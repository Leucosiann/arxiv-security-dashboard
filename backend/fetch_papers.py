#!/usr/bin/env python3
"""
Arxiv Security Papers Fetcher

Bu script Arxiv'den gÃ¼venlik ve yapay zeka kesiÅŸimindeki makaleleri Ã§eker,
Google Gemini API ile TÃ¼rkÃ§e Ã¶zetler oluÅŸturur ve data.json dosyasÄ±nÄ± gÃ¼nceller.

KullanÄ±m:
    python fetch_papers.py

Ortam DeÄŸiÅŸkenleri:
    GEMINI_API_KEY: Google Gemini API anahtarÄ±
"""

import arxiv
import json
import os
from datetime import datetime, timedelta
from pathlib import Path
import time
import google.generativeai as genai
from dotenv import load_dotenv

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# YapÄ±landÄ±rma
MAX_RESULTS = 50  # Her Ã§alÄ±ÅŸmada Ã§ekilecek maksimum makale
DAYS_LOOKBACK = 7  # Son kaÃ§ gÃ¼nÃ¼n makalelerine bakÄ±lacak
DATA_FILE = Path(__file__).parent.parent / "public" / "data.json"

# Arxiv sorgusu: cs.CR (Security) VE (cs.AI VEYA cs.LG VEYA cs.PL)
ARXIV_QUERY = "cat:cs.CR AND (cat:cs.AI OR cat:cs.LG OR cat:cs.PL)"


def setup_gemini():
    """Gemini API'yi yapÄ±landÄ±r."""
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("âš ï¸  GEMINI_API_KEY bulunamadÄ±. TÃ¼rkÃ§e Ã¶zetler oluÅŸturulamayacak.")
        return None
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-1.5-flash')
    return model


def generate_turkish_summary(model, title: str, abstract: str) -> str:
    """
    Makale baÅŸlÄ±ÄŸÄ± ve abstract'Ä±ndan TÃ¼rkÃ§e Ã¶zet oluÅŸtur.
    Markdown formatÄ±nda dÃ¶ndÃ¼rÃ¼r.
    """
    if not model:
        return abstract  # API yoksa orijinal abstract'Ä± dÃ¶ndÃ¼r
    
    prompt = f"""AÅŸaÄŸÄ±daki akademik makale iÃ§in TÃ¼rkÃ§e Ã¶zet hazÄ±rla.

Kurallar:
1. Markdown formatÄ±nda yaz (## Ã–zet baÅŸlÄ±ÄŸÄ±, madde iÅŸaretleri, **kalÄ±n** yazÄ± kullan)
2. Makaleyi'nin ana katkÄ±sÄ±nÄ±, metodolojisini ve sonuÃ§larÄ±nÄ± Ã¶zetle
3. Teknik terimleri TÃ¼rkÃ§e karÅŸÄ±lÄ±klarÄ±yla birlikte kullan (Ã¶rn: "Derin Ã–ÄŸrenme (Deep Learning)")
4. Maksimum 150 kelime olsun
5. Akademik ama anlaÅŸÄ±lÄ±r bir dil kullan

Makale BaÅŸlÄ±ÄŸÄ±: {title}

Abstract:
{abstract}

TÃ¼rkÃ§e Ã–zet:"""

    try:
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"âš ï¸  Gemini API hatasÄ±: {e}")
        return abstract


def fetch_arxiv_papers(query: str, max_results: int, days_back: int) -> list:
    """
    Arxiv'den makaleleri Ã§ek.
    
    Args:
        query: Arxiv sorgu stringi
        max_results: Maksimum sonuÃ§ sayÄ±sÄ±
        days_back: Son kaÃ§ gÃ¼nÃ¼n makaleleri
    
    Returns:
        Makale listesi
    """
    print(f"ğŸ” Arxiv sorgusu: {query}")
    print(f"ğŸ“… Son {days_back} gÃ¼n aranÄ±yor...")
    
    # Tarih filtresi iÃ§in cutoff
    cutoff_date = datetime.now() - timedelta(days=days_back)
    
    # Arxiv aramasÄ±
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )
    
    papers = []
    client = arxiv.Client()
    
    for result in client.results(search):
        # Tarih kontrolÃ¼
        published = result.published.replace(tzinfo=None)
        if published < cutoff_date:
            continue
            
        # Kategorileri al
        categories = [cat for cat in result.categories]
        
        paper = {
            "id": result.entry_id.split("/")[-1],  # arxiv:2412.12345 -> 2412.12345
            "title": result.title.replace("\n", " "),
            "authors": [author.name for author in result.authors[:5]],  # Ä°lk 5 yazar
            "published_date": published.strftime("%Y-%m-%d"),
            "tags": categories,
            "link": result.entry_id,
            "pdf_link": result.pdf_url,
            "content": {
                "en": result.summary.replace("\n", " "),
                "tr": ""  # Sonra doldurulacak
            }
        }
        papers.append(paper)
    
    print(f"âœ… {len(papers)} makale bulundu")
    return papers


def load_existing_data() -> list:
    """Mevcut data.json dosyasÄ±nÄ± yÃ¼kle."""
    if not DATA_FILE.exists():
        return []
    
    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        return []


def save_data(papers: list):
    """Makaleleri data.json'a kaydet."""
    # Dizini oluÅŸtur
    DATA_FILE.parent.mkdir(parents=True, exist_ok=True)
    
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(papers, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ {len(papers)} makale kaydedildi: {DATA_FILE}")


def merge_papers(existing: list, new: list) -> list:
    """
    Yeni makaleleri mevcut listeyle birleÅŸtir.
    DuplikasyonlarÄ± Ã¶nle ve tarihe gÃ¶re sÄ±rala.
    """
    # Mevcut ID'leri al
    existing_ids = {p["id"] for p in existing}
    
    # Yeni makaleleri ekle
    merged = existing.copy()
    new_count = 0
    
    for paper in new:
        if paper["id"] not in existing_ids:
            merged.append(paper)
            new_count += 1
    
    # Tarihe gÃ¶re sÄ±rala (yeniden eskiye)
    merged.sort(key=lambda x: x["published_date"], reverse=True)
    
    print(f"ğŸ“Š {new_count} yeni makale eklendi")
    return merged


def main():
    """Ana fonksiyon."""
    print("=" * 60)
    print("ğŸ” Arxiv Security Papers Fetcher")
    print("=" * 60)
    
    # Gemini'yi ayarla
    model = setup_gemini()
    
    # Mevcut verileri yÃ¼kle
    existing_papers = load_existing_data()
    print(f"ğŸ“‚ Mevcut makale sayÄ±sÄ±: {len(existing_papers)}")
    
    # Yeni makaleleri Ã§ek
    new_papers = fetch_arxiv_papers(ARXIV_QUERY, MAX_RESULTS, DAYS_LOOKBACK)
    
    if not new_papers:
        print("â„¹ï¸  Yeni makale bulunamadÄ±")
        return
    
    # Mevcut ID'leri al
    existing_ids = {p["id"] for p in existing_papers}
    
    # Sadece yeni makaleler iÃ§in TÃ¼rkÃ§e Ã¶zet oluÅŸtur
    for i, paper in enumerate(new_papers):
        if paper["id"] in existing_ids:
            continue  # Zaten var, atla
            
        print(f"\nğŸ“ [{i+1}/{len(new_papers)}] {paper['title'][:60]}...")
        
        # TÃ¼rkÃ§e Ã¶zet oluÅŸtur
        turkish_summary = generate_turkish_summary(
            model, 
            paper["title"], 
            paper["content"]["en"]
        )
        paper["content"]["tr"] = turkish_summary
        
        # Rate limiting
        if model:
            time.sleep(1)  # API rate limit iÃ§in bekle
    
    # BirleÅŸtir ve kaydet
    all_papers = merge_papers(existing_papers, new_papers)
    save_data(all_papers)
    
    print("\n" + "=" * 60)
    print("âœ… Ä°ÅŸlem tamamlandÄ±!")
    print("=" * 60)


if __name__ == "__main__":
    main()
