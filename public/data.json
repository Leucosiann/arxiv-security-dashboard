[
  {
    "id": "2602.12092v1",
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "authors": [
      "Bo Zhang",
      "Jiaxuan Guo",
      "Lijun Li",
      "Dongrui Liu",
      "Sujin Chen"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2602.12092v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12092v1",
    "content": {
      "en": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.",
      "tr": "**Makale Başlığı:** DeepSight: Kapsamlı Bir LM Güvenlik Araç Seti\n\n**Özet:**\n\nBüyük Modellerin (LMs) gelişimi hızla ilerlerken, güvenlikleri de öncelik taşımaktadır. Mevcut Büyük Dil Modelleri (LLMs) ve Çok Modlu Büyük Dil Modelleri (MLLMs) güvenlik iş akışında, değerlendirme, teşhis ve hizalama genellikle ayrı araçlarla ele alınmaktadır. Özellikle, güvenlik değerlendirmesi yalnızca harici davranışsal riskleri belirleyebilmekte ancak içsel kök nedenleri ortaya çıkaramamaktadır. Bu esnada, güvenlik teşhisi sıklıkla somut risk senaryolarından sapmakta ve açıklanabilir düzeyde kalmaktadır. Bu şekilde, güvenlik hizalaması, içsel mekanizmalardaki değişikliklere dair özel açıklamalar eksikliğinden dolayı, genel yetenekleri potansiyel olarak degrade edebilir. Bu sorunları sistematik olarak ele almak için, yeni bir güvenlik değerlendirme-teşhis entegre paradigmasını uygulamak üzere açık kaynaklı bir proje olan DeepSight'ı öneriyoruz. DeepSight, değerlendirme araç seti DeepSafe ve teşhis araç seti DeepScan'den oluşan, düşük maliyetli, tekrarlanabilir, verimli ve son derece ölçeklenebilir büyük ölçekli model güvenlik değerlendirme projesidir. Görev ve veri protokollerini birleştirerek, iki aşama arasında bir bağlantı kuruyor ve güvenlik değerlendirmesini black-box'tan white-box içgörüye dönüştürüyoruz. Ayrıca DeepSight, öncü AI risk değerlendirmesini ve ortak güvenlik değerlendirme ve teşhisi destekleyen ilk açık kaynaklı araç setidir."
    }
  },
  {
    "id": "2602.11897v1",
    "title": "Agentic AI for Cybersecurity: A Meta-Cognitive Architecture for Governable Autonomy",
    "authors": [
      "Andrei Kojukhov",
      "Arkady Bovshover"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11897v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11897v1",
    "content": {
      "en": "Contemporary AI-driven cybersecurity systems are predominantly architected as model-centric detection and automation pipelines optimized for task-level performance metrics such as accuracy and response latency. While effective for bounded classification tasks, these architectures struggle to support accountable decision-making under adversarial uncertainty, where actions must be justified, governed, and aligned with organizational and regulatory constraints. This paper argues that cybersecurity orchestration should be reconceptualized as an agentic, multi-agent cognitive system, rather than a linear sequence of detection and response components. We introduce a conceptual architectural framework in which heterogeneous AI agents responsible for detection, hypothesis formation, contextual interpretation, explanation, and governance are coordinated through an explicit meta-cognitive judgement function. This function governs decision readiness and dynamically calibrates system autonomy when evidence is incomplete, conflicting, or operationally risky. By synthesizing distributed cognition theory, multi-agent systems research, and responsible AI governance frameworks, we demonstrate that modern security operations already function as distributed cognitive systems, albeit without an explicit organizing principle. Our contribution is to make this cognitive structure architecturally explicit and governable by embedding meta-cognitive judgement as a first-class system function. We discuss implications for security operations centers, accountable autonomy, and the design of next-generation AI-enabled cyber defence architectures. The proposed framework shifts the focus of AI in cybersecurity from optimizing isolated predictions to governing autonomy under uncertainty.",
      "tr": "**Makale Başlığı: Siber Güvenlik için Agentic AI: Yönetilebilir Özerklik için Meta-Bilişsel Bir Mimari**\n\n**Özet:**\n\nGünümüz AI destekli siber güvenlik sistemleri, ağırlıklı olarak doğruluk ve yanıt gecikmesi gibi görev düzeyinde performans metrikleri için optimize edilmiş model merkezli tespit ve otomasyon hatları şeklinde tasarlanmaktadır. Sınırlı sınıflandırma görevleri için etkili olmalarına rağmen, bu mimariler, eylemlerin gerekçelendirilmesi, yönetilmesi ve kurumsal ile düzenleyici kısıtlamalarla uyumlu olması gereken düşmanca belirsizlik altında hesap verebilir karar verme mekanizmalarını desteklemekte zorlanmaktadır. Bu makale, siber güvenlik orkestrasyonunun, doğrusal bir tespit ve yanıt bileşenleri dizisi yerine, bir agentic, multi-agent bilişsel sistem olarak yeniden kavramsallaştırılması gerektiğini savunmaktadır. Tespit, hipotez oluşturma, bağlamsal yorumlama, açıklama ve yönetişimden sorumlu heterojen AI ajanlarının açık bir meta-bilişsel yargı fonksiyonu aracılığıyla koordine edildiği kavramsal bir mimari çerçeve sunuyoruz. Bu fonksiyon, karar hazırlığını yönetir ve kanıtların eksik, çelişkili veya operasyonel olarak riskli olduğu durumlarda sistem özerkliğini dinamik olarak kalibre eder. Dağıtılmış biliş teorisini, multi-agent sistemler araştırmasını ve sorumlu AI yönetişim çerçevelerini sentezleyerek, modern güvenlik operasyonlarının, açık bir organize edici ilke olmasa da, zaten dağıtılmış bilişsel sistemler olarak işlev gördüğünü gösteriyoruz. Katkımız, meta-bilişsel yargıyı birinci sınıf bir sistem fonksiyonu olarak gömerek bu bilişsel yapıyı mimari olarak açık ve yönetilebilir hale getirmektir. Güvenlik operasyon merkezleri, hesap verebilir özerklik ve yeni nesil AI destekli siber savunma mimarilerinin tasarımı üzerindeki etkileri tartışılmaktadır. Önerilen çerçeve, siber güvenlikte AI'nın odağını izole edilmiş tahminleri optimize etmekten, belirsizlik altında özerkliği yönetmeye kaydırmaktadır."
    }
  },
  {
    "id": "2602.11851v1",
    "title": "Resource-Aware Deployment Optimization for Collaborative Intrusion Detection in Layered Networks",
    "authors": [
      "André García Gómez",
      "Ines Rieger",
      "Wolfgang Hotwagner",
      "Max Landauer",
      "Markus Wurzenberger"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11851v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11851v1",
    "content": {
      "en": "Collaborative Intrusion Detection Systems (CIDS) are increasingly adopted to counter cyberattacks, as their collaborative nature enables them to adapt to diverse scenarios across heterogeneous environments. As distributed critical infrastructure operates in rapidly evolving environments, such as drones in both civil and military domains, there is a growing need for CIDS architectures that can flexibly accommodate these dynamic changes. In this study, we propose a novel CIDS framework designed for easy deployment across diverse distributed environments. The framework dynamically optimizes detector allocation per node based on available resources and data types, enabling rapid adaptation to new operational scenarios with minimal computational overhead. We first conducted a comprehensive literature review to identify key characteristics of existing CIDS architectures. Based on these insights and real-world use cases, we developed our CIDS framework, which we evaluated using several distributed datasets that feature different attack chains and network topologies. Notably, we introduce a public dataset based on a realistic cyberattack targeting a ground drone aimed at sabotaging critical infrastructure. Experimental results demonstrate that the proposed CIDS framework can achieve adaptive, efficient intrusion detection in distributed settings, automatically reconfiguring detectors to maintain an optimal configuration, without requiring heavy computation, since all experiments were conducted on edge devices.",
      "tr": "Makale Başlığı: Katmanlı Ağlarda İşbirlikçi Saldırı Tespit Sistemleri İçin Kaynak-Farkındalıklı Konuşlandırma Optimizasyonu\n\nÖzet:\nİşbirlikçi Saldırı Tespit Sistemleri (CIDS), siber saldırılara karşı koymak amacıyla giderek daha fazla benimsenmektedir, zira işbirlikçi doğaları heterojen ortamlarda çeşitli senaryolara uyum sağlamalarını mümkün kılmaktadır. Dağıtık kritik altyapı, hem sivil hem de askeri alanlardaki drone'lar gibi hızla gelişen ortamlarda faaliyet gösterdiğinden, bu dinamik değişiklikleri esnek bir şekilde karşılayabilen CIDS mimarilerine olan ihtiyaç artmaktadır. Bu çalışmada, çeşitli dağıtık ortamlara kolay konuşlandırma için tasarlanmış yeni bir CIDS çerçevesi öneriyoruz. Çerçeve, mevcut kaynaklara ve veri türlerine dayalı olarak düğüm başına dedektör tahsisini dinamik olarak optimize ederek, minimum hesaplama yükü ile yeni operasyonel senaryolara hızlı adaptasyonu sağlar. İlk olarak, mevcut CIDS mimarilerinin temel özelliklerini belirlemek için kapsamlı bir literatür taraması gerçekleştirdik. Bu içgörülere ve gerçek dünya kullanım örneklerine dayanarak, CIDS çerçevemizi geliştirdik ve farklı saldırı zincirleri ile ağ topolojilerini içeren birkaç dağıtık veri kümesi kullanarak değerlendirdik. Özellikle, kritik altyapıyı sabote etmeyi amaçlayan ve bir yer drone'unu hedef alan gerçekçi bir siber saldırıya dayanan kamuya açık bir veri kümesi sunuyoruz. Deneysel sonuçlar, önerilen CIDS çerçevesinin dağıtık ortamlarda uyarlanabilir, verimli saldırı tespiti sağlayabildiğini ve tüm deneyler edge devices üzerinde gerçekleştirildiğinden, ağır hesaplama gerektirmeden, dedektörleri otomatik olarak yeniden yapılandırarak optimal bir konfigürasyonu sürdürebildiğini göstermektedir."
    }
  },
  {
    "id": "2602.11655v1",
    "title": "LoRA-based Parameter-Efficient LLMs for Continuous Learning in Edge-based Malware Detection",
    "authors": [
      "Christian Rondanini",
      "Barbara Carminati",
      "Elena Ferrari",
      "Niccolò Lardo",
      "Ashish Kundu"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "link": "http://arxiv.org/abs/2602.11655v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11655v1",
    "content": {
      "en": "The proliferation of edge devices has created an urgent need for security solutions capable of detecting malware in real time while operating under strict computational and memory constraints. Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in recognizing complex patterns, yet their deployment on edge devices remains impractical due to their resource demands. However, in edge malware detection, static or centrally retrained models degrade under evolving threats and heterogeneous traffic; locally trained models become siloed and fail to transfer across domains. To overcome these limitations, in this paper, we present a continuous learning architecture for edge-based malware detection that combines local adaptation on each device with global knowledge sharing through parameter-efficient LoRA adapters. Lightweight transformer models (DistilBERT, DistilGPT-2, TinyT5) run on edge nodes and are incrementally fine-tuned on device-specific traffic; only the resulting LoRA modules are aggregated by a lightweight coordinator and redistributed, enabling cross-device generalization without exchanging raw data. We evaluate on two public IoT security datasets, Edge-IIoTset and TON-IoT, under multi-round learning to simulate evolving threats. Compared to isolated fine-tuning, the LoRA-based exchange yields up to 20-25% accuracy gains when models encounter previously unseen attacks from another domain, while maintaining stable loss and F1 across rounds. LoRA adds less than 1% to model size (~0.6-1.8 MB), making updates practical for constrained edge hardware.",
      "tr": "Elbette, istenen şekilde çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** Kenar Tabanlı Kötü Amaçlı Yazılım Tespitinde Sürekli Öğrenme İçin LoRA Tabanlı Parametre Verimli LLM'ler\n\n**Özet:**\n\nKenar cihazlarının yaygınlaşması, katı hesaplama ve bellek kısıtlamaları altında gerçek zamanlı kötü amaçlı yazılım tespit edebilen güvenlik çözümlerine acil bir ihtiyaç yaratmıştır. Son zamanlarda, Large Language Models (LLM'ler) karmaşık örüntüleri tanımada dikkate değer yetenekler sergilemişlerdir; ancak kaynak gereksinimleri nedeniyle kenar cihazlarına dağıtımları pratik olmaktan uzaktır. Bununla birlikte, kenar kötü amaçlı yazılım tespitinde statik veya merkezi olarak yeniden eğitilmiş modeller gelişen tehditler ve heterojen trafik altında bozulur; yerel olarak eğitilmiş modeller silolaşır ve etki alanları arasında aktarımda başarısız olur. Bu sınırlamaların üstesinden gelmek için bu makalede, her cihazdaki yerel adaptasyonu parametre verimli LoRA adaptörleri aracılığıyla küresel bilgi paylaşımıyla birleştiren kenar tabanlı kötü amaçlı yazılım tespiti için bir sürekli öğrenme mimarisi sunuyoruz. Hafif transformer modelleri (DistilBERT, DistilGPT-2, TinyT5) kenar düğümlerinde çalışır ve cihazlara özgü trafik üzerinde artımlı olarak ince ayarlanır; yalnızca sonuçta ortaya çıkan LoRA modülleri hafif bir koordinatör tarafından toplanır ve yeniden dağıtılır, bu da ham veri alışverişi yapmadan cihazlar arası genelleme sağlar. İki genel IoT güvenlik veri kümesi olan Edge-IIoTset ve TON-IoT üzerinde, gelişen tehditleri simüle etmek için çok turlu öğrenme altında değerlendirme yapıyoruz. İzole ince ayara kıvıldığında, LoRA tabanlı değişim, modeller daha önce görülmemiş saldırılarla başka bir etki alanından karşılaştığında, turlar boyunca kararlı kayıp ve F1'i korurken %20-25'e kadar doğruluk kazançları sağlamaktadır. LoRA, model boyutuna %1'den az ekleme yapar (~0.6-1.8 MB), bu da güncellemeleri kısıtlı kenar donanımları için pratik hale getirir."
    }
  },
  {
    "id": "2602.11651v1",
    "title": "DMind-3: A Sovereign Edge--Local--Cloud AI System with Controlled Deliberation and Correction-Based Tuning for Safe, Low-Latency Transaction Execution",
    "authors": [
      "Enhao Huang",
      "Frank Li",
      "Tony Lin",
      "Lowes Yang"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11651v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11651v1",
    "content": {
      "en": "This paper introduces DMind-3, a sovereign Edge-Local-Cloud intelligence stack designed to secure irreversible financial execution in Web3 environments against adversarial risks and strict latency constraints. While existing cloud-centric assistants compromise privacy and fail under network congestion, and purely local solutions lack global ecosystem context, DMind-3 resolves these tensions by decomposing capability into three cooperating layers: a deterministic signing-time intent firewall at the edge, a private high-fidelity reasoning engine on user hardware, and a policy-governed global context synthesizer in the cloud. We propose policy-driven selective offloading to route computation based on privacy sensitivity and uncertainty, supported by two novel training objectives: Hierarchical Predictive Synthesis (HPS) for fusing time-varying macro signals, and Contrastive Chain-of-Correction Supervised Fine-Tuning (C$^3$-SFT) to enhance local verification reliability. Extensive evaluations demonstrate that DMind-3 achieves a 93.7% multi-turn success rate in protocol-constrained tasks and superior domain reasoning compared to general-purpose baselines, providing a scalable framework where safety is bound to the edge execution primitive while maintaining sovereignty over sensitive user intent.",
      "tr": "**Makale Başlığı:** DMind-3: Güvenli, Düşük Gecikmeli İşlem Yürütme için Kontrollü Deliberation ve Correction-Based Tuning ile Egemen Edge--Local--Cloud Yapay Zeka Sistemi\n\n**Özet:**\n\nBu makale, DMind-3'ü tanıtmaktadır. DMind-3, Web3 ortamlarında geri döndürülemez finansal işlemlerin, saldırgan risklere ve katı gecikme kısıtlamalarına karşı güvenliğini sağlamak üzere tasarlanmış egemen bir Edge-Local-Cloud zeka yığınıdır. Mevcut bulut merkezli yardımcılar gizliliği tehlikeye atarken ve ağ tıkanıklığında başarısız olurken, tamamen yerel çözümler küresel ekosistem bağlamından yoksundur. DMind-3, bu gerilimleri yeteneği üç işbirliği yapan katmana ayırarak çözer: kenarda deterministik bir signing-time intent firewall, kullanıcı donanımında özel bir high-fidelity reasoning engine ve bulutta politika güdümlü bir global context synthesizer. Gizlilik hassasiyetine ve belirsizliğe göre hesaplamayı yönlendirmek için politika güdümlü seçici offloading öneriyoruz. Bu, iki yeni eğitim hedefi ile desteklenmektedir: zamanla değişen makro sinyalleri birleştirmek için Hierarchical Predictive Synthesis (HPS) ve yerel doğrulama güvenilirliğini artırmak için Contrastive Chain-of-Correction Supervised Fine-Tuning (C$^3$-SFT). Kapsamlı değerlendirmeler, DMind-3'ün protokol kısıtlamalı görevlerde %93,7 çoklu tur başarı oranı ve genel amaçlı temel sistemlere kıyasla üstün alan reasoning yeteneği elde ettiğini göstermektedir. Bu sistem, güvenliğin kenar yürütme ilkelere bağlı kaldığı, hassas kullanıcı niyeti üzerinde egemenliği koruyarak ölçeklenebilir bir çerçeve sunmaktadır."
    }
  },
  {
    "id": "2602.11528v1",
    "title": "Stop Tracking Me! Proactive Defense Against Attribute Inference Attack in LLMs",
    "authors": [
      "Dong Yan",
      "Jian Liang",
      "Ran He",
      "Tieniu Tan"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2602.11528v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11528v1",
    "content": {
      "en": "Recent studies have shown that large language models (LLMs) can infer private user attributes (e.g., age, location, gender) from user-generated text shared online, enabling rapid and large-scale privacy breaches. Existing anonymization-based defenses are coarse-grained, lacking word-level precision in anonymizing privacy-leaking elements. Moreover, they are inherently limited as altering user text to hide sensitive cues still allows attribute inference to occur through models' reasoning capabilities. To address these limitations, we propose a unified defense framework that combines fine-grained anonymization (TRACE) with inference-preventing optimization (RPS). TRACE leverages attention mechanisms and inference chain generation to identify and anonymize privacy-leaking textual elements, while RPS employs a lightweight two-stage optimization strategy to induce model rejection behaviors, thereby preventing attribute inference. Evaluations across diverse LLMs show that TRACE-RPS reduces attribute inference accuracy from around 50\\% to below 5\\% on open-source models. In addition, our approach offers strong cross-model generalization, prompt-variation robustness, and utility-privacy tradeoffs. Our code is available at https://github.com/Jasper-Yan/TRACE-RPS.",
      "tr": "Kesinlikle, işte akademik makalenin başlığı ve özetinin talep ettiğiniz şekilde çevrilmiş hali:\n\n**Makale Başlığı:** Beni İzlemeyi Kes! LLM'lerde Öznitelik Çıkarım Saldırılarına Karşı Proaktif Savunma\n\n**Özet:**\nSon çalışmalar, büyük dil modellerinin (LLM'ler) çevrimiçi paylaşılan kullanıcı tarafından oluşturulan metinlerden özel kullanıcı özniteliklerini (örneğin, yaş, konum, cinsiyet) çıkarabildiğini ve bu durumun hızlı ve büyük ölçekli gizlilik ihlallerine yol açabildiğini göstermiştir. Mevcut anonimleştirme tabanlı savunmalar kaba tanelidir ve gizlilik sızdıran unsurların anonimleştirilmesinde kelime düzeyinde hassasiyetten yoksundur. Dahası, bu savunmalar doğası gereği sınırlıdır, çünkü hassas ipuçlarını gizlemek için kullanıcı metnini değiştirmek, modellerin **reasoning** yetenekleri aracılığıyla öznitelik çıkarımının gerçekleşmesine hala olanak tanır. Bu sınırlamaları gidermek için, ince taneli anonimleştirmeyi (TRACE) çıkarım önleyici optimizasyon (RPS) ile birleştiren birleşik bir savunma çerçevesi öneriyoruz. TRACE, gizlilik sızdıran metinsel unsurları belirlemek ve anonimleştirmek için dikkat mekanizmalarını ve **inference chain generation**'ı kullanırken, RPS öznitelik çıkarımını önlemek amacıyla model reddetme davranışları uyandırmak için hafif bir iki aşamalı optimizasyon stratejisi kullanır. Çeşitli LLM'ler üzerinde yapılan değerlendirmeler, TRACE-RPS'nin açık kaynaklı modellerde öznitelik çıkarım doğruluğunu yaklaşık %50'den %5'in altına düşürdüğünü göstermektedir. Ek olarak, yaklaşımımız güçlü bir modeller arası genelleme, prompt-varyasyon sağlamlığı ve fayda-gizlilik ödünleşmeleri sunmaktadır. Kodumuz şu adresten temin edilebilir: https://github.com/Jasper-Yan/TRACE-RPS."
    }
  },
  {
    "id": "2602.11513v1",
    "title": "Differentially Private and Communication Efficient Large Language Model Split Inference via Stochastic Quantization and Soft Prompt",
    "authors": [
      "Yujie Gu",
      "Richeng Jin",
      "Xiaoyu Ji",
      "Yier Jin",
      "Wenyuan Xu"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11513v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11513v1",
    "content": {
      "en": "Large Language Models (LLMs) have achieved remarkable performance and received significant research interest. The enormous computational demands, however, hinder the local deployment on devices with limited resources. The current prevalent LLM inference paradigms require users to send queries to the service providers for processing, which raises critical privacy concerns. Existing approaches propose to allow the users to obfuscate the token embeddings before transmission and utilize local models for denoising. Nonetheless, transmitting the token embeddings and deploying local models may result in excessive communication and computation overhead, preventing practical implementation. In this work, we propose \\textbf{DEL}, a framework for \\textbf{D}ifferentially private and communication \\textbf{E}fficient \\textbf{L}LM split inference. More specifically, an embedding projection module and a differentially private stochastic quantization mechanism are proposed to reduce the communication overhead in a privacy-preserving manner. To eliminate the need for local models, we adapt soft prompt at the server side to compensate for the utility degradation caused by privacy. To the best of our knowledge, this is the first work that utilizes soft prompt to improve the trade-off between privacy and utility in LLM inference, and extensive experiments on text generation and natural language understanding benchmarks demonstrate the effectiveness of the proposed method.",
      "tr": "Makale Başlığı: Diferansiyel Gizlilik ve İletişim Verimliliğine Sahip Büyük Dil Modeli Bölünmüş Çıkarımı için Stokastik Kuantizasyon ve Soft Prompt Kullanımı\n\nÖzet:\nLarge Language Models (LLMs), dikkate değer bir performans sergilemiş ve önemli araştırma ilgisi görmüştür. Ancak, muazzam hesaplama gereksinimleri, sınırlı kaynaklara sahip cihazlarda yerel dağıtımı engellemektedir. Mevcut yaygın LLM inference paradigmaları, kullanıcıların işleme için sorguları servis sağlayıcılara göndermesini gerektirir, bu da kritik gizlilik endişelerini gündeme getirir. Mevcut yaklaşımlar, kullanıcıların iletimden önce token embeddings'ini karartmasına ve gürültüyü gidermek için yerel modeller kullanmasına olanak tanımayı önermektedir. Bununla birlikte, token embeddings'inin iletilmesi ve yerel modellerin konuşlandırılması aşırı iletişim ve hesaplama yüküne neden olabilir ve pratik uygulamayı engelleyebilir. Bu çalışmada, Differentially private ve communication Efficient LLM split inference için bir framework olan **DEL**'i (Differentially private and communication Efficient LLM split inference) öneriyoruz. Daha spesifik olarak, iletişim yükünü gizlilik koruyucu bir şekilde azaltmak için bir embedding projection modülü ve differentially private stochastic quantization mekanizması önerilmektedir. Yerel modellere olan ihtiyacı ortadan kaldırmak için, gizlilikten kaynaklanan utility degradation'ı telafi etmek üzere sunucu tarafında soft prompt'u adapte ediyoruz. Bilgimiz dahilinde, LLM inference'ında privacy ve utility arasındaki ödünleşmeyi iyileştirmek için soft prompt kullanan ilk çalışma budur ve metin üretimi ile natural language understanding benchmark'ları üzerindeki kapsamlı deneyler, önerilen yöntemin etkinliğini göstermektedir."
    }
  },
  {
    "id": "2602.11416v1",
    "title": "Optimizing Agent Planning for Security and Autonomy",
    "authors": [
      "Aashish Kolluri",
      "Rishi Sharma",
      "Manuel Costa",
      "Boris Köpf",
      "Tobias Nießen"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.11416v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11416v1",
    "content": {
      "en": "Indirect prompt injection attacks threaten AI agents that execute consequential actions, motivating deterministic system-level defenses. Such defenses can provably block unsafe actions by enforcing confidentiality and integrity policies, but currently appear costly: they reduce task completion rates and increase token usage compared to probabilistic defenses. We argue that existing evaluations miss a key benefit of system-level defenses: reduced reliance on human oversight. We introduce autonomy metrics to quantify this benefit: the fraction of consequential actions an agent can execute without human-in-the-loop (HITL) approval while preserving security. To increase autonomy, we design a security-aware agent that (i) introduces richer HITL interactions, and (ii) explicitly plans for both task progress and policy compliance. We implement this agent design atop an existing information-flow control defense against prompt injection and evaluate it on the AgentDojo and WASP benchmarks. Experiments show that this approach yields higher autonomy without sacrificing utility.",
      "tr": "Makale Başlığı: Güvenlik ve Otonomi için Ajan Planlamasının Optimizasyonu\n\nÖzet:\nDolaylı prompt injection saldırıları, sonuç doğuran eylemleri gerçekleştiren yapay zeka ajanları için tehdit oluşturmakta ve deterministik sistem düzeyinde savunmaları gerektirmektedir. Bu tür savunmalar, gizlilik ve bütünlük politikalarını uygulayarak güvenli olmayan eylemleri kanıtlanabilir şekilde engelleyebilir, ancak şu anda maliyetli görünmektedir: olasılıksal savunmalara kıyasla görev tamamlama oranlarını düşürmekte ve token kullanımını artırmaktadır. Sistem düzeyinde savunmaların önemli bir faydasının, insan denetimine olan bağımlılığın azalması olduğunu mevcut değerlendirmelerin gözden kaçırdığını savunuyoruz. Bu faydayı nicelleştirmek için otonomi metriklerini tanıtıyoruz: güvenliği korurken bir ajanın insan-döngüde (HITL) onayına ihtiyaç duymadan gerçekleştirebileceği sonuç doğuran eylemlerin oranı. Otonomiyi artırmak için, (i) daha zengin HITL etkileşimleri sunan ve (ii) hem görev ilerlemesini hem de politika uyumluluğunu açıkça planlayan güvenlik-farkındalığına sahip bir ajan tasarlıyoruz. Bu ajan tasarımını, prompt injection'a karşı mevcut bir information-flow control defense üzerine inşa ediyor ve AgentDojo ile WASP benchmark'larında değerlendiriyoruz. Deneyler, bu yaklaşımın fayda kaybı olmaksızın daha yüksek otonomi sağladığını göstermektedir."
    }
  },
  {
    "id": "2602.11327v1",
    "title": "Security Threat Modeling for Emerging AI-Agent Protocols: A Comparative Analysis of MCP, A2A, Agora, and ANP",
    "authors": [
      "Zeynab Anbiaee",
      "Mahdi Rabbani",
      "Mansur Mirani",
      "Gunjan Piya",
      "Igor Opushnyev"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11327v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11327v1",
    "content": {
      "en": "The rapid development of the AI agent communication protocols, including the Model Context Protocol (MCP), Agent2Agent (A2A), Agora, and Agent Network Protocol (ANP), is reshaping how AI agents communicate with tools, services, and each other. While these protocols support scalable multi-agent interaction and cross-organizational interoperability, their security principles remain understudied, and standardized threat modeling is limited; no protocol-centric risk assessment framework has been established yet. This paper presents a systematic security analysis of four emerging AI agent communication protocols. First, we develop a structured threat modeling analysis that examines protocol architectures, trust assumptions, interaction patterns, and lifecycle behaviors to identify protocol-specific and cross-protocol risk surfaces. Second, we introduce a qualitative risk assessment framework that identifies twelve protocol-level risks and evaluates security posture across the creation, operation, and update phases through systematic assessment of likelihood, impact, and overall protocol risk, with implications for secure deployment and future standardization. Third, we provide a measurement-driven case study on MCP that formalizes the risk of missing mandatory validation/attestation for executable components as a falsifiable security claim by quantifying wrong-provider tool execution under multi-server composition across representative resolver policies. Collectively, our results highlight key design-induced risk surfaces and provide actionable guidance for secure deployment and future standardization of agent communication ecosystems.",
      "tr": "The rapid development of the AI agent communication protocols, including the Model Context Protocol (MCP), Agent2Agent (A2A), Agora, and Agent Network Protocol (ANP), is reshaping how AI agents communicate with tools, services, and each other. While these protocols support scalable multi-agent interaction and cross-organizational interoperability, their security principles remain understudied, and standardized threat modeling is limited; no protocol-centric risk assessment framework has been established yet. This paper presents a systematic security analysis of four emerging AI agent communication protocols. First, we develop a structured threat modeling analysis that examines protocol architectures, trust assumptions, interaction patterns, and lifecycle behaviors to identify protocol-specific and cross-protocol risk surfaces. Second, we introduce a qualitative risk assessment framework that identifies twelve protocol-level risks and evaluates security posture across the creation, operation, and update phases through systematic assessment of likelihood, impact, and overall protocol risk, with implications for secure deployment and future standardization. Third, we provide a measurement-driven case study on MCP that formalizes the risk of missing mandatory validation/attestation for executable components as a falsifiable security claim by quantifying wrong-provider tool execution under multi-server composition across representative resolver policies. Collectively, our results highlight key design-induced risk surfaces and provide actionable guidance for secure deployment and future standardization of agent communication ecosystems."
    }
  },
  {
    "id": "2602.11304v1",
    "title": "CryptoAnalystBench: Failures in Multi-Tool Long-Form LLM Analysis",
    "authors": [
      "Anushri Eswaran",
      "Oleg Golev",
      "Darshan Tank",
      "Sidhant Rahi",
      "Himanshu Tyagi"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11304v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11304v1",
    "content": {
      "en": "Modern analyst agents must reason over complex, high token inputs, including dozens of retrieved documents, tool outputs, and time sensitive data. While prior work has produced tool calling benchmarks and examined factuality in knowledge augmented systems, relatively little work studies their intersection: settings where LLMs must integrate large volumes of dynamic, structured and unstructured multi tool outputs. We investigate LLM failure modes in this regime using crypto as a representative high data density domain. We introduce (1) CryptoAnalystBench, an analyst aligned benchmark of 198 production crypto and DeFi queries spanning 11 categories; (2) an agentic harness equipped with relevant crypto and DeFi tools to generate responses across multiple frontier LLMs; and (3) an evaluation pipeline with citation verification and an LLM as a judge rubric spanning four user defined success dimensions: relevance, temporal relevance, depth, and data consistency. Using human annotation, we develop a taxonomy of seven higher order error types that are not reliably captured by factuality checks or LLM based quality scoring. We find that these failures persist even in state of the art systems and can compromise high stakes decisions. Based on this taxonomy, we refine the judge rubric to better capture these errors. While the judge does not align with human annotators on precise scoring across rubric iterations, it reliably identifies critical failure modes, enabling scalable feedback for developers and researchers studying analyst style agents. We release CryptoAnalystBench with annotated queries, the evaluation pipeline, judge rubrics, and the error taxonomy, and outline mitigation strategies and open challenges in evaluating long form, multi tool augmented systems.",
      "tr": "Modern analyst agents must reason over complex, high token inputs, including dozens of retrieved documents, tool outputs, and time sensitive data. While prior work has produced tool calling benchmarks and examined factuality in knowledge augmented systems, relatively little work studies their intersection: settings where LLMs must integrate large volumes of dynamic, structured and unstructured multi tool outputs. We investigate LLM failure modes in this regime using crypto as a representative high data density domain. We introduce (1) CryptoAnalystBench, an analyst aligned benchmark of 198 production crypto and DeFi queries spanning 11 categories; (2) an agentic harness equipped with relevant crypto and DeFi tools to generate responses across multiple frontier LLMs; and (3) an evaluation pipeline with citation verification and an LLM as a judge rubric spanning four user defined success dimensions: relevance, temporal relevance, depth, and data consistency. Using human annotation, we develop a taxonomy of seven higher order error types that are not reliably captured by factuality checks or LLM based quality scoring. We find that these failures persist even in state of the art systems and can compromise high stakes decisions. Based on this taxonomy, we refine the judge rubric to better capture these errors. While the judge does not align with human annotators on precise scoring across rubric iterations, it reliably identifies critical failure modes, enabling scalable feedback for developers and researchers studying analyst style agents. We release CryptoAnalystBench with annotated queries, the evaluation pipeline, judge rubrics, and the error taxonomy, and outline mitigation strategies and open challenges in evaluating long form, multi tool augmented systems."
    }
  }
]