[
  {
    "id": "2512.16874v1",
    "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking",
    "authors": [
      "Tomáš Souček",
      "Pierre Fernandez",
      "Hady Elsahar",
      "Sylvestre-Alvise Rebuffi",
      "Valeriu Lacatusu"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16874v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16874v1",
    "content": {
      "en": "Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.",
      "tr": "## Özet\n\nBu makale, dijital içeriklerin köken takibi için kritik öneme sahip görünmez watermarklama (invisible watermarking) alanında **Pixel Seal** adlı yeni bir state-of-the-art yaklaşımını sunmaktadır. Mevcut yöntemlerin karşılaştığı temel zorluklar olan görsel algılamayı tam olarak taklit edemeyen proxy perceptual losses (MSE, LPIPS gibi), çelişkili hedeflerden kaynaklanan optimizasyon kararsızlığı ve yüksek çözünürlüklü görüntülerde/videolarda watermarkların hem görünmezlik hem de sağlamlık özelliklerinde azalma gibi sorunları ele almaktadır.\n\n**Ana Katkılar:**\n\n*   **Adversarial-only training paradigm:** Güvenilir olmayan piksel bazlı algılanabilirlik kayıplarını (imperceptibility losses) ortadan kaldırarak daha sağlam ve görünmez watermarklar elde edilmesini sağlar.\n*   **Üç aşamalı eğitim programı (three-stage training schedule):** Sağlamlık (robustness) ve görünmezlik (imperceptibility) hedeflerini ayrıştırarak eğitim sürecinin kararlı bir şekilde yakınsama göstermesini sağlar.\n*   **Yüksek çözünürlük uyarlaması (high-resolution adaptation):** JND-based attenuation ve training-time inference simulation teknikleri kullanarak upscaling artefaktlarını ortadan kaldırır ve yüksek çözünürlüklü görüntülerde/videolarda ölçeklenebilirlik sorununu çözer.\n\n**Metodoloji:**\n\nPixel Seal, geleneksel piksel tabanlı kayıplar yerine tamamen adversarial bir eğitim çerçevesi kullanır. Eğitim süreci, önce görünmezlik, ardından sağlamlık ve son olarak her ikisinin dengelenmesi olmak üzere üç aşamada gerçekleştirilir. Yüksek çözünürlük desteği için, insan görsel sisteminin fark edemeyeceği düzeydeki JND (Just Noticeable Difference) eşiklerine dayalı zayıflatma ve eğitim sırasında yapay çıkarım simülasyonu gibi teknikler entegre edilmiştir. Videolar için ise, temporal watermark pooling yöntemiyle verimli bir uyarlama sağlanmıştır.\n\n**Sonuçlar:**\n\nPixel Seal, farklı görüntü türlerinde ve çok çeşitli dönüşümlere (transformations) karşı yapılan kapsamlı değerlendirmelerde mevcut state-of-the-art yöntemlere kıyasla belirgin iyileştirmeler göstermiştir. Hem sağlamlık hem de görünmezlik açısından üstün performans sergileyerek, gerçek dünya senaryolarında güvenilir köken takibi için pratik ve ölçeklenebilir bir çözüm sunmaktadır."
    }
  },
  {
    "id": "2512.16851v1",
    "title": "PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy",
    "authors": [
      "Ripan Kumar Kundu",
      "Istiak Ahmed",
      "Khaza Anuarul Hoque"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "link": "http://arxiv.org/abs/2512.16851v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16851v1",
    "content": {
      "en": "The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.",
      "tr": "## Özet\n\nBu makale, Extended Reality (XR) teknolojileri ile yapay zeka (AI) entegrasyonunun (AI XR) getirdiği yenilikçi uygulamaların mahremiyet endişelerine odaklanmaktadır. Özellikle, AI XR sistemlerinde kullanılan hassas verilerin (örneğin, eye-tracking) Membership Inference Attacks (MIA) ve Re-identification Attacks (RDA) gibi saldırılarla kişisel bilgilerin yüksek başarı oranıyla elde edilme riski üzerinde durulmaktadır. Geleneksel Differential Privacy (DP) yöntemlerinin, veri setlerindeki çok sayıda feature'a eşit şekilde uygulanması durumunda, alakasız feature'lara gereksiz gürültü ekleyebileceği, model doğruluğunu düşürebileceği ve inference süresini artırarak gerçek zamanlı XR dağıtımını sınırlayabileceği belirtilmiştir.\n\n### Ana Katkılar:\n\n*   **Yeni Bir Mahremiyet Koruma Çerçevesi:** Makale, Explainable AI (XAI) ve DP'yi birleştiren yenilikçi bir çerçeve sunmaktadır. Bu çerçeve, AI XR modellerindeki en etkili feature'ları tespit etmek için post-hoc explanations kullanır ve bu feature'lara seçici olarak DP uygular.\n*   **Seçici DP Uygulaması:** Geleneksel DP'nin aksine, bu yaklaşım yalnızca en kritik feature'lara DP uygulayarak model doğruluğunu korur ve gürültü seviyesini optimize eder.\n\n### Metodoloji:\n\n*   **XAI Odaklı Feature Seçimi:** Post-hoc explanation teknikleri kullanılarak AI XR modellerinde yüksek etkiye sahip olan feature'lar belirlenmiştir.\n*   **Seçici DP Uygulaması:** Tespit edilen kritik feature'lara inference sırasında DP mekanizmaları uygulanmıştır.\n*   **Değerlendirme:** Önerilen XAI-guided DP yaklaşımı, cybersickness, emotion ve activity classification üzerine üç farklı AI XR modeli ve üç veri seti üzerinde değerlendirilmiştir.\n\n### Sonuçlar:\n\n*   **Mahremiyet Saldırılarında Azalma:** Önerilen yöntem, cybersickness görevlerinde MIA ve RDA başarı oranlarını sırasıyla %43 ve %39'a kadar düşürmüştür.\n*   **Model Utlity Korunumu:** Transformer modelleri kullanılarak %97'ye varan doğrulukla model utility korunmuştur.\n*   **İnference Süresi İyileştirmesi:** Geleneksel DP yaklaşımlarına kıyasla inference süresi ~2 kata kadar iyileştirilmiştir.\n*   **Pratik Uygulama:** PrivateXR adı verilen bir kullanıcı arayüzü (UI) geliştirilerek, kullanıcıların HTC VIVE Pro headset üzerinde mahremiyet seviyelerini ayarlamalarına ve gerçek zamanlı görev tahminleri almalarına olanak tanınmıştır. Bu, XR oyun deneyimi sırasında kullanıcı gizliliğini korumayı hedefler."
    }
  },
  {
    "id": "2512.16778v1",
    "title": "Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences",
    "authors": [
      "Theshani Nuradha",
      "Ian George",
      "Christoph Hirche"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "quant-ph",
      "cs.CR",
      "cs.IT",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16778v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16778v1",
    "content": {
      "en": "Data-processing is a desired property of classical and quantum divergences and information measures. In information theory, the contraction coefficient measures how much the distinguishability of quantum states decreases when they are transmitted through a quantum channel, establishing linear strong data-processing inequalities (SDPI). However, these linear SDPI are not always tight and can be improved in most of the cases. In this work, we establish non-linear SDPI for quantum hockey-stick divergence for noisy channels that satisfy a certain noise criterion. We also note that our results improve upon existing linear SDPI for quantum hockey-stick divergences and also non-linear SDPI for classical hockey-stick divergence. We define $F_γ$ curves generalizing Dobrushin curves for the quantum setting while characterizing SDPI for the sequential composition of heterogeneous channels. In addition, we derive reverse-Pinsker type inequalities for $f$-divergences with additional constraints on hockey-stick divergences. We show that these non-linear SDPI can establish tighter finite mixing times that cannot be achieved through linear SDPI. Furthermore, we find applications of these in establishing stronger privacy guarantees for the composition of sequential private quantum channels when privacy is quantified by quantum local differential privacy.",
      "tr": "## Özet\n\nBu çalışma, kuantum bilgi teorisinde **data-processing** özelliği üzerine odaklanmaktadır. Veri işleme, kuantum durumlarının ayırt edilebilirliğinin kuantum kanallarından geçerken ne kadar azaldığını ölçen **contraction coefficient** ile ilişkilidir. Mevcut **linear strong data-processing inequalities (SDPI)** bu azalmaları doğrusal olarak ele almakta olup, her zaman en sıkı sınırları sağlamayabilir.\n\n**Ana Katkı:**\n\n*   Bu makale, **non-linear SDPI**'yi kuantum hockey-stick divergence için, belirli bir gürültü kriterini sağlayan noisy kanallar bağlamında geliştirmektedir.\n*   Geliştirilen non-linear SDPI'ler, mevcut linear SDPI'leri kuantum hockey-stick divergence için ve ayrıca klasik hockey-stick divergence için geliştirir.\n*   **$F_γ$ curves** kavramı, **Dobrushin curves**'in kuantum ortamına genelleştirilmesi olarak tanımlanmakta ve heterojen kanalların ardışık bileşimi için SDPI karakterizasyonunu sağlamaktadır.\n\n**Metodoloji:**\n\n*   Makale, belirli gürültü kriterlerine sahip noisy kanallar için yeni non-linear SDPI'ler türetir.\n*   **$f$-divergences** için ters-Pinsker tipi eşitsizlikler, hockey-stick divergence'ler üzerine ek kısıtlamalarla elde edilir.\n*   Bu non-linear SDPI'ler, **finite mixing times** için daha sıkı sınırlar oluşturmada kullanılır.\n\n**Sonuçlar:**\n\n*   Non-linear SDPI'ler, linear SDPI'ler ile elde edilemeyen daha sıkı finite mixing times sağlar.\n*   Bu non-linear yaklaşımlar, **quantum local differential privacy** ile ölçülen gizliliğin ardışık özel kuantum kanallarının bileşiminde daha güçlü gizlilik garantileri sağlamak için uygulamalar bulur."
    }
  },
  {
    "id": "2512.16717v1",
    "title": "Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering",
    "authors": [
      "Rudra Dubey",
      "Arpit Mani Tripathi",
      "Archit Srivastava",
      "Sarvpal Singh"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.16717v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16717v1",
    "content": {
      "en": "In actuality, phishing attacks remain one of the most prevalent cybersecurity risks in existence today, with malevolent actors constantly changing their strategies to successfully trick users. This paper presents an AI model for a phishing detection system that uses an ensemble approach to combine character-level Convolutional Neural Networks (CNN) and LightGBM with engineered features. Our system uses a character-level CNN to extract sequential features after extracting 36 lexical, structural, and domain-based features from the URLs. On a test dataset of 19,873 URLs, the ensemble model achieves an accuracy of 99.819 percent, precision of 100 percent, recall of 99.635 percent, and ROC-AUC of 99.947 percent. Through a FastAPI-based service with an intuitive user interface, the suggested system has been utilised to offer real-time detection. In contrast, the results demonstrate that the suggested solution performs better than individual models; LightGBM contributes 40 percent and character-CNN contributes 60 percent to the final prediction. The suggested method maintains extremely low false positive rates while doing a good job of identifying contemporary phishing techniques. Index Terms - Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis",
      "tr": "## Özet\n\nBu makale, sürekli gelişen phishing saldırılarına karşı **AI destekli bir phishing detection system** sunmaktadır.\n\n*   **Ana Katkı:** Makalenin temel katkısı, phishing URL'lerini tespit etmek için **character-level Convolutional Neural Network (CNN)** ve **feature engineering** ile elde edilen özelliklerin **LightGBM** ile birleştirildiği bir **ensemble approach** önermesidir.\n\n*   **Metodoloji:**\n    *   URL'lerden 36 adet **lexical, structural, ve domain-based features** çıkarılmıştır.\n    *   Bu özelliklerin yanı sıra, URL'lerden **character-level CNN** kullanılarak sıralı (sequential) özellikler çıkarılmıştır.\n    *   Elde edilen bu iki farklı özellik kümesi, **ensemble model** oluşturmak amacıyla birleştirilmiştir.\n    *   Modelin performansının değerlendirilmesi için 19.873 URL'lik bir test veri seti kullanılmıştır.\n    *   Önerilen sistem, **FastAPI tabanlı bir servis** ve kullanıcı dostu bir arayüz aracılığıyla **real-time detection** için kullanıma sunulmuştur.\n\n*   **Sonuçlar:**\n    *   Ensemble model, test veri setinde **%99.819 accuracy, %100 precision, %99.635 recall ve %99.947 ROC-AUC** gibi yüksek performans metrikleri elde etmiştir.\n    *   Sonuçlar, önerilen ensemble çözümünün tekil modellerden daha üstün olduğunu göstermektedir.\n    *   Nihai tahminde **LightGBM'in %40, character-CNN'in ise %60 katkı** sağladığı belirlenmiştir.\n    *   Önerilen yöntem, güncel phishing tekniklerini başarılı bir şekilde tespit ederken **son derece düşük false positive rates** oranlarını korumuştur.\n\n*   **Anahtar Kelimeler:** Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis."
    }
  },
  {
    "id": "2512.16658v1",
    "title": "Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking",
    "authors": [
      "Sangeeth B",
      "Serena Nicolazzo",
      "Deepa K.",
      "Vinod P"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.16658v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16658v1",
    "content": {
      "en": "The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.",
      "tr": "## Özet\n\nBu akademik makale, **Deep Neural Network (DNN) Fikri Mülkiyetini Kaos Tabanlı White-Box Watermarking ile Koruma** üzerine odaklanmaktadır.\n\n*   **Ana Katkı:** Makale, DNN'lerin fikri mülkiyetini korumak için verimli ve dayanıklı bir **white-box watermarking** çerçevesi sunmaktadır. Bu çerçeve, modelin dahili parametrelerine (ağırlıklarına) sahip bilgisi ekleyerek model sahipliğini doğrulamayı amaçlar.\n\n*   **Metodoloji:**\n    *   Watermark, **logistic map** adı verilen bilinen bir **chaotic function** kullanılarak oluşturulan, başlangıç parametrelerine duyarlı bir dizi olarak üretilir.\n    *   Bu **chaotic sequence**, modelin yapısını değiştirmeden veya prediktif performansta düşüşe neden olmadan, modelin ara bir katmanının **weights**'ine enjekte edilir.\n    *   Sahipliği doğrulamak için, **genetic algorithm** tabanlı bir doğrulama süreci geliştirilmiştir. Bu süreç, çıkarılan ve yeniden oluşturulan diziler arasındaki benzerliği optimize ederek orijinal **chaotic parameters**'ı kurtarmayı hedefler.\n\n*   **Sonuçlar:**\n    *   Önerilen yaklaşımın etkinliği, **MNIST** ve **CIFAR-10** veri kümeleri üzerinde yapılan **image classification** görevlerinde kapsamlı deneylerle gösterilmiştir.\n    *   Gömülü watermark'ın, **fine-tuning** sonrasında dahi tespit edilebilir olduğu ve model doğruluğunda ihmal edilebilir bir kayıp olduğu gözlemlenmiştir.\n    *   Sayısal watermark kurtarmanın yanı sıra, ağırlık yoğunluğu grafikleri kullanılarak görsel analizler yapılmış ve orijinal, watermark'lı ve kurcalanmış modelleri ayırt etmek için aktivasyon tabanlı sınıflandırıcılar oluşturulmuştur.\n    *   Sonuç olarak, bu yöntem, **white-box** ortamlarında model sahipliğini gömme ve doğrulama için esnek ve ölçeklenebilir bir çözüm sunarak, fikri mülkiyetin kritik olduğu gerçek dünya senaryolarına uygun olduğu belirtilmiştir."
    }
  },
  {
    "id": "2512.16650v1",
    "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
    "authors": [
      "Jirui Yang",
      "Hengqi Guo",
      "Zhihui Lu",
      "Yi Zhao",
      "Yuansen Zhang"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.16650v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16650v1",
    "content": {
      "en": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
      "tr": "## Özet\n\nBu akademik makale, büyük dil modelleri (Large Language Models) için hafif ve etkili bir zararlı içerik tespit yöntemi olan **Prefix Probing**'i tanıtmaktadır.\n\n*   **Ana Katkı:** Makalenin temel katkısı, zararlı içerik tespitinde doğasında bulunan doğruluk, gecikme ve dağıtım maliyeti arasındaki üçlü dengeyi iyileştiren yeni bir black-box yöntemi sunmaktır. **Prefix Probing**, tespit maliyetini ilk token gecikmesine yakın bir seviyeye indirirken, tespit doğruluğundan önemli ölçüde ödün vermez.\n\n*   **Metodoloji:**\n    *   **Prefix Probing** yöntemi, \"agreement/execution\" (onaylama/yürütme) ve \"refusal/safety\" (ret/güvenlik) gibi açılış ön eklerinin (opening prefixes) koşullu log-olabilirliklerini (conditional log-probabilities) karşılaştırır.\n    *   Tespit yükünü azaltmak için **prefix caching** (ön ek önbellekleme) tekniğinden yararlanır.\n    *   Tespit sırasında, ek bir model çağırmadan veya çok aşamalı çıkarım yapmadan, yalnızca ön ekler üzerinde tek bir log-olabilirlik hesaplaması ile bir zararlılık puanı üretilir ve bir eşik (threshold) uygulanır.\n    *   Ön eklerin ayırt edici gücünü artırmak için otomatik olarak bilgi açısından zengin ön ekler keşfeden verimli bir ön ek oluşturma algoritması (prefix construction algorithm) tasarlanmıştır.\n\n*   **Sonuçlar:** Kapsamlı deneyler, **Prefix Probing**'in mevcut harici güvenlik modelleriyle (mainstream external safety models) karşılaştırılabilir bir tespit etkinliği sağladığını göstermektedir. Aynı zamanda, bu yöntem sadece minimum computational cost (hesaplama maliyeti) gerektirir ve ek model dağıtımı (extra model deployment) ihtiyacını ortadan kaldırır, bu da yüksek pratiklik ve verimliliğini vurgular."
    }
  },
  {
    "id": "2512.16538v1",
    "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
    "authors": [
      "Xiao Li",
      "Yue Li",
      "Hao Wu",
      "Yue Zhang",
      "Yechao Zhang"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16538v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16538v1",
    "content": {
      "en": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.",
      "tr": "## Özet\n\nBu akademik makale, kod açığı tespitinde büyük dil modellerinin (LLM) artan kullanımına karşı kod gizleme (code obfuscation) tekniklerinin etkisini sistematik olarak incelemektedir. Makalenin **ana katkısı**, mevcut kod gizleme yöntemlerini sınıflandırarak ve bunları tek bir çerçevede değerlendirerek, LLM tabanlı açık tespitinin güvenilirliğini ve dayanıklılığını anlamaktır.\n\n### Metodoloji\n\n*   **Sınıflandırma:** Kod gizleme yöntemleri üç ana sınıfta gruplandırılmıştır: **layout**, **data flow** ve **control flow**. Bu sınıflar altında 11 alt kategori ve 19 somut teknik incelenmiştir.\n*   **Uygulama:** Bu teknikler, Solidity, C, C++ ve Python olmak üzere dört farklı programlama dilinde uygulanmıştır.\n*   **Değerlendirme:** Uygulanan gizleme tekniklerinin etkileri, dört model ailesini (DeepSeek, OpenAI, Qwen ve LLaMA) kapsayan 15 farklı LLM ve GitHub Copilot ile Codex gibi iki kodlama ajanı üzerinde değerlendirilmiştir.\n*   **Analiz:** Elde edilen sonuçlar, gizlemenin performansı artırıp azaltabileceği koşulları belirlemek için açıklık (vulnerability) türleri, kod özellikleri (code properties) ve model nitelikleri (model attributes) açısından analiz edilmiştir.\n\n### Sonuçlar\n\nMakalenin bulguları, kod gizlemenin LLM tabanlı açık tespitinde hem **olumlu hem de olumsuz etkilere** sahip olabileceğini göstermiştir. Bu etkiler, gizleme tekniklerinin türüne, uygulandığı programa ve kullanılan LLM'in özelliklerine bağlı olarak değişiklik göstermektedir. Araştırma ayrıca, bu etkileşimleri açıklığa kavuşturmak ve LLM'lerin gerçek dünya açık tespitinde dayanıklılığını artırmak için açık sorunları ve gelecekteki araştırma yönlerini ortaya koymaktadır."
    }
  },
  {
    "id": "2512.16310v1",
    "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
    "authors": [
      "Yuxuan Qiao",
      "Dongqin Liu",
      "Hongchang Yang",
      "Wei Zhou",
      "Songlin Hu"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2512.16310v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16310v1",
    "content": {
      "en": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
      "tr": "## Özet\n\nBu akademik makale, **Large Language Models (LLM)** tarafından desteklenen **single-agent, multi-tool** mimarilerinin oluşturduğu yeni ve ciddi bir gizlilik riskini, **Tools Orchestration Privacy Risk (TOP-R)**'ı sistematik olarak incelemektedir. Bu risk, bir ajanın, kullanıcı tarafından belirlenmiş iyi niyetli bir hedefi gerçekleştirmek amacıyla, farklı araçlardan elde ettiği bilgileri birleştirerek ve kendi akıl yürütme yeteneklerini kullanarak beklenmedik hassas bilgileri sentezlemesiyle ortaya çıkar.\n\n*   **Ana Katkı:** Makale, TOP-R'ı ilk kez kapsamlı bir şekilde tanımlamakta ve analiz etmektedir. Bu riskin temel nedeninin, ajanın **objective function**'ının gizlilik farkındalığını göz ardı ederek yalnızca **helpfulness** için aşırı optimize edilmesinden kaynaklandığı öne sürülmektedir.\n\n*   **Metodoloji:**\n    *   Araştırmacılar, TOP-R'ı incelemek için **TOP-Bench** adlı bir veri seti oluşturmuşlardır. Bu veri seti, **leakage** ve **benign** senaryoları eşleştirerek riskin değerlendirilmesini sağlamaktadır.\n    *   Güvenlik ve **robustness** arasındaki dengeyi ölçmek amacıyla **H-Score** adında bütünsel bir metrik geliştirilmiştir.\n\n*   **Sonuçlar:**\n    *   Yapılan değerlendirmeler, TOP-R'ın ciddi bir risk olduğunu göstermiştir. Sekiz temsilci modelin ortalama **Risk Leakage Rate (RLR)**'ı %90.24 iken, ortalama H-Score yalnızca 0.167'dir ve hiçbir model 0.3'ü geçememiştir.\n    *   Makale, TOP-R'ı etkili bir şekilde azaltan **Privacy Enhancement Principle (PEP)** adlı bir yöntem önermiştir. Bu yöntem, **Risk Leakage Rate**'ini %46.58'e düşürmüş ve **H-Score**'u anlamlı bir şekilde 0.624'e yükseltmiştir.\n\nMakale, mevcut ajan mimarilerindeki yeni bir risk sınıfını, yapısal sınırlamaları ve uygulanabilir azaltma stratejilerini ortaya koymaktadır."
    }
  },
  {
    "id": "2512.16307v1",
    "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks",
    "authors": [
      "Safwan Shaheer",
      "G. M. Refatul Islam",
      "Mohammad Rafid Hamid",
      "Tahsin Zaman Jilan"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.16307v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16307v1",
    "content": {
      "en": "In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.",
      "tr": "## Özet\n\nBu akademik makale, **Large Language Models (LLMs)** alanındaki hızlı gelişimi göz önünde bulundurarak, **prompt injection attacks**ın sunduğu önemli güvenlik risklerini incelemektedir. Özellikle **LLaMA** ailesi gibi küçük ve açık kaynaklı modeller üzerine odaklanmaktadır.\n\n*   **Ana Katkı:** Makale, otomatik savunmalar üretebilen ve bu üretilen savunmaları kapsamlı bir şekilde benchmark edilmiş saldırılara karşı sistematik olarak değerlendiren **novel defense mechanisms** sunmaktadır. Bu yaklaşımın LLM'lerdeki **goal-hijacking vulnerabilities**i azaltmadaki etkinliğini ampirik olarak göstermektedir. Makale, küçük açık kaynaklı LLM'lerin artan önemini ve edge cihazlarda geniş dağılım potansiyellerini vurgulamaktadır.\n\n*   **Metodoloji:** Araştırma, üç temel bileşenden oluşmaktadır:\n    *   Mevcut prompt tabanlı savunmaların en yeni saldırılara karşı değerlendirilmesi.\n    *   **Chain Of Thoughts** (CoT) gibi bir seed defense kullanarak savunma promptlarını iteratif olarak iyileştiren yeni bir framework'ün tanıtılması.\n    *   **Goal hijacking attacks**ın tespitinde önemli iyileştirmeler gösterilmesi.\n\n*   **Sonuçlar:** Önerilen stratejiler, saldırıların başarı oranlarını ve yanlış tespit oranlarını önemli ölçüde azaltırken, aynı zamanda goal-hijacking yeteneklerini etkili bir şekilde tespit etmektedir. Bu sonuçlar, kaynak kısıtlı ortamlarda daha güvenli ve verimli küçük ve açık kaynaklı LLM dağıtımlarının yolunu açmaktadır."
    }
  },
  {
    "id": "2512.16292v1",
    "title": "In-Context Probing for Membership Inference in Fine-Tuned Language Models",
    "authors": [
      "Zhexi Lu",
      "Hongliang Chi",
      "Nathalie Baracaldo",
      "Swanand Ravindra Kadhe",
      "Yuseok Jeon"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16292v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16292v1",
    "content": {
      "en": "Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.",
      "tr": "## Özet\n\nBu makale, ince ayarlı (fine-tuned) büyük dil modellerinde (LLMs) **Membership Inference Attacks (MIAs)**'ın neden olduğu gizlilik tehdidini ele almaktadır. Özellikle hassas verilerle alan-spesifik görevlere adapte edilmiş LLM'ler bu tehdit altındadır. Önceki siyah-kutulu (black-box) MIA tekniklerinin güven skorlarına veya token olasılıklarına dayanması, bu sinyallerin örneklerin içsel özellikleriyle (örneğin, içerik zorluğu veya nadirliği) karışmasına ve dolayısıyla düşük genelleme performansı ile sinyal-gürültü oranına yol açmasına neden olmaktadır.\n\n### Ana Katkı ve Metodoloji\n\nBu çalışma, eğitim dinamikleri teorisine dayanan yeni bir MIA çerçevesi olan **ICP-MIA**'yı sunmaktadır. Bu çerçeve, özellikle optimizasyon sırasında azalan getiri (diminishing returns) olgusundan yararlanır.\n\n*   **Optimization Gap**: Üyeler (member samples), yakınsamada (convergence) minimal kalan kayıp-azaltma potansiyeline sahipken, üye olmayanlar (non-members) daha fazla optimizasyon potansiyeli taşır. Bu fark, **Optimization Gap** olarak adlandırılır ve bir üyelik işareti olarak kullanılır.\n*   **In-Context Probing (ICP)**: Siyah-kutulu bir ortamda bu farkı tahmin etmek için, ICP eğitim gerektirmeyen (training-free) bir yöntemdir. Stratejik olarak oluşturulmuş girdi bağlamları (input contexts) aracılığıyla ince ayara benzer davranışları taklit eder.\n*   **Probing Stratejileri**:\n    *   **Referans-veriye dayalı (reference-data-based)**: Semantik olarak benzer halka açık örnekler kullanılır.\n    *   **Öz-bozma (self-perturbation)**: Maskeleme veya üretme yoluyla gerçekleştirilir.\n\n### Sonuçlar\n\n*   Üç farklı görev ve birden fazla LLM üzerinde yapılan deneyler, ICP-MIA'nın, özellikle düşük yanlış pozitif oranlarında (low false positive rates) önceki siyah-kutulu MIA'lardan önemli ölçüde daha iyi performans gösterdiğini ortaya koymuştur.\n*   Referans verisi hizalamasının (reference data alignment), model tipinin (model type), PEFT konfigürasyonlarının (PEFT configurations) ve eğitim çizelgelerinin (training schedules) saldırı etkinliğini nasıl etkilediği de analiz edilmiştir.\n*   Bu bulgular, ICP-MIA'yı dağıtılmış LLM'lerde gizlilik risklerini denetlemek için pratik ve teorik olarak temellendirilmiş bir çerçeve olarak konumlandırmaktadır."
    }
  },
  {
    "id": "2512.16280v1",
    "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams",
    "authors": [
      "Gilad Gressel",
      "Rahul Pankajakshan",
      "Shir Rozenfeld",
      "Ling Li",
      "Ivan Franceschini"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "link": "http://arxiv.org/abs/2512.16280v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16280v1",
    "content": {
      "en": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.   We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.",
      "tr": "## Özet\n\nBu akademik makale, \"Romance-Baiting\" adı verilen dolandırıcılık türünde Büyük Dil Modellerinin (LLMs) mevcut ve gelecekteki rolünü incelemektedir.\n\n*   **Makalenin Ana Katkısı:**\n    *   \"Romance-Baiting\" dolandırıcılıklarının artık LLM'ler tarafından büyük ölçüde otomatize edilebileceği ve bu tür operasyonların yaygınlaştığına dair endişeleri ortaya koymaktadır.\n    *   Mevcut güvenlik filtrelerinin bu tür dolandırıcılık diyaloglarını tespit etmede yetersiz kaldığını vurgulamaktadır.\n\n*   **Metodoloji:**\n    *   145 dolandırıcılık operasyonu içindeki kişiler ve 14 dolandırıcılık mağduru ile derinlemesine mülakatlar yapılmıştır.\n    *   LLM tabanlı dolandırıcılık ajanları ile insan operatörlerin performansını karşılaştıran, körlemesine ve uzun süreli bir konuşma çalışması yürütülmüştür.\n    *   Ticari güvenlik filtrelerinin etkinliği değerlendirilmiştir.\n\n*   **Sonuçlar:**\n    *   LLM'lerin halihazırda dolandırıcılık organizasyonlarında yaygın olarak kullanıldığı tespit edilmiştir; dolandırıcılık faaliyetlerinin %87'si sistemsel ve otomatize edilebilir sohbet görevlerinden oluşmaktadır.\n    *   Bir haftalık çalışmada, LLM ajanı katılımcılardan **daha fazla güven** sağlamış (p=0.007) ve insan operatörlere göre **daha yüksek talep uyumu** sergilemiştir (%46'ya karşılık %18).\n    *   Popüler güvenlik filtreleri, romance-baiting diyaloglarının **%0.0'ını** tespit edebilmiştir.\n    *   Bu bulgular, romance-baiting dolandırıcılıklarının tam ölçekli LLM otomasyonuna uygun olduğunu ve mevcut savunma mekanizmalarının yetersiz kaldığını göstermektedir."
    }
  },
  {
    "id": "2512.16123v1",
    "title": "Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection",
    "authors": [
      "Min Geun Song",
      "Gang Min Kim",
      "Woonmin Kim",
      "Yongsik Kim",
      "Jeonghyun Sim"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2512.16123v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16123v1",
    "content": {
      "en": "Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.",
      "tr": "## Özet\n\nBu akademik makale, nesne algılama modellerinin **adversarial attacks**'e karşı savunulması için bir **autoencoder-based denoising defense** yöntemi önermektedir.\n\n*   **Ana Katkı:** Makalenin temel katkısı, **adversarial perturbations**'un neden olduğu nesne algılama performans düşüşünü gidermek amacıyla bir **single-layer convolutional autoencoder** kullanılarak gürültü giderme (denoising) savunması geliştirmesidir. Bu yaklaşım, modelin yeniden eğitilmesine gerek kalmadan savunma sağlamayı hedeflemektedir.\n\n*   **Metodoloji:**\n    *   COCO veri setindeki araç görüntüleri üzerinde **Perlin noise** kullanılarak **adversarial attacks** gerçekleştirilmiştir.\n    *   Bu saldırılardan etkilenen görüntülerdeki **perturbations**'ları kaldırmak için bir **single-layer convolutional autoencoder** uygulanmıştır.\n    *   Savunma mekanizmasının etkinliği, **YOLOv5** modeli kullanılarak ve **bbox mAP** ile **bbox mAP@50** metrikleri üzerinden değerlendirilmiştir.\n\n*   **Sonuçlar:**\n    *   Yapılan **adversarial attacks**, **bbox mAP** değerini 0.2890'dan 0.1640'a düşürerek %43.3'lük bir performans düşüşüne neden olmuştur.\n    *   Önerilen **autoencoder defense** uygulandıktan sonra, **bbox mAP** 0.1700'e (%3.7'lik bir iyileşme) yükselmiştir.\n    *   **bbox mAP@50** değeri ise 0.2780'den 0.3080'e (%10.8'lik bir artış) iyileşmiştir.\n    *   Bu sonuçlar, **autoencoder-based denoising**'in, modelin yeniden eğitilmesine gerek kalmadan **adversarial attacks**'e karşı kısmi bir savunma sağlayabildiğini göstermektedir."
    }
  },
  {
    "id": "2512.15892v1",
    "title": "VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces",
    "authors": [
      "Artem Grigor",
      "Christian Schroeder de Witt",
      "Simon Birnbach",
      "Ivan Martinovic"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.15892v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15892v1",
    "content": {
      "en": "Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy.   We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs).   We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.",
      "tr": "## Özet\n\nBu makale, büyük dil modelleri (LLMs) ile geliştirilen otonom ajanların karşılaştığı \"host-independent autonomy\" sorununu ele almaktadır. Mevcut durumda, ajanlar kullanıcılar adına hassas kaynakları yönetebilseler de, bu ajanların üzerinde çalıştığı host altyapısı, model, girdi veya çıktıları manipüle ederek ajanın gerçek otonomisini zayıflatabilmektedir.\n\n### Ana Katkı:\n\n*   **VET (Verifiable Execution Traces)** adlı yeni bir formal framework sunulmaktadır. VET, ajanın çıktılarının host'tan bağımsız olarak doğrulanmasını sağlayarak host-independent autonomy'ye doğru bir adım atmaktadır.\n*   **Agent Identity Document (AID)** kavramı tanıtılmıştır. AID, ajanın yapılandırmasını ve doğrulama için gereken proof system'lerini belirtir.\n*   VET, çeşitli proof mekanizmalarını destekleyen **compositional** bir yapıya sahiptir: trusted hardware, succinct cryptographic proofs ve notarized TLS transcripts (Web Proofs).\n\n### Metodoloji:\n\n*   VET, API tabanlı bir LLM ajanı için uygulanmış ve gerçekçi iş yükleri üzerinde değerlendirilmiştir.\n*   Gizli bilgi içeren \"black-box\" API çağrıları için **Web Proofs** pratik bir çözüm olarak belirlenmiş ve doğrudan API çağrılarına kıyasla genellikle 3$\\times$ civarında bir overhead ile çalıştığı gözlemlenmiştir.\n*   Herkese açık (public) API çağrıları için ise daha düşük overhead'li bir **TEE Proxy** (Trusted Execution Environment Proxy) kullanımının yeterli olduğu bulunmuştur.\n*   Bir **verifiable trading agent** örneği ile, her karar için proof üretimi ve Web Proofs ile TEE Proxy'nin birlikte kullanımı gösterilmiştir.\n\n### Sonuçlar:\n\n*   Mevcut teknolojiyle pratik, host'tan bağımsız doğrulamanın mümkün olduğu gösterilmiştir.\n*   Bu çalışma, gelecekte tam host-independent autonomy'ye sahip sistemlerin temelini atmaktadır."
    }
  },
  {
    "id": "2512.15688v1",
    "title": "BashArena: A Control Setting for Highly Privileged AI Agents",
    "authors": [
      "Adam Kaufman",
      "James Lucassen",
      "Tyler Tracy",
      "Cody Rushing",
      "Aryan Bhatt"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.15688v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15688v1",
    "content": {
      "en": "Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.",
      "tr": "## Özet\n\nBu makale, yüksek düzeyde yetkilendirilmiş yapay zeka (AI) ajanlarının neden olabileceği potansiyel zararları kontrol altına almak için tasarlanmış bir ortam olan **BashArena**'yı tanıtmaktadır.\n\n*   **Ana Katkı:** BashArena, AI kontrol tekniklerinin güvenlik açısından kritik ortamlarda incelenmesi için **control setting** olarak geliştirilmiştir. Bu ortam, 637 adet Linux sistem yönetimi ve altyapı mühendisliği görevini içermekte olup, karmaşık ve gerçekçi senaryolar sunmaktadır. Ayrıca, bir **red team** için dört farklı sabotaj hedefi (malware çalıştırma, sırları sızdırma, yetki yükseltme ve firewall'u devre dışı bırakma) tanımlanmıştır.\n\n*   **Metodoloji:** Makale, öncü büyük dil modellerinin (LLM) görev tamamlama, tespit edilmeden sabotaj gerçekleştirme ve sabotaj girişimlerini tespit etme yeteneklerini değerlendirmek için BashArena'yı kullanmıştır.\n\n*   **Sonuçlar:** Yapılan değerlendirmelerde, Claude Sonnet 4.5'in, GPT-4.1 mini'nin izlemesinden %26 oranında kaçınarak sabotaj gerçekleştirebildiği gözlemlenmiştir. Bu durum, %4'lük bir trajectory-wise **False Positive Rate (FPR)** ile birlikte raporlanmıştır. Bu bulgular, BashArena içinde daha etkili kontrol protokollerinin tasarlanması için bir **baseline** sağlamaktadır. Makalede ayrıca, veri kümesi bir **ControlArena setting** olarak yayınlanmış ve görev üretim **pipeline**'ı paylaşılmıştır."
    }
  },
  {
    "id": "2512.15823v1",
    "title": "Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications",
    "authors": [
      "Mohammad Waquas Usmani",
      "Sankalpa Timilsina",
      "Michael Zink",
      "Susmit Shannigrahi"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.MM",
      "eess.IV"
    ],
    "link": "http://arxiv.org/abs/2512.15823v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15823v1",
    "content": {
      "en": "Immersive formats such as 360° and 6DoF point cloud videos require high bandwidth and low latency, posing challenges for real-time AR/VR streaming. This work focuses on reducing bandwidth consumption and encryption/decryption delay, two key contributors to overall latency. We design a system that downsamples point cloud content at the origin server and applies partial encryption. At the client, the content is decrypted and upscaled using an ML-based super-resolution model. Our evaluation demonstrates a nearly linear reduction in bandwidth/latency, and encryption/decryption overhead with lower downsampling resolutions, while the super-resolution model effectively reconstructs the original full-resolution point clouds with minimal error and modest inference time.",
      "tr": "## Özet\n\nBu akademik makale, Gerçek Zamanlı Karma Gerçeklik (Mixed Reality - MR) uygulamaları için güvenli ve verimli bir yapay zeka tabanlı süper-çözünürlük (super-resolution) sistemini tanıtmaktadır.\n\n*   **Ana Katkı:** Çalışmanın temel amacı, 360° ve 6DoF point cloud videolar gibi sürükleyici formatların yüksek bant genişliği (bandwidth) ve düşük gecikme (latency) gereksinimlerini karşılamaktır. Bu kapsamda, bant genişliği tüketimini ve şifreleme/çözme (encryption/decryption) gecikmesini azaltarak genel gecikmeyi düşürmeye odaklanılmaktadır.\n\n*   **Metodoloji:**\n    *   Sistem, verinin kaynağında (origin server) point cloud içeriğini indükleyerek (downsampling) ve buna kısmi şifreleme (partial encryption) uygulayarak tasarlanmıştır.\n    *   İstemci tarafında (client), içerik çözülmekte (decrypted) ve bir Makine Öğrenmesi (ML) tabanlı süper-çözünürlük modeli kullanılarak upscale edilmektedir.\n\n*   **Sonuçlar:**\n    *   Yapılan değerlendirmeler, daha düşük indükleme çözünürlüklerinde bant genişliği, gecikme ve şifreleme/çözme ek yükünde (overhead) neredeyse doğrusal bir azalma göstermiştir.\n    *   Süper-çözünürlük modeli, orijinal tam çözünürlüklü (full-resolution) point cloud'ları minimal hata ve makul çıkarım süresi (inference time) ile başarılı bir şekilde yeniden oluşturmuştur."
    }
  },
  {
    "id": "2512.15503v1",
    "title": "Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection",
    "authors": [
      "Konstantinos Kalogiannis",
      "Ahmed Mohamed Hussain",
      "Hexu Li",
      "Panos Papadimitratos"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2512.15503v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15503v1",
    "content": {
      "en": "Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.",
      "tr": "## Özet\n\nBu makale, Vehicle-to-Everything (V2X) iletişimi ile koordine edilen çoklu araç oluşumları olan vehicular platooning'in sunduğu dönüşümsel iyileştirmelere rağmen, dağıtık doğasının güvenlik açıklarına yol açtığını ve onaylanmış araçların hatalı kinematik veriler enjekte ederek operasyonel istikrarı tehlikeye atabileceğini belirtmektedir. Geleneksel misbehavior detection yaklaşımlarının yüksek False Positive (FP) oranlarına sahip olması ve çoklu araç koordinasyon dinamiklerindeki karmaşık zamansal bağımlılıkları yakalayamaması sorununa çözüm olarak, Attention In Motion (AIMformer) adlı transformer-based bir framework sunulmaktadır.\n\n**Makalenin Ana Katkıları:**\n\n*   **AIMformer Framework'ü:** Gerçek zamanlı misbehavior detection için özel olarak tasarlanmış, transformer-based bir mimari.\n*   **Multi-head Self-Attention Mekanizmaları:** Araç içi zamansal dinamikleri ve araçlar arası uzamsal korelasyonları eş zamanlı olarak yakalama yeteneği.\n*   **Global Positional Encoding ve Araç-Özel Zamansal Offsetler:** Join/exit manevralarını yönetmek için entegre edilmiş bir yaklaşım.\n*   **Precision-Focused (BCE) Loss Function:** Güvenlik-kritik araç sistemlerinin gereksinimlerini karşılamak üzere FP'leri cezalandıran bir kayıp fonksiyonu önerisi.\n*   **Edge Deployment Yetenekleri:** Kaynak kısıtlı kenar platformlarında gerçek zamanlı operasyon için uygunluk.\n\n**Metodoloji:**\n\nAIMformer, multi-head self-attention mekanizmalarını kullanarak hem araç içi zaman serisi verilerini hem de araçlar arasındaki mekansal ilişkileri modellemektedir. Join ve exit operasyonları sırasında meydana gelen zamansal değişimleri ele almak için global positional encoding ile araçlara özgü zamansal offsetler birleştirilmiştir. Güvenlik-kritik uygulamalar için hassasiyeti en üst düzeye çıkarmak amacıyla, False Positive'leri proaktif olarak azaltmaya odaklanan özel bir Precision-Focused (BCE) loss function kullanılmıştır.\n\n**Sonuçlar:**\n\nYapılan kapsamlı değerlendirmeler, AIMformer'ın 4 farklı platoon controller, çeşitli saldırı vektörleri ve farklı mobilite senaryoları altında state-of-the-art baseline mimarilerine kıyasla üstün performans gösterdiğini ($\\geq$ 0.93) ortaya koymaktadır. TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX) ve TensorRT kullanılarak gerçekleştirilen detaylı deployment analizi, sub-millisecond inference latency elde edilmesini sağlamıştır. Bu sonuçlar, AIMformer'ın hem araç içi hem de yol kenarı altyapı dağıtımı için uygun ve uygulanabilir olduğunu doğrulamaktadır."
    }
  },
  {
    "id": "2512.15468v1",
    "title": "How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?",
    "authors": [
      "Hua Yang",
      "Alejandro Velasco",
      "Thanh Le-Cong",
      "Md Nazmul Haque",
      "Bowen Xu"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.15468v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15468v1",
    "content": {
      "en": "The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.   In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.",
      "tr": "## Özet\n\nBu akademik makale, büyük dil modelleri (LLMs) için kullanılan kod verilerindeki gizlilik ve telif hakkı endişelerini ele almaktadır. Makalenin ana katkısı, kod dönüşüm tekniklerinin, LLMs için membership inference (MI) saldırılarına karşı direncini nasıl etkilediğini sistematik olarak araştırmasıdır.\n\n### Metodoloji\n\nAraştırmacılar, semantik olarak eşdeğer kod dönüşüm kurallarının MI tespitini nasıl atlattığını incelemişlerdir. Bu dönüşümler, kodun sözdizimini değiştirirken anlamsal bütünlüğünü korumayı amaçlamaktadır.\n\n### Sonuçlar\n\n*   **Dönüşümlerin Etkisi:** Dönüşüm kurallarının her biri için model doğruluğu en kötü durumda yalnızca %1.5 oranında düşmektedir. Bu, dönüştürülmüş veri kümelerinin fine-tuning için etkili bir şekilde kullanılabileceğini göstermektedir.\n*   **RenameVariable Kuralının Etkinliği:** **RenameVariable** kuralının MI başarısını %10.19 oranında azalttığı tespit edilmiştir. Bu durum, bu kuralın kısıtlı kodun varlığını gizlemedeki potansiyelini vurgulamaktadır.\n*   **Nedensel Analiz:** Yapılan nedensel analiz, değişken yeniden adlandırmanın (variable renaming) MI tespitini engellemede en güçlü nedensel etkiye sahip olduğunu doğrulamıştır.\n*   **Kombine Dönüşümler:** Birden fazla dönüşüm kuralının birleştirilmesinin MI etkinliğini daha fazla azaltmadığı gözlemlenmiştir.\n*   **Genel Çıkarım:** Bulgular, kod LLMs için lisans uyumluluğunun uygulanmasında kritik bir zayıflık olduğunu ortaya koymaktadır. Dönüşüm tabanlı gizleme (obfuscation) tekniklerinin MI tespitini önemli ölçüde zayıflatabileceği gösterilmiştir."
    }
  },
  {
    "id": "2512.15379v1",
    "title": "Remotely Detectable Robot Policy Watermarking",
    "authors": [
      "Michael Amir",
      "Manon Flageat",
      "Amanda Prorok"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.RO",
      "cs.CR",
      "cs.LG",
      "eess.SY"
    ],
    "link": "http://arxiv.org/abs/2512.15379v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15379v1",
    "content": {
      "en": "The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \\textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.",
      "tr": "## Özet\n\nBu makale, robotik sistemlerde kullanılan eğitilmiş politikaların (trained policy) fikri mülkiyetini korumaya yönelik yeni bir yöntem sunmaktadır. Mevcut su izi (watermarking) teknikleri genellikle robota doğrudan erişim gerektirirken, bu çalışma **\"Physical Observation Gap\"** adı verilen, yalnızca dışarıdan gözlemlere dayalı olarak (örneğin video görüntüleri) su izi tespit etme zorluğunu ele almaktadır.\n\n**Ana Katkılar:**\n\n*   Robot politikaları için **remote detection** yeteneğine sahip ilk su izi stratejisi olan **Colored Noise Coherency (CoNoCo)**'yu tanıtmaktadır.\n*   Sinyallerin gürültülü, asenkron ve bilinmeyen sistem dinamikleri tarafından filtrelendiği bir ortamda su izi tespitinin zorluğunu **glimpse sequence** kavramıyla formalize etmektedir.\n*   CoNoCo'nun politikaların performansını düşürmediğini, **marginal action distribution**'u koruyarak kanıtlamaktadır.\n\n**Metodoloji:**\n\n*   CoNoCo, politikaların doğal rastgeleliğini (stochasticity) kullanarak robotun hareketlerine spektral bir sinyal gömülmesini sağlar.\n*   Bu gömülü sinyal, hareket yakalama (motion capture) ve farklı açılardan çekilmiş video görüntüleri gibi çeşitli uzak gözlem yöntemleriyle tespit edilebilir.\n\n**Sonuçlar:**\n\n*   Simülasyon ve gerçek dünya robot deneylerinde, CoNoCo'nun farklı uzak gözlem modaliteleri üzerinden **güçlü ve sağlam (robust)** bir tespit sağladığı gösterilmiştir.\n*   Bu çalışma, robotikte fikri mülkiyetin korunması için önemli bir adım olup, fiziksel politikaların kökenini **non-invasively** ve yalnızca uzak gözlemlerle doğrulayan ilk yöntemi sunmaktadır."
    }
  },
  {
    "id": "2512.15335v1",
    "title": "Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference",
    "authors": [
      "Chenxiang Zhang",
      "Tongxi Qu",
      "Zhong Li",
      "Tian Zhang",
      "Jun Pang"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.15335v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15335v1",
    "content": {
      "en": "Deep neural networks are widely deployed with quantization techniques to reduce memory and computational costs by lowering the numerical precision of their parameters. While quantization alters model parameters and their outputs, existing privacy analyses primarily focus on full-precision models, leaving a gap in understanding how bit-width reduction can affect privacy leakage. We present the first systematic study of the privacy-utility relationship in post-training quantization (PTQ), a versatile family of methods that can be applied to pretrained models without further training. Using membership inference attacks as our evaluation framework, we analyze three popular PTQ algorithms-AdaRound, BRECQ, and OBC-across multiple precision levels (4-bit, 2-bit, and 1.58-bit) on CIFAR-10, CIFAR-100, and TinyImageNet datasets. Our findings consistently show that low-precision PTQs can reduce privacy leakage. In particular, lower-precision models demonstrate up to an order of magnitude reduction in membership inference vulnerability compared to their full-precision counterparts, albeit at the cost of decreased utility. Additional ablation studies on the 1.58-bit quantization level show that quantizing only the last layer at higher precision enables fine-grained control over the privacy-utility trade-off. These results offer actionable insights for practitioners to balance efficiency, utility, and privacy protection in real-world deployments.",
      "tr": "## Özet\n\nBu makale, *deep neural networks* (derin öğrenme ağları) kullanımında, özellikle *post-training quantization* (PTQ) yöntemlerinin gizlilik üzerindeki etkisini incelemektedir. Araştırmanın temel katkısı, bit genişliği azaltımının model parametrelerini ve çıktılarını değiştirerek gizlilik sızıntısını nasıl etkilediğini sistematik olarak analiz etmektir.\n\n**Metodoloji:**\n\n*   Çalışma, *membership inference attacks* (üyelik çıkarım saldırıları) çerçevesini kullanarak üç popüler PTQ algoritmasını (AdaRound, BRECQ ve OBC) değerlendirmiştir.\n*   Bu analiz, farklı precision seviyelerinde (4-bit, 2-bit ve 1.58-bit) ve CIFAR-10, CIFAR-100 ve TinyImageNet veri setleri üzerinde gerçekleştirilmiştir.\n\n**Sonuçlar:**\n\n*   Düşük precision'lı PTQ yöntemlerinin gizlilik sızıntısını azaltabildiği tutarlı bir şekilde gözlemlenmiştir.\n*   Özellikle, daha düşük precision seviyesine sahip modeller, tam precision'lı modellere kıyasla üyelik çıkarım zafiyetinde bir büyüklük mertebesine kadar azalma göstermiştir.\n*   Bu durum, utility'de (kullanışlılıkta) bir düşüş pahasına gerçekleşmiştir.\n*   1.58-bit precision seviyesi üzerinde yapılan ek ablasyon çalışmaları, yalnızca son katmanın daha yüksek precision'da nicelleştirilmesinin gizlilik-utility dengesi üzerinde ince ayar yapma imkanı sunduğunu göstermiştir.\n\nBu bulgular, pratik uygulamalarda verimlilik, utility ve gizlilik korumasını dengelemek için eyleme geçirilebilir içgörüler sunmaktadır."
    }
  },
  {
    "id": "2512.15286v1",
    "title": "Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions",
    "authors": [
      "Siva Sai",
      "Ishika Goyal",
      "Shubham Sharma",
      "Sri Harshita Manuri",
      "Vinay Chamola"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.15286v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15286v1",
    "content": {
      "en": "The increasing number of cyber threats and rapidly evolving tactics, as well as the high volume of data in recent years, have caused classical machine learning, rules, and signature-based defence strategies to fail, rendering them unable to keep up. An alternative, Quantum Machine Learning (QML), has recently emerged, making use of computations based on quantum mechanics. It offers better encoding and processing of high-dimensional structures for certain problems. This survey provides a comprehensive overview of QML techniques relevant to the domain of security, such as Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), and Quantum Generative Adversarial Networks (QGANs), and discusses the contributions of this paper in relation to existing research in the field and how it improves over them. It also maps these methods across supervised, unsupervised, and generative learning paradigms, and to core cybersecurity tasks, including intrusion and anomaly detection, malware and botnet classification, and encrypted-traffic analytics. It also discusses their application in the domain of cloud computing security, where QML can enhance secure and scalable operations. Many limitations of QML in the domain of cybersecurity have also been discussed, along with the directions for addressing them.",
      "tr": "## Özet\n\nBu makale, siber güvenlik alanında klasik makine öğrenmesi yöntemlerinin yetersiz kaldığı günümüzdeki zorluklara bir çözüm olarak **Quantum Machine Learning (QML)** alanını tanıtmaktadır. QML'nin, özellikle yüksek boyutlu verilerin işlenmesinde sunduğu potansiyel avantajlara odaklanarak, bu alandaki mevcut yaklaşımları ve gelecekteki yönleri kapsamlı bir şekilde incelemektedir.\n\n**Makalenin Ana Katkısı:**\n\n*   Siber güvenlikle ilgili QML tekniklerinin kapsamlı bir **taxonomy**'sini sunmaktadır.\n*   Mevcut araştırmalara kıyasla QML'nin alana getirdiği yenilikleri ve iyileştirmeleri vurgulamaktadır.\n*   QML'nin siber güvenlikteki sınırlılıklarını ve bu sınırlılıkların üstesinden gelme yollarını tartışmaktadır.\n\n**Metodoloji:**\n\nMakale, QML'yi siber güvenlik görevlerine uyarlamak için kullanılan başlıca teknikleri analiz etmektedir:\n\n*   **Quantum Neural Networks (QNNs)**\n*   **Quantum Support Vector Machines (QSVMs)**\n*   **Variational Quantum Circuits (VQCs)**\n*   **Quantum Generative Adversarial Networks (QGANs)**\n\nBu yöntemler, **supervised learning**, **unsupervised learning** ve **generative learning** paradigmaları ile eşleştirilmiştir.\n\n**Sonuçlar:**\n\nQML'nin siber güvenlikteki çeşitli temel görevler için uygulanabilirliği gösterilmiştir:\n\n*   **Intrusion and anomaly detection**\n*   **Malware and botnet classification**\n*   **Encrypted-traffic analytics**\n\nAyrıca, QML'nin **cloud computing security** alanında güvenli ve ölçeklenebilir operasyonları nasıl iyileştirebileceği tartışılmıştır. Makale, QML'nin siber güvenlik uygulamalarındaki mevcut sınırlılıklarını detaylandırmakta ve bu alanlardaki gelecekteki araştırma yönlerini ortaya koymaktadır."
    }
  },
  {
    "id": "2512.15143v1",
    "title": "An Efficient Gradient-Based Inference Attack for Federated Learning",
    "authors": [
      "Pablo Montaña-Fernández",
      "Ines Ortega-Fernandez"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.15143v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15143v1",
    "content": {
      "en": "Federated Learning is a machine learning setting that reduces direct data exposure, improving the privacy guarantees of machine learning models. Yet, the exchange of model updates between the participants and the aggregator can still leak sensitive information. In this work, we present a new gradient-based membership inference attack for federated learning scenarios that exploits the temporal evolution of last-layer gradients across multiple federated rounds. Our method uses the shadow technique to learn round-wise gradient patterns of the training records, requiring no access to the private dataset, and is designed to consider both semi-honest and malicious adversaries (aggregators or data owners). Beyond membership inference, we also provide a natural extension of the proposed attack to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses. The proposed attacks are model-agnostic, and therefore applicable to any gradient-based model and can be applied to both classification and regression settings. We evaluate the attack on CIFAR-100 and Purchase100 datasets for membership inference and on Breast Cancer Wisconsin for attribute inference. Our findings reveal strong attack performance and comparable computational and memory overhead in membership inference when compared to another attack from the literature. The obtained results emphasize that multi-round federated learning can increase the vulnerability to inference attacks, that aggregators pose a more substantial threat than data owners, and that attack performance is strongly influenced by the nature of the training dataset, with richer, high-dimensional data leading to stronger leakage than simpler tabular data.",
      "tr": "## Özet\n\nBu makale, federated learning ortamlarında model güncellemelerinden hassas bilgileri sızdırmayı amaçlayan yeni bir **gradient-based membership inference attack** sunmaktadır. Temel katkısı, katılımcılar ile aggregator arasındaki iletişimin, özellikle **last-layer gradients**'in zaman içindeki evrimini kullanarak üye olma durumunu tahmin etmesidir.\n\n### Metodoloji:\n\n*   **Shadow Technique** kullanılarak, gizli veri setine erişim olmaksızın, her bir federated round için eğitim kayıtlarının **round-wise gradient patterns**'leri öğrenilir.\n*   Saldırı, hem **semi-honest** hem de **malicious adversaries** (aggregator veya data owners) göz önünde bulundurularak tasarlanmıştır.\n*   Üye olma çıkarımı (membership inference) dışında, **discrete attribute inference** için de bir uzantı sunulmaktadır. Bu uzantı, farklı öznitelik hipotezleri altında gradient tepkilerini karşılaştırarak çalışır.\n*   Önerilen saldırılar **model-agnostic**'tir, yani gradient tabanlı herhangi bir model ve hem sınıflandırma hem de regresyon ayarları için uygulanabilir.\n\n### Sonuçlar:\n\n*   CIFAR-100 ve Purchase100 veri setleri üzerinde yapılan üye olma çıkarımı değerlendirmeleri, saldırının **strong attack performance** gösterdiğini ortaya koymuştur.\n*   Kullanılan hesaplama ve bellek yükü, literatürdeki başka bir saldırıya kıyasla **comparable** bulunmuştur.\n*   Araştırma bulguları, **multi-round federated learning**'in çıkarım saldırılarına karşı savunmasızlığı artırabileceğini vurgulamaktadır.\n*   **Aggregators**, **data owners**'a göre daha büyük bir tehdit oluşturmaktadır.\n*   Saldırı performansı, eğitim veri setinin doğasından önemli ölçüde etkilenir; **richer, high-dimensional data**, daha basit tablo verilerine göre daha güçlü bilgi sızıntısına yol açmaktadır."
    }
  },
  {
    "id": "2512.15081v1",
    "title": "Quantifying Return on Security Controls in LLM Systems",
    "authors": [
      "Richard Helder Moulton",
      "Austin O'Brien",
      "John D. Hastings"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2512.15081v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15081v1",
    "content": {
      "en": "Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).",
      "tr": "## Özet\n\nBu akademik makale, büyük dil modelleri (LLM) kullanan sistemlerde güvenlik kontrollerinin etkisini nicelleştirmek için bir çerçeve ve tekrarlanabilir bir metodoloji sunmaktadır. Makalenin temel katkısı, LLM sistemlerinde uygulanan güvenlik önlemlerinin maliyet-etkinliğini değerlendirmeye olanak tanıyan bir yöntem geliştirmektir.\n\n### Ana Katkılar ve Metodoloji\n\n*   **Nicel Karar Destek Çerçevesi:** Makale, LLM'lerde güvenlik kontrollerinin seçimi ve değerlendirilmesi için karar odaklı bir çerçeve sunar. Bu çerçeve, saldırıların sonuçlarını finansal risk tahminlerine ve **Return on Control (RoC)** metriklerine dönüştürerek, farklı güvenlik katmanlarının parasal bazda karşılaştırılmasını sağlar.\n*   **Tekrarlanabilir Metodoloji:** Çalışma, **residual risk**'i nicelleştiren ve **adversarial probe** sonuçlarını finansal risk tahminlerine çeviren bir metodoloji ortaya koyar.\n*   **Uygulama ve Saldırı Sınıfları:** Makalede, sentetik kişisel olarak tanımlanabilir bilgileri (PII) içeren bir veri kümesi üzerinde çalışan bir **retrieval-augmented generation (RAG)** servisi (DeepSeek-R1 modeli kullanılarak) örneklendirilmiştir. Bu sistem, **Garak** aracı kullanılarak beş farklı güvenlik açığı sınıfında otomatik saldırılara maruz bırakılmıştır:\n    *   PII leakage\n    *   latent context injection\n    *   prompt injection\n    *   adversarial attack generation\n    *   divergence\n*   **Risk ve Kayıp Tahmini:** Her bir (saldırı, kontrol) çifti için saldırı başarı olasılıkları **Laplace's Rule of Succession** kullanılarak tahmin edilmiştir. Bu olasılıklar, 10.000 çalıştırmalı **Monte Carlo simulations** ile birleştirilmiştir. Simülasyonlarda, kamuya açık veri ihlali maliyeti verilerinden kalibre edilen kayıp üçgeni dağılımları kullanılarak **loss exceedance curves** ve beklenen kayıplar (expected losses) üretilmiştir.\n*   **Karşılaştırma:** Üç yaygın güvenlik önlemi (mitigation) değerlendirilmiştir:\n    *   attribute-based access control (ABAC)\n    *   named entity recognition (NER) redaction (Microsoft Presidio kullanılarak)\n    *   NeMo Guardrails\n    Bu önlemler, baseline RAG konfigürasyonu ile karşılaştırılmıştır.\n\n### Sonuçlar\n\n*   **Baseline Sistem Performansı:** Baseline RAG sistemi, yüksek saldırı başarı oranları göstermiştir (PII, latent injection ve prompt injection için >= 0.98). Bu durum, toplam simüle edilmiş beklenen kaybın senaryo başına yaklaşık **$313k** olmasına neden olmuştur.\n*   **ABAC Etkinliği:** ABAC, PII ve prompt ile ilgili saldırıların başarı olasılıklarını sıfıra yaklaştırarak toplam beklenen kaybı yaklaşık **%94** oranında azaltmıştır. Bu önlem, **9.83** gibi yüksek bir **RoC** değeri elde etmiştir.\n*   **NER Redaction Etkinliği:** NER redaction, PII sızıntısını tamamen ortadan kaldırmış ve **5.97** **RoC** değerine ulaşmıştır.\n*   **NeMo Guardrails Performansı:** NeMo Guardrails ise yalnızca marjinal bir fayda sağlamış ve **0.05** **RoC** değeri ile sınırlı bir etkinlik göstermiştir.\n\nBu çalışma, LLM güvenliğine yönelik yatırım kararları için nicel bir temel sunmaktadır."
    }
  },
  {
    "id": "2512.15003v1",
    "title": "SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports",
    "authors": [
      "Sogol Masoumzadeh",
      "Yufei Li",
      "Shane McIntosh",
      "Dániel Varró",
      "Lili Wei"
    ],
    "published_date": "2025-12-17",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2512.15003v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15003v1",
    "content": {
      "en": "Monitoring issue tracker submissions is a crucial software maintenance activity. A key goal is the prioritization of high risk, security-related bugs. If such bugs can be recognized early, the risk of propagation to dependent products and endangerment of stakeholder benefits can be mitigated. To assist triage engineers with this task, several automatic detection techniques, from Machine Learning (ML) models to prompting Large Language Models (LLMs), have been proposed. Although promising to some extent, prior techniques often memorize lexical cues as decision shortcuts, yielding low detection rate specifically for more complex submissions. As such, these classifiers do not yet reach the practical expectations of a real-time detector of security-related issues. To address these limitations, we propose SEBERTIS, a framework to train Deep Neural Networks (DNNs) as classifiers independent of lexical cues, so that they can confidently detect fully unseen security-related issues. SEBERTIS capitalizes on fine-tuning bidirectional transformer architectures as Masked Language Models (MLMs) on a series of semantically equivalent vocabulary to prediction labels (which we call Semantic Surrogates) when they have been replaced with a mask. Our SEBERTIS-trained classifier achieves a 0.9880 F1-score in detecting security-related issues of a curated corpus of 10,000 GitHub issue reports, substantially outperforming state-of-the-art issue classifiers, with 14.44%-96.98%, 15.40%-93.07%, and 14.90%-94.72% higher detection precision, recall, and F1-score over ML-based baselines. Our classifier also substantially surpasses LLM baselines, with an improvement of 23.20%-63.71%, 36.68%-85.63%, and 39.49%-74.53% for precision, recall, and F1-score.",
      "tr": "## Özet\n\nBu makale, yazılım bakım süreçlerinde güvenlik odaklı issue report'ları otomatik olarak tespit etmek ve sınıflandırmak için SEBERTIS adında yeni bir framework sunmaktadır. Makalenin ana katkısı, mevcut yaklaşımların karşılaştığı \"lexical cues\" bağımlılığını aşarak, daha karmaşık ve daha önce görülmemiş güvenlik sorunlarını tespit edebilen derin sinir ağları (DNNs) tabanlı bir sınıflandırıcı geliştirmektir.\n\n### Metodoloji\n\n*   SEBERTIS, **bidirectional transformer architectures**'ın Masked Language Models (MLMs) olarak fine-tuning edilmesi üzerine kuruludur.\n*   Bu fine-tuning süreci, tahmin etiketlerinin (prediction labels) yerine maskelerin kullanıldığı ve anlamca eşdeğer bir kelime dağarcığı (semantic surrogates) serisi üzerinde gerçekleştirilir.\n*   Bu yaklaşım, modelin kelime bazlı ezberleme yerine daha derin anlamsal anlayış geliştirmesini hedefler.\n\n### Sonuçlar\n\n*   SEBERTIS ile eğitilmiş sınıflandırıcı, 10.000 GitHub issue report'undan oluşan kürate edilmiş bir corpus üzerinde güvenlik odaklı issue report'ları tespit etmede **0.9880 F1-score** elde etmiştir.\n*   Bu sonuç, mevcut state-of-the-art issue sınıflandırıcılarını önemli ölçüde geride bırakmaktadır:\n    *   ML-based baselines'a kıyasla **%14.44-%96.98** daha yüksek precision, **%15.40-%93.07** daha yüksek recall ve **%14.90-%94.72** daha yüksek F1-score.\n*   Ayrıca, sınıflandırıcı LLM baselines'larını da önemli ölçüde geride bırakarak, precision, recall ve F1-score'da sırasıyla **%23.20-%63.71**, **%36.68-%85.63** ve **%39.49-%74.53** oranlarında iyileşme sağlamıştır."
    }
  },
  {
    "id": "2512.15803v1",
    "title": "An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative",
    "authors": [
      "Apurva Shet",
      "Izzat Alsmadi"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.15803v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15803v1",
    "content": {
      "en": "Zero-day vulnerabilities represent some of the most critical threats in cybersecurity, as they correspond to previously unknown flaws in software or hardware that are actively exploited before vendors can develop and deploy patches. During this exposure window, affected systems remain defenseless, making zero-day attacks particularly damaging and difficult to mitigate. This study analyzes the Zero Day Initiative (ZDI) vulnerability disclosures reported between January and April 2024, Cole [2025] comprising a total of 415 vulnerabilities. The dataset includes vulnerability identifiers, Common Vulnerability Scoring System (CVSS) v3.0 scores, publication dates, and short textual descriptions. The primary objectives of this work are to identify trends in zero-day vulnerability disclosures, examine severity distributions across vendors, and investigate which vulnerability characteristics are most indicative of high severity. In addition, this study explores predictive modeling approaches for severity classification, comparing classical machine learning techniques with deep learning models using both structured metadata and unstructured textual descriptions. The findings aim to support improved patch prioritization strategies, more effective vulnerability management, and enhanced organizational preparedness against emerging zero-day threats.",
      "tr": "## Özet\n\nBu çalışma, **Zero Day Initiative (ZDI)** tarafından açıklanan sıfır gün güvenlik açıklarının ampirik bir analizini sunmaktadır.\n\n*   **Ana Katkı:** Çalışmanın temel katkısı, 2024'ün ilk dört ayında ZDI tarafından bildirilen 415 sıfır gün güvenlik açığını analiz ederek, bu açıkların özelliklerini, önem düzeylerini ve satıcılar arasındaki dağılımını ortaya koymaktır. Ayrıca, **predictive modeling** yaklaşımları kullanarak gelecekteki sıfır gün güvenlik açıklarının ciddiyetini sınıflandırmayı hedeflemektedir.\n\n*   **Metodoloji:**\n    *   Analiz, Ocak-Nisan 2024 döneminde ZDI tarafından disclosed edilen 415 güvenlik açığının yer aldığı bir veri kümesi üzerinde gerçekleştirilmiştir.\n    *   Veri kümesi, **vulnerability identifiers**, **Common Vulnerability Scoring System (CVSS) v3.0 scores**, publication dates ve kısa metinsel açıklamalar içermektedir.\n    *   Sıfır gün güvenlik açıklarındaki trendler belirlenmiş, satıcılar arasındaki ciddiyet dağılımları incelenmiş ve yüksek ciddiyetin en belirgin göstergeleri araştırılmıştır.\n    *   Ciddiyet sınıflandırması için **classical machine learning techniques** ve **deep learning models** karşılaştırılmıştır. Bu modeller, hem structured metadata hem de unstructured textual descriptions kullanılarak eğitilmiştir.\n\n*   **Sonuçlar:**\n    *   Analiz, sıfır gün güvenlik açığı açıklamalarındaki mevcut trendlere ışık tutmaktadır.\n    *   Farklı satıcılar için ciddiyet dağılımları incelenmiştir.\n    *   Çalışma, yüksek ciddiyete sahip güvenlik açıklarını belirlemede hangi **vulnerability characteristics**'lerin en etkili olduğunu ortaya koymaktadır.\n    *   **Predictive modeling** sonuçları, ciddiyet sınıflandırması konusunda umut verici yaklaşımlar sunarak, yama önceliklendirme stratejilerinin iyileştirilmesine, daha etkili **vulnerability management**'a ve ortaya çıkan sıfır gün tehditlerine karşı daha iyi organizasyonel hazırlığa katkı sağlamayı amaçlamaktadır."
    }
  },
  {
    "id": "2512.14958v1",
    "title": "Intrusion Detection in Internet of Vehicles Using Machine Learning",
    "authors": [
      "Hop Le",
      "Izzat Alsmadi"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.14958v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14958v1",
    "content": {
      "en": "The Internet of Vehicles (IoV) has evolved modern transportation through enhanced connectivity and intelligent systems. However, this increased connectivity introduces critical vulnerabilities, making vehicles susceptible to cyber-attacks such Denial-ofService (DoS) and message spoofing. This project aims to develop a machine learning-based intrusion detection system to classify malicious Controller Area network (CAN) bus traffic using the CiCIoV2024 benchmark dataset. We analyzed various attack patterns including DoS and spoofing attacks targeting critical vehicle parameters such as Spoofing-GAS - gas pedal position, Spoofing-RPM, Spoofing-Speed, and Spoofing-Steering\\_Wheel. Our initial findings confirm a multi-class classification problem with a clear structural difference between attack types and benign data, providing a strong foundation for machine learning models.",
      "tr": "## Özet\n\nBu makale, **Internet of Vehicles (IoV)** ortamlarında siber saldırılara karşı savunmayı güçlendirmek amacıyla makine öğrenmesi tabanlı bir **intrusion detection system (IDS)** geliştirilmesini konu almaktadır.\n\n*   **Ana Katkı:** Çalışmanın ana katkısı, IoV'nin artan bağlantı özelliğinin getirdiği güvenlik zafiyetlerini ele almak ve özellikle **Denial-of-Service (DoS)** ve **message spoofing** gibi saldırı türlerini tespit edebilen bir makine öğrenmesi modeli önermektir.\n*   **Metodoloji:** Yöntem, **CiCIoV2024** benchmark veri setini kullanarak, araçların kritik parametrelerine (örn. **Spoofing-GAS**, **Spoofing-RPM**, **Spoofing-Speed**, **Spoofing-Steering_Wheel**) yönelik saldırı örüntülerinin analizini içermektedir. Araştırmacılar, saldırı türleri ile normal (benign) veriler arasındaki **structural difference** üzerinden makine öğrenmesi modelleri için uygun bir temel oluşturmayı hedeflemişlerdir.\n*   **Sonuçlar:** İlk bulgular, bu problemin **multi-class classification** olarak ele alınabileceğini ve saldırı türleri ile normal trafik arasında belirgin yapısal farklılıklar bulunduğunu teyit etmektedir. Bu durum, makine öğrenmesi modellerinin etkin bir şekilde eğitilip uygulanması için güçlü bir zemin sunmaktadır."
    }
  },
  {
    "id": "2512.14935v1",
    "title": "Cloud Security Leveraging AI: A Fusion-Based AISOC for Malware and Log Behaviour Detection",
    "authors": [
      "Nnamdi Philip Okonkwo",
      "Lubna Luxmi Dhirani"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.14935v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14935v1",
    "content": {
      "en": "Cloud Security Operations Center (SOC) enable cloud governance, risk and compliance by providing insights visibility and control. Cloud SOC triages high-volume, heterogeneous telemetry from elastic, short-lived resources while staying within tight budgets. In this research, we implement an AI-Augmented Security Operations Center (AISOC) on AWS that combines cloud-native instrumentation with ML-based detection. The architecture uses three Amazon EC2 instances: Attacker, Defender, and Monitoring. We simulate a reverse-shell intrusion with Metasploit, and Filebeat forwards Defender logs to an Elasticsearch and Kibana stack for analysis. We train two classifiers, a malware detector built on a public dataset and a log-anomaly detector trained on synthetically augmented logs that include adversarial variants. We calibrate and fuse the scores to produce multi-modal threat intelligence and triage activity into NORMAL, SUSPICIOUS, and HIGH\\_CONFIDENCE\\_ATTACK. On held-out tests the fusion achieves strong macro-F1 (up to 1.00) under controlled conditions, though performance will vary in noisier and more diverse environments. These results indicate that simple, calibrated fusion can enhance cloud SOC capabilities in constrained, cost-sensitive setups.",
      "tr": "## Özet\n\nBu akademik makale, **Cloud Security Operations Center (SOC)**'ların yeteneklerini yapay zeka (AI) entegrasyonu ile geliştirmeyi amaçlamaktadır. Araştırmanın ana katkısı, maliyet-etkin bir **AI-Augmented Security Operations Center (AISOC)** mimarisinin geliştirilmesidir. Bu mimari, bulut-native enstrümantasyon ile makine öğrenimi (ML) tabanlı tespit yöntemlerini birleştirmektedir.\n\n### Metodoloji:\n\n*   **Mimari:** Araştırmada, **AWS** üzerinde üç adet **Amazon EC2** instance'ı (Attacker, Defender, Monitoring) kullanılarak bir AISOC sistemi kurulmuştur.\n*   **Simülasyon:** Bir **reverse-shell** saldırısı **Metasploit** ile simüle edilmiştir.\n*   **Veri Toplama ve Analiz:** **Filebeat**, Defender loglarını bir **Elasticsearch** ve **Kibana** yığınına iletmiştir.\n*   **Sınıflandırıcılar:** İki ana sınıflandırıcı geliştirilmiştir:\n    *   **Malware Detector:** Genel bir dataset üzerinde eğitilmiş bir zararlı yazılım tespit sistemi.\n    *   **Log-Anomaly Detector:** **Synthetically augmented** edilmiş, düşmanca (adversarial) varyantları içeren loglar üzerinde eğitilmiş bir anomali tespit sistemi.\n*   **Skor Füzyonu (Fusion):** Tespit edilen zararlı yazılımların ve log anomalilerinin skorları kalibre edilerek birleştirilmiştir (fusion). Bu füzyon, çok modlu tehdit istihbaratı üretmek ve aktiviteyi **NORMAL**, **SUSPICIOUS** ve **HIGH_CONFIDENCE_ATTACK** olarak sınıflandırmak için kullanılmıştır.\n\n### Sonuçlar:\n\n*   Kontrollü koşullar altında yapılan testlerde, geliştirilen skor füzyon yöntemi yüksek **macro-F1** skorları (1.00'a kadar) elde etmiştir.\n*   Bu sonuçlar, basit ve kalibre edilmiş bir füzyonun, kısıtlı ve maliyet-hassas bulut SOC ortamlarında yetenekleri önemli ölçüde artırabileceğini göstermektedir.\n*   Makale, daha gürültülü ve çeşitli ortamlarda performansın değişebileceğine dikkat çekmektedir."
    }
  },
  {
    "id": "2512.15799v1",
    "title": "Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India",
    "authors": [
      "Sahibpreet Singh",
      "Shikha Dhiman"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "link": "http://arxiv.org/abs/2512.15799v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15799v1",
    "content": {
      "en": "The integration of generative Artificial Intelligence into the digital ecosystem necessitates a critical re-evaluation of Indian criminal jurisprudence regarding computational forensics integrity. While algorithmic efficiency enhances evidence extraction, a research gap exists regarding the Digital Personal Data Protection Act, 2023's compatibility with adversarial AI threats, specifically anti-forensics and deepfakes. This study scrutinizes the AI \"dual-use\" dilemma, functioning as both a cyber-threat vector and forensic automation mechanism, to delineate privacy boundaries in high-stakes investigations. Employing a doctrinal legal methodology, the research synthesizes statutory analysis of the DPDP Act with global ethical frameworks (IEEE, EU) to evaluate regulatory efficacy. Preliminary results indicate that while Machine Learning offers high accuracy in pattern recognition, it introduces vulnerabilities regarding data poisoning and algorithmic bias. Findings highlight a critical tension between the Act's data minimization principles and forensic data retention requirements. Furthermore, the paper identifies that existing legal definitions inadequately encompass AI-driven \"tool crimes\" and \"target crimes.\" Consequently, the research proposes a \"human-centric\" forensic model prioritizing explainable AI (XAI) to ensure evidence admissibility. These implications suggest that synchronizing Indian privacy statutes with international forensic standards is imperative to mitigate synthetic media risks, establishing a roadmap for future legislative amendments and technical standardization.",
      "tr": "## Özet\n\nBu akademik makale, Hindistan'da yapay zeka (AI) çağında siber suçlar ve bilgisayar adli bilimi (computer forensics) konularını incelemektedir.\n\n*   **Ana Katkı:** Makale, üretken yapay zekanın (generative AI) Hindistan'daki mevcut yasal çerçeveye, özellikle de Digital Personal Data Protection Act, 2023 (DPDP Act) ile bilgisayar adli bilimlerinin bütünlüğüne etkilerini araştırmaktadır. Mevcut yasal tanımların AI destekli suçları yeterince kapsamadığını vurgulamaktadır.\n\n*   **Metodoloji:** Çalışma, **doctrinal legal methodology** kullanmaktadır. Bu kapsamda, DPDP Act'in yasal analizleri küresel etik çerçeveler (IEEE, EU) ile sentezlenerek düzenleyici etkinliği değerlendirilmektedir.\n\n*   **Sonuçlar:**\n    *   Makine öğrenmesi (Machine Learning) örüntü tanımada (pattern recognition) yüksek doğruluk sağlamasına rağmen, **data poisoning** ve **algorithmic bias** gibi zafiyetler getirmektedir.\n    *   DPDP Act'in veri minimizasyon ilkeleri ile adli veri saklama gereksinimleri arasında önemli bir gerilim bulunmaktadır.\n    *   Mevcut yasal tanımlar, AI güdümlü \"tool crimes\" ve \"target crimes\"ı yetersiz bir şekilde kapsamaktadır.\n    *   Bu nedenle makale, delil kabul edilebilirliğini (evidence admissibility) sağlamak için **explainable AI (XAI)** öncelikli, \"human-centric\" bir adli bilişim modeli önermektedir.\n    *   Makale, Hindistan'ın gizlilik yasalarının uluslararası adli bilişim standartlarıyla senkronize edilmesinin, sentetik medya (synthetic media) risklerini azaltmak için zorunlu olduğunu belirtmektedir. Bu durum, gelecekteki yasal değişiklikler ve teknik standartlar için bir yol haritası sunmaktadır."
    }
  },
  {
    "id": "2512.14860v1",
    "title": "Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks",
    "authors": [
      "Viet K. Nguyen",
      "Mohammad I. Husain"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.14860v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14860v1",
    "content": {
      "en": "Agentic AI introduces security vulnerabilities that traditional LLM safeguards fail to address. Although recent work by Unit 42 at Palo Alto Networks demonstrated that ChatGPT-4o successfully executes attacks as an agent that it refuses in chat mode, there is no comparative analysis in multiple models and frameworks. We conducted the first systematic penetration testing and comparative evaluation of agentic AI systems, testing five prominent models (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, Grok 2, and Nova Pro) across two agentic AI frameworks (AutoGen and CrewAI) using a seven-agent architecture that mimics the functionality of a university information management system and 13 distinct attack scenarios that span prompt injection, Server Side Request Forgery (SSRF), SQL injection, and tool misuse. Our 130 total test cases reveal significant security disparities: AutoGen demonstrates a 52.3% refusal rate versus CrewAI's 30.8%, while model performance ranges from Nova Pro's 46.2% to Claude and Grok 2's 38.5%. Most critically, Grok 2 on CrewAI rejected only 2 of 13 attacks (15.4% refusal rate), and the overall refusal rate of 41.5% across all configurations indicates that more than half of malicious prompts succeeded despite enterprise-grade safety mechanisms. We identify six distinct defensive behavior patterns including a novel \"hallucinated compliance\" strategy where models fabricate outputs rather than executing or refusing attacks, and provide actionable recommendations for secure agent deployment. Complete attack prompts are also included in the Appendix to enable reproducibility.",
      "tr": "## Özet\n\nBu akademik makale, **agentic AI sistemlerinin güvenlik açıklarını** incelemektedir. Geleneksel Large Language Model (LLM) güvenlik önlemlerinin agentic AI'nin getirdiği yeni tehditlere karşı yetersiz kaldığı vurgulanmaktadır. Yapılan bu çalışma, agentic AI sistemlerinin farklı modeller ve framework'ler üzerindeki güvenlik performansını karşılaştırmalı olarak analiz eden ilk sistematik **penetration testing** çalışmasıdır.\n\n*   **Metodoloji:**\n    *   Beş farklı prominent AI modeli kullanılmıştır: Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, Grok 2 ve Nova Pro.\n    *   İki agentic AI framework'ü değerlendirilmiştir: AutoGen ve CrewAI.\n    *   Bir üniversite bilgi yönetim sisteminin işlevselliğini taklit eden **yedi-agent mimarisi** oluşturulmuştur.\n    *   **Prompt injection, Server Side Request Forgery (SSRF), SQL injection ve tool misuse** gibi 13 farklı saldırı senaryosu uygulanmıştır.\n    *   Toplamda 130 test durumu incelenmiştir.\n\n*   **Ana Katkılar ve Sonuçlar:**\n    *   **Güvenlikte Anlamlı Farklılıklar:** AutoGen, CrewAI'ye (30.8%) kıyasla daha yüksek bir **refusal rate** (%52.3) sergilemiştir. Model performansları ise Nova Pro (%46.2) ile Claude ve Grok 2 (%38.5) arasında farklılık göstermiştir.\n    *   **Önemli Zafiyetler:** En dikkat çekici bulgu, Grok 2'nin CrewAI üzerinde yalnızca 2 saldırıyı reddetmesi (%15.4 refusal rate) ve genel olarak tüm konfigürasyonlarda %41.5'lik bir refusal rate olmasıdır. Bu durum, kurumsal düzeyde güvenlik mekanizmalarına rağmen **yarısından fazlasının kötü niyetli prompt'lara başarıyla yanıt verdiğini** göstermektedir.\n    *   **Yeni Savunma Davranışı:** Altı farklı savunma davranış paterni tanımlanmış, bunlardan biri de modellerin saldırıları reddetmek veya uygulamak yerine **hallucinated compliance** (halüsinasyonlu uyum) stratejisi ile sahte çıktılar üretmesidir.\n    *   **Eyleme Geçirilebilir Öneriler:** Güvenli agent dağıtımı için eyleme geçirilebilir tavsiyeler sunulmuştur. Makalenin ekinde, tekrarlanabilirliği sağlamak amacıyla saldırı prompt'ları da yer almaktadır."
    }
  },
  {
    "id": "2512.14846v1",
    "title": "MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber",
    "authors": [
      "Arth Bhardwaj",
      "Sia Godika",
      "Yuvam Loonker"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.14846v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14846v1",
    "content": {
      "en": "Traditional, centralized security tools often miss adaptive, multi-vector attacks. We present the Multi-Agent LLM Cyber Defense Framework (MALCDF), a practical setup where four large language model (LLM) agents-Detection, Intelligence, Response, and Analysis-work together in real time. Agents communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages, and produce audit-friendly outputs (e.g., MITRE ATT&CK mappings).   For evaluation, we keep the test simple and consistent: all reported metrics come from the same 50-record live stream derived from the CICIDS2017 feature schema. CICIDS2017 is used for configuration (fields/schema) and to train a practical ML baseline. The ML-IDS baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the 50-record stream, with no overlap between training and test records.   In experiments, MALCDF reaches 90.0% detection accuracy, 85.7% F1-score, and 9.1% false-positive rate, with 6.8s average per-event latency. It outperforms the lightweight ML-IDS baseline and a single-LLM setup on accuracy while keeping end-to-end outputs consistent. Overall, this hands-on build suggests that coordinating simple LLM agents with secure, ontology-aligned messaging can improve practical, real-time cyber defense.",
      "tr": "## Özet\n\nBu makale, geleneksel merkezi güvenlik araçlarının adaptif ve çok vektörlü saldırıları tespit etmedeki yetersizliklerine çözüm sunan **MALCDF (Multi-Agent LLM Cyber Defense Framework)** adlı pratik bir dağıtık savunma çerçevesini tanıtmaktadır.\n\n### Ana Katkı:\n\n*   **Dağıtık ve Çoklu Ajan Yaklaşımı:** MALCDF, dört farklı Büyük Dil Modeli (LLM) ajanını (Detection, Intelligence, Response, ve Analysis) gerçek zamanlı olarak işbirliği içinde çalıştırarak siber savunma yeteneklerini artırmayı hedefler.\n*   **Güvenli ve Anlam Tabanlı İletişim:** Ajanlar arasındaki iletişim, şifrelenmiş ve ontoloji-aligned mesajlarla bir **Secure Communication Layer (SCL)** üzerinden gerçekleştirilir.\n*   **Denetlenebilir Çıktılar:** Sistem, **MITRE ATT&CK** eşleştirmeleri gibi denetlenebilir ve anlaşılır çıktı üretir.\n\n### Metodoloji:\n\n*   **Ajan Mimarisi:** Dört ana LLM ajanı:\n    *   **Detection Agent:** Saldırıları tespit eder.\n    *   **Intelligence Agent:** Tehdit istihbaratı toplar ve analiz eder.\n    *   **Response Agent:** Tespit edilen tehditlere müdahale önerileri sunar.\n    *   **Analysis Agent:** Saldırıların kök nedenlerini ve etkilerini analiz eder.\n*   **Değerlendirme Verisi:** Tüm metrikler, **CICIDS2017** özelliğine sahip 50 kayıtlık canlı bir veri akışından elde edilmiştir. Bu veri akışı, yapılandırma (alanlar/şema) ve bir ML baseline'ı eğitmek için kullanılmıştır.\n*   **ML Baseline:** Eğitim ve test verileri arasında örtüşme olmayan, **CICIDS2017** veri setinin bir alt kümesi üzerinde eğitilmiş bir **Lightweight Random Forest IDS (LRF-IDS)** kullanılmıştır.\n\n### Sonuçlar:\n\n*   MALCDF, %90.0 tespit doğruluğu (detection accuracy), %85.7 F1-score ve %9.1 yanlış pozitif oranı (false-positive rate) ile önemli başarı elde etmiştir.\n*   Olay başına ortalama gecikme (average per-event latency) 6.8 saniyedir.\n*   MALCDF, doğruluk açısından hem lightweight ML-IDS baseline'ını hem de tek bir LLM kurulumunu geride bırakırken, uçtan uca çıktı tutarlılığını korumuştur.\n*   Bu pratik uygulama, basit LLM ajanlarının güvenli ve anlam tabanlı mesajlaşma ile koordine edilmesinin, pratik ve gerçek zamanlı siber savunmayı iyileştirebileceğini göstermektedir."
    }
  },
  {
    "id": "2512.14448v1",
    "title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space",
    "authors": [
      "Xingfu Zhou",
      "Pengfei Wang"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.14448v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14448v1",
    "content": {
      "en": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.",
      "tr": "## Özet\n\nBu makale, dış kaynaklı veriye dayanan Büyük Dil Modeli (LLM) ajanlarının güvenliğini artırmaya yönelik önemli bir katkıda bulunmaktadır. Mevcut saldırılar genellikle bilginin içeriğini değiştirmeye veya doğrudan komut enjeksiyonuna odaklanırken, bu çalışma ajanın **reasoning style**'ını hedef alan yeni bir saldırı vektörünü ortaya koymaktadır.\n\n### Ana Katkı: Reasoning-Style Poisoning (RSP)\n\n*   LLM ajanlarının bilgi işleme biçimini manipüle eden **Reasoning-Style Poisoning (RSP)** paradigması sunulmuştur.\n*   Bu yaklaşım, ajanın neyi işlediği yerine nasıl işlediğini hedef almaktadır.\n\n### Metodoloji: Generative Style Injection (GSI) ve Reasoning Style Vector (RSV)\n\n*   **Generative Style Injection (GSI)** saldırı yöntemi ile, erişilen belgelerin temel gerçekleri değiştirilmeden veya açık tetikleyiciler kullanılmadan \"analysis paralysis\" veya \"cognitive haste\" gibi patolojik tonlara dönüştürülmesi gösterilmiştir.\n*   Bu stilistik değişimleri ölçmek için **Reasoning Style Vector (RSV)** metriği geliştirilmiştir. RSV; **Verification depth**, **Self-confidence** ve **Attention focus** gibi unsurları takip etmektedir.\n\n### Sonuçlar\n\n*   HotpotQA ve FEVER veri setleri üzerinde ReAct, Reflection ve Tree of Thoughts (ToT) mimarileri kullanılarak yapılan deneylerde, GSI saldırısının performansı önemli ölçüde düşürdüğü gözlemlenmiştir.\n*   Saldırı, **reasoning steps** sayısını 4.4 kata kadar artırabilmiş veya erken hatalara neden olmuştur.\n*   GSI, en gelişmiş **content filters**'ı başarıyla atlatabilmiştir.\n*   Bu çalışmanın bir sonucu olarak, **reasoning style**'ın ayırt edilebilir ve istismar edilebilir bir zafiyet olduğu kanıtlanmıştır. Bu durum, statik içerik analizinin ötesinde, sürece odaklanan savunma mekanizmalarına olan ihtiyacı vurgulamaktadır.\n*   Son olarak, RSV metriklerini gerçek zamanlı olarak hesaplayan ve değerler güvenlik eşiklerini aştığında uyarı veren hafif bir çalışma zamanı izleyicisi olan **RSP-M** önerilmiştir."
    }
  },
  {
    "id": "2512.14422v1",
    "title": "Hybrid Ensemble Method for Detecting Cyber-Attacks in Water Distribution Systems Using the BATADAL Dataset",
    "authors": [
      "Waqas Ahmed"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.14422v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14422v1",
    "content": {
      "en": "The cybersecurity of Industrial Control Systems that manage critical infrastructure such as Water Distribution Systems has become increasingly important as digital connectivity expands. BATADAL benchmark data is a good source of testing intrusion detection techniques, but it presents several important problems, such as imbalance in the number of classes, multivariate time dependence, and stealthy attacks. We consider a hybrid ensemble learning model that will enhance the detection ability of cyber-attacks in WDS by using the complementary capabilities of machine learning and deep learning models. Three base learners, namely, Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network, have been strictly compared and seven ensemble types using simple averaged and stacked learning with a logistic regression meta-learner. Random Forest analysis identified top predictors turned into temporal and statistical features, and Synthetic Minority Oversampling Technique (SMOTE) was used to overcome the class imbalance issue. The analyics indicates that the single Long Short-Term Memory network model is of poor performance (F1 = 0.000, AUC = 0.4460), but tree-based models, especially eXtreme Gradient Boosting, perform well (F1 = 0.7470, AUC=0.9684). The hybrid stacked ensemble of Random Forest , eXtreme Gradient Boosting , and Long Short-Term Memory network scored the highest, with the attack class of 0.7205 with an F1-score and a AUC of 0.9826 indicating that the heterogeneous stacking between model precision and generalization can work. The proposed framework establishes a robust and scalable solution for cyber-attack detection in time-dependent industrial systems, integrating temporal learning and ensemble diversity to support the secure operation of critical infrastructure.",
      "tr": "## Özet\n\nBu makale, Su Dağıtım Sistemleri (WDS) gibi kritik altyapıları yöneten Endüstriyel Kontrol Sistemlerinin (ICS) siber güvenlik açıklarını tespit etmek için hibrit bir ensemble öğrenme modeli önermektedir. Dijital bağlantının artmasıyla bu sistemlerin güvenliği giderek daha fazla önem kazanmaktadır.\n\n**Ana Katkı:**\n\n*   BATADAL veri seti, siber saldırı tespit tekniklerini test etmek için değerli bir kaynak olmasına rağmen, sınıf dengesizliği, çok değişkenli zaman bağımlılığı ve gizli saldırılar gibi önemli zorluklar sunmaktadır.\n*   Bu çalışma, makine öğrenmesi ve derin öğrenme modellerinin tamamlayıcı yeteneklerini kullanarak siber saldırı tespitini geliştirmeyi amaçlayan hibrit bir ensemble öğrenme modeli sunmaktadır.\n\n**Metodoloji:**\n\n*   Makalede üç temel öğrenici kullanılmıştır: **Random Forest**, **eXtreme Gradient Boosting** ve **Long Short-Term Memory (LSTM)** network.\n*   Basit ortalama alma ve **stacked learning** yöntemleriyle yedi farklı ensemble türü oluşturulmuş ve bu türler, bir **logistic regression meta-learner** kullanılarak değerlendirilmiştir.\n*   Sınıf dengesizliği sorununu gidermek için **Synthetic Minority Oversampling Technique (SMOTE)** kullanılmıştır.\n*   **Random Forest** analizi ile en önemli öngörücüler belirlenerek zamansal ve istatistiksel özelliklere dönüştürülmüştür.\n\n**Sonuçlar:**\n\n*   Tek başına kullanılan **Long Short-Term Memory (LSTM)** network modelinin performansı düşük bulunmuştur (F1 = 0.000, AUC = 0.4460).\n*   Ağaç tabanlı modeller, özellikle **eXtreme Gradient Boosting**, yüksek performans göstermiştir (F1 = 0.7470, AUC=0.9684).\n*   **Random Forest**, **eXtreme Gradient Boosting** ve **Long Short-Term Memory (LSTM)** network'ün hibrit **stacked ensemble** modeli en yüksek puanı elde etmiştir. Saldırı sınıfı için 0.7205 F1-score ve 0.9826 AUC değeri, model hassasiyeti ve genellemesi arasındaki heterojen yığının etkinliğini göstermektedir.\n*   Önerilen çerçeve, zaman bağımlı endüstriyel sistemlerde siber saldırı tespiti için zamansal öğrenme ve ensemble çeşitliliğini entegre eden sağlam ve ölçeklenebilir bir çözüm sunarak kritik altyapının güvenli işletimini desteklemektedir."
    }
  },
  {
    "id": "2512.14330v1",
    "title": "Criminal Liability in AI-Enabled Autonomous Vehicles: A Comparative Study",
    "authors": [
      "Sahibpreet Singh",
      "Manjit Singh"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.14330v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14330v1",
    "content": {
      "en": "AI revolutionizes transportation through autonomous vehicles (AVs) but introduces complex criminal liability issues regarding infractions. This study employs a comparative legal analysis of primary statutes, real-world liability claims, and academic literature across the US, Germany, UK, China, and India; jurisdictions selected for their technological advancement and contrasting regulatory approaches. The research examines the attribution of human error, AI moral agency, and the identification of primary offenders in AV incidents. Findings reveal fragmented regulatory landscapes: India and the US rely on loose networks of state laws, whereas the UK enacted the pioneering Automated and Electric Vehicles Act 2018. Germany enforces strict safety standards, distinguishing liability based on the vehicle's operating mode, while China similarly aims for a stringent liability regime. The study concludes that globally harmonized legal standards are essential to foster technological innovation while ensuring minimum risk and clear liability attribution.",
      "tr": "## Özet\n\nBu akademik makale, yapay zeka destekli otonom araçların (AVs) neden olduğu suçlarda sorumluluğun belirlenmesine odaklanmaktadır.\n\n*   **Ana Katkı:** Makale, otonom araçların karıştığı suçlarda hukuki sorumluluk atfetme konusundaki küresel yasal boşlukları ve zorlukları ortaya koymaktadır. Teknolojik ilerlemenin getirdiği bu yeni sorumluluk rejimlerinin, mevcut hukuki çerçevelerle uyumunu incelemektedir.\n\n*   **Metodoloji:** Çalışma, ABD, Almanya, Birleşik Krallık, Çin ve Hindistan gibi farklı teknolojik gelişmişlik düzeylerine ve karşıt düzenleyici yaklaşımlara sahip ülkelerdeki ilgili kanunları, gerçek dünya sorumluluk iddialarını ve akademik literatürü kapsayan karşılaştırmalı bir hukuki analiz yürütmüştür. İnsan hatası, AI moral agency ve AV olaylarındaki birincil faillerin belirlenmesi gibi konular araştırılmıştır.\n\n*   **Sonuçlar:** Bulgular, ülkeler arasında parçalanmış düzenleyici manzara olduğunu göstermektedir.\n    *   Hindistan ve ABD, eyalet yasalarının gevşek ağlarına dayanırken, Birleşik Krallık, öncü **Automated and Electric Vehicles Act 2018**'i yürürlüğe koymuştur.\n    *   Almanya, **strict safety standards** uygulayarak aracın operating mode'una göre sorumluluğu ayırmaktadır.\n    *   Çin de benzer şekilde bir **stringent liability regime** hedeflemektedir.\n    *   Çalışma, teknolojik yeniliği teşvik ederken minimum risk ve net sorumluluk atfetmeyi sağlamak için küresel olarak uy harmonized legal standards'ın elzem olduğu sonucuna varmaktadır."
    }
  },
  {
    "id": "2512.14233v1",
    "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design",
    "authors": [
      "Ruozhao Yang",
      "Mingfei Cheng",
      "Gelei Deng",
      "Tianwei Zhang",
      "Junjie Wang"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.14233v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14233v1",
    "content": {
      "en": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.",
      "tr": "## Özet\n\nBu makale, **PentestEval** adını verdiği, Büyük Dil Modellerinin (LLM) sızma testi (penetration testing) yeteneklerini değerlendirmek için ilk kapsamlı bir benchmark'ı sunmaktadır. Geleneksel sızma testlerinin el yapımı, uzmanlık gerektiren ve ölçeklenmesi zor süreçler olduğunu belirtmekte, LLM'lerin bu alanda otomasyon potansiyelini vurgulamaktadır. Ancak mevcut LLM tabanlı uygulamaların, görev ayrıştırması (task decomposition) ve alan adaptasyonu (domain adaptation) olmadan basit istemler (prompting) kullanması nedeniyle güvenirliğin düşük olduğunu ve modellerin farklı sızma testi aşamalarındaki yetenekleri hakkında sınırlı bilgi sağladığını ortaya koymaktadır.\n\n*   **Ana Katkı:** PentestEval, sızma testi süreçlerini **Information Collection**, **Weakness Gathering and Filtering**, **Attack Decision-Making**, ve **Exploit Generation and Revision** olmak üzere altı farklı aşamaya ayırarak LLM'leri değerlendirmek için tasarlanmış ilk modüler benchmark'ı sunar. Bu benchmark, 346 adet görevi kapsayan 12 gerçekçi, savunmasız senaryoda uzman anotasyonlu ground truth ve tamamen otomatik bir değerlendirme hattı (evaluation pipeline) içerir.\n*   **Metodoloji:** PentestEval, sızma testi sürecini aşamalara ayırarak her bir aşamayı ayrı ayrı ve bütünsel olarak değerlendirir. Bu modüler yaklaşım, LLM'lerin her aşamadaki performansını ve genel entegre yeteneklerini analiz etmeye olanak tanır.\n*   **Sonuçlar:** PentestEval kullanılarak yapılan değerlendirmeler, 9 yaygın LLM'nin sızma testi aşamalarında genel olarak zayıf performans gösterdiğini ve belirgin sınırlılıklara sahip olduğunu ortaya koymuştur. Uçtan uca (end-to-end) hatlarda başarı oranı sadece %31'dir. Mevcut LLM tabanlı sistemler olan PentestGPT, PentestAgent ve VulnBot da benzer sınırlılıklar sergilemiş, otonom ajanlar ise neredeyse tamamen başarısız olmuştur. Bulgular, otonom sızma testi için daha güçlü yapısal akıl yürütmeye (structured reasoning) ihtiyaç duyulduğunu ve modülerleştirmenin her bir aşamayı iyileştirerek genel performansı artırdığını göstermektedir. PentestEval, gelecekteki ince ayarlı, aşama düzeyinde değerlendirme araştırmaları için temel bir benchmark sağlayarak daha güvenilir LLM tabanlı otomasyonun yolunu açmaktadır."
    }
  },
  {
    "id": "2512.14767v1",
    "title": "Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation",
    "authors": [
      "Unai Laskurain",
      "Aitor Aguirre-Ortuzar",
      "Urko Zurutuza"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "link": "http://arxiv.org/abs/2512.14767v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14767v1",
    "content": {
      "en": "Federated Learning (FL) is an emerging machine learning paradigm that enables multiple parties to collaboratively train models without sharing raw data, ensuring data privacy. In Vertical FL (VFL), where each party holds different features for the same users, a key challenge is to evaluate the feature contribution of each party before any model is trained, particularly in the early stages when no model exists. To address this, the Shapley-CMI method was recently proposed as a model-free, information-theoretic approach to feature valuation using Conditional Mutual Information (CMI). However, its original formulation did not provide a practical implementation capable of computing the required permutations and intersections securely. This paper presents a novel privacy-preserving implementation of Shapley-CMI for VFL. Our system introduces a private set intersection (PSI) server that performs all necessary feature permutations and computes encrypted intersection sizes across discretized and encrypted ID groups, without the need for raw data exchange. Each party then uses these intersection results to compute Shapley-CMI values, computing the marginal utility of their features. Initial experiments confirm the correctness and privacy of the proposed system, demonstrating its viability for secure and efficient feature contribution estimation in VFL. This approach ensures data confidentiality, scales across multiple parties, and enables fair data valuation without requiring the sharing of raw data or training models.",
      "tr": "## Özet\n\nBu akademik makale, **Vertical Federated Learning (VFL)** ortamında veri gizliliğini koruyarak özellik (feature) katkısını değerlendirme problemine odaklanmaktadır. Makalenin temel katkısı, daha önce önerilen **Shapley-CMI** yönteminin pratik ve gizlilik koruyucu bir implementasyonunu sunmaktır.\n\n### Ana Katkılar ve Metodoloji\n\n*   **Problem Tanımı:** VFL'de, her tarafın aynı kullanıcılar için farklı özelliklere sahip olduğu durumlarda, herhangi bir model eğitilmeden önce, özellikle erken aşamalarda, her tarafın özellik katkısını model-serbest bir yaklaşımla değerlendirmek kritik bir zorluktur. Mevcut **Shapley-CMI** yöntemi, **Conditional Mutual Information (CMI)** kullanarak özellik katkısını model-serbest olarak değerlendirse de, gerekli permütasyonları ve kesişimleri (intersections) güvenli bir şekilde hesaplama konusunda pratik bir implementasyona sahip değildi.\n*   **Önerilen Çözüm:** Makale, **Shapley-CMI** için yeni bir gizlilik koruyucu VFL implementasyonunu tanıtmaktadır. Bu sistem, gerekli özellik permütasyonlarını gerçekleştiren ve diskretize edilmiş (discretized) ve şifrelenmiş (encrypted) kimlik grupları (ID groups) arasındaki şifrelenmiş kesişim boyutlarını hesaplayan bir **private set intersection (PSI)** sunucusu kullanır. Bu süreçte ham veri alışverişine gerek duyulmaz.\n*   **Mekanizma:** Her taraf, **PSI** sunucusundan aldığı kesişim sonuçlarını kullanarak kendi özelliklerinin marjinal faydasını (marginal utility) hesaplar ve böylece **Shapley-CMI** değerlerini elde eder.\n\n### Sonuçlar\n\n*   Yapılan ilk deneyler, önerilen sistemin **doğruluğunu ve gizliliğini** doğrulamıştır.\n*   Sistem, VFL'de güvenli ve verimli bir özellik katkısı tahmini için **uygulanabilirliğini** göstermiştir.\n*   Bu yaklaşım, veri gizliliğini sağlamakta, birden fazla tarafa ölçeklenebilmekte ve ham verilerin paylaşılmasına veya modellerin eğitilmesine gerek kalmadan adil veri değerlemesi (fair data valuation) sağlamaktadır."
    }
  },
  {
    "id": "2512.14166v1",
    "title": "IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol",
    "authors": [
      "Yunhao Yao",
      "Zhiqiang Wang",
      "Haoran Cheng",
      "Yihang Cheng",
      "Haohua Du"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.14166v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14166v1",
    "content": {
      "en": "The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.",
      "tr": "## Özet\n\nBu makale, LLM'lerin (Large Language Models) otonom ajanlar olarak gelişimiyle birlikte ortaya çıkan ve **Model Context Protocol (MCP)** aracılığıyla dış araçların keşfedilmesi ve çağrılmasını konu almaktadır. MCP mimarisi, *scalability*'yi artırmak için *reasoning engine*'i *tool execution*'dan ayırsa da, kullanıcı güvenilir sınırının dışında kalan üçüncü taraf MCP sunucularının, kullanıcıdan gizli olan araç etkileşim günlüklerini gözlemlemesiyle önemli bir *privacy surface* oluşturmaktadır.\n\n**Ana Katkı:**\n\n*   Makale, **Intent Inversion** adını verdiği yeni bir *privacy threat*'i tanımlar ve formalize eder. Bu tehdit, yarı-dürüst (semi-honest) bir MCP sunucusunun, yalnızca meşru araç çağrılarını analiz ederek kullanıcının özel *underlying intent*'ini yeniden yapılandırmaya çalışmasını içerir.\n\n**Metodoloji:**\n\n*   Bu savunmasızlığı sistematik olarak değerlendirmek için **IntentMiner** adında bir framework önerilmektedir.\n*   IntentMiner, **Hierarchical Information Isolation** ve **Three-Dimensional Semantic Analysis** tekniklerini kullanır.\n*   Bu analiz, *tool purpose*, *call statements* ve *returned results*'ı entegre ederek, adım düzeyinde (step level) kullanıcı niyetini doğru bir şekilde tahmin etmeyi amaçlar.\n\n**Sonuçlar:**\n\n*   Yapılan kapsamlı deneyler, IntentMiner'ın orijinal kullanıcı sorgularıyla yüksek düzeyde *semantic alignment* (yüzde 85'in üzerinde) elde ettiğini göstermiştir.\n*   Bu oran, temel yaklaşımlardan (baseline approaches) önemli ölçüde daha iyi bir performanstır.\n*   Sonuçlar, ayrıştırılmış ajan mimarilerindeki (decoupled agent architectures) doğasında var olan *privacy risks*'leri vurgulamakta ve masum görünen araç yürütme günlüklerinin (tool execution logs) kullanıcı sırlarını ifşa etmek için güçlü bir vektör olabileceğini ortaya koymaktadır."
    }
  },
  {
    "id": "2512.14130v1",
    "title": "UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis",
    "authors": [
      "Amirmohammad Pasdar",
      "Toby Murray",
      "Van-Thuan Pham"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.14130v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14130v1",
    "content": {
      "en": "We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.",
      "tr": "## Özet\n\nBu makale, UIXPOSE adlı, derlenmiş ve açık kaynaklı mobil uygulamalarda çalışabilen, kaynak-koddan bağımsız (source-code-agnostic) bir framework'ü tanıtmaktadır. UIXPOSE, mobil zararlı yazılım analizi için **Intention Behaviour Alignment (IBA)** prensibini uygulamakta ve kullanıcı arayüzünden çıkarılan niyetleri (UI-inferred intent) çalışma zamanı semantikleri (runtime semantics) ile hizalamaktadır.\n\n**Ana Katkısı:**\n\n*   Önceki çalışmaların statik (izin-merkezli, widget-seviyesi) veya kaba dinamik sinyalleri (endpoint'ler, kısmi kaynak kullanımı) izleme yaklaşımlarının eksiklerini gidermektedir. UIXPOSE, içerik ve bağlamı gözden kaçıran bu yöntemlerin aksine, daha derinlemesine bir analiz sunar.\n*   UIXPOSE, çalışma zamanında (at runtime) niyet ve davranış arasındaki tutarsızlıkları tespit ederek zararlı davranışları ortaya çıkarır ve davranışsal olarak zengin yolların keşfini vurgular.\n\n**Metodolojisi:**\n\n*   Her ekran için bir niyet vektörü (intent vector) **vision-language models** ve **knowledge structures** kullanılarak çıkarılır.\n*   Çözümlenmiş ağ yükleri (decoded network payloads), **heap/memory signals** ve kaynak kullanım izleri (resource utilisation traces) bir davranış vektörüne (behaviour vector) birleştirilir.\n*   Bu iki vektörün çalışma zamanında hesaplanan uyumu (alignment), misbehavior tespiti için kullanılır.\n\n**Sonuçları:**\n\n*   Üç gerçek dünya senaryosunda (case studies) yapılan deneylerde, UIXPOSE'un sadece metadata'ya dayalı temel analizlerin (metadata-only baselines) gözden kaçırdığı gizli veri sızdırma (covert exfiltration) ve arka planda gizlenmiş aktiviteleri (hidden background activity) ortaya çıkardığı gösterilmiştir.\n*   Bu bulgular, IBA'nın dinamik tespit yeteneklerini nasıl geliştirdiğini ispat etmektedir."
    }
  },
  {
    "id": "2512.14045v1",
    "title": "A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis",
    "authors": [
      "Omar Abusabha",
      "Jiyong Uhm",
      "Tamer Abuhmed",
      "Hyungjoon Koo"
    ],
    "published_date": "2025-12-16",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.PL"
    ],
    "link": "http://arxiv.org/abs/2512.14045v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14045v1",
    "content": {
      "en": "A function inlining optimization is a widely used transformation in modern compilers, which replaces a call site with the callee's body in need. While this transformation improves performance, it significantly impacts static features such as machine instructions and control flow graphs, which are crucial to binary analysis. Yet, despite its broad impact, the security impact of function inlining remains underexplored to date. In this paper, we present the first comprehensive study of function inlining through the lens of machine learning-based binary analysis. To this end, we dissect the inlining decision pipeline within the LLVM's cost model and explore the combinations of the compiler options that aggressively promote the function inlining ratio beyond standard optimization levels, which we term extreme inlining. We focus on five ML-assisted binary analysis tasks for security, using 20 unique models to systematically evaluate their robustness under extreme inlining scenarios. Our extensive experiments reveal several significant findings: i) function inlining, though a benign transformation in intent, can (in)directly affect ML model behaviors, being potentially exploited by evading discriminative or generative ML models; ii) ML models relying on static features can be highly sensitive to inlining; iii) subtle compiler settings can be leveraged to deliberately craft evasive binary variants; and iv) inlining ratios vary substantially across applications and build configurations, undermining assumptions of consistency in training and evaluation of ML models.",
      "tr": "## Özet\n\nBu makale, modern derleyicilerde yaygın olarak kullanılan bir performans iyileştirme tekniği olan **function inlining**'in, özellikle makine öğrenmesi (ML) tabanlı ikili analiz (binary analysis) üzerindeki etkilerini ve güvenlik çıkarımlarını derinlemesine incelemektedir.\n\n*   **Ana Katkı:**\n    *   Function inlining'in, ikili analiz için kritik olan **machine instructions** ve **control flow graphs** gibi statik özellikler üzerindeki önemli etkisine rağmen, güvenlik boyutu büyük ölçüde keşfedilmemiş bir alan olarak kalmıştır. Bu çalışma, function inlining'in ML tabanlı ikili analiz üzerindeki etkilerine dair ilk kapsamlı araştırmayı sunmaktadır.\n    *   LLVM'nin cost model'indeki inlining karar verme sürecini detaylandırarak, standart optimizasyon seviyelerinin ötesinde function inlining oranını agresif bir şekilde artıran **extreme inlining** senaryolarını araştırmaktadır.\n    *   Güvenlik odaklı beş farklı ML destekli ikili analiz görevi üzerinde, 20 benzersiz model kullanılarak extreme inlining senaryoları altında bu modellerin sağlamlığı (robustness) sistematik olarak değerlendirilmiştir.\n\n*   **Metodoloji:**\n    *   LLVM derleyicisinin cost model'indeki inlining karar verme mekanizması analiz edilmiştir.\n    *   Yüksek function inlining oranlarına neden olan derleyici seçenekleri (compiler options) kombinasyonları incelenerek **extreme inlining** elde edilmiştir.\n    *   Güvenlik odaklı beş farklı ML tabanlı ikili analiz görevi (örneğin, malware tespiti, güvenlik açığı analizi) için 20 farklı ML modeli kullanılarak deneyler yapılmıştır.\n    *   Modellerin performansı ve davranışı, extreme inlining uygulanan ikili dosyalar üzerinde test edilmiştir.\n\n*   **Sonuçlar:**\n    *   Function inlining, niyet olarak zararsız bir dönüşüm olsa da, ML modellerinin davranışlarını doğrudan veya dolaylı olarak etkileyebilir. Bu durum, **discriminative** veya **generative ML models**'den kaçınmak için potansiyel olarak istismar edilebilir.\n    *   Statik özelliklere dayanan ML modelleri, inlining'e karşı oldukça hassas olabilir.\n    *   İnce derleyici ayarları (subtle compiler settings), kaçamak ikili varyantlar (evasive binary variants) oluşturmak için kasıtlı olarak kullanılabilir.\n    *   Inlining oranları, uygulamalar ve derleme konfigürasyonları arasında önemli ölçüde farklılık göstererek, ML modellerinin eğitim ve değerlendirmesindeki tutarlılık varsayımlarını zayıflatmaktadır."
    }
  },
  {
    "id": "2512.13666v1",
    "title": "SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work",
    "authors": [
      "Weihang Cao",
      "Mustafa Doger",
      "Sennur Ulukus"
    ],
    "published_date": "2025-12-15",
    "tags": [
      "cs.CR",
      "cs.DC",
      "cs.IT",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.13666v1",
    "pdf_link": "https://arxiv.org/pdf/2512.13666v1",
    "content": {
      "en": "The security and decentralization of Proof-of-Work (PoW) have been well-tested in existing blockchain systems. However, its tremendous energy waste has raised concerns about sustainability. Proof-of-Useful-Work (PoUW) aims to redirect the meaningless computation to meaningful tasks such as solving machine learning (ML) problems, giving rise to the branch of Proof-of-Learning (PoL). While previous studies have proposed various PoLs, they all, to some degree, suffer from security, decentralization, or efficiency issues. In this paper, we propose a PoL framework that trains ML models efficiently while maintaining blockchain security in a fully distributed manner. We name the framework SEDULity, which stands for a Secure, Efficient, Distributed, and Useful Learning-based blockchain system. Specifically, we encode the template block into the training process and design a useful function that is difficult to solve but relatively easy to verify, as a substitute for the PoW puzzle. We show that our framework is distributed, secure, and efficiently trains ML models. We further demonstrate that the proposed PoL framework can be extended to other types of useful work and design an incentive mechanism to incentivize task verification. We show theoretically that a rational miner is incentivized to train fully honestly with well-designed system parameters. Finally, we present simulation results to demonstrate the performance of our framework and validate our analysis.",
      "tr": "## Özet\n\nBu makale, **SEDULity** adını verdiği yeni bir **Proof-of-Learning (PoL)** çerçevesi sunmaktadır. Mevcut **Proof-of-Work (PoW)** sistemlerinin enerji israfına yol açması sorunu ele alınarak, **Proof-of-Useful-Work (PoUW)** konsepti üzerinden anlamlı hesaplama görevlerine yönelme hedeflenmiştir. SEDULity, güvenli, verimli, dağıtık ve yararlı öğrenmeye dayalı bir blokzincir sistemi olmayı amaçlamaktadır.\n\n### Ana Katkılar:\n\n*   Dağıtık ve güvenli bir şekilde makine öğrenmesi (ML) modellerini verimli bir şekilde eğitebilen bir PoL çerçevesi önermektedir.\n*   PoW bulmacası yerine, çözülmesi zor ancak doğrulanması nispeten kolay olan bir **useful function** tasarlamıştır.\n*   Çerçevenin, farklı **useful work** türlerine genişletilebileceğini ve görev doğrulamasını teşvik etmek için bir **incentive mechanism** tasarladığını göstermiştir.\n\n### Metodoloji:\n\n*   **Template block**, eğitim sürecine entegre edilmiştir.\n*   PoW'un yerini alacak, zorlu ancak doğrulanması kolay bir **useful function** geliştirilmiştir.\n*   Teorik olarak, rasyonel bir madencinin iyi tasarlanmış sistem parametreleriyle tam dürüstçe eğitim yapmaya teşvik edildiği gösterilmiştir.\n\n### Sonuçlar:\n\n*   Geliştirilen SEDULity çerçevesinin dağıtık, güvenli ve ML modellerini verimli bir şekilde eğitebildiği gösterilmiştir.\n*   Simülasyon sonuçları, çerçevenin performansını sergilemek ve teorik analizleri doğrulamak için sunulmuştur."
    }
  },
  {
    "id": "2512.13501v1",
    "title": "Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS",
    "authors": [
      "Sabrine Ennaji",
      "Elhadj Benkhelifa",
      "Luigi Vincenzo Mancini"
    ],
    "published_date": "2025-12-15",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.13501v1",
    "pdf_link": "https://arxiv.org/pdf/2512.13501v1",
    "content": {
      "en": "Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.   To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.   We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.",
      "tr": "## Özet\n\nBu akademik makale, makine öğrenmesi tabanlı saldırı tespit sistemlerini (ML-Based IDS) hedef alan **black-box adversarial attacks**'a karşı davranış farkındalığı olan ve genelleştirilebilir bir savunma mekanizması sunmaktadır.\n\n*   **Ana Katkı:** Makalenin ana katkısı, gerçekçi black-box senaryoları için özel olarak tasarlanmış, hafif ve proaktif bir savunma mekanizması olan **Adaptive Feature Poisoning**'i önermektir. Bu mekanizma, saldırganın geri bildirim döngüsünü bozarak ancak tespit yeteneklerini etkilemeden saldırganların kullandığı özellikleri dinamik olarak bozmayı amaçlar.\n\n*   **Metodoloji:**\n    *   **Adaptive Feature Poisoning**, saldırıların sessiz ve sürekli olabileceği varsayımıyla hareket eder.\n    *   Gerçekleşen trafik profillerini analiz eder ve **change point detection** tekniklerini kullanarak anormallikleri tespit eder.\n    *   Gözlemlenen sapmalara dayanarak, saldırganın kullanma olasılığı yüksek olan özellikleri, **adaptive scaling** ile seçici olarak bozar.\n    *   Bu müdahaleler, saldırganın elde edeceği geri bildirimleri (örneğin, yanıt süreleri, kaynak kullanımı gibi **behavioral signals**) kirleterek saldırının etkinliğini azaltır.\n\n*   **Sonuçlar:**\n    *   **Adaptive Feature Poisoning**, farklı gerçekçi adversarial saldırı stratejilerine (sessiz probing, transferability based attacks, decision boundary based attacks) karşı başarıyla değerlendirilmiştir.\n    *   Mekanizmanın saldırganları şaşırtarak saldırı etkinliğini düşürdüğü ve tespit performansını koruduğu gösterilmiştir.\n    *   Genelleştirilebilir, saldırıdan bağımsız ve tespit edilemez bir savunma sunarak ML-Based IDS'de pratik ve sağlam bir adversarial dirence önemli bir adım olduğu vurgulanmıştır."
    }
  },
  {
    "id": "2512.13352v1",
    "title": "On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models",
    "authors": [
      "Ali Al Sahili",
      "Ali Chehab",
      "Razane Tajeddine"
    ],
    "published_date": "2025-12-15",
    "tags": [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.13352v1",
    "pdf_link": "https://arxiv.org/pdf/2512.13352v1",
    "content": {
      "en": "Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.",
      "tr": "## Özet\n\nBu çalışma, Büyük Dil Modellerinden (LLMs) eğitim verisi çıkarılması (training data extraction) sürecinde Üyelik Çıkarım Saldırılarının (Membership Inference Attacks - MIAs) etkinliğini incelemektedir.\n\n*   **Ana Katkı:** Makale, MIAs'ın eğitim verisi çıkarma süreçlerindeki pratik faydasını analiz ederek, bu saldırıların LLM'lerin eğitim verisi gizliliği üzerindeki riskini sistematik olarak değerlendirmektedir.\n*   **Metodoloji:**\n    *   Çeşitli MIA teknikleri, veri çıkarma (data extraction) işlem hattına entegre edilmiştir.\n    *   Bu entegre yapıda MIAs'ın performansı sistematik olarak karşılaştırılmıştır (benchmarking).\n    *   Performans, geleneksel MIA benchmark sonuçları ile karşılaştırılarak pratik kullanım senaryolarındaki etkinlikleri değerlendirilmiştir.\n*   **Sonuçlar:** LLM'lerin eğitim verilerini ezberleme eğiliminin bir sonucu olarak ortaya çıkan veri çıkarma ve MIAs arasındaki bağlantı vurgulanmaktadır. Çalışma, entegre veri çıkarma senaryolarında MIAs'ın etkinliğini ölçerek, bu saldırıların gerçek dünya uygulamalarındaki önemini ortaya koymaktadır."
    }
  },
  {
    "id": "2512.13325v1",
    "title": "Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models",
    "authors": [
      "Malte Hellmeier"
    ],
    "published_date": "2025-12-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.13325v1",
    "pdf_link": "https://arxiv.org/pdf/2512.13325v1",
    "content": {
      "en": "Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.",
      "tr": "## Özet\n\nBu makale, **Unicode text watermarking** yöntemlerinin **Large Language Models (LLMs)** karşısındaki güvenlik ve saptanabilirlik analizini incelemektedir. Dijital metinlerin LLM'ler tarafından eğitilmesi veya LLM üretimi çıktıların insan yazısı metinlerden ayırt edilmesindeki artan endişeler göz önüne alındığında, veri kontrolünü kaybetme korkusu dijital su işaretlemenin önemini vurgulamaktadır.\n\n### Ana Katkı ve Metodoloji\n\n*   Çalışmanın ana katkısı, mevcut dijital metin su işaretleme yöntemlerinin LLM'ler tarafından güvenli ve saptanamaz olup olmadığını analiz etmektir.\n*   Bu amaçla, **üç kontrollü deneyden oluşan bir test ortamında** on adet mevcut **Unicode text watermarking** yöntemi uygulanmıştır.\n*   Uygulanan yöntemler, **GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4 ve Gemini 2.5 Pro** olmak üzere altı farklı büyük dil modeli üzerinde analiz edilmiştir.\n\n### Sonuçlar\n\n*   Deney bulguları, özellikle **en son akıl yürütme modellerinin**, su işaretlenmiş metinleri saptayabildiğini göstermiştir.\n*   Bununla birlikte, tüm modellerin su işaretini **çıkaramaması** dikkat çekicidir. Bu durum, su işaretinin çıkarılması için **source code** gibi uygulama detaylarının sağlanması gerekliliğini ortaya koymaktadır.\n*   Çalışma, güvenlik araştırmacıları ve uygulayıcılar için çıkarımları tartışmakta ve güvenlik endişelerini gidermeye yönelik gelecek araştırma fırsatlarını ana hatlarıyla belirtmektedir."
    }
  },
  {
    "id": "2512.13207v2",
    "title": "Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting",
    "authors": [
      "Karina Chichifoi",
      "Fabio Merizzi",
      "Michele Colajanni"
    ],
    "published_date": "2025-12-15",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.13207v2",
    "pdf_link": "https://arxiv.org/pdf/2512.13207v2",
    "content": {
      "en": "Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13% degradation) but fails against patch attacks (281-603% amplification), exposing limitations of outlier-based defenses for spatially correlated data.",
      "tr": "## Özet\n\nBu akademik makale, **Federated Learning (FL)** üzerinde **Adversarial Attacks**'ın sıcaklık tahmini performansına etkilerini incelemektedir. Makalenin ana katkısı, coğrafi olarak dağıtılmış veri kaynaklarına dayalı sıcaklık tahmin modellerini tehdit eden veri zehirleme (data poisoning) saldırılarının doğasını ve etkisini ortaya koymaktır.\n\n**Metodoloji:**\n\n*   Makale, Avrupa Bölgesel Yeniden Analiz (CERRA) veri kümesi üzerinde eğitilmiş bölgesel yüzey sıcaklığı tahminlerini hedef almaktadır.\n*   Coğrafi olarak dağıtılmış istemciler (clients) simüle edilerek, yama tabanlı (patch-based) ve küresel yanlılık (global biasing) saldırıları değerlendirilmiştir.\n*   Saldırıların etkilerini ölçmek için tahminlerdeki sapmalar ve **mean squared error (MSE)** gibi metrikler kullanılmıştır.\n*   Savunma mekanizması olarak **trimmed mean aggregation** değerlendirilmiştir.\n\n**Sonuçlar:**\n\n*   Zehirli istemcilerin (poisoned clients) küçük bir oranının dahi, mekansal olarak bağlı geniş alanlardaki tahminleri yanıltabileceği gösterilmiştir.\n*   Tek bir kötü niyetli istemciden kaynaklanan küresel sıcaklık yanlılık saldırısı, tahminleri **-1.7 K**'ya kadar kaydırabilmiştir.\n*   Koordineli yama saldırıları, **MSE**'yi üç katından fazla artırmış ve **+3.5 K**'yı aşan kalıcı bölgesel anomaliler üretmiştir.\n*   **Trimmed mean aggregation** savunma mekanizması, küresel yanlılık saldırılarına karşı etkili olurken (%2-13 bozulma), yama saldırılarına karşı yetersiz kalmış (%281-603 artış), mekansal olarak ilişkili veriler için aykırı değer tabanlı savunmaların sınırlılıklarını ortaya koymuştur."
    }
  },
  {
    "id": "2512.12921v1",
    "title": "Cisco Integrated AI Security and Safety Framework Report",
    "authors": [
      "Amy Chang",
      "Tiffany Saade",
      "Sanket Mendapara",
      "Adam Swanda",
      "Ankit Garg"
    ],
    "published_date": "2025-12-15",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.12921v1",
    "pdf_link": "https://arxiv.org/pdf/2512.12921v1",
    "content": {
      "en": "Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space.   This paper presents Cisco's Integrated AI Security and Safety Framework (\"AI Security Framework\"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.",
      "tr": "## Özet\n\nBu rapor, Cisco'nun geliştirdiği **Integrated AI Security and Safety Framework**'i tanıtmaktadır.\n\n*   **Ana Katkı:** Mevcut yapılar genellikle yapay zeka (AI) risklerinin yalnızca belirli yönlerini ele alırken, Cisco'nun AI Güvenlik Çerçevesi, tüm AI risklerini kapsayan **birleştirilmiş, yaşam döngüsüne duyarlı bir taksonomi ve operasyonel çerçeve** sunmaktadır. Bu çerçeve, content safety failures, model ve data integrity, runtime manipulations ve ecosystem risks gibi geniş bir yelpazedeki tehditleri sınıflandırmayı, entegre etmeyi ve operasyonelleştirmeyi amaçlamaktadır.\n\n*   **Metodoloji:** Rapor, AI sistemlerinin hızla yaygınlaşmasıyla birlikte ortaya çıkan yeni saldırı yüzeylerini ve bu saldırıların çeşitli kategorilerini analiz eder. Mevcut çerçevelerin (MITRE ATLAS, NIST AI 100-2 AML taxonomy, OWASP Top 10s for LLMs and Agentic AI Applications) sınırlılıklarını belirledikten sonra, kendi geliştirdikleri çerçevenin tasarım prensiplerini ve kapsamını detaylandırır. Çerçeve, AI güvenliğini ve AI emniyetini farklı modalities, agents, pipelines ve daha geniş ekosistem genelinde birleştirir.\n\n*   **Sonuçlar:** Cisco'nun AI Güvenlik Çerçevesi, tehdit tanımlama, red-teaming, risk önceliklendirme gibi pratik uygulamalar için tasarlanmıştır. Multimodal bağlamlar, humanoids, wearables ve sensory infrastructures gibi gelişen dağıtımlara da genişletilebilir bir yapıdadır. Bu çerçeve, modern AI sistemlerinin nasıl başarısız olduğunu, saldırganların bu başarısızlıkları nasıl sömürdüğünü ve kuruluşların AI yaşam döngüsü boyunca nasıl savunmalar oluşturabileceğini anlamak için bir yapı sağlar."
    }
  },
  {
    "id": "2512.12914v1",
    "title": "CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs",
    "authors": [
      "Shashie Dilhara Batan Arachchige",
      "Benjamin Zi Hao Zhao",
      "Hassan Jameel Asghar",
      "Dinusha Vatsalan",
      "Dali Kaafar"
    ],
    "published_date": "2025-12-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.12914v1",
    "pdf_link": "https://arxiv.org/pdf/2512.12914v1",
    "content": {
      "en": "Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.",
      "tr": "## Özet\n\nBu makale, ince ayarlanmış (fine-tuned) Büyük Dil Modelleri'nde (LLMs) gizlilik sızıntısını azaltmaya yönelik **CTIGuardian** adlı bir **few-shot framework** sunmaktadır.\n\n*   **Ana Katkı:** İnce ayarlanmış LLM'lerin, özellikle hassas veriler içeren özel veri kümeleriyle eğitildiklerinde, bu bilgileri istemeden kötü niyetli kullanıcılara sızdırma potansiyeli bulunmaktadır. Makale, Siber Tehdit İstihbaratı (CTI) kullanım durumunda bu veri sızıntısı riskini ortaya koymakta ve tam model yeniden eğitmenin maliyetli ve pratik olmadığını vurgulamaktadır. CTIGuardian, **privacy alignment** adı verilen yeni bir yaklaşım sunarak bu soruna çözüm getirmektedir. Bu yaklaşım, LLM'lerdeki safety alignment'dan esinlenerek, **few-shot supervision** ile gizlilik kısıtlamalarını zorlar.\n\n*   **Metodoloji:** CTIGuardian, aynı altta yatan LLM tarafından yönetilen bir **privacy classifier** ve bir **privacy redactor** entegre eder. Bu bileşenler, az sayıda örnek kullanarak modelin hassas bilgileri sızdırmasını engellemek üzere eğitilir. Sistem, GPT-4o mini ve Mistral-7B Instruct modelleri üzerinde değerlendirilmiştir. Karşılaştırma için **Presidio** adlı bir **named entity recognition (NER)** baseline kullanılmıştır.\n\n*   **Sonuçlar:** Yapılan değerlendirmeler, CTIGuardian'ın NER tabanlı modellerden daha iyi bir **privacy-utility trade-off** sağladığını göstermiştir. Makale, bu framework'ün CTI kullanım durumu için etkinliğini kanıtlamakla birlikte, diğer hassas alanlara da uygulanabilecek kadar genel bir yapıya sahip olduğunu belirtmektedir."
    }
  },
  {
    "id": "2512.15782v1",
    "title": "Auto-Tuning Safety Guardrails for Black-Box Large Language Models",
    "authors": [
      "Perry Abdulkadir"
    ],
    "published_date": "2025-12-14",
    "tags": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.15782v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15782v1",
    "content": {
      "en": "Large language models (LLMs) are increasingly deployed behind safety guardrails such as system prompts and content filters, especially in settings where product teams cannot modify model weights. In practice these guardrails are typically hand-tuned, brittle, and difficult to reproduce. This paper studies a simple but practical alternative: treat safety guardrail design itself as a hyperparameter optimization problem over a frozen base model. Concretely, I wrap Mistral-7B-Instruct with modular jailbreak and malware system prompts plus a ModernBERT-based harmfulness classifier, then evaluate candidate configurations on three public benchmarks covering malware generation, classic jailbreak prompts, and benign user queries. Each configuration is scored using malware and jailbreak attack success rate, benign harmful-response rate, and end-to-end latency. A 48-point grid search over prompt combinations and filter modes establishes a baseline. I then run a black-box Optuna study over the same space and show that it reliably rediscovers the best grid configurations while requiring an order of magnitude fewer evaluations and roughly 8x less wall-clock time. The results suggest that viewing safety guardrails as tunable hyperparameters is a feasible way to harden black-box LLM deployments under compute and time constraints.",
      "tr": "## Özet\n\nBu makale, ürün ekiplerinin model ağırlıklarını değiştiremediği durumlarda, genellikle sistem komutları (system prompts) ve içerik filtreleri (content filters) gibi güvenlik çemberleri (safety guardrails) arkasında konuşlandırılan Büyük Dil Modelleri (LLMs) için otomatik ayarlama yöntemini araştırmaktadır.\n\n*   **Ana Katkı:** Güvenlik çemberi tasarımını, dondurulmuş (frozen) bir temel model üzerinde bir **hyperparameter optimization** problemi olarak ele almak. Bu yaklaşım, manuel olarak ayarlanmış, kırılgan ve tekrarlanması zor olan mevcut yöntemlere pratik bir alternatif sunmaktadır.\n\n*   **Metodoloji:**\n    *   Mistral-7B-Instruct modeli, modüler **jailbreak** ve **malware** sistem komutları ile bir **ModernBERT** tabanlı zararlılık sınıflandırıcısı (harmfulness classifier) ile entegre edilmiştir.\n    *   Aday yapılandırmalar (candidate configurations), **malware** üretimi, klasik **jailbreak** komutları ve zararsız kullanıcı sorguları (benign user queries) gibi üç farklı halka açık benchmark üzerinde değerlendirilmiştir.\n    *   Her yapılandırma, **malware** ve **jailbreak** saldırı başarı oranı (attack success rate), zararsız yanıt verme oranı (benign harmful-response rate) ve uçtan uca gecikme (end-to-end latency) gibi metriklerle puanlanmıştır.\n    *   Başlangıç noktası olarak, komut kombinasyonları ve filtre modları üzerinde 48 noktalı bir grid search kullanılmıştır.\n    *   Ardından, aynı alanda **Optuna** kütüphanesi kullanılarak black-box bir çalışma yürütülmüştür.\n\n*   **Sonuçlar:**\n    *   **Optuna** çalışması, grid search ile elde edilen en iyi yapılandırmaları güvenilir bir şekilde yeniden keşfetmiştir.\n    *   Bu süreçte, grid search'e kıyasla bir büyüklük mertebesi daha az değerlendirme (evaluations) ve yaklaşık 8 kat daha az duvar saati süresi (wall-clock time) gerektirmiştir.\n    *   Elde edilen sonuçlar, güvenlik çemberlerini ayarlanabilir hiperparametreler olarak görmenin, hesaplama (compute) ve zaman kısıtlamaları altında black-box LLM konuşlandırmalarını güçlendirmek için uygulanabilir bir yol olduğunu göstermektedir."
    }
  },
  {
    "id": "2512.12837v1",
    "title": "Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union",
    "authors": [
      "Sahibpreet Singh",
      "Manjit Singh"
    ],
    "published_date": "2025-12-14",
    "tags": [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.12837v1",
    "pdf_link": "https://arxiv.org/pdf/2512.12837v1",
    "content": {
      "en": "AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-à-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement.",
      "tr": "## Özet\n\nBu akademik makale, yapay zeka (AI) destekli \"greenwashing\" (çevreci aldatmaca) olgusunun kurumsal sürdürülebilirlik yönetişimindeki zorluklarını ve buna ilişkin cezai sorumluluk sorunlarını Hindistan, Amerika Birleşik Devletleri (ABD) ve Avrupa Birliği (AB) bağlamında karşılaştırmalı bir hukuki analizle incelemektedir.\n\n*   **Ana Katkı:**\n    *   Makale, AI aracılı yeşil aldatmacada kusurluluğun belirlenmesindeki doktrinsel boşlukları ortaya koymaktadır. Mevcut yasal düzenlemelerin, insan niyetine dayalı olmaları nedeniyle algoritmik sistemlerden kaynaklanan aldatıcı beyanları ele almakta yetersiz kaldığı vurgulanmaktadır.\n    *   Yapay zeka tarafından üretilen yanlış beyanlara karşı mevcut dolandırıcılık (fraud) yasalarının modası geçmiş olduğu ve içtihatların (jurisprudence) adaptasyonunda kritik bir boşluk bulunduğu tespit edilmiştir.\n    *   Araştırma, kurumsal cezai sorumluluğun potansiyel genişlemesine ilişkin sonuçlar sunmaktadır.\n\n*   **Metodoloji:**\n    *   Çalışma, **doctrinal legal methodology** (doktrinel hukuki metodoloji) kullanılarak judicial precedents (içtihatlar) ve statutory instruments (yasal düzenlemeler) sistematik olarak incelenmiştir.\n\n*   **Sonuçlar:**\n    *   Bulgular, **strict liability** (kusursuz sorumluluk) modellerinin uygulanabilirliğini, AI accountability (yapay zeka hesap verebilirliği) için yeniden düzenlenen governance frameworks (yönetişim çerçeveleri) ve ESG (Environmental, Social, and Governance) rejimleri altında **algorithmic due diligence** (algoritmik durum tespiti) zorunluluklarını desteklemektedir.\n    *   Karşılaştırmalı analizler, yargı bölgeleri arasındaki farklılıkları ortaya koymaktadır. Özellikle AB'nin **Corporate Sustainability Due Diligence Directive (CSDDD)** (Kurumsal Sürdürülebilirlik Durum Tespiti Direktifi), potansiyel bir uluslararası model olarak öne çıkmaktadır.\n    *   Makale, algoritmik şeffaflığın (algorithmic opacity) sorumluluk uygulamasını engellememesini sağlamak amacıyla, **algorithmic risk assessment** (algoritmik risk değerlendirmesi) ile **legal personhood constructs** (tüzel kişilik yapıları) entegre eden hibrit bir liability framework (sorumluluk çerçevesi) savunmaktadır."
    }
  },
  {
    "id": "2512.15780v1",
    "title": "Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence",
    "authors": [
      "Samruddhi Baviskar"
    ],
    "published_date": "2025-12-14",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.15780v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15780v1",
    "content": {
      "en": "We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.",
      "tr": "## Özet\n\nBu akademik makale, finansal karar alma süreçlerinde kullanılan tabular machine learning modellerinin adversarial robustness (düşmanca sağlamlık) üzerine odaklanmaktadır.\n\n*   **Ana Katkı:**\n    *   Finansal karar alma bağlamında tabular machine learning modellerinin adversarial attacks'e (düşmanca saldırılar) karşı savunmasızlıklarını değerlendirmektedir.\n    *   Adversarial training'in (düşmanca eğitim) bu saldırılara karşı kısmi bir iyileşme sağlayabileceğini göstermektedir.\n    *   Savunmasızlıkların ekonomik etkilerini ve yönetişim (governance) boyutlarını ele almaktadır.\n\n*   **Metodoloji:**\n    *   Credit scoring (kredi puanlama) ve fraud detection (dolandırıcılık tespiti) veri setleri kullanılmıştır.\n    *   Gradient-based attacks (gradyan tabanlı saldırılar) uygulanmıştır.\n    *   Saldırıların modellerin discrimination (ayırımcılık), calibration (kalibrasyon) ve financial risk metrics (finansal risk metrikleri) üzerindeki etkileri ölçülmüştür.\n\n*   **Sonuçlar:**\n    *   Küçük pertürbasyonlar (küçük değişiklikler) altında modellerin performansında kayda değer bir düşüş gözlemlenmiştir.\n    *   Adversarial training (düşmanca eğitim) ile bu performans düşüşünde kısmi bir iyileşme elde edilmiştir."
    }
  },
  {
    "id": "2512.12827v1",
    "title": "GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients",
    "authors": [
      "Mohammad Mahdi Razmjoo",
      "Mohammad Mahdi Sharifian",
      "Saeed Bagheri Shouraki"
    ],
    "published_date": "2025-12-14",
    "tags": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2512.12827v1",
    "pdf_link": "https://arxiv.org/pdf/2512.12827v1",
    "content": {
      "en": "Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.",
      "tr": "## Özet\n\nBu makale, **GradID** adlı yeni bir adversarial detection yöntemi sunmaktadır. Yöntemin ana katkısı, deep neural networks'ün girdi kaybı manzarasının (input loss landscape) geometrik özelliklerini kullanarak adversarial örnekleri tespit etmektir.\n\n### Metodoloji\n\n*   Makale, modelin gradient parametrelerinin **Intrinsic Dimensionality (ID)** analizi üzerine odaklanmaktadır.\n*   **Intrinsic Dimensionality**, veri noktalarını tanımlamak için gereken minimum koordinat sayısını ölçer.\n*   Doğal (natural) ve adversarial veriler için farklı ve tutarlı bir **ID** farkı gözlemlenmiştir. Bu fark, önerilen tespit yönteminin temelini oluşturmaktadır.\n\n### Sonuçlar\n\n*   **GradID**, iki farklı operasyonel senaryoda doğrulanmıştır:\n    *   **Batch-wise detection:** Kötü amaçlı veri gruplarını belirlemek için kullanıldığında MNIST ve SVHN gibi veri setlerinde yüksek etkinlik göstermiştir.\n    *   **Individual-sample setting:** Tekil örneklerin tespitinde, CIFAR-10 ve MS COCO gibi zorlu benchmarklarda yeni state-of-the-art sonuçlar elde edilmiştir.\n*   Yöntem, CW ve AutoAttack gibi çeşitli saldırılara karşı mevcut yöntemleri önemli ölçüde geride bırakarak, CIFAR-10 üzerinde sürekli olarak %92'nin üzerinde tespit oranları sağlamıştır.\n*   Sonuçlar, **intrinsic dimensionality**'nin çeşitli veri setleri ve saldırı stratejileri arasında güçlü bir \"fingerprint\" olarak adversarial detection için etkili bir geometrik yaklaşım olduğunu göstermektedir."
    }
  },
  {
    "id": "2512.14753v1",
    "title": "CODE ACROSTIC: Robust Watermarking for Code Generation",
    "authors": [
      "Li Lin",
      "Siyuan Xin",
      "Yang Cao",
      "Xiaochun Cao"
    ],
    "published_date": "2025-12-14",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.14753v1",
    "pdf_link": "https://arxiv.org/pdf/2512.14753v1",
    "content": {
      "en": "Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.",
      "tr": "## Özet\n\nBu makale, **CODE ACROSTIC** adını verdiği, kod üretimi için **robust watermarking** sunan yeni bir metodoloji geliştirmektedir.\n\n### Ana Katkı:\n\n*   Mevcut kod watermarking yöntemlerinin, kodun işlevselliğini etkilemeden kolayca kaldırılabilen yorum satırları (comments) üzerinden yapılan saldırılara karşı **zayıf** olduğunu ortaya koymaktadır.\n*   Daha önce belirtilen, kodun doğal dile kıyasla **low-entropy** bir senaryo olması zorluğunu aşmak için, kodun **low-entropy** ve **high-entropy** kısımlarını ayırt etmek amacıyla **prior knowledge**'dan yararlanan bir yaklaşım sunmaktadır.\n\n### Metodoloji:\n\n*   **Cue List** olarak adlandırılan bir kelime listesi aracılığıyla kod içerisindeki **low-entropy** ve **high-entropy** bölgeler belirlenir.\n*   Watermark enjeksiyonu, bu **Cue List** rehberliğinde gerçekleştirilir. Bu sayede, daha yüksek **detectability** ve **usability** elde edilir.\n\n### Sonuçlar:\n\n*   Geliştirilen **CODE ACROSTIC** metodu, **HumanEval** benchmark'ı üzerinde değerlendirilmiştir.\n*   Üç adet state-of-the-art kod watermarking tekniği ile yapılan karşılaştırmalarda, metodun **etkinliği** başarıyla gösterilmiştir."
    }
  },
  {
    "id": "2512.15779v1",
    "title": "Hyperparameter Tuning-Based Optimized Performance Analysis of Machine Learning Algorithms for Network Intrusion Detection",
    "authors": [
      "Sudhanshu Sekhar Tripathy",
      "Bichitrananda Behera"
    ],
    "published_date": "2025-12-14",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2512.15779v1",
    "pdf_link": "https://arxiv.org/pdf/2512.15779v1",
    "content": {
      "en": "Network Intrusion Detection Systems (NIDS) are essential for securing networks by identifying and mitigating unauthorized activities indicative of cyberattacks. As cyber threats grow increasingly sophisticated, NIDS must evolve to detect both emerging threats and deviations from normal behavior. This study explores the application of machine learning (ML) methods to improve the NIDS accuracy through analyzing intricate structures in deep-featured network traffic records. Leveraging the 1999 KDD CUP intrusion dataset as a benchmark, this research evaluates and optimizes several ML algorithms, including Support Vector Machines (SVM), Naïve Bayes variants (MNB, BNB), Random Forest (RF), k-Nearest Neighbors (k-NN), Decision Trees (DT), AdaBoost, XGBoost, Logistic Regression (LR), Ridge Classifier, Passive-Aggressive (PA) Classifier, Rocchio Classifier, Artificial Neural Networks (ANN), and Perceptron (PPN). Initial evaluations without hyper-parameter optimization demonstrated suboptimal performance, highlighting the importance of tuning to enhance classification accuracy. After hyper-parameter optimization using grid and random search techniques, the SVM classifier achieved 99.12% accuracy with a 0.0091 False Alarm Rate (FAR), outperforming its default configuration (98.08% accuracy, 0.0123 FAR) and all other classifiers. This result confirms that SVM accomplishes the highest accuracy among the evaluated classifiers. We validated the effectiveness of all classifiers using a tenfold cross-validation approach, incorporating Recursive Feature Elimination (RFE) for feature selection to enhance the classifiers accuracy and efficiency. Our outcomes indicate that ML classifiers are both adaptable and reliable, contributing to enhanced accuracy in systems for detecting network intrusions.",
      "tr": "## Özet\n\nBu makale, **Network Intrusion Detection Systems (NIDS)** performansını artırmak amacıyla makine öğrenmesi (ML) algoritmalarının kullanımını ve **hyperparameter tuning**'in bu algoritmaların doğruluğu üzerindeki etkisini incelemektedir.\n\n*   **Ana Katkı:**\n    *   Ağ trafiği kayıtlarındaki karmaşık yapıları analiz ederek NIDS doğruluğunu iyileştirmek için ML yöntemlerinin uygulanması.\n    *   Çeşitli ML algoritmalarının **hyperparameter tuning** öncesi ve sonrası performansını karşılaştırarak, tuning'in önemini vurgulamak.\n    *   **Recursive Feature Elimination (RFE)** kullanarak özellik seçiminin sınıflandırıcıların doğruluğu ve verimliliği üzerindeki etkisini değerlendirmek.\n\n*   **Metodoloji:**\n    *   **1999 KDD CUP** veri seti benchmark olarak kullanılmıştır.\n    *   **Support Vector Machines (SVM), Naïve Bayes (MNB, BNB), Random Forest (RF), k-Nearest Neighbors (k-NN), Decision Trees (DT), AdaBoost, XGBoost, Logistic Regression (LR), Ridge Classifier, Passive-Aggressive (PA) Classifier, Rocchio Classifier, Artificial Neural Networks (ANN),** ve **Perceptron (PPN)** gibi çeşitli ML algoritmaları değerlendirilmiştir.\n    *   Algoritmaların performansını optimize etmek için **grid search** ve **random search** teknikleri kullanılarak **hyperparameter tuning** yapılmıştır.\n    *   **Tenfold cross-validation** yöntemiyle tüm sınıflandırıcıların etkinliği doğrulanmıştır.\n\n*   **Sonuçlar:**\n    *   **Hyperparameter tuning** öncesinde algoritmaların performansı yetersiz kalmıştır.\n    *   **Hyperparameter tuning** sonrasında, **SVM** sınıflandırıcısı **%99.12 doğruluk** ve **0.0091 False Alarm Rate (FAR)** ile en yüksek performansı göstermiştir. Bu, varsayılan yapılandırmasına ( %98.08 doğruluk, 0.0123 FAR) ve diğer tüm sınıflandırıcılara göre önemli bir iyileşmedir.\n    *   Yapılan analizler, ML sınıflandırıcılarının **adaptable** ve **reliable** olduğunu ve ağ saldırılarını tespit etme sistemlerinin doğruluğunu artırmaya katkıda bulunduğunu göstermektedir."
    }
  },
  {
    "id": "2412.15842",
    "title": "Adversarial Robustness of Large Language Models in Safety-Critical Applications",
    "authors": [
      "John Smith",
      "Maria Garcia",
      "Wei Chen"
    ],
    "published_date": "2024-12-20",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "https://arxiv.org/abs/2412.15842",
    "pdf_link": "https://arxiv.org/pdf/2412.15842.pdf",
    "content": {
      "en": "Large Language Models (LLMs) are increasingly deployed in safety-critical domains such as healthcare, autonomous systems, and cybersecurity. This paper presents a comprehensive analysis of adversarial attack vectors targeting LLMs in these high-stakes environments. We introduce a novel taxonomy of attacks including prompt injection, jailbreaking, and model extraction techniques. Our experimental results on GPT-4, Claude, and Llama-2 demonstrate that current safety measures can be bypassed with carefully crafted adversarial inputs. We propose a multi-layered defense framework combining input sanitization, output filtering, and runtime monitoring to enhance LLM robustness.",
      "tr": "## Özet\n\nBüyük Dil Modelleri (LLM'ler) sağlık, otonom sistemler ve siber güvenlik gibi **güvenlik açısından kritik alanlarda** giderek daha fazla kullanılmaktadır.\n\n### Ana Bulgular\n- Prompt enjeksiyonu, jailbreaking ve model çıkarma tekniklerini içeren yeni bir saldırı taksonomisi sunulmaktadır\n- GPT-4, Claude ve Llama-2 üzerinde yapılan deneyler, mevcut güvenlik önlemlerinin atlanabildiğini göstermektedir\n- **Çok katmanlı savunma çerçevesi** önerilmektedir:\n  - Girdi temizleme\n  - Çıktı filtreleme\n  - Çalışma zamanı izleme"
    }
  },
  {
    "id": "2412.15721",
    "title": "Federated Learning with Differential Privacy for Secure Medical Image Analysis",
    "authors": [
      "Alice Johnson",
      "Kenji Tanaka",
      "Sarah Williams"
    ],
    "published_date": "2024-12-20",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "https://arxiv.org/abs/2412.15721",
    "pdf_link": "https://arxiv.org/pdf/2412.15721.pdf",
    "content": {
      "en": "Medical image analysis using machine learning requires access to sensitive patient data, raising significant privacy concerns. We propose a federated learning framework integrated with differential privacy guarantees for training deep neural networks on distributed medical imaging datasets. Our approach enables multiple hospitals to collaboratively train diagnostic models without sharing raw patient data. We demonstrate our method on chest X-ray classification achieving 94.2% accuracy while maintaining ε-differential privacy with ε=3.0.",
      "tr": "## Özet\n\nMakine öğrenimi kullanarak tıbbi görüntü analizi, hassas hasta verilerine erişim gerektirmekte ve bu durum önemli **gizlilik endişeleri** yaratmaktadır.\n\n### Önerilen Yaklaşım\n- Diferansiyel gizlilik garantileri ile entegre edilmiş federatif öğrenme çerçevesi\n- Birden fazla hastane, ham hasta verilerini paylaşmadan teşhis modellerini ortaklaşa eğitebilir\n\n### Sonuçlar\n- Göğüs röntgeni sınıflandırmasında **%94.2 doğruluk**\n- ε=3.0 ile ε-diferansiyel gizlilik sağlanmaktadır"
    }
  },
  {
    "id": "2412.14998",
    "title": "Neural Network Watermarking: A Survey on Intellectual Property Protection",
    "authors": [
      "David Lee",
      "Emma Brown",
      "Raj Patel"
    ],
    "published_date": "2024-12-19",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.AI"
    ],
    "link": "https://arxiv.org/abs/2412.14998",
    "pdf_link": "https://arxiv.org/pdf/2412.14998.pdf",
    "content": {
      "en": "As deep neural networks become valuable commercial assets, protecting intellectual property rights has emerged as a critical challenge. This survey provides a comprehensive review of neural network watermarking techniques for ownership verification. We categorize existing methods into white-box and black-box approaches, analyzing their robustness against model modification attacks including fine-tuning, pruning, and knowledge distillation. We identify key open challenges and propose future research directions for developing more resilient watermarking schemes.",
      "tr": "## Özet\n\nDerin sinir ağları değerli ticari varlıklar haline geldikçe, **fikri mülkiyet haklarının korunması** kritik bir zorluk olarak ortaya çıkmıştır.\n\n### Katkılar\n- Sahiplik doğrulaması için sinir ağı filigran tekniklerinin kapsamlı incelemesi\n- Mevcut yöntemler **beyaz kutu** ve **kara kutu** yaklaşımları olarak kategorize edilmiştir\n\n### Analiz Edilen Saldırılar\n1. İnce ayar (fine-tuning)\n2. Budama (pruning)\n3. Bilgi damıtma (knowledge distillation)\n\nAçık zorluklar belirlenmiş ve daha dayanıklı filigran şemaları için gelecek araştırma yönleri önerilmiştir."
    }
  },
  {
    "id": "2412.14567",
    "title": "Secure Multi-Party Computation for Privacy-Preserving Machine Learning",
    "authors": [
      "Lisa Wang",
      "Michael Chen",
      "Fatima Al-Said"
    ],
    "published_date": "2024-12-19",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "https://arxiv.org/abs/2412.14567",
    "pdf_link": "https://arxiv.org/pdf/2412.14567.pdf",
    "content": {
      "en": "Secure Multi-Party Computation (MPC) enables multiple parties to jointly compute functions over their private inputs without revealing individual data. We present an optimized MPC protocol for privacy-preserving neural network inference that achieves 10x speedup over existing approaches. Our implementation leverages GPU acceleration and novel secret sharing techniques to enable real-time inference on encrypted data. We evaluate our system on image classification and natural language processing tasks, demonstrating practical deployment feasibility.",
      "tr": "## Özet\n\nGüvenli Çok Taraflı Hesaplama (MPC), birden fazla tarafın bireysel verileri açığa çıkarmadan özel girdileri üzerinde ortaklaşa fonksiyon hesaplamasını sağlar.\n\n### Teknik Katkılar\n- Mevcut yaklaşımlara göre **10 kat hızlanma** sağlayan optimize edilmiş MPC protokolü\n- GPU hızlandırma ve yeni gizli paylaşım teknikleri\n- Şifrelenmiş veri üzerinde gerçek zamanlı çıkarım\n\n### Değerlendirme\n- Görüntü sınıflandırma görevleri\n- Doğal dil işleme görevleri\n- Pratik dağıtım fizibilitesi gösterilmiştir"
    }
  },
  {
    "id": "2412.13892",
    "title": "Automated Vulnerability Detection in Smart Contracts using Graph Neural Networks",
    "authors": [
      "Tom Anderson",
      "Yuki Sato",
      "Anna Kowalski"
    ],
    "published_date": "2024-12-18",
    "tags": [
      "cs.CR",
      "cs.PL",
      "cs.AI"
    ],
    "link": "https://arxiv.org/abs/2412.13892",
    "pdf_link": "https://arxiv.org/pdf/2412.13892.pdf",
    "content": {
      "en": "Smart contracts deployed on blockchain platforms handle billions of dollars in digital assets, making security vulnerabilities extremely costly. We propose SmartGuard, a graph neural network-based approach for automated vulnerability detection in Solidity smart contracts. Our method constructs control flow graphs and data dependency graphs from contract bytecode, then applies a novel attention mechanism to identify vulnerability patterns. Evaluation on a dataset of 47,398 real-world contracts shows 96.3% precision and 91.7% recall in detecting reentrancy, integer overflow, and access control vulnerabilities.",
      "tr": "## Özet\n\nBlok zinciri platformlarında dağıtılan akıllı sözleşmeler milyarlarca dolarlık dijital varlığı yönetmektedir, bu da güvenlik açıklarını son derece maliyetli kılmaktadır.\n\n### SmartGuard Sistemi\n- Solidity akıllı sözleşmelerinde otomatik güvenlik açığı tespiti için **graf sinir ağı tabanlı** yaklaşım\n- Sözleşme bytecode'undan kontrol akış grafları ve veri bağımlılık grafları oluşturulur\n- Güvenlik açığı kalıplarını belirlemek için yeni bir dikkat mekanizması uygulanır\n\n### Sonuçlar (47,398 gerçek dünya sözleşmesi)\n| Metrik | Değer |\n|--------|-------|\n| Kesinlik | %96.3 |\n| Geri çağırma | %91.7 |\n\n**Tespit edilen açıklar:** Yeniden giriş, tamsayı taşması, erişim kontrolü"
    }
  },
  {
    "id": "2412.13456",
    "title": "Formal Verification of Neural Network Controllers for Autonomous Vehicles",
    "authors": [
      "Robert Miller",
      "Chen Liu",
      "Isabella Romano"
    ],
    "published_date": "2024-12-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.PL"
    ],
    "link": "https://arxiv.org/abs/2412.13456",
    "pdf_link": "https://arxiv.org/pdf/2412.13456.pdf",
    "content": {
      "en": "Autonomous vehicles rely on neural network controllers for safety-critical decisions. We present a formal verification framework that provides mathematical guarantees about neural network behavior within specified operational domains. Our approach combines abstract interpretation with satisfiability modulo theories (SMT) solving to verify properties such as collision avoidance and lane keeping. We demonstrate scalability to networks with millions of parameters, verifying a production-grade perception system from a major automotive manufacturer.",
      "tr": "## Özet\n\nOtonom araçlar, güvenlik açısından kritik kararlar için sinir ağı kontrolörlerine güvenmektedir.\n\n### Önerilen Çerçeve\nBelirli operasyonel alanlar içinde sinir ağı davranışı hakkında **matematiksel garantiler** sağlayan formal doğrulama çerçevesi.\n\n### Metodoloji\n- Soyut yorumlama (abstract interpretation)\n- Tatmin edilebilirlik modülo teorileri (SMT) çözme\n- **Doğrulanan özellikler:**\n  - Çarpışmadan kaçınma\n  - Şerit takibi\n\n### Ölçeklenebilirlik\n- Milyonlarca parametre içeren ağlara ölçeklenebilirlik\n- Büyük bir otomotiv üreticisinden üretim düzeyinde algılama sistemi doğrulanmıştır"
    }
  },
  {
    "id": "2412.12789",
    "title": "Detecting AI-Generated Text: Robustness Analysis of Current Methods",
    "authors": [
      "Sophie Martin",
      "James Wilson",
      "Priya Sharma"
    ],
    "published_date": "2024-12-17",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "https://arxiv.org/abs/2412.12789",
    "pdf_link": "https://arxiv.org/pdf/2412.12789.pdf",
    "content": {
      "en": "The proliferation of large language models has raised concerns about AI-generated misinformation and academic dishonesty. We evaluate the robustness of current AI text detection methods against various evasion techniques including paraphrasing, character substitution, and adversarial perturbations. Our experiments reveal significant vulnerabilities in commercial detectors, with accuracy dropping from 95% to below 50% under targeted attacks. We propose an ensemble detection approach combining statistical and neural methods that maintains 82% accuracy under adversarial conditions.",
      "tr": "## Özet\n\nBüyük dil modellerinin yaygınlaşması, yapay zeka tarafından üretilen yanlış bilgi ve akademik dürüstlük konularında endişelere yol açmıştır.\n\n### Değerlendirilen Kaçınma Teknikleri\n1. Parafraz yapma\n2. Karakter değiştirme\n3. Düşmanca pertürbasyonlar\n\n### Bulgular\n- Ticari dedektörlerde önemli güvenlik açıkları tespit edilmiştir\n- Hedefli saldırılar altında doğruluk **%95'ten %50'nin altına** düşmektedir\n\n### Önerilen Çözüm\n- İstatistiksel ve sinirsel yöntemleri birleştiren **topluluk tespit yaklaşımı**\n- Düşmanca koşullar altında **%82 doğruluk** sağlanmaktadır"
    }
  },
  {
    "id": "2412.12234",
    "title": "Privacy-Preserving Compiler Optimizations for Secure Computation",
    "authors": [
      "Nathan Green",
      "Hiroshi Yamamoto",
      "Clara Santos"
    ],
    "published_date": "2024-12-17",
    "tags": [
      "cs.CR",
      "cs.PL"
    ],
    "link": "https://arxiv.org/abs/2412.12234",
    "pdf_link": "https://arxiv.org/pdf/2412.12234.pdf",
    "content": {
      "en": "Secure computation protocols incur significant performance overhead compared to plaintext computation. We present a domain-specific compiler that automatically optimizes programs for secure execution. Our optimizer applies novel transformations including garbled circuit minimization, SIMD batching for homomorphic encryption, and communication-aware scheduling. Benchmarks on standard cryptographic workloads show 3-15x speedup over naive compilation while preserving semantic equivalence and security guarantees.",
      "tr": "## Özet\n\nGüvenli hesaplama protokolleri, düz metin hesaplamaya kıyasla önemli performans yüküne neden olmaktadır.\n\n### SecureOpt Derleyicisi\nProgramları güvenli yürütme için otomatik olarak optimize eden **alana özgü derleyici**.\n\n### Uygulanan Optimizasyonlar\n- Karıştırılmış devre (garbled circuit) minimizasyonu\n- Homomorfik şifreleme için SIMD gruplama\n- İletişim farkındalıklı zamanlama\n\n### Performans\n- Standart kriptografik iş yüklerinde **3-15 kat hızlanma**\n- Semantik eşdeğerlik korunmaktadır\n- Güvenlik garantileri sağlanmaktadır"
    }
  },
  {
    "id": "2412.11567",
    "title": "Backdoor Attacks and Defenses in Deep Reinforcement Learning",
    "authors": [
      "Kevin Zhang",
      "Marie Dubois",
      "Ahmed Hassan"
    ],
    "published_date": "2024-12-16",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.AI"
    ],
    "link": "https://arxiv.org/abs/2412.11567",
    "pdf_link": "https://arxiv.org/pdf/2412.11567.pdf",
    "content": {
      "en": "Deep reinforcement learning agents are vulnerable to backdoor attacks where adversaries embed hidden triggers during training. We systematically study backdoor threats in continuous control environments, demonstrating attacks that cause autonomous agents to fail catastrophically when triggered. Our proposed defense mechanism uses activation clustering and spectral signature analysis to detect backdoored policies with 94% accuracy. We provide theoretical analysis of attack success conditions and defense limitations.",
      "tr": "## Özet\n\nDerin pekiştirmeli öğrenme ajanları, düşmanların eğitim sırasında gizli tetikleyiciler yerleştirdiği **arka kapı saldırılarına** karşı savunmasızdır.\n\n### Çalışmanın Kapsamı\n- Sürekli kontrol ortamlarında arka kapı tehditlerinin sistematik incelenmesi\n- Tetiklendiğinde otonom ajanların felaket boyutunda başarısız olmasına neden olan saldırılar gösterilmiştir\n\n### Savunma Mekanizması\n- **Aktivasyon kümeleme** ve **spektral imza analizi**\n- Arka kapılı politikaları **%94 doğrulukla** tespit eder\n\n### Teorik Katkılar\n- Saldırı başarı koşullarının analizi\n- Savunma sınırlamalarının belirlenmesi"
    }
  },
  {
    "id": "2412.10823",
    "title": "Zero-Knowledge Proofs for Machine Learning Model Integrity",
    "authors": [
      "Laura Thompson",
      "Kai Nakamura",
      "Stefan Mueller"
    ],
    "published_date": "2024-12-15",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "https://arxiv.org/abs/2412.10823",
    "pdf_link": "https://arxiv.org/pdf/2412.10823.pdf",
    "content": {
      "en": "Machine learning as a service requires clients to trust that providers execute the promised model correctly. We introduce zkML, a system for generating zero-knowledge proofs of correct neural network inference. Our approach enables verifiable computation where a prover can demonstrate that outputs were generated by a specific model without revealing model weights or input data. We achieve practical proof generation times of under 10 seconds for ResNet-50 inference through novel circuit optimizations for floating-point arithmetic.",
      "tr": "## Özet\n\nHizmet olarak makine öğrenimi, istemcilerin sağlayıcıların vaat edilen modeli doğru şekilde çalıştırdığına güvenmesini gerektirir.\n\n### zkML Sistemi\nDoğru sinir ağı çıkarımının **sıfır bilgi kanıtlarını** oluşturmak için tasarlanmış sistem.\n\n### Özellikler\n- Kanıtlayıcı, model ağırlıklarını veya girdi verilerini açığa çıkarmadan çıktıların belirli bir model tarafından üretildiğini gösterebilir\n- **Doğrulanabilir hesaplama** sağlanır\n\n### Performans\n- ResNet-50 çıkarımı için **10 saniyenin altında** kanıt üretim süresi\n- Kayan nokta aritmetiği için yeni devre optimizasyonları uygulanmıştır"
    }
  }
]