[
  {
    "id": "2602.05838v1",
    "title": "FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation",
    "authors": [
      "Mayank Kumar",
      "Qian Lou",
      "Paulo Barreto",
      "Martine De Cock",
      "Sikha Pentyala"
    ],
    "published_date": "2026-02-05",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.05838v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05838v1",
    "content": {
      "en": "Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.",
      "tr": "Elbette, makale başlığı ve özetinin çevirisi aşağıdadır:\n\n**Makale Başlığı:** FHAIM: Özel Sentetik Veri Üretimi İçin Tam Homomorfik AIM\n\n**Özet:**\nYapay zeka için veri hayati önem taşımaktadır, ancak en değerli verilerin çoğu gizlilik ve düzenlemeler nedeniyle veri silolarında kilitli kalmaktadır. Sonuç olarak, sağlık, eğitim ve finans gibi en önemli alanların çoğunda yapay zeka büyük ölçüde yetersiz kullanılmaktadır. Sentetik veri üretimi (SDG), yani gerçek veriler üzerinde eğitilmiş bir sentezleyici ile yapay verilerin üretilmesi, verileri gizlilik endişelerini giderirken kullanılabilir hale getirmek için cazip bir çözüm sunmaktadır. Ancak mevcut SDG-as-a-service iş akışları, veri sahiplerinin özel verilere erişim konusunda sağlayıcılara güvenmelerini gerektirmektedir. Biz, şifrelenmiş tablo verileri üzerinde marjinal tabanlı bir sentetik veri üreteci eğitmek için ilk tam homomorfik şifreleme (FHE) çerçevesi olan FHAIM'ı öneriyoruz. FHAIM, yaygın olarak kullanılan AIM algoritmasını yeni FHE protokolleri kullanarak FHE ortamına uyarlar ve özel verilerin tüm süreç boyunca şifreli kalmasını ve yalnızca differential privacy garantileriyle serbest bırakılmasını sağlar. Ampirik analizlerimiz, FHAIM'ın AIM'ın performansını korurken uygulanabilir çalışma sürelerini de sürdürdüğünü göstermektedir."
    }
  },
  {
    "id": "2602.05817v1",
    "title": "Interpreting Manifolds and Graph Neural Embeddings from Internet of Things Traffic Flows",
    "authors": [
      "Enrique Feito-Casares",
      "Francisco M. Melgarejo-Meseguer",
      "Elena Casiraghi",
      "Giorgio Valentini",
      "José-Luis Rojo-Álvarez"
    ],
    "published_date": "2026-02-05",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2602.05817v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05817v1",
    "content": {
      "en": "The rapid expansion of Internet of Things (IoT) ecosystems has led to increasingly complex and heterogeneous network topologies. Traditional network monitoring and visualization tools rely on aggregated metrics or static representations, which fail to capture the evolving relationships and structural dependencies between devices. Although Graph Neural Networks (GNNs) offer a powerful way to learn from relational data, their internal representations often remain opaque and difficult to interpret for security-critical operations. Consequently, this work introduces an interpretable pipeline that generates directly visualizable low-dimensional representations by mapping high-dimensional embeddings onto a latent manifold. This projection enables the interpretable monitoring and interoperability of evolving network states, while integrated feature attribution techniques decode the specific characteristics shaping the manifold structure. The framework achieves a classification F1-score of 0.830 for intrusion detection while also highlighting phenomena such as concept drift. Ultimately, the presented approach bridges the gap between high-dimensional GNN embeddings and human-understandable network behavior, offering new insights for network administrators and security analysts.",
      "tr": "**Makale Başlığı:** Nesnelerin İnterneti Trafik Akışlarından Manifoldları ve Graph Neural Embeddings'i Yorumlama\n\n**Özet:**\n\nNesnelerin İnterneti (IoT) ekosistemlerinin hızlı genişlemesi, giderek daha karmaşık ve heterojen ağ topolojilerine yol açmıştır. Geleneksel ağ izleme ve görselleştirme araçları, cihazlar arasındaki gelişen ilişkileri ve yapısal bağımlılıkları yakalayamayan toplu metrik veya statik temsiller üzerine dayanmaktadır. Graph Neural Networks (GNNs), ilişkisel verilerden öğrenmek için güçlü bir yol sunarken, iç temsilleri genellikle güvenlik açısından kritik operasyonlar için opak ve yorumlanması zor kalmaktadır. Sonuç olarak, bu çalışma, yüksek boyutlu embeddings'leri latent bir manifold üzerine haritalayarak doğrudan görselleştirilebilir düşük boyutlu temsiller üreten yorumlanabilir bir pipeline sunmaktadır. Bu projeksiyon, gelişen ağ durumlarının yorumlanabilir izlenmesini ve birlikte çalışabilirliğini sağlarken, entegre feature attribution teknikleri manifold yapısını şekillendiren özel karakteristikleri çözmektedir. Çerçeve, saldırı tespiti için 0.830'luk bir sınıflandırma F1-score'u elde etmekte ve aynı zamanda concept drift gibi olguları da vurgulamaktadır. Nihayetinde, sunulan yaklaşım, yüksek boyutlu GNN embeddings'leri ile insan tarafından anlaşılabilir ağ davranışı arasındaki boşluğu kapatmakta, ağ yöneticileri ve güvenlik analistleri için yeni içgörüler sunmaktadır."
    }
  },
  {
    "id": "2602.05486v1",
    "title": "Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems",
    "authors": [
      "Matteo Esposito",
      "Lodovica Marchesi",
      "Roberto Tonelli",
      "Valentina Lenarduzzi"
    ],
    "published_date": "2026-02-05",
    "tags": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "link": "http://arxiv.org/abs/2602.05486v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05486v1",
    "content": {
      "en": "Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.",
      "tr": "İşte makale başlığının ve özetinin çevirisi:\n\n**Makale Başlığı:** Sovereign-by-Design Yapay Zeka ve Blockchain Destekli Sistemler İçin Bir Referans Mimari\n\n**Özet:**\nDijital egemenlik, egemen olmayan bulut altyapılarının hakimiyeti, Üretken Yapay Zekanın (Generative AI) hızlı benimsenmesi ve giderek sıkılaşan düzenleyici gereklilikler tarafından yönlendirilen modern yazılım yoğun sistemler için merkezi bir endişe kaynağı haline gelmiştir. Mevcut girişimler yönetişim, uyumluluk ve güvenliği izole bir şekilde ele alırken, egemenliğin mimari düzeyde nasıl operasyonelleştirilebileceği konusunda sınırlı rehberlik sunmaktadırlar. Bu makalede, egemenliğin yalnızca düzenleyici bir hedef olmaktan ziyade birinci sınıf bir mimari özellik olarak ele alınması gerektiğini savunmaktayız. Kendi kendine egemen kimliği (self-sovereign identity), blockchain tabanlı güven ve denetlenebilirlik, egemen veri yönetişimi ve açık mimari kontrol altında konuşlandırılmış Üretken Yapay Zeka'yı (Generative AI) entegre eden bir Egemen Referans Mimari (Sovereign Reference Architecture) sunuyoruz. Mimari, Üretken Yapay Zekanın (Generative AI) hem yönetişim riskleri kaynağı hem de uygun şekilde kısıtlandığında uyumluluk, hesap verebilirlik ve sürekli güvence sağlayıcısı olarak ikili rolünü açıkça yakalamaktadır. Egemenliği bir mimari kalite özelliği olarak çerçeveleyerek, çalışmamız düzenleyici niyet ile somut sistem tasarımı arasında köprü kurmakta, denetlenebilir, evrilebilir ve yargı alanına duyarlı (jurisdiction-aware) Yapay Zeka destekli sistemler inşa etmek için tutarlı bir temel sunmaktadır. Önerilen referans mimari, yazılım mimarisi, Üretken Yapay Zeka (Generative AI) ve dijital egemenlik kesişiminde gelecekteki araştırma ve uygulama için prensipli bir başlangıç noktası sağlamaktadır."
    }
  },
  {
    "id": "2602.05386v1",
    "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening",
    "authors": [
      "Zhenxiong Yu",
      "Zhi Yang",
      "Zhiheng Jin",
      "Shuhe Wang",
      "Heng Zhang"
    ],
    "published_date": "2026-02-05",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.05386v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05386v1",
    "content": {
      "en": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.",
      "tr": "**Makale Başlığı:** Spider-Sense: Hiyerarşik Uyarlanabilir Tarama ile Etkin Ajan Savunması için İçsel Risk Algılama\n\n**Özet:**\n\nBüyük dil modelleri (LLM'ler) otonom ajanlara dönüştükçe, gerçek dünyadaki uygulanabilirlikleri önemli ölçüde genişlemiş ve bununla birlikte yeni güvenlik zorlukları da ortaya çıkmıştır. Mevcut ajan savunma mekanizmalarının çoğu zorunlu bir kontrol paradigması benimsemektedir; bu paradigmada güvenlik doğrulaması, ajan yaşam döngüsünün önceden tanımlanmış aşamalarında zorla tetiklenir. Bu çalışmada, etkin ajan güvenliğinin mimari olarak ayrılmış ve zorunlu olmaktan ziyade içsel ve seçici olması gerektiğini savunuyoruz. Risk algılama üzerine kurulu, olay güdümlü bir savunma çerçevesi olan Spider-Sense framework'ünü öneriyoruz; bu çerçeve, ajanların gizli bir tetikte kalmasını ve yalnızca risk algılandığında savunmaları tetiklemesini sağlar. Tetiklendiğinde, Spider-Sense verimlilik ve hassasiyet arasında denge kuran hiyerarşik bir savunma mekanizmasını devreye sokar: hafif ağırlıklı similarity matching aracılığıyla bilinen kalıpları çözerken, belirsiz durumları deep internal reasoning'e yükselterek harici modellere olan bağımlılığı ortadan kaldırır. Titiz bir değerlendirmeyi kolaylaştırmak için, gerçekçi araç yürütme ve çok aşamalı saldırılar içeren yaşam döngüsü farkındalığına sahip bir benchmark olan S$^2$Bench'i sunuyoruz. Kapsamlı deneyler, Spider-Sense'in rekabetçi veya üstün savunma performansı elde ettiğini, en düşük Attack Success Rate (ASR) ve False Positive Rate (FPR)'yi sağladığını ve yalnızca %8.3'lük marjinal bir latency overhead'e sahip olduğunu göstermektedir."
    }
  },
  {
    "id": "2602.05279v1",
    "title": "Hallucination-Resistant Security Planning with a Large Language Model",
    "authors": [
      "Kim Hammar",
      "Tansu Alpcan",
      "Emil Lupu"
    ],
    "published_date": "2026-02-05",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.05279v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05279v1",
    "content": {
      "en": "Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.",
      "tr": "**Makale Başlığı:** Büyük Bir Dil Modeli ile Halüsinasyona Dirençli Güvenlik Planlaması\n\n**Özet:**\n\nBüyük dil modelleri (LLM'ler), olay müdahale planlaması gibi güvenlik yönetimi görevlerini desteklemek için umut vadeden araçlardır. Ancak, güvenilirliklerinin düşük olması ve halüsinasyon eğilimleri önemli zorluklar olmaya devam etmektedir. Bu makalede, bir LLM'yi güvenlik yönetiminde karar destek sistemi olarak kullanmak için ilkeli bir çerçeve sunarak bu zorlukları ele almaktayız. Çerçevemiz, LLM'yi yinelemeli bir döngüye entegre eder; bu döngüde LLM, sistem kısıtlamaları ve ileriye dönük tahminlerle tutarlılığı kontrol edilen aday eylemler üretir. Tutarlılık düşük olduğunda, üretilen eylemlerden kaçınırız ve bunun yerine, örneğin bir dijital ikizde eylemleri değerlendirerek dış geri bildirim toplarız. Bu geri bildirim daha sonra in-context learning (ICL) yoluyla aday eylemleri iyileştirmek için kullanılır. Bu tasarımın, tutarlılık eşiğini ayarlayarak halüsinasyon riskini kontrol etmemizi sağladığını kanıtlıyoruz. Ayrıca, belirli varsayımlar altında ICL'nin pişmanlık (regret) miktarını sınırlandırıyoruz. Çerçevemizi değerlendirmek için, sistem günlüklerine dayalı bir yanıt ve kurtarma planı oluşturma hedefi olan bir olay müdahale kullanım durumuna uyguluyoruz. Dört genel veri kümesi üzerinde yapılan deneyler, çerçevemizin en güncel LLM'lere kıyasla kurtarma sürelerini %30'a kadar azalttığını göstermektedir."
    }
  },
  {
    "id": "2602.05089v1",
    "title": "Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning",
    "authors": [
      "Ethan Rathbun",
      "Wo Wei Lin",
      "Alina Oprea",
      "Christopher Amato"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.RO"
    ],
    "link": "http://arxiv.org/abs/2602.05089v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05089v1",
    "content": {
      "en": "Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger'', leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent's training pipeline, enabling them to both alter and observe agent's rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze'' which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze's effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks.",
      "tr": "**Makale Başlığı: Güvenilmeyen Simülatörlere Dikkat Edin -- Pekiştirmeli Öğrenmede Ödül-Gerektirmeyen Arka Kapı Saldırıları**\n\n**Özet:**\nPekiştirmeli Öğrenme (RL) alanındaki gelişmelerde simüle edilmiş ortamlar kritik bir rol oynamaktadır. Bu ortamlar, uygulayıcıların ve araştırmacıların pahalı gerçek donanım deneylerine başvurmadan karar verme ajanlarını eğitmesini sağlamaktadır. Ancak simülatörler, güvenlik açısından bir kör nokta olmaya devam etmektedir. Bu durum, kötü niyetli geliştiricilerin yayınladıkları simülatörlerin dinamiklerini kötü amaçlarla değiştirmelerine olanak tanımaktadır. Bu nedenle, bu çalışmada yeni bir tehdidi ortaya koymaktayız. Simülatör dinamiklerinin, RL ajanlarına gizlice eylem seviyesinde arka kapılar yerleştirmek için nasıl kullanılabileceğini göstermekteyiz. Bu arka kapı, bir saldırganın önceden tanımlanmış bir \"tetikleyici\" gözlemlediğinde, bir ajanda hedeflenen eylemleri güvenilir bir şekilde aktive etmesine olanak tanır ve bu da potansiyel olarak tehlikeli sonuçlara yol açabilir. Geleneksel arka kapı saldırıları, saldırganın ajanın eğitim hattı üzerinde neredeyse tam kontrole sahip olduğunu varsayan güçlü tehdit modelleriyle sınırlıdır. Bu modeller, saldırganın hem ödülleri değiştirmesine hem de gözlemlemesine imkan tanır. Bu varsayımlar bir simülatör içinde gerçekleştirilmesi zor olduğundan, gerçek dünya görevleri için eğitilmiş RL ajanlarına, ödüllerini değiştirmeden veya hatta gözlemlemeden, güvenilir ve gizli bir şekilde arka kapılar yerleştirebilen yeni bir saldırı olan \"Daze\"yi öneriyoruz. Daze'nin genel RL görevlerinde saldırı başarısını garanti etmedeki etkinliğine dair resmi kanıtlar sunmanın yanı sıra, hem ayrık hem de sürekli eylem uzayı alanlarında kapsamlı ampirik değerlendirmeler yapmaktayız. Ek olarak, RL arka kapı saldırılarının gerçek robot donanımlarına aktarılmasının ilk örneğini sunuyoruz. Bu gelişmeler, kötü niyetli saldırıları önlemek için RL eğitim hattının tüm bileşenlerini güvence altına almaya yönelik daha fazla araştırmayı teşvik etmektedir."
    }
  },
  {
    "id": "2602.05066v1",
    "title": "Bypassing AI Control Protocols via Agent-as-a-Proxy Attacks",
    "authors": [
      "Jafar Isbarov",
      "Murat Kantarcioglu"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.05066v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05066v1",
    "content": {
      "en": "As AI agents automate critical workloads, they remain vulnerable to indirect prompt injection (IPI) attacks. Current defenses rely on monitoring protocols that jointly evaluate an agent's Chain-of-Thought (CoT) and tool-use actions to ensure alignment with user intent. We demonstrate that these monitoring-based defenses can be bypassed via a novel Agent-as-a-Proxy attack, where prompt injection attacks treat the agent as a delivery mechanism, bypassing both agent and monitor simultaneously. While prior work on scalable oversight has focused on whether small monitors can supervise large agents, we show that even frontier-scale monitors are vulnerable. Large-scale monitoring models like Qwen2.5-72B can be bypassed by agents with similar capabilities, such as GPT-4o mini and Llama-3.1-70B. On the AgentDojo benchmark, we achieve a high attack success rate against AlignmentCheck and Extract-and-Evaluate monitors under diverse monitoring LLMs. Our findings suggest current monitoring-based agentic defenses are fundamentally fragile regardless of model scale.",
      "tr": "Makale Başlığı: Agent-as-a-Proxy Saldırıları Yoluyla Yapay Zeka Kontrol Protokollerini Aşma\n\nÖzet:\nYapay zeka ajanları kritik iş yüklerini otomatikleştirdikça, dolaylı prompt injection (IPI) saldırılarına karşı savunmasız kalmaktadırlar. Mevcut savunmalar, bir ajanın Chain-of-Thought (CoT) ve araç kullanma eylemlerini kullanıcı niyetiyle uyumu sağlamak üzere ortaklaşa değerlendiren izleme protokollerine dayanmaktadır. Bu izleme tabanlı savunmaların, ajanı bir dağıtım mekanizması olarak ele alan ve hem ajanı hem de monitörü eş zamanlı olarak atlayan yeni bir Agent-as-a-Proxy saldırısı yoluyla aşılabileceğini gösteriyoruz. Ölçeklenebilir denetim üzerine yapılan önceki çalışmalar, küçük monitörlerin büyük ajanları denetleyip denetleyemeyeceğine odaklanmışken, biz sınır ölçeğindeki monitörlerin bile savunmasız olduğunu gösteriyoruz. Qwen2.5-72B gibi büyük ölçekli izleme modelleri, GPT-4o mini ve Llama-3.1-70B gibi benzer yeteneklere sahip ajanlar tarafından aşılabilemektedir. AgentDojo benchmark'ında, çeşitli izleme LLM'leri altında AlignmentCheck ve Extract-and-Evaluate monitörlerine karşı yüksek bir saldırı başarı oranı elde ettik. Bulgularımız, mevcut izleme tabanlı ajanlık savunmalarının model ölçeğinden bağımsız olarak temelden kırılgan olduğunu düşündürmektedir."
    }
  },
  {
    "id": "2602.05056v1",
    "title": "VEXA: Evidence-Grounded and Persona-Adaptive Explanations for Scam Risk Sensemaking",
    "authors": [
      "Heajun An",
      "Connor Ng",
      "Sandesh Sharma Dulal",
      "Junghwan Kim",
      "Jin-Hee Cho"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.05056v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05056v1",
    "content": {
      "en": "Online scams across email, short message services, and social media increasingly challenge everyday risk assessment, particularly as generative AI enables more fluent and context-aware deception. Although transformer-based detectors achieve strong predictive performance, their explanations are often opaque to non-experts or misaligned with model decisions. We propose VEXA, an evidence-grounded and persona-adaptive framework for generating learner-facing scam explanations by integrating GradientSHAP-based attribution with theory-informed vulnerability personas. Evaluation across multi-channel datasets shows that grounding explanations in detector-derived evidence improves semantic reliability without increasing linguistic complexity, while persona conditioning introduces interpretable stylistic variation without disrupting evidential alignment. These results reveal a key design insight: evidential grounding governs semantic correctness, whereas persona-based adaptation operates at the level of presentation under constraints of faithfulness. Together, VEXA demonstrates the feasibility of persona-adaptive, evidence-grounded explanations and provides design guidance for trustworthy, learner-facing security explanations in non-formal contexts.",
      "tr": "Aşağıda, verilen akademik makale başlığının ve özetinin teknik terimlerin İngilizce bırakılarak, akademik ve resmi bir dille yapılmış Türkçe çevirisi bulunmaktadır:\n\n**Makale Başlığı:** VEXA: Dolandırıcılık Riski Anlama İçin Kanıt Temelli ve Persona Uyumlu Açıklamalar\n\n**Özet:**\nE-posta, kısa mesaj hizmetleri ve sosyal medya aracılığıyla gerçekleşen çevrimiçi dolandırıcılıklar, özellikle üretken yapay zeka daha akıcı ve bağlama duyarlı aldatmacaları mümkün kıldıkça, günlük risk değerlendirmesine yönelik zorlukları artırmaktadır. Transformer tabanlı tespit sistemleri güçlü öngörüsel performans sergilese de, açıklamaları genellikle uzman olmayanlar için şeffaf değildir veya model kararlarıyla uyumsuzdur. Biz, GradientSHAP tabanlı atıfı teori odaklı vulnerability personas ile entegre ederek, öğrenci odaklı dolandırıcılık açıklamaları üretmek için kanıt temelli ve persona uyumlu bir çerçeve olan VEXA'yı öneriyoruz. Çok kanallı veri kümeleri üzerinden yapılan değerlendirme, açıklamaları tespit sisteminden türetilen kanıtlara dayandırmanın, dilsel karmaşıklığı artırmadan anlamsal güvenilirliği geliştirdiğini göstermektedir. Aynı zamanda, persona koşullandırması, kanıtsal uyumu bozmadan yorumlanabilir stilistik çeşitlilik sunmaktadır. Bu sonuçlar önemli bir tasarım içgörüsünü ortaya koymaktadır: kanıtsal temellendirme anlamsal doğruluğu yönetirken, persona tabanlı uyum sadakat kısıtlamaları altında sunum düzeyinde işler. Birlikte, VEXA persona uyumlu, kanıt temelli açıklamaların fizibilitesini göstermekte ve gayri resmi bağlamlarda güvenilir, öğrenci odaklı güvenlik açıklamaları için tasarım rehberliği sunmaktadır."
    }
  },
  {
    "id": "2602.05023v1",
    "title": "Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?",
    "authors": [
      "Ruixin Yang",
      "Ethan Mendes",
      "Arthur Wang",
      "James Hays",
      "Sauvik Das"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.05023v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05023v1",
    "content": {
      "en": "Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity, we introduce VLM-GEOPRIVACY, a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.",
      "tr": "**Makale Başlığı:** Vision-Language Modelleri Konumsal Açıklamalarda Bağlamsal Bütünlüğe Uygun Davranıyor mu?\n\n**Özet:**\n\nVision-language modelleri (VLMs), görüntü coğrafi konum belirleme konusunda güçlü performans sergilemiştir ve bu yetenek, gelişmiş multimodal large reasoning modelleri (MLRMs) ile daha da keskinleşmiştir. Bu durum, yaygın olarak erişilebilen bu modellerin, genellikle sokak seviyesinde hassasiyetle, paylaşıcının rıza gösterdiğinden veya açıklamayı amaçladığından daha detaylı bilgi açığa çıkarabilen, özensizce paylaşılan fotoğraflardan hassas konumların çıkarılması için istismar edilebileceği önemli bir gizlilik riski oluşturmaktadır. Yakın zamanda yapılan çalışmalar bu riske karşı koymak için coğrafi konum açıklamasını genel bir kısıtlamayla ele almayı önermiş olsa da, bu önlemler geçerli coğrafi konum kullanımlarını kötü niyetli davranışlardan ayırt edememektedir. Bunun yerine, VLMs'lerin gizlilik ve fayda dengesini kurarak, bilgi açıklamasının uygun düzeyini belirlemek için bir görüntüdeki öğeler üzerinde reasoning yaparak bağlamsal bütünlüğü koruması gerekmektedir. Modellerin bağlamsal bütünlüğe ne kadar uyduğunu değerlendirmek için, VLMs'yi gerçek dünya görüntülerindeki gizli sosyal normları ve bağlamsal ipuçlarını yorumlamaya ve uygun konum açıklama düzeyini belirlemeye zorlayan bir benchmark olan VLM-GEOPRIVACY'yi sunuyoruz. On dört önde gelen VLM üzerindeki değerlendirmemiz, görüntüleri hassas bir şekilde coğrafi konumlandırabilmelerine rağmen, modellerin insan gizlilik beklentileriyle kötü bir şekilde hizalandığını göstermektedir. Hassas bağlamlarda sıkça aşırı açıklama yapmaktadırlar ve prompt-based attacks'lere karşı savunmasızdırlar. Sonuçlarımız, multimodal sistemlerde bağlam koşullu gizlilik reasoning'ini entegre etmek için yeni tasarım ilkelerine ihtiyaç duymaktadır."
    }
  },
  {
    "id": "2602.04753v1",
    "title": "Comparative Insights on Adversarial Machine Learning from Industry and Academia: A User-Study Approach",
    "authors": [
      "Vishruti Kakkad",
      "Paul Chung",
      "Hanan Hibshi",
      "Maverick Woo"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.04753v1",
    "pdf_link": "https://arxiv.org/pdf/2602.04753v1",
    "content": {
      "en": "An exponential growth of Machine Learning and its Generative AI applications brings with it significant security challenges, often referred to as Adversarial Machine Learning (AML). In this paper, we conducted two comprehensive studies to explore the perspectives of industry professionals and students on different AML vulnerabilities and their educational strategies. In our first study, we conducted an online survey with professionals revealing a notable correlation between cybersecurity education and concern for AML threats. For our second study, we developed two CTF challenges that implement Natural Language Processing and Generative AI concepts and demonstrate a poisoning attack on the training data set. The effectiveness of these challenges was evaluated by surveying undergraduate and graduate students at Carnegie Mellon University, finding that a CTF-based approach effectively engages interest in AML threats. Based on the responses of the participants in our research, we provide detailed recommendations emphasizing the critical need for integrated security education within the ML curriculum.",
      "tr": "İşte akademik makale başlığının ve özetinin Türkçeye çevrilmiş hali:\n\n**Makale Başlığı:** Sektör ve Akademi Perspektifinden Saldırgan Makine Öğrenmesi Üzerine Karşılaştırmalı İçgörüler: Bir Kullanıcı-Çalışması Yaklaşımı\n\n**Özet:**\nMakine Öğrenmesi ve Üretken Yapay Zeka uygulamalarının katlanarak artması, genellikle Saldırgan Makine Öğrenmesi (AML) olarak adlandırılan önemli güvenlik zorluklarını beraberinde getirmektedir. Bu makalede, sektör profesyonelleri ve öğrencilerin farklı AML açıklıkları ve eğitim stratejileri konusundaki bakış açılarını keşfetmek amacıyla iki kapsamlı çalışma yürütülmüştür. İlk çalışmamızda, siber güvenlik eğitimi ile AML tehditlerine yönelik endişe arasında dikkate değer bir korelasyon ortaya koyan profesyonellerle çevrimiçi bir anket gerçekleştirilmiştir. İkinci çalışmamızda ise Doğal Dil İşleme ve Üretken Yapay Zeka konseptlerini uygulayan ve eğitim veri kümesi üzerinde bir poisoning attack gösteren iki CTF meydan okuması geliştirilmiştir. Bu meydan okumaların etkinliği, Carnegie Mellon Üniversitesi'ndeki lisans ve lisansüstü öğrencileriyle yapılan bir anketle değerlendirilmiş ve CTF tabanlı bir yaklaşımın AML tehditlerine olan ilgiyi etkili bir şekilde çektiği bulunmuştur. Araştırmamızdaki katılımcıların yanıtlarına dayanarak, ML müfredatı içinde entegre edilmiş güvenlik eğitiminin kritik gerekliliğini vurgulayan ayrıntılı öneriler sunmaktayız."
    }
  }
]