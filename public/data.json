[
  {
    "id": "2512.21241v1",
    "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
    "authors": [
      "Xinjie Xu",
      "Shuyu Cheng",
      "Dongwei Xu",
      "Qi Xuan",
      "Chen Ma"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2512.21241v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21241v1",
    "content": {
      "en": "In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum $\\ell_2$-norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, which proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum. We provide a theoretical analysis of its convergence behavior, showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization. To further accelerate convergence, we incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance. The superiority of our approach is supported by theoretical guarantees under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that our method surpasses 13 state-of-the-art approaches in query efficiency.",
      "tr": "İşte akademik makale başlığının ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** Sorgu Verimliliği Yüksek Zorlu Etiket Saldırıları İçin Işın Arama Optimizasyonunun Yakınsama Oranının İyileştirilmesi\n\n**Özet:**\n\nZorlu etiketli (hard-label) siyah-kutu (black-box) zararlı saldırılarda, yalnızca en yüksek puana sahip tahmin edilen etiketin erişilebilir olduğu durumlarda, kabul edilemez query complexity pratik uygulamalar önünde büyük bir engel teşkil etmektedir. Bu çalışmada, zararsız bir görüntüyü zararlı bölgeye taşımak için gereken minimum $\\ell_2$-norm pertürbasyonunu veren optimum ışın yönünü (ray direction) arayan, saldırıların temsili bir sınıfını optimize etmeye odaklanıyoruz. Nesterov's Accelerated Gradient (NAG)'dan ilham alarak, bir momentum tabanlı algoritma olan ARS-OPT'yi öneriyoruz. Bu algoritma, birikmiş momentumdan çıkarılan gelecek bir ışın yönüne ilişkin gradient'i proaktif olarak tahmin eder. Yakınsama davranışının teorik analizini sunarak, ARS-OPT'nin daha doğru yönel güncelleştirmeleri sağladığını ve daha hızlı, daha kararlı bir optimizasyon gerçekleştirdiğini gösteriyoruz. Yakınsamayı daha da hızlandırmak için, ARS-OPT'nin gradient tahminine surrogate-model priors'ları entegre ederek, geliştirilmiş performans sergileyen PARS-OPT'yi elde ediyoruz. Yaklaşımımızın üstünlüğü, standart varsayımlar altında teorik garantilerle desteklenmektedir. ImageNet ve CIFAR-10 üzerindeki kapsamlı deneyler, yöntemimizin query efficiency açısından 13 adet son teknoloji (state-of-the-art) yaklaşımın önüne geçtiğini göstermektedir."
    }
  },
  {
    "id": "2512.21238v1",
    "title": "Assessing the Software Security Comprehension of Large Language Models",
    "authors": [
      "Mohammed Latif Siddiq",
      "Natalie Sekerak",
      "Antonio Karam",
      "Maria Leal",
      "Arvin Islam-Gomes"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.SE",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.21238v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21238v1",
    "content": {
      "en": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.",
      "tr": "**Makale Başlığı:** Büyük Dil Modellerinin Yazılım Güvenliği Kavrayışının Değerlendirilmesi\n\n**Özet:**\n\nBüyük dil modelleri (LLM'ler) yazılım geliştirmede giderek daha fazla kullanılmakla birlikte, yazılım güvenliği uzmanlıkları düzeyleri belirsizliğini korumaktadır. Bu çalışma, Blooms Taksonomisi'ni bir çerçeve olarak kullanarak beş önde gelen LLM'nin - GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1 ve Qwen-2.5 - güvenlik kavrayışını sistematik olarak değerlendirmektedir. Altı bilişsel boyutu değerlendirmekteyiz: hatırlama, anlama, uygulama, analiz etme, değerlendirme ve yaratma. Metodolojimiz, özenle hazırlanmış çoktan seçmeli sorular, güvenlik açığı içeren kod parçacıkları (SALLM), bir Giriş Düzeyinde Yazılım Güvenliği dersinden alınan kurs değerlendirmeleri, gerçek dünya vaka çalışmaları (XBOW) ve Güvenli Yazılım Mühendisliği dersinden proje tabanlı yaratma görevleri dahil olmak üzere çeşitli veri kümelerini entegre etmektedir. Sonuçlar, LLM'lerin bilgileri hatırlama ve bilinen güvenlik açıklarını belirleme gibi alt düzey bilişsel görevlerde iyi performans gösterdiğini, ancak reasoning, mimari değerlendirme ve güvenli sistem oluşturma gerektiren üst düzey görevlerde performanslarının önemli ölçüde düştüğünü göstermektedir. Toplam doğruluğu raporlamanın ötesinde, bir modelin güvenilir performansı sürekli olarak sürdürdüğü en yüksek bilişsel düzeyi belirleyen bir yazılım güvenliği knowledge boundary sunuyoruz. Ek olarak, Blooms seviyelerinde LLM'lerin sergilediği 51 tekrarlayan yanlış anlama paterni tespit ediyoruz."
    }
  },
  {
    "id": "2512.21236v1",
    "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
    "authors": [
      "Yifan Huang",
      "Xiaojun Jia",
      "Wenbo Guo",
      "Yuqiang Sun",
      "Yihao Huang"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2512.21236v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21236v1",
    "content": {
      "en": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",
      "tr": "Makale Başlığı: SPELL'i Yansıtmak: LLM Sınırlılıklarını Kırma İçin Cümle Eşleştirme Keşfi\n\nÖzet:\nLarge language models (LLMs), yazılım geliştirme alanında AI-assisted coding tools aracılığıyla devrim yaratmıştır ve sınırlı programlama uzmanlığına sahip geliştiricilerin sofistike uygulamalar oluşturmasını sağlamıştır. Ancak bu erişilebilirlik, bu güçlü araçları zararlı yazılımlar üretmek için istismar edebilecek kötü niyetli aktörlere de uzanmaktadır. Mevcut jailbreaking araştırmaları öncelikle LLM'lere karşı genel saldırı senaryolarına odaklanmakta, malicious code generation'ı bir jailbreak hedefi olarak keşfetme konusunda ise sınırlı kalmaktadır. Bu boşluğu gidermek için, malicious code generation'daki security alignment'ın zayıflıklarını değerlendirmek üzere özel olarak tasarlanmış kapsamlı bir testing framework olan SPELL'i öneriyoruz. Framework'ümüz, novel attack patterns'ın keşfini başarılı tekniklerin istismarıyla dengeleyerek, önceden oluşturulmuş bir knowledge dataset'inden cümleleri akıllıca birleştirerek jailbreaking prompts'ları sistematik olarak oluşturan bir time-division selection strategy benimsemektedir. Üç gelişmiş code model (GPT-4.1, Claude-3.5 ve Qwen2.5-Coder) üzerinde yapılan kapsamlı değerlendirme, SPELL'in etkinliğini göstermekte ve sekiz adet malicious code kategorisinde sırasıyla %83.75, %19.38 ve %68.12 saldırı başarı oranları elde etmektedir. Üretilen prompts, Cursor gibi gerçek dünya AI development tools'larında başarıyla malicious code üretmekte, çıktıları ise %73'ün üzerinde oranlarda state-of-the-art detection systems tarafından zararlı olarak doğrulanmaktadır. Bu bulgular, mevcut LLM implementations'ındaki önemli security gaps'leri ortaya koymakta ve code generation applications'larında AI safety alignment'ı iyileştirmek için değerli içgörüler sunmaktadır."
    }
  },
  {
    "id": "2512.21132v1",
    "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
    "authors": [
      "Tobias von Arx",
      "Niels Mündler",
      "Mark Vero",
      "Maximilian Baader",
      "Martin Vechev"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "link": "http://arxiv.org/abs/2512.21132v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21132v1",
    "content": {
      "en": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.",
      "tr": "**Makale Başlığı:** AutoBaxBuilder: Kod Güvenliği Kıyaslamasını Başlatma\n\n**Özet:**\n\nYazılım mühendisliğinde LLM'lerin yaygın olarak benimsenmesiyle birlikte, LLM tarafından üretilen kodun doğruluğunun ve güvenliğinin güvenilir bir şekilde değerlendirilmesi hayati önem taşımaktadır. Özellikle önceki çalışmalar, güvenliğin genellikle göz ardı edildiğini ve LLM'lerin güvenlik açıkları içeren kod üretme eğiliminde olduğunu ortaya koymuştur. Bu bulgular, güvenlik uzmanları tarafından büyük manuel çaba gerektirerek oluşturulan özel kıyaslamalar sayesinde mümkün olmuştur. Ancak, manuel olarak hazırlanmış kıyaslamalara güvenmek uzun vadede yetersiz kalmaktadır, çünkü kıyaslamalar (i) doğal olarak eğitim verilerini kirletir, (ii) daha eksiksiz bir resim sunmak için yeni görevlere genişletilmeli ve (iii) daha yetenekli LLM'leri zorlamak için zorluk seviyeleri artırılmalıdır. Bu çalışmada, bu zorlukları ele alıyoruz ve kod güvenliği kıyaslamaları için sıfırdan görevler ve testler üreten bir çerçeve olan AutoBaxBuilder'ı sunuyoruz. LLM'lerin kod anlama yeteneklerinden yararlanarak işlevsellik testleri ve uçtan uca güvenlik sorgulama istismarları oluşturmak için, ince taneli makuliyet kontrolleri içeren sağlam bir işlem hattı tanıtıyoruz. Üretilen kıyaslamanın kalitesini doğrulamak için, hem nitel bir analiz yürütüyor hem de insan uzmanları tarafından oluşturulan görevlerle karşılaştırarak nicel deneyler gerçekleştiriyoruz. AutoBaxBuilder'ı tamamen yeni görevler oluşturmak için kullanıyoruz ve bunları, LLM'lerin bu görevlerdeki güvenlik yeteneklerinin kapsamlı bir değerlendirmesiyle birlikte AutoBaxBench olarak kamuoyuna sunuyoruz. Yeni bir görevin 2 saatten kısa sürede ve 10 USD'den daha az bir maliyetle üretilebildiğini buluyoruz."
    }
  },
  {
    "id": "2512.21110v1",
    "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent",
    "authors": [
      "Ahmed M. Hussain",
      "Salahuddin Salahuddin",
      "Panos Papadimitratos"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ],
    "link": "http://arxiv.org/abs/2512.21110v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21110v1",
    "content": {
      "en": "Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.",
      "tr": "Makale Başlığı: Bağlamın Ötesi: Büyük Dil Modellerinin Kullanıcı Niyetini Kavramada Başarısızlığı\n\nÖzet:\nMevcut Büyük Dil Modelleri (LLMs) güvenlik yaklaşımları, açıkça zararlı içeriğe odaklanırken kritik bir zafiyeti göz ardı etmektedir: bağlamı anlama ve kullanıcı niyetini tanıma konusundaki yetersizlik. Bu durum, kötü niyetli kullanıcıların güvenlik mekanizmalarını aşmak için sistematik olarak kullanabilecekleri istismar edilebilir zafiyetler yaratmaktadır. ChatGPT, Claude, Gemini ve DeepSeek dahil olmak üzere birden fazla en gelişmiş LLM'yi ampirik olarak değerlendiriyoruz. Analizimiz, duygusal çerçeveleme, aşamalı ifşa ve akademik gerekçelendirme teknikleri aracılığıyla güvenilir güvenlik mekanizmalarının atlatıldığını göstermektedir. Özellikle, reasoning-enabled konfigürasyonlar istismarın etkinliğini hafifletmek yerine artırmış, olgusal kesinliği artırırken altta yatan niyeti sorgulamakta başarısız olmuştur. Bunun istisnası, bazı kullanım durumlarında bilgi sağlama yerine niyet tespitine öncelik veren Claude Opus 4.1 olmuştur. Bu örüntü, mevcut mimari tasarımların sistematik zafiyetler yarattığını ortaya koymaktadır. Bu sınırlamalar, sonradan koruyucu mekanizmalar yerine bağlamsal anlama ve niyet tanımayı temel güvenlik yetenekleri olarak benimseyen paradigmatik değişimler gerektirmektedir."
    }
  },
  {
    "id": "2512.21048v1",
    "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
    "authors": [
      "Savvy Sharma",
      "George Petrovic",
      "Sarthak Kaushik"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.21048v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21048v1",
    "content": {
      "en": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.",
      "tr": "**Makale Başlığı:** zkFL-Health: Tıbbi Yapay Zeka Gizliliği İçin Blokzincir Destekli Sıfır Bilgi Bileşik Öğrenme\n\n**Özet:**\n\nSağlık yapay zekası, büyük ve çeşitli veri kümelerine ihtiyaç duymaktadır. Ancak, sıkı gizlilik ve yönetişim kısıtlamaları, kurumlar arasında ham veri paylaşımını engellemektedir. Bileşik öğrenme (FL), verinin bulunduğu yerde model eğitimi yaparak ve yalnızca model güncellemelerini değiş tokuş ederek bunu hafifletir. Ancak, pratik uygulamalar hala iki temel riskle karşı karşıyadır: (1) gizlilik sızıntısı (membership inference, gradient inversion) gradientler veya güncellemeler aracılığıyla ve (2) tespit edilmeden katkıları düşürebilen, değiştirebilen veya enjekte edebilen tek hata noktası olan aggregator'a güven. Biz, tıbbi yapay zeka için gizliliği koruyan, doğrulanabilir şekilde doğru işbirlikçi eğitimi sunmak üzere FL'yi zero-knowledge proofs (ZKPs) ve Trusted Execution Environments (TEEs) ile birleştiren bir mimari olan zkFL-Health'i sunuyoruz. Müşteriler yerel olarak modellerini eğitir ve güncellemelerini commit ederler; aggregator, global güncellemeyi hesaplamak için bir TEE içinde çalışır ve herhangi bir müşteri güncellemesini host'a açıklamadan, tam olarak commit edilmiş girdileri ve doğru aggregation rule'u kullandığını gösteren özlü bir ZK proof (Halo2/Nova aracılığıyla) üretir. Doğrulayıcı düğümler, proof'u doğrular ve on-chain üzerine kriptografik commitment'ları kaydeder. Bu, değişmez bir denetim izi sağlar ve herhangi tek bir tarafa güvenme ihtiyacını ortadan kaldırır. Sağlık hizmetlerine özel sistem ve tehdit modellerini, zkFL-Health protokolünü, güvenlik/gizlilik garantilerini ve accuracy, privacy risk, latency ve cost'u kapsayan bir performans değerlendirme planını ana hatlarıyla belirtiyoruz. Bu çerçeve, klinik kabul ve mevzuata uygunluk için temel özellikler olan güçlü gizlilik, bütünlük ve denetlenebilirlik ile çok kurumsal tıbbi yapay zekayı mümkün kılar."
    }
  },
  {
    "id": "2512.20872v1",
    "title": "Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification",
    "authors": [
      "Jakir Hossain",
      "Gurvinder Singh",
      "Lukasz Ziarek",
      "Ahmet Erdem Sarıyüce"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.20872v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20872v1",
    "content": {
      "en": "Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.",
      "tr": "**Makale Başlığı:** Better Call Graphs: Kötü Amaçlı Yazılım Sınıflandırması İçin Yeni Bir Fonksiyon Çağrı Grafikleri Veri Kümesi\n\n**Özet:**\n\nFunction call graphs (FCGs), uygulamaların davranışsal yapısını yüzeyel imzaların ötesinde yakalayarak kötü amaçlı yazılım tespiti için güçlü bir soyutlama olarak ortaya çıkmıştır. Geleneksel program analizindeki faydaları, kötü amaçlı yazılımların etkili sınıflandırılmasına ve analizine olanak tanıdığı için iyi bir şekilde kurulmuştur. Mobil alanda, özellikle Android ekosisteminde, FCG tabanlı kötü amaçlı yazılım sınıflandırması, platformun yaygın benimsenmesi ve Android uygulamalarının karmaşık, bileşen tabanlı yapısı nedeniyle özellikle kritiktir. Ancak, bu yöndeki ilerleme, büyük ölçekli, yüksek kaliteli Android'e özgü FCG veri kümelerinin eksikliği ile engellenmektedir. Mevcut veri kümeleri genellikle güncelliğini yitirmiş, uygulama yeniden paketlenmesinden kaynaklanan küçük veya gereksiz grafikler tarafından domine edilmiş ve gerçek dünya kötü amaçlı yazılımlarının çeşitliliğini yansıtamamaktadır. Bu sınırlamalar, aşırı uyumlanmaya (overfitting) ve grafik tabanlı sınıflandırma yöntemlerinin güvenilmez değerlendirilmesine yol açmaktadır. Bu boşluğu gidermek için, en son Android uygulama paketlerinden (APKs) çıkarılan büyük ve benzersiz FCG'lerden oluşan kapsamlı bir veri kümesi olan Better Call Graphs'ı (BCG) sunuyoruz. BCG, her APK için grafik seviyesi özelliklerin yanı sıra çeşitli aileleri ve türleri kapsayan hem zararsız hem de kötü amaçlı örnekleri içermektedir. Temel sınıflandırıcılar (baseline classifiers) kullanılarak yapılan kapsamlı deneyler aracılığıyla, mevcut veri kümelerine kıyasla BCG'nin gerekliliğini ve değerini gösteriyoruz. BCG, https://erdemub.github.io/BCG-dataset adresinde kamuya açık olarak mevcuttur."
    }
  },
  {
    "id": "2512.20712v1",
    "title": "Real-World Adversarial Attacks on RF-Based Drone Detectors",
    "authors": [
      "Omer Gazit",
      "Yael Itzhakev",
      "Yuval Elovici",
      "Asaf Shabtai"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.20712v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20712v1",
    "content": {
      "en": "Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.",
      "tr": "**Makale Başlığı: RF Tabanlı Drone Dedektörlerine Yönelik Gerçek Dünya Çevresel Saldırıları**\n\n**Özet:**\n\nRadyo frekansı (RF) tabanlı sistemler, drone'ların RF sinyal kalıplarını analiz ederek, bunları nesne tespit modelleri tarafından işlenen spectrogram görüntüleri haline dönüştürerek drone'ları tespit etmek için giderek daha fazla kullanılmaktadır. Görüntü tabanlı modellere yönelik mevcut RF saldırıları, dijital özellikleri değiştirerek, dijital pertürbasyonların senkronizasyon hataları ve parazit oluşturabilecek iletilebilir dalga formlarına dönüştürülmesindeki zorluk ve donanım sınırlamaları nedeniyle over-the-air (OTA) uygulamasını zorlaştırmaktadır. Biz, RF görüntü tabanlı drone dedektörlerine yönelik ilk fiziksel saldırıyı sunuyoruz; burada sınıf özel evrensel karma temel bant (I/Q) pertürbasyon dalga formları, meşru iletişimlerle birlikte iletilmek üzere optimize edilmiştir. Saldırıyı dört farklı drone türüyle yapılan RF kayıtları ve OTA deneyleri kullanarak değerlendirdik. Sonuçlarımız, mütevazı, yapılandırılmış I/Q pertürbasyonlarının standart RF zincirleriyle uyumlu olduğunu ve meşru drone'ların tespitini korurken hedef drone tespitini güvenilir bir şekilde azalttığını göstermektedir."
    }
  },
  {
    "id": "2512.20423v1",
    "title": "Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit",
    "authors": [
      "Adam Elaoumari"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2512.20423v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20423v1",
    "content": {
      "en": "The purpose of this project is to assess how well defenders can detect DNS-over-HTTPS (DoH) file exfiltration, and which evasion strategies can be used by attackers. While providing a reproducible toolkit to generate, intercept and analyze DoH exfiltration, and comparing Machine Learning vs threshold-based detection under adversarial scenarios. The originality of this project is the introduction of an end-to-end, containerized pipeline that generates configurable file exfiltration over DoH using several parameters (e.g., chunking, encoding, padding, resolver rotation). It allows for file reconstruction at the resolver side, while extracting flow-level features using a fork of DoHLyzer. The pipeline contains a prediction side, which allows the training of machine learning models based on public labelled datasets and then evaluates them side-by-side with threshold-based detection methods against malicious and evasive DNS-Over-HTTPS traffic. We train Random Forest, Gradient Boosting and Logistic Regression classifiers on a public DoH dataset and benchmark them against evasive DoH exfiltration scenarios. The toolkit orchestrates traffic generation, file capture, feature extraction, model training and analysis. The toolkit is then encapsulated into several Docker containers for easy setup and full reproducibility regardless of the platform it is run on. Future research regarding this project is directed at validating the results on mixed enterprise traffic, extending the protocol coverage to HTTP/3/QUIC request, adding a benign traffic generation, and working on real-time traffic evaluation. A key objective is to quantify when stealth constraints make DoH exfiltration uneconomical and unworthy for the attacker.",
      "tr": "Elbette, makale başlığını ve özetini akademik ve resmi bir dille Türkçeye çevirdim. Teknik terimler olduğu gibi İngilizce bırakıldı.\n\n**Makale Başlığı:** DNS-over-HTTPS Veri Sızdırma Olaylarının Engellenmeye Dayanıklı Tespiti: Pratik Bir Değerlendirme ve Toolkit\n\n**Özet:**\nBu projenin amacı, savunmacıların DNS-over-HTTPS (DoH) dosya sızdırma olaylarını ne kadar iyi tespit edebildiğini ve saldırganların hangi engelleme stratejilerini kullanabileceğini değerlendirmektir. DoH sızdırmayı üreten, araya giren ve analiz eden tekrarlanabilir bir toolkit sunarken, saldırgan senaryoları altında Machine Learning ve eşik tabanlı tespit yöntemlerini karşılaştırmaktadır. Bu projenin özgünlüğü, birkaç parametre (örneğin, chunking, encoding, padding, resolver rotation) kullanarak DoH üzerinden yapılandırılabilir dosya sızdırmayı üreten uçtan uca, containerized bir pipeline'ın tanıtılmasıdır. Bu pipeline, resolver tarafında dosya yeniden yapılandırmaya izin verirken, DoHLyzer'ın bir çatalını kullanarak flow-level features'ı çıkarmaktadır. Pipeline, kamuya açık etiketli veri setlerine dayalı machine learning modellerini eğitmeye ve ardından saldırgan ve engellemeye eğilimli DNS-Over-HTTPS trafiğine karşı bu modelleri eşik tabanlı tespit yöntemleriyle yan yana değerlendirmeye olanak tanıyan bir prediction side içermektedir. Random Forest, Gradient Boosting ve Logistic Regression sınıflandırıcılarını kamuya açık bir DoH veri setinde eğitiyor ve onları engellemeye eğilimli DoH sızdırma senaryolarına karşı benchmark ediyoruz. Toolkit, trafik üretimi, dosya yakalama, feature extraction, model eğitimi ve analizi koordine etmektedir. Toolkit, üzerinde çalıştırıldığı platformdan bağımsız olarak kolay kurulum ve tam tekrarlanabilirlik için birkaç Docker container'ına kapsüllenmiştir. Bu projeyle ilgili gelecekteki araştırmalar, sonuçları karma kurumsal trafik üzerinde doğrulamaya, protokol kapsamını HTTP/3/QUIC request'lerine genişletmeye, benign trafik üretimi eklemeye ve gerçek zamanlı trafik değerlendirmesi üzerinde çalışmaya yöneliktir. Temel bir hedef, stealth constraints'in DoH sızdırmayı saldırgan için ekonomik olmayan ve değersiz kıldığı noktaları nicel olarak belirlemektir."
    }
  },
  {
    "id": "2512.20396v1",
    "title": "Symmaries: Automatic Inference of Formal Security Summaries for Java Programs",
    "authors": [
      "Narges Khakpour",
      "Nicolas Berthier"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.FL",
      "cs.PL",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2512.20396v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20396v1",
    "content": {
      "en": "We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.",
      "tr": "Aşağıda, verilen akademik makale başlığının ve özetinin Türkçe çevirisi bulunmaktadır:\n\n**Makale Başlığı:** Symmaries: Java Programları İçin Formal Güvenlik Özelliklerinin Otomatik Çıkarımı\n\n**Özet:**\nJava bytecode programları için method özetleri biçiminde formal güvenlik özelliklerini otomatik olarak oluşturan ölçeklenebilir, modüler ve sağlam bir yaklaşım sunmaktayız. Bir özet, methodun güvenli bir şekilde çağrılabileceği koşulları, bilgi akışları ve aliasing güncellemelerine ilişkin özellikler ile birlikte, methodun güvenlik davranışının soyut bir temsilini sağlar. Bu tür özetler, statik kod analizi araçları tarafından tüketilebilir ve aynı zamanda geliştiricilerin, özellikle kütüphaneler gibi kod segmentlerinin davranışlarını anlamalarına, uygulamalarda yeniden kullanıldıklarında güvenlik etkilerini değerlendirmelerine yardımcı olur. Yaklaşımımız, güvenlik özetlerinin üretimini otomatikleştiren Symmaries adlı bir araçta uygulanmıştır. Symmaries'i Java API kütüphanelerine uygulayarak güvenlik özelliklerini çıkardık ve ölçeklenebilirliğini değerlendirmek için büyük gerçek dünya uygulamalarına uyguladık. Sonuçlarımız, aracın yüz binlerce satır koda sahip uygulamaları analiz etme konusunda başarılı bir şekilde ölçeklenebildiğini ve kullanılan heap modeline bağlı olarak Symmaries'in umut verici bir precision elde ettiğini göstermektedir. Yaklaşımımızın, termination-insensitive non-interference'ı garanti etme açısından sağlamlığını kanıtladık."
    }
  }
]