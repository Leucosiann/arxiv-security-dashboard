[
  {
    "id": "2601.22159v1",
    "title": "RedSage: A Cybersecurity Generalist LLM",
    "authors": [
      "Naufal Suryanto",
      "Muzammal Naseer",
      "Pengfei Li",
      "Syed Talal Wasim",
      "Jinhui Yi"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2601.22159v1",
    "pdf_link": "https://arxiv.org/pdf/2601.22159v1",
    "content": {
      "en": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.",
      "tr": "İşte akademik makale başlığı ve özetinin çevirisi:\n\n**Makale Başlığı:** RedSage: Bir Siber Güvenlik Genel Uzmanı LLM\n\n**Özet:**\nSiber güvenlik operasyonları, hassas verileri ifşa etmeden çeşitli iş akışlarını destekleyen yardımcı LLM'leri gerektirmektedir. Mevcut çözümler ya gizlilik riskleri taşıyan özel API'lere ya da alan adaptasyonundan yoksun açık modellere dayanmaktadır. Bu boşluğu doldurmak için, büyük ölçekli web filtrelemesi ve çerçeveleri, saldırı tekniklerini ve güvenlik araçlarını kapsayan 28.6K belge boyunca yüksek kaliteli kaynakların manuel toplanması yoluyla siber güvenliğe odaklanmış 11.8B token'lık devamlı ön eğitim verisi derledik. Buna dayanarak, denetimli ince ayar için 266K çok turlu siber güvenlik örneği üretmek üzere uzman iş akışlarını simüle eden bir agentic augmentation pipeline tasarlıyoruz. Genel açık kaynaklı LLM verileriyle birleştirilen bu kaynaklar, alan farkındalığına sahip ön eğitim ve son eğitim ile açık kaynaklı, yerel olarak dağıtılabilir bir siber güvenlik yardımcısı olan RedSage'in eğitimini mümkün kılmaktadır. Modelleri titizlikle değerlendirmek için, siber güvenlik bilgisini, becerilerini ve araç uzmanlığını kapsayan 30K çoktan seçmeli ve 240 açık uçlu Soru-Cevap öğesinden oluşan bir benchmark olan RedSage-Bench'i tanıtıyoruz. RedSage, daha geniş genelleme yeteneğini değerlendirmek için yerleşik siber güvenlik benchmarklarında (örneğin, CTI-Bench, CyberMetric, SECURE) ve genel LLM benchmarklarında daha da değerlendirilmektedir. 8B ölçeğinde RedSage, siber güvenlik benchmarklarında +5.59 puana kadar ve Open LLM Leaderboard görevlerinde +5.05 puana kadar temel modelleri geride bırakarak tutarlı olarak daha iyi sonuçlar elde etmektedir. Bu bulgular, alan farkındalığına sahip agentic augmentation ve ön/son eğitimin yalnızca siber güvenliğe özgü uzmanlığı geliştirmekle kalmayıp aynı zamanda genel reasoning ve instruction-following'i iyileştirmeye de yardımcı olabileceğini göstermektedir. Tüm modeller, veri kümeleri ve kodlar kamuya açıktır."
    }
  },
  {
    "id": "2601.22136v1",
    "title": "StepShield: When, Not Whether to Intervene on Rogue Agents",
    "authors": [
      "Gloria Felicia",
      "Michael Eniolade",
      "Jinfeng He",
      "Zitha Sasindran",
      "Hemant Kumar"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2601.22136v1",
    "pdf_link": "https://arxiv.org/pdf/2601.22136v1",
    "content": {
      "en": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.",
      "tr": "**Makale Başlığı:** StepShield: Sahte Ajanlara Ne Zaman, Müdahale Edilip Edilmeyeceğine Değil\n\n**Özet:**\n\nMevcut ajan güvenliği karşılaştırmalı değerlendirmeleri ikili doğruluk oranları bildirmekte, erken müdahaleyi ölüm sonrası analizle karıştırmaktadır. 8. adımda bir ihlali işaretleyen bir tespit mekanizması müdahaleyi mümkün kılarken, 48. adımda bunu bildiren yalnızca adli inceleme değeri sunar. Bu ayrım kritiktir, ancak mevcut karşılaştırmalı değerlendirmeler bunu ölçememektedir. StepShield'i, ihlallerin yalnızca tespit edilip edilmediğini değil, ne zaman tespit edildiğini de değerlendiren ilk karşılaştırmalı değerlendirmeyi tanıtıyoruz. StepShield, 9.213 kod ajanı yörüngesi içermekte olup, titizlikle açıklanmış 1.278 eğitim çifti ve gerçekçi %8,1 sahte oranına sahip 7.935 yörüngeli bir test kümesi bulunmaktadır. Sahte davranışlar, altı kategoriye yayılan gerçek dünya güvenlik olaylarına dayanmaktadır. Üç yeni zamansal metrik öneriyoruz: Early Intervention Rate (EIR), Intervention Gap ve Tokens Saved. Şaşırtıcı bir şekilde, değerlendirmemiz bir LLM tabanlı hakemin %59 EIR elde ederken, bir static analyzer'ın yalnızca %26 oranında bir performans elde ettiğini ortaya koymaktadır; bu, standart doğruluk metriklerine tamamen görünmez olan 2,3 katlık bir performans farkıdır. Ayrıca, erken tespitin doğrudan ekonomik faydalar sağladığını gösteriyoruz: basamaklı HybridGuard tespit mekanizmamız izleme maliyetlerini %75 oranında azaltmakta ve kurumsal ölçekte beş yıl boyunca 108 milyon dolarlık kümülatif tasarruf öngörmektedir. Değerlendirme odağını olup olmadığından ne zaman'a kaydırarak, StepShield daha güvenli ve daha ekonomik olarak uygulanabilir AI ajanları oluşturmak için yeni bir temel sağlamaktadır. Kod ve veriler Apache 2.0 lisansı altında yayınlanmıştır."
    }
  },
  {
    "id": "2601.21902v1",
    "title": "Hardware-Triggered Backdoors",
    "authors": [
      "Jonas Möller",
      "Erik Imgrund",
      "Thorsten Eisenhofer",
      "Konrad Rieck"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.21902v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21902v1",
    "content": {
      "en": "Machine learning models are routinely deployed on a wide range of computing hardware. Although such hardware is typically expected to produce identical results, differences in its design can lead to small numerical variations during inference. In this work, we show that these variations can be exploited to create backdoors in machine learning models. The core idea is to shape the model's decision function such that it yields different predictions for the same input when executed on different hardware. This effect is achieved by locally moving the decision boundary close to a target input and then refining numerical deviations to flip the prediction on selected hardware. We empirically demonstrate that these hardware-triggered backdoors can be created reliably across common GPU accelerators. Our findings reveal a novel attack vector affecting the use of third-party models, and we investigate different defenses to counter this threat.",
      "tr": "İşte istenen çeviri:\n\n**Makale Başlığı:** Hardware-Triggered Backdoors\n\n**Özet:**\nMachine learning modelleri, çok çeşitli bilgi işlem donanımlarında rutin olarak konuşlandırılmaktadır. Bu tür donanımların tipik olarak aynı sonuçları üreteceği beklense de, tasarımındaki farklılıklar çıkarım sırasında küçük sayısal değişikliklere yol açabilir. Bu çalışmada, bu değişikliklerin machine learning modellerinde backdoors oluşturmak için istismar edilebileceğini gösteriyoruz. Temel fikir, modelin karar fonksiyonunu, farklı donanımlarda yürütüldüğünde aynı girdi için farklı tahminler verecek şekilde şekillendirmektir. Bu etki, karar sınırını hedef bir girdiye yakın bir yere yerel olarak taşıyarak ve ardından seçilen donanımda tahmini değiştirmek için sayısal sapmaları iyileştirerek elde edilir. Yaygın GPU hızlandırıcılar genelinde bu hardware-triggered backdoors'ların güvenilir bir şekilde oluşturulabileceğini ampirik olarak göstermekteyiz. Bulgularımız, üçüncü taraf modellerin kullanımını etkileyen yeni bir saldırı vektörünü ortaya koymaktadır ve bu tehditle mücadele etmek için farklı savunma mekanizmalarını araştırmaktayız."
    }
  },
  {
    "id": "2601.21898v1",
    "title": "Making Models Unmergeable via Scaling-Sensitive Loss Landscape",
    "authors": [
      "Minwoo Jang",
      "Hoyoung Kim",
      "Jabin Koo",
      "Jungseul Ok"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.21898v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21898v1",
    "content": {
      "en": "The rise of model hubs has made it easier to access reusable model components, making model merging a practical tool for combining capabilities. Yet, this modularity also creates a \\emph{governance gap}: downstream users can recompose released weights into unauthorized mixtures that bypass safety alignment or licensing terms. Because existing defenses are largely post-hoc and architecture-specific, they provide inconsistent protection across diverse architectures and release formats in practice. To close this gap, we propose \\textsc{Trap}$^{2}$, an architecture-agnostic protection framework that encodes protection into the update during fine-tuning, regardless of whether they are released as adapters or full models. Instead of relying on architecture-dependent approaches, \\textsc{Trap}$^{2}$ uses weight re-scaling as a simple proxy for the merging process. It keeps released weights effective in standalone use, but degrades them under re-scaling that often arises in merging, undermining unauthorized merging.",
      "tr": "Elbette, makalenizin başlığını ve özetini akademik ve resmi bir dille Türkçeye çevirdim. Teknik terimleri belirttiğiniz gibi İngilizce orijinal halleriyle bıraktım:\n\n**Makale Başlığı:** Making Models Unmergeable via Scaling-Sensitive Loss Landscape\n\n**Özet:**\n\nModel merkezlerinin (model hubs) yükselişi, yeniden kullanılabilir model bileşenlerine erişimi kolaylaştırmış ve model birleştirme (model merging) işlemini yetenekleri birleştirmek için pratik bir araç haline getirmiştir. Ancak bu modülerlik aynı zamanda bir *yönetişim boşluğu* (governance gap) yaratmaktadır: Son kullanıcılar, yayımlanan ağırlıkları (weights) güvenlik uyumluluğunu (safety alignment) veya lisans koşullarını atlayan yetkisiz karışımlara yeniden birleştirebilirler. Mevcut savunma mekanizmalarının büyük ölçüde sonradan müdahale eden (post-hoc) ve mimariye özgü (architecture-specific) olması nedeniyle, pratikte çeşitli mimariler ve yayın formatları genelinde tutarsız bir koruma sağlamaktadırlar. Bu boşluğu kapatmak için, mimariden bağımsız (architecture-agnostic) bir koruma çerçevesi olan \\textsc{Trap}$^{2}$'yi öneriyoruz. Bu çerçeve, ağırlıklar adaptörler (adapters) olarak mı yoksa tam modeller (full models) olarak mı yayımlandığına bakılmaksızın, ince ayar (fine-tuning) sırasında güncellemelerin içine korumayı kodlar. Mimariye bağımlı yaklaşımlara güvenmek yerine, \\textsc{Trap}$^{2}$ birleştirme (merging) işleminin basit bir vekilini (proxy) olarak ağırlık yeniden ölçeklendirmeyi (weight re-scaling) kullanır. Bu yaklaşım, yayımlanan ağırlıkları bağımsız kullanımda etkili tutarken, birleştirme (merging) sırasında sıklıkla ortaya çıkan yeniden ölçeklendirme (re-scaling) altında performanslarını düşürerek yetkisiz birleştirmeyi (unauthorized merging) baltalamaktadır."
    }
  },
  {
    "id": "2601.21682v1",
    "title": "FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning",
    "authors": [
      "Xiaoyu Xu",
      "Minxin Du",
      "Kun Fang",
      "Zi Liang",
      "Yaxin Xiao"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.21682v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21682v1",
    "content": {
      "en": "Large language models (LLMs) demonstrate impressive capabilities across diverse tasks but raise concerns about privacy, copyright, and harmful materials. Existing LLM unlearning methods rarely consider the continual and high-volume nature of real-world deletion requests, which can cause utility degradation and catastrophic forgetting as requests accumulate. To address this challenge, we introduce \\fit, a framework for continual unlearning that handles large numbers of deletion requests while maintaining robustness against both catastrophic forgetting and post-unlearning recovery. \\fit mitigates degradation through rigorous data \\underline{F}iltering, \\underline{I}mportance-aware updates, and \\underline{T}argeted layer attribution, enabling stable performance across long sequences of unlearning operations and achieving a favorable balance between forgetting effectiveness and utility retention. To support realistic evaluation, we present \\textbf{PCH}, a benchmark covering \\textbf{P}ersonal information, \\textbf{C}opyright, and \\textbf{H}armful content in sequential deletion scenarios, along with two symmetric metrics, Forget Degree (F.D.) and Retain Utility (R.U.), which jointly assess forgetting quality and utility preservation. Extensive experiments on four open-source LLMs with hundreds of deletion requests show that \\fit achieves the strongest trade-off between F.D. and R.U., surpasses existing methods on MMLU, CommonsenseQA, and GSM8K, and remains resistant against both relearning and quantization recovery attacks.",
      "tr": "**Makale Başlığı: FIT: Continual LLM Unlearning'de Katastrofik Unutmaya Meydan Okuma**\n\n**Özet:**\n\nBüyük dil modelleri (LLM'ler) çeşitli görevlerde etkileyici yetenekler sergilerken, gizlilik, telif hakkı ve zararlı materyaller konusunda endişelere yol açmaktadır. Mevcut LLM unlearning yöntemleri, gerçek dünya silme isteklerinin devam eden ve yüksek hacimli doğasını nadiren dikkate almaktadır; bu durum, istekler biriktikçe fayda kaybına ve katastrofik forgetting'e neden olabilir. Bu zorluğun üstesinden gelmek için, çok sayıda silme isteğini yöneten ve hem katastrofik forgetting'e hem de unlearning sonrası kurtarmaya karşı dayanıklılığı koruyan continual unlearning için bir framework olan \\fit'i tanıtıyoruz. \\fit, titiz veri \\underline{F}iltering, \\underline{I}mportance-aware updates ve \\underline{T}argeted layer attribution aracılığıyla degradation'ı azaltır, bu da uzun unlearning operasyon dizileri boyunca istikrarlı bir performans sağlar ve forgetting effectiveness ile utility retention arasında uygun bir denge elde eder. Gerçekçi değerlendirmeyi desteklemek için, sıralı silme senaryolarında \\textbf{P}ersonal information, \\textbf{C}opyright ve \\textbf{H}armful content'i kapsayan bir benchmark olan \\textbf{PCH}'yi ve forgetting kalitesi ile utility preservation'ı birlikte değerlendiren iki simetrik metrik olan Forget Degree (F.D.) ve Retain Utility (R.U.)'yu sunuyoruz. Yüzlerce silme isteği ile dört açık kaynak LLM üzerinde yapılan kapsamlı deneyler, \\fit'in F.D. ve R.U. arasında en güçlü trade-off'u elde ettiğini, MMLU, CommonsenseQA ve GSM8K'da mevcut yöntemleri geride bıraktığını ve hem relearning hem de quantization recovery attacks'lara karşı dirençli kaldığını göstermektedir."
    }
  },
  {
    "id": "2601.21636v1",
    "title": "Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation",
    "authors": [
      "Jan Schuchardt",
      "Nikita Kalinin"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "link": "http://arxiv.org/abs/2601.21636v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21636v1",
    "content": {
      "en": "We study privacy amplification for differentially private model training with matrix factorization under random allocation (also known as the balls-in-bins model). Recent work by Choquette-Choo et al. (2025) proposes a sampling-based Monte Carlo approach to compute amplification parameters in this setting. However, their guarantees either only hold with some high probability or require random abstention by the mechanism. Furthermore, the required number of samples for ensuring $(ε,δ)$-DP is inversely proportional to $δ$. In contrast, we develop sampling-free bounds based on Rényi divergence and conditional composition. The former is facilitated by a dynamic programming formulation to efficiently compute the bounds. The latter complements it by offering stronger privacy guarantees for small $ε$, where Rényi divergence bounds inherently lead to an over-approximation. Our framework applies to arbitrary banded and non-banded matrices. Through numerical comparisons, we demonstrate the efficacy of our approach across a broad range of matrix mechanisms used in research and practice.",
      "tr": "**Makale Başlığı:** Rastgele Dağılım Altında Matris Mekanizmaları İçin Örneklemesiz Gizlilik Hesaplaması\n\n**Özet:**\n\nBu çalışmada, rastgele dağılım (diğer adıyla 'balls-in-bins' modeli) altında matris çarpanlarına ayırma ile differansiyel olarak özel model eğitiminde gizlilik amplifikasyonu konusunu inceliyoruz. Choquette-Choo ve arkadaşları (2025) tarafından yakın zamanda önerilen çalışma, bu bağlamda amplifikasyon parametrelerini hesaplamak için örneklem tabanlı bir Monte Carlo yaklaşımı sunmaktadır. Ancak, bu yaklaşımın sağladığı güvenceler ya sadece yüksek bir olasılıkla geçerlidir ya da mekanizma tarafından rastgele soyutlama gerektirir. Dahası, $(ε,δ)$-DP'yi sağlamak için gereken örneklem sayısı $δ$'ya ters orantılıdır. Buna karşılık, biz Rényi divergence ve conditional composition'a dayanan örneklemesiz sınırlar geliştiriyoruz. Birincisi, sınırları verimli bir şekilde hesaplamak için dinamik programlama formülasyonu ile kolaylaştırılmıştır. İkincisi ise, Rényi divergence sınırlarımızın doğası gereği aşırı yaklaşıma yol açtığı küçük $ε$ değerleri için daha güçlü gizlilik güvenceleri sunarak ilkini tamamlar. Geliştirdiğimiz çerçeve, rastgele bantlı ve bantlı olmayan matrislere uygulanabilir. Sayısal karşılaştırmalar aracılığıyla, araştırma ve uygulamada kullanılan geniş bir matris mekanizmaları yelpazesinde yaklaşımımızın etkinliğini göstermekteyiz."
    }
  },
  {
    "id": "2601.21628v1",
    "title": "Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise",
    "authors": [
      "Puwei Lian",
      "Yujun Cai",
      "Songze Li",
      "Bingkun Bao"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.21628v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21628v1",
    "content": {
      "en": "Diffusion models have achieved remarkable progress in image generation, but their increasing deployment raises serious concerns about privacy. In particular, fine-tuned models are highly vulnerable, as they are often fine-tuned on small and private datasets. Membership inference attacks (MIAs) are used to assess privacy risks by determining whether a specific sample was part of a model's training data. Existing MIAs against diffusion models either assume obtaining the intermediate results or require auxiliary datasets for training the shadow model. In this work, we utilized a critical yet overlooked vulnerability: the widely used noise schedules fail to fully eliminate semantic information in the images, resulting in residual semantic signals even at the maximum noise step. We empirically demonstrate that the fine-tuned diffusion model captures hidden correlations between the residual semantics in initial noise and the original images. Building on this insight, we propose a simple yet effective membership inference attack, which injects semantic information into the initial noise and infers membership by analyzing the model's generation result. Extensive experiments demonstrate that the semantic initial noise can strongly reveal membership information, highlighting the vulnerability of diffusion models to MIAs.",
      "tr": "**Makale Başlığı:** Gürültü Bir Sorgulama Aracı Olarak: Başlangıç Gürültüsünden Faydalanan Yayılma Modellerine Yönelik Üyelik Çıkarım Saldırıları\n\n**Özet:**\n\nYayılma modelleri (Diffusion models), görüntü üretiminde dikkate değer ilerlemeler kaydetmiş olsa da, giderek artan kullanımları ciddi gizlilik endişelerine yol açmaktadır. Özellikle, ince ayarlanmış (fine-tuned) modeller, genellikle küçük ve özel veri kümeleri üzerinde ince ayarlandıkları için yüksek düzeyde savunmasızdır. Üyelik çıkarım saldırıları (Membership inference attacks - MIAs), belirli bir örneğin bir modelin eğitim verisinin bir parçası olup olmadığını belirleyerek gizlilik risklerini değerlendirmek için kullanılır. Yayılma modellerine karşı mevcut MIAs, ya ara sonuçların elde edilmesini varsayar ya da gölge modelin (shadow model) eğitilmesi için yardımcı veri kümeleri gerektirir. Bu çalışmada, kritik ancak göz ardı edilmiş bir zafiyetten yararlandık: yaygın olarak kullanılan gürültü çizelgeleri (noise schedules), görüntülerdeki anlamsal bilgiyi tamamen ortadan kaldırmakta başarısız olarak, maksimum gürültü adımında bile artık anlamsal sinyaller bırakmaktadır. İnce ayarlanmış yayılma modelinin, başlangıç gürültüsündeki artık anlamsal özellikler ile orijinal görüntüler arasındaki gizli korelasyonları yakaladığını ampirik olarak gösteriyoruz. Bu içgörüden yola çıkarak, başlangıç gürültüsüne anlamsal bilgi enjekte eden ve modelin üretim sonucunu analiz ederek üyelik çıkarımı yapan basit ancak etkili bir üyelik çıkarım saldırısı öneriyoruz. Kapsamlı deneyler, anlamsal başlangıç gürültüsünün üyelik bilgilerini güçlü bir şekilde ortaya çıkarabileceğini göstermekte ve yayılma modellerinin MIAs'a karşı savunmasızlığını vurgulamaktadır."
    }
  },
  {
    "id": "2601.21531v1",
    "title": "On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression",
    "authors": [
      "Xinwei Zhang",
      "Hangcheng Liu",
      "Li Bai",
      "Hao Wang",
      "Qingqing Ye"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2601.21531v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21531v1",
    "content": {
      "en": "Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.",
      "tr": "Elbette, istenen çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** Görüntüsel Token Sıkıştırması Altında Büyük Görsel-Dil Modellerinin Rakip Dayanıklılığı Üzerine\n\n**Özet:**\n\nGörüntüsel token sıkıştırması, görsel tokenların budanması veya birleştirilmesi yoluyla büyük görsel-dil modellerini (LVLMs) hızlandırmak için yaygın olarak kullanılmaktadır, ancak rakip dayanıklılığı henüz keşfedilmemiştir. Mevcut encoder tabanlı saldırıların, sıkıştırılmış LVLMs'lerin dayanıklılığını önemli ölçüde abartabildiğini göstermekteyiz. Bunun nedeni bir optimizasyon-çıkarım uyumsuzluğudur: pertürbasyonlar tam-token temsili üzerinde optimize edilirken, çıkarım bir token-sıkıştırma darboğazı aracılığıyla gerçekleştirilir. Bu açığı gidermek için, konuşlandırılmış sıkıştırma mekanizmasına veya token bütçesine erişim varsaymadan, pertürbasyon optimizasyonunu sıkıştırma çıkarımıyla hizalayan Compression-AliGnEd saldırısını (CAGE) öneriyoruz. CAGE şunları birleştirir: (i) makul bütçeler boyunca hayatta kalması muhtemel tokenlar üzerindeki distorsiyonu yoğunlaştıran beklenen özellik bozulması ve (ii) derece-distorsiyon hizalaması, ki bu, derece puanlarıyla token distorsiyonlarını aktif olarak hizalayarak yüksek düzeyde bozulmuş kanıtların tutulmasını teşvik eder. Çeşitli temsili tak-çalıştır sıkıştırma mekanizmaları ve veri kümeleri genelinde elde ettiğimiz sonuçlar, CAGE'nin temel modele göre tutarlı bir şekilde daha düşük sağlam doğruluk elde ettiğini göstermektedir. Bu çalışma, sıkıştırmayı göz ardı eden dayanıklılık değerlendirmelerinin aşırı iyimser olabileceğini vurgulamakta, verimli LVLMs'ler için sıkıştırmaya duyarlı güvenlik değerlendirmesi ve savunmalarına işaret etmektedir."
    }
  },
  {
    "id": "2601.21189v1",
    "title": "Adaptive and Robust Cost-Aware Proof of Quality for Decentralized LLM Inference Networks",
    "authors": [
      "Arther Tian",
      "Alex Ding",
      "Frank Chen",
      "Simon Wu",
      "Aaron Chan"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.21189v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21189v1",
    "content": {
      "en": "Decentralized large language model inference networks require lightweight mechanisms to reward high quality outputs under heterogeneous latency and cost. Proof of Quality provides scalable verification by sampling evaluator nodes that score candidate outputs, then aggregating their scores into a consensus signal that determines rewards. However, evaluator heterogeneity and malicious score manipulation can distort consensus and inflate payouts, which weakens incentive alignment in open participation settings.   This paper extends a cost-aware Proof of Quality mechanism by adding adversary-resilient consensus formation. We study robust aggregation rules, including median and trimmed mean, and an adaptive trust-weighted consensus that updates evaluator weights from deviation signals. Using question answering and summarization workloads with a ground truth proxy for offline analysis, we quantify evaluator reliability and show strong variance across evaluators, including task-dependent misalignment that can invert correlations. We then evaluate robustness under four adversarial strategies, including noise injection, boosting, sabotage, and intermittent manipulation, across a sweep of malicious ratios and evaluator sample sizes. Our results show that robust aggregation improves consensus alignment with the ground truth proxy and reduces sensitivity to noisy and strategic attacks compared with simple averaging. We further characterize the operational trade-off introduced by evaluator sampling, where larger evaluator sets reduce evaluator rewards and increase payoff variance while inference rewards remain relatively stable in our configuration. These findings motivate robust consensus as a default component for cost-aware Proof of Quality and provide practical guidance for selecting evaluator sampling parameters under adversarial risk and resource constraints.",
      "tr": "**Akademik Makale Başlığı ve Özeti Çevirisi**\n\n**Makale Başlığı:** Adaptive and Robust Cost-Aware Proof of Quality for Decentralized LLM Inference Networks\n\n**Özet:**\n\nMerkeziyetsiz large language model inference networks, heterojen gecikme ve maliyet altında yüksek kaliteli çıktılara ödül vermek için hafif mekanizmalara ihtiyaç duyar. Proof of Quality, aday çıktıları puanlayan değerlendirici düğümleri örnekleyerek, ardından puanlarını ödülleri belirleyen bir fikir birliği sinyaline dönüştürerek ölçeklenebilir doğrulama sağlar. Ancak, değerlendirici heterojenliği ve kötü niyetli puan manipülasyonu, fikir birliğini bozabilir ve ödemeleri şişirebilir, bu da açık katılım ortamlarında teşvik uyumunu zayıflatır. Bu makale, hasım-dirençli fikir birliği oluşumu ekleyerek cost-aware Proof of Quality mekanizmasını genişletmektedir. median ve trimmed mean gibi robust aggregation kurallarını ve sapma sinyallerinden değerlendirici ağırlıklarını güncelleyen adaptive trust-weighted consensus'u inceliyoruz. Çevrimdışı analiz için bir ground truth proxy ile question answering ve summarization iş yükleri kullanarak, değerlendirici güvenilirliğini ölçüyor ve puanlar arasındaki güçlü değişkenliği, ters korelasyonlara neden olabilecek görev bağımlı uyumsuzluklar dahil olmak üzere gösteriyoruz. Ardından, kötü niyetli oranların ve değerlendirici örneklem boyutlarının bir incelemesi boyunca gürültü enjeksiyonu, boosting, sabotage ve aralıklı manipülasyon dahil dört hasım stratejisi altındaki robustness'u değerlendiriyoruz. Sonuçlarımız, robust aggregation'ın, basit ortalamaya kıyasla fikir birliğini ground truth proxy ile uyumunu iyileştirdiğini ve gürültülü ve stratejik saldırılara duyarlılığı azalttığını göstermektedir. Ayrıca, değerlendirici örneklemesinin getirdiği operasyonel trade-off'u karakterize ediyoruz; burada daha büyük değerlendirici kümeleri değerlendirici ödüllerini azaltır ve ödeme değişkenliğini artırırken, yapılandırmamızda inference ödülleri nispeten stabil kalmaktadır. Bu bulgular, cost-aware Proof of Quality için varsayılan bir bileşen olarak robust consensus'u teşvik etmekte ve hasım riski ve kaynak kısıtlamaları altında değerlendirici örnekleme parametrelerini seçmek için pratik rehberlik sağlamaktadır."
    }
  },
  {
    "id": "2601.21051v1",
    "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
    "authors": [
      "Zhuoran Yang",
      "Ed Li",
      "Jianliang He",
      "Aman Priyanshu",
      "Baturay Saglam"
    ],
    "published_date": "2026-01-28",
    "tags": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.21051v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21051v1",
    "content": {
      "en": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.",
      "tr": "**Makale Başlığı:** Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Teknik Raporu\n\n**Özet:**\n\nBu çalışmada, siber güvenlik alanına özgü ilk açık kaynaklı native reasoning modeli olan Foundation-Sec-8B-Reasoning'i sunmaktayız. Daha önce yayımladığımız Foundation-Sec-8B temel modeli üzerine inşa edilen bu model (Llama-3.1-8B-Base'den türetilmiştir), denetimli ince ayar (SFT) ve doğrulanabilir ödüllerden pekiştirmeli öğrenme (RLVR) yöntemlerinin birleşimini içeren iki aşamalı bir süreçle eğitilmiştir. Eğitimimiz, siber güvenlik analizi, talimat takibi ve matematiksel reasoning alanlarını kapsayan özel reasoning verilerinden faydalanmaktadır. 10 siber güvenlik ve 10 genel amaçlı benchmark üzerinde yapılan değerlendirmeler, daha büyük modellere kıyasla siber güvenlik görevlerinde rekabetçi bir performans sergilediğini ve güçlü genel yeteneklerini koruduğunu göstermektedir. Model, multi-hop reasoning görevlerinde etkili bir genelleme yeteneği ve uygun sistem promptları ve guardrail'ler ile konuşlandırıldığında güçlü bir safety performance sergilemektedir. Bu çalışma, alana özel reasoning modellerinin geniş genel yeteneklerini korurken özel görevlerde güçlü performans gösterebileceğini ortaya koymaktadır. Modeli https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning adresinden kamuya açık olarak sunuyoruz."
    }
  }
]