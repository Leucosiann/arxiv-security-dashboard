[
  {
    "id": "2601.05988v1",
    "title": "CyberGFM: Graph Foundation Models for Lateral Movement Detection in Enterprise Networks",
    "authors": [
      "Isaiah J. King",
      "Bernardo Trindade",
      "Benjamin Bowman",
      "H. Howie Huang"
    ],
    "published_date": "2026-01-09",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.05988v1",
    "pdf_link": "https://arxiv.org/pdf/2601.05988v1",
    "content": {
      "en": "Representing networks as a graph and training a link prediction model using benign connections is an effective method of anomaly-based intrusion detection. Existing works using this technique have shown great success using temporal graph neural networks and skip-gram-based approaches on random walks. However, random walk-based approaches are unable to incorporate rich edge data, while the GNN-based approaches require large amounts of memory to train. In this work, we propose extending the original insight from random walk-based skip-grams--that random walks through a graph are analogous to sentences in a corpus--to the more modern transformer-based foundation models. Using language models that take advantage of GPU optimizations, we can quickly train a graph foundation model to predict missing tokens in random walks through a network of computers. The graph foundation model is then finetuned for link prediction and used as a network anomaly detector. This new approach allows us to combine the efficiency of random walk-based methods and the rich semantic representation of deep learning methods. This system, which we call CyberGFM, achieved state-of-the-art results on three widely used network anomaly detection datasets, delivering a up to 2$\\times$ improvement in average precision. We found that CyberGFM outperforms all prior works in unsupervised link prediction for network anomaly detection, using the same number of parameters, and with equal or better efficiency than the previous best approaches.",
      "tr": "Makale Başlığı: Kurumsal Ağlarda Yanal Hareket Tespiti İçin SiberGFM: Graph Foundation Models\n\nÖzet:\nAğları bir grafik olarak temsil etmek ve kötü niyetli olmayan bağlantıları kullanarak bir link prediction modeli eğitmek, anomali tabanlı saldırı tespiti için etkili bir yöntemdir. Bu tekniği kullanan mevcut çalışmalar, rastgele yürüyüşler üzerinde zamansal grafik sinir ağları (temporal graph neural networks) ve skip-gram tabanlı yaklaşımlar kullanarak büyük başarı göstermiştir. Ancak, rastgele yürüyüş tabanlı yaklaşımlar zengin kenar verilerini (rich edge data) dahil edememekte, GNN tabanlı yaklaşımlar ise eğitmek için büyük miktarda bellek gerektirmektedir. Bu çalışmada, rastgele yürüyüş tabanlı skip-gram'lardan gelen orijinal içgörüyü - grafik üzerinden rastgele yürüyüşlerin bir veri kümesindeki cümlelere benzetilmesi - daha modern transformer tabanlı foundation models'lara genişletmeyi öneriyoruz. GPU optimizasyonlarından faydalanan dil modellerini kullanarak, bir bilgisayar ağındaki rastgele yürüyüşlerde eksik token'ları (missing tokens) tahmin etmek üzere bir graph foundation modelini hızla eğitebiliriz. Ardından, graph foundation model link prediction için finetune edilerek bir ağ anomali dedektörü olarak kullanılır. Bu yeni yaklaşım, rastgele yürüyüş tabanlı yöntemlerin verimliliğini ve deep learning yöntemlerinin zengin anlamsal temsilini (rich semantic representation) birleştirmemize olanak tanır. SiberGFM olarak adlandırdığımız bu sistem, üç yaygın olarak kullanılan ağ anomali tespiti veri kümesi üzerinde state-of-the-art sonuçlar elde etmiş ve ortalama precision'da (average precision) 2 katına kadar iyileşme sağlamıştır. SiberGFM'nin, parametre sayısı aynı olmakla birlikte, önceki en iyi yaklaşımlarla eşit veya daha iyi verimlilikle, ağ anomali tespiti için denetimsiz link prediction'da (unsupervised link prediction) tüm önceki çalışmalardan daha iyi performans gösterdiğini bulduk."
    }
  },
  {
    "id": "2601.05918v1",
    "title": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
    "authors": [
      "Tianshi Li"
    ],
    "published_date": "2026-01-09",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "link": "http://arxiv.org/abs/2601.05918v1",
    "pdf_link": "https://arxiv.org/pdf/2601.05918v1",
    "content": {
      "en": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
      "tr": "Elbette, makale başlığını ve özetini istediğiniz şekilde Türkçeye çevirdim:\n\n**Makale Başlığı:** Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset\n\n**Özet:**\n4 Aralık 2025 tarihinde Anthropic, nicel görüşmeleri geniş ölçekte yürütmek için tasarlanmış bir yapay zeka aracı olan Anthropic Interviewer'ı ve araştırmada yapay zeka kullanımına ilişkin 125'i bilim insanı olmak üzere 1.250 profesyonel ile yapılan görüşmelerden oluşan bir kamu veri setini yayınlamıştır. Bilim insanı alt kümesine odaklanarak, web araması ve agentic yeteneklere sahip yaygın olarak erişilebilen LLM'lerin, yirmi dört görüşmenin altısını belirli bilimsel çalışmalarla ilişkilendirebildiğini, ilgili yazarları geri alabildiğini ve bazı durumlarda görüşmecileri benzersiz bir şekilde tanımlayabildiğini göstermekteyim. Katkım, modern LLM tabanlı ajanların bu tür yeniden tanımlama saldırılarını kolay ve düşük çaba gerektiren hale getirdiğini göstermektir: hazır araçlar, birkaç doğal dil istemiyle web'de arama yapabilir, ayrıntıları çapraz referansla kontrol edebilir ve olası eşleşmeler önerebilir, böylece teknik engeli etkili bir şekilde düşürmektedir. Yeniden tanımlamayı zararsız görevlere bölerek mevcut güvenlik önlemleri aşılabilir. Saldırıyı üst düzeyde özetlemekte, LLM ajanları çağında zengin nitel verilerin yayınlanmasına ilişkin çıkarımları tartışmakta ve azaltma önerileri ile açık sorunları sunmaktayım. Bulgularımı Anthropic'e bildirmiş bulunmaktayım."
    }
  },
  {
    "id": "2601.05828v1",
    "title": "Influence of Parallelism in Vector-Multiplication Units on Correlation Power Analysis",
    "authors": [
      "Manuel Brosch",
      "Matthias Probst",
      "Stefan Kögler",
      "Georg Sigl"
    ],
    "published_date": "2026-01-09",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.IR"
    ],
    "link": "http://arxiv.org/abs/2601.05828v1",
    "pdf_link": "https://arxiv.org/pdf/2601.05828v1",
    "content": {
      "en": "The use of neural networks in edge devices is increasing, which introduces new security challenges related to the neural networks' confidentiality. As edge devices often offer physical access, attacks targeting the hardware, such as side-channel analysis, must be considered. To enhance the performance of neural network inference, hardware accelerators are commonly employed. This work investigates the influence of parallel processing within such accelerators on correlation-based side-channel attacks that exploit power consumption. The focus is on neurons that are part of the same fully-connected layer, which run parallel and simultaneously process the same input value. The theoretical impact of concurrent multiply-and-accumulate operations on overall power consumption is evaluated, as well as the success rate of correlation power analysis. Based on the observed behavior, equations are derived that describe how the correlation decreases with increasing levels of parallelism. The applicability of these equations is validated using a vector-multiplication unit implemented on an FPGA.",
      "tr": "**Makale Başlığı:** Influence of Parallelism in Vector-Multiplication Units on Correlation Power Analysis\n\n**Özet:**\n\nKenar cihazlarda yapay sinir ağlarının kullanımının artması, bu ağların gizliliğiyle ilgili yeni güvenlik zorlukları ortaya çıkarmaktadır. Kenar cihazlar genellikle fiziksel erişim imkanı sunduğundan, yan kanal analizi gibi donanımı hedef alan saldırılar göz önünde bulundurulmalıdır. Yapay sinir ağı çıkarım performansını artırmak için donanım hızlandırıcıları yaygın olarak kullanılmaktadır. Bu çalışma, söz konusu hızlandırıcılardaki paralel işlemenin, güç tüketimini kullanan korelasyon tabanlı yan kanal saldırıları üzerindeki etkisini incelemektedir. Odak noktası, aynı tam bağlı katmanın parçası olan ve aynı girdi değerini paralel ve eş zamanlı olarak işleyen nöronlardır. Eş zamanlı çarpma-biriktirme (multiply-and-accumulate) işlemlerinin toplam güç tüketimi üzerindeki teorik etkisi ile korelasyon gücü analizinin (correlation power analysis) başarı oranı değerlendirilmektedir. Gözlemlenen davranışa dayanarak, korelasyonun paralelcilik düzeyi arttıkça nasıl azaldığını tanımlayan denklemler türetilmiştir. Bu denklemlerin uygulanabilirliği, bir FPGA üzerinde uygulanan bir vector-multiplication unit kullanılarak doğrulanmıştır."
    }
  },
  {
    "id": "2601.05755v1",
    "title": "VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit",
    "authors": [
      "Junda Lin",
      "Zhaomeng Zhou",
      "Zhi Zheng",
      "Shuochen Liu",
      "Tong Xu"
    ],
    "published_date": "2026-01-09",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.05755v1",
    "pdf_link": "https://arxiv.org/pdf/2601.05755v1",
    "content": {
      "en": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \\textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \\textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \\textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \\textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility. Code is available at https://anonymous.4open.science/r/VIGIL-378B/.",
      "tr": "**Makale Başlığı:** VIGIL: LLM Agent'larını Verify-Before-Commit Aracılığıyla Tool Stream Injection'a Karşı Savunmak\n\n**Özet:**\nAçık ortamlarda çalışan LLM agent'ları, özellikle manipüle edilmiş metadata ve çalışma zamanı geri bildirimlerinin yürütme akışını ele geçirdiği tool stream'de, dolaylı prompt injection'dan kaynaklanan artan risklerle karşı karşıyadır. Mevcut savunmalar, gelişmiş modellerin sıkı hizalamaları nedeniyle enjekte edilmiş kurallara öncelik vermesi ve statik koruma mekanizmalarının uyarlanabilir reasoning için gereken geri bildirim döngüsünü kesmesi gibi kritik bir ikilemle karşılaşır. Bu çatışmayı çözmek için, kısıtlayıcı izolasyondan verify-before-commit protokolüne paradigma değiştiren \\textbf{VIGIL} çerçevesini öneriyoruz. Spekülatif hipotez üretimini kolaylaştırarak ve niyet temelli verification yoluyla güvenliği sağlayarak, \\textbf{VIGIL} sağlam kontrol sağlarken reasoning esnekliğini korur. Ayrıca, dinamik bağımlılıklarla karakterize edilen yaygın tehditleri simüle etmek üzere tasarlanmış 959 tool stream injection vakasından oluşan bir benchmark olan \\textbf{SIREN}'ı sunuyoruz. Kapsamlı deneyler, \\textbf{VIGIL}'in saldırı başarı oranını %22'den fazla azaltırken, statik temellere kıyasla saldırı altındaki faydayı iki katından fazla artırarak en son teknoloji dinamik savunmalardan daha iyi performans gösterdiğini ve böylece güvenlik ile fayda arasında optimum bir denge sağladığını göstermektedir. Kod https://anonymous.4open.science/r/VIGIL-378B/ adresinde mevcuttur."
    }
  },
  {
    "id": "2601.05742v1",
    "title": "The Echo Chamber Multi-Turn LLM Jailbreak",
    "authors": [
      "Ahmad Alobaid",
      "Martí Jordà Roca",
      "Carlos Castillo",
      "Joan Vendrell"
    ],
    "published_date": "2026-01-09",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.05742v1",
    "pdf_link": "https://arxiv.org/pdf/2601.05742v1",
    "content": {
      "en": "The availability of Large Language Models (LLMs) has led to a new generation of powerful chatbots that can be developed at relatively low cost. As companies deploy these tools, security challenges need to be addressed to prevent financial loss and reputational damage. A key security challenge is jailbreaking, the malicious manipulation of prompts and inputs to bypass a chatbot's safety guardrails. Multi-turn attacks are a relatively new form of jailbreaking involving a carefully crafted chain of interactions with a chatbot. We introduce Echo Chamber, a new multi-turn attack using a gradual escalation method. We describe this attack in detail, compare it to other multi-turn attacks, and demonstrate its performance against multiple state-of-the-art models through extensive evaluation.",
      "tr": "İşte makale başlığı ve özetinin Türkçeye çevrilmiş hali:\n\n**Makale Başlığı:** The Echo Chamber Multi-Turn LLM Jailbreak\n\n**Özet:**\nBüyük Dil Modellerinin (LLM) yaygınlaşması, nispeten düşük maliyetle geliştirilebilen yeni nesil güçlü sohbet botlarının ortaya çıkmasına yol açmıştır. Şirketler bu araçları kullanıma sundukça, finansal kayıp ve itibar zedelenmesini önlemek için güvenlik zorluklarının ele alınması gerekmektedir. Önemli bir güvenlik zorluğu, sohbet botunun güvenlik mekanizmalarını atlatmak amacıyla prompt ve girdilerin kötü niyetli manipülasyonu olan jailbreaking'dir. Multi-turn saldırılar, sohbet botu ile dikkatlice tasarlanmış bir dizi etkileşim içeren nispeten yeni bir jailbreaking biçimidir. Biz, kademeli bir tırmanma yöntemi kullanan yeni bir multi-turn saldırı olan Echo Chamber'ı sunuyoruz. Bu saldırıyı ayrıntılı olarak tanımlıyor, diğer multi-turn saldırılarla karşılaştırıyor ve kapsamlı bir değerlendirme yoluyla birden fazla state-of-the-art model üzerinde performansını gösteriyoruz."
    }
  },
  {
    "id": "2601.05739v1",
    "title": "PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility",
    "authors": [
      "G M Shahariar",
      "Zabir Al Nazi",
      "Md Olid Hasan Bhuiyan",
      "Zhouxing Shi"
    ],
    "published_date": "2026-01-09",
    "tags": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2601.05739v1",
    "pdf_link": "https://arxiv.org/pdf/2601.05739v1",
    "content": {
      "en": "Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.",
      "tr": "**Makale Başlığı:** PII-VisBench: Görsel Dil Modellerinde Kişisel Olarak Tanımlanabilir Bilgi Güvenliğinin Görünürlük Sürekliliği Boyunca Değerlendirilmesi\n\n**Özet:**\n\nGörsel Dil Modelleri (VLMs), giderek artan bir şekilde gizlilik açısından kritik alanlara entegre edilmektedir. Ancak, kişisel olarak tanımlanabilir bilgi (PII) sızıntısına yönelik mevcut değerlendirmeler, gizliliği büyük ölçüde statik bir extraction task olarak ele almakta ve bir bireyin online varlığının – çevrimiçi olarak mevcut olan verilerinin hacminin – gizlilik uyumunu nasıl etkilediğini göz ardı etmektedir. Çevrimiçi varlığın sürekliliği aracılığıyla VLM güvenliğini değerlendirmek üzere tasarlanmış 4000 benzersiz probe içeren yeni bir benchmark olan PII-VisBench'i sunuyoruz. Benchmark, çevrimiçi olarak mevcut olan bilgilerinin kapsamı ve doğasına dayanarak 200 bireyi yüksek, orta, düşük ve sıfır olmak üzere dört visibility kategorisine ayırmaktadır. İki temel metriğe dayanarak 18 açık kaynaklı VLM (0.3B-32B) değerlendirmekteyiz: reddedilen PII probing queries yüzdesi (Refusal Rate) ve PII içeren olarak işaretlenen non-refusal response'ların oranı (Conditional PII Disclosure Rate). Modeller genelinde tutarlı bir örüntü gözlemlemekteyiz: birey visibility'si düştükçe, refusal'lar artmakta ve PII disclosure'lar azalmaktadır (yüksek %9.10'dan düşük %5.34'e). Yüksek visibility'li bireyler için modellerin PII'yi açıklama olasılığının daha yüksek olduğunu, bununla birlikte önemli model-family heterogeneity'si ve PII-type disparities'inin bulunduğunu tespit etmekteyiz. Son olarak, paraphrasing ve jailbreak-style prompt'lar, saldırı ve modele bağlı başarısızlıkları ortaya çıkarmakta, visibility-aware safety evaluation ve eğitim müdahalelerini motive etmektedir."
    }
  },
  {
    "id": "2601.05703v1",
    "title": "AIBoMGen: Generating an AI Bill of Materials for Secure, Transparent, and Compliant Model Training",
    "authors": [
      "Wiebe Vandendriessche",
      "Jordi Thijsman",
      "Laurens D'hooge",
      "Bruno Volckaert",
      "Merlijn Sebrechts"
    ],
    "published_date": "2026-01-09",
    "tags": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.05703v1",
    "pdf_link": "https://arxiv.org/pdf/2601.05703v1",
    "content": {
      "en": "The rapid adoption of complex AI systems has outpaced the development of tools to ensure their transparency, security, and regulatory compliance. In this paper, the AI Bill of Materials (AIBOM), an extension of the Software Bill of Materials (SBOM), is introduced as a standardized, verifiable record of trained AI models and their environments. Our proof-of-concept platform, AIBoMGen, automates the generation of signed AIBOMs by capturing datasets, model metadata, and environment details during training. The training platform acts as a neutral, third-party observer and root of trust. It enforces verifiable AIBOM creation for every job. The system uses cryptographic hashing, digital signatures, and in-toto attestations to ensure integrity and protect against threats such as artifact tampering by dishonest model creators. Our evaluation demonstrates that AIBoMGen reliably detects unauthorized modifications to all artifacts and can generate AIBOMs with negligible performance overhead. These results highlight the potential of AIBoMGen as a foundational step toward building secure and transparent AI ecosystems, enabling compliance with regulatory frameworks like the EUs AI Act.",
      "tr": "**Makale Başlığı:** AIBoMGen: Güvenli, Şeffaf ve Uyumlu Model Eğitimi için Bir Yapay Zeka Malzeme Beyanı (AI Bill of Materials) Oluşturulması\n\n**Özet:**\nKarmaşık yapay zeka sistemlerinin hızlı benimsenmesi, şeffaflık, güvenlik ve düzenleyici uyumluluklarını sağlamaya yönelik araçların geliştirilme hızını geride bırakmıştır. Bu makalede, Yapay Zeka Malzeme Beyanı (AI Bill of Materials - AIBOM), Yazılım Malzeme Beyanı'nın (Software Bill of Materials - SBOM) bir uzantısı olarak, eğitilmiş yapay zeka modelleri ve çevreleri için standartlaştırılmış, doğrulanabilir bir kayıt olarak tanıtılmaktadır. Kavram kanıtı platformumuz AIBoMGen, eğitim sırasında veri kümelerini, model metadata'sını ve çevre detaylarını yakalayarak imzalı AIBOM'ların oluşturulmasını otomatikleştirir. Eğitim platformu, tarafsız bir üçüncü taraf gözlemcisi ve güven kökü (root of trust) olarak işlev görür. Her iş için doğrulanabilir AIBOM oluşturulmasını zorunlu kılar. Sistem, bütünlüğü sağlamak ve kötü niyetli model yaratıcıları tarafından yapılan artifact tampering gibi tehditlere karşı korumak için kriptografik hashing, dijital imzalar ve in-toto attestations kullanır. Değerlendirmemiz, AIBoMGen'in tüm artifact'lerdeki yetkisiz değişiklikleri güvenilir bir şekilde tespit edebildiğini ve ihmal edilebilir performans yüküyle AIBOM'lar oluşturabildiğini göstermektedir. Bu sonuçlar, AIBoMGen'in güvenli ve şeffaf yapay zeka ekosistemleri inşa etme yolunda temel bir adım olarak potansiyelini vurgulamakta ve AB Yapay Zeka Yasası gibi düzenleyici çerçevelere uyumluluğu mümkün kılmaktadır."
    }
  },
  {
    "id": "2601.05587v1",
    "title": "HogVul: Black-box Adversarial Code Generation Framework Against LM-based Vulnerability Detectors",
    "authors": [
      "Jingxiao Yang",
      "Ping He",
      "Tianyu Du",
      "Sun Bing",
      "Xuhong Zhang"
    ],
    "published_date": "2026-01-09",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.05587v1",
    "pdf_link": "https://arxiv.org/pdf/2601.05587v1",
    "content": {
      "en": "Recent advances in software vulnerability detection have been driven by Language Model (LM)-based approaches. However, these models remain vulnerable to adversarial attacks that exploit lexical and syntax perturbations, allowing critical flaws to evade detection. Existing black-box attacks on LM-based vulnerability detectors primarily rely on isolated perturbation strategies, limiting their ability to efficiently explore the adversarial code space for optimal perturbations. To bridge this gap, we propose HogVul, a black-box adversarial code generation framework that integrates both lexical and syntax perturbations under a unified dual-channel optimization strategy driven by Particle Swarm Optimization (PSO). By systematically coordinating two-level perturbations, HogVul effectively expands the search space for adversarial examples, enhancing the attack efficacy. Extensive experiments on four benchmark datasets demonstrate that HogVul achieves an average attack success rate improvement of 26.05\\% over state-of-the-art baseline methods. These findings highlight the potential of hybrid optimization strategies in exposing model vulnerabilities.",
      "tr": "İşte makale başlığı ve özetinin istediğiniz şekilde çevrilmiş hali:\n\n**Makale Başlığı:** HogVul: LM Tabanlı Güvenlik Açığı Tespit Cihazlarına Karşı Siyah Kutu Çekişmeli Kod Üretimi Çerçevesi\n\n**Özet:**\nYazılım güvenlik açığı tespitindeki son gelişmeler, Language Model (LM) tabanlı yaklaşımlarla yönlendirilmiştir. Ancak, bu modeller, kritik kusurların tespit edilmesinden kaçmasına olanak tanıyan leksikal ve sözdizimsel bozulmaları (perturbations) istismar eden çekişmeli saldırılara (adversarial attacks) karşı savunmasız kalmaktadır. LM tabanlı güvenlik açığı tespit cihazlarına yönelik mevcut siyah kutu (black-box) saldırılar, öncelikli olarak izole bozulma stratejilerine dayanmaktadır, bu da optimal bozulmalar için çekişmeli kod alanını (adversarial code space) verimli bir şekilde keşfetme yeteneklerini sınırlamaktadır. Bu boşluğu doldurmak için, Particle Swarm Optimization (PSO) tarafından yönlendirilen birleşik çift kanallı optimizasyon stratejisi (unified dual-channel optimization strategy) altında hem leksikal hem de sözdizimsel bozulmaları entegre eden bir siyah kutu çekişmeli kod üretimi çerçevesi olan HogVul'u öneriyoruz. İki seviyeli bozulmaları sistematik olarak koordine ederek, HogVul çekişmeli örnekler için arama alanını (search space) etkili bir şekilde genişletir ve saldırı etkinliğini artırır. Dört kıyaslama veri kümesi üzerinde yapılan kapsamlı deneyler, HogVul'un en gelişmiş temel yöntemlere (state-of-the-art baseline methods) kıyasla ortalama %26,05'lik bir saldırı başarı oranı iyileştirmesi elde ettiğini göstermektedir. Bu bulgular, modelin savunmasızlıklarını ortaya çıkarmada hibrit optimizasyon stratejilerinin potansiyelini vurgulamaktadır."
    }
  },
  {
    "id": "2601.05466v1",
    "title": "Jailbreaking Large Language Models through Iterative Tool-Disguised Attacks via Reinforcement Learning",
    "authors": [
      "Zhaoqi Wang",
      "Zijian Zhang",
      "Daqing He",
      "Pengtao Kou",
      "Xin Li"
    ],
    "published_date": "2026-01-09",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.05466v1",
    "pdf_link": "https://arxiv.org/pdf/2601.05466v1",
    "content": {
      "en": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse applications, however, they remain critically vulnerable to jailbreak attacks that elicit harmful responses violating human values and safety guidelines. Despite extensive research on defense mechanisms, existing safeguards prove insufficient against sophisticated adversarial strategies. In this work, we propose iMIST (\\underline{i}nteractive \\underline{M}ulti-step \\underline{P}rogre\\underline{s}sive \\underline{T}ool-disguised Jailbreak Attack), a novel adaptive jailbreak method that synergistically exploits vulnerabilities in current defense mechanisms. iMIST disguises malicious queries as normal tool invocations to bypass content filters, while simultaneously introducing an interactive progressive optimization algorithm that dynamically escalates response harmfulness through multi-turn dialogues guided by real-time harmfulness assessment. Our experiments on widely-used models demonstrate that iMIST achieves higher attack effectiveness, while maintaining low rejection rates. These results reveal critical vulnerabilities in current LLM safety mechanisms and underscore the urgent need for more robust defense strategies.",
      "tr": "**Makale Başlığı:** Pekiştirmeli Öğrenme Yoluyla Tekrarlayan Araç-Gizlenmiş Saldırılar Aracılığıyla Büyük Dil Modellerini Jailbreak Etme\n\n**Özet:**\n\nBüyük Dil Modelleri (LLM'ler), çeşitli uygulamalarda dikkate değer yetenekler sergilemiş olmalarına rağmen, insan değerlerini ve güvenlik yönergelerini ihlal eden zararlı yanıtlar oluşturan jailbreak saldırılarına karşı kritik düzeyde savunmasız kalmaktadır. Savunma mekanizmaları üzerine kapsamlı araştırmalara rağmen, mevcut korumalar gelişmiş düşmanca stratejilere karşı yetersiz kalmaktadır. Bu çalışmada, mevcut savunma mekanizmalarındaki zafiyetleri sinerjik olarak sömüren yeni bir adaptif jailbreak yöntemi olan iMIST (interactive Multi-step Progressive Tool-disguised Jailbreak Attack) önermekteyiz. iMIST, içerik filtrelerini aşmak için kötü amaçlı sorguları normal araç çağrıları olarak gizlerken, aynı zamanda gerçek zamanlı harmfulness assessment ile yönlendirilen çok turlu diyaloglar aracılığıyla yanıt zararlılığını dinamik olarak artıran interaktif bir progressive optimization algoritması sunmaktadır. Yaygın olarak kullanılan modeller üzerindeki deneylerimiz, iMIST'in düşük reddetme oranlarını korurken daha yüksek saldırı etkinliği sağladığını göstermektedir. Bu sonuçlar, mevcut LLM güvenlik mekanizmalarındaki kritik zafiyetleri ortaya koymakta ve daha sağlam savunma stratejilerine olan acil ihtiyacın altını çizmektedir."
    }
  },
  {
    "id": "2601.05445v1",
    "title": "Knowledge-Driven Multi-Turn Jailbreaking on Large Language Models",
    "authors": [
      "Songze Li",
      "Ruishi He",
      "Xiaojun Jia",
      "Jun Wang",
      "Zhihui Fu"
    ],
    "published_date": "2026-01-09",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.05445v1",
    "pdf_link": "https://arxiv.org/pdf/2601.05445v1",
    "content": {
      "en": "Large Language Models (LLMs) face a significant threat from multi-turn jailbreak attacks, where adversaries progressively steer conversations to elicit harmful outputs. However, the practical effectiveness of existing attacks is undermined by several critical limitations: they struggle to maintain a coherent progression over long interactions, often losing track of what has been accomplished and what remains to be done; they rely on rigid or pre-defined patterns, and fail to adapt to the LLM's dynamic and unpredictable conversational state. To address these shortcomings, we introduce Mastermind, a multi-turn jailbreak framework that adopts a dynamic and self-improving approach. Mastermind operates in a closed loop of planning, execution, and reflection, enabling it to autonomously build and refine its knowledge of model vulnerabilities through interaction. It employs a hierarchical planning architecture that decouples high-level attack objectives from low-level tactical execution, ensuring long-term focus and coherence. This planning is guided by a knowledge repository that autonomously discovers and refines effective attack patterns by reflecting on interactive experiences. Mastermind leverages this accumulated knowledge to dynamically recombine and adapt attack vectors, dramatically improving both effectiveness and resilience. We conduct comprehensive experiments against state-of-the-art models, including GPT-5 and Claude 3.7 Sonnet. The results demonstrate that Mastermind significantly outperforms existing baselines, achieving substantially higher attack success rates and harmfulness ratings. Moreover, our framework exhibits notable resilience against multiple advanced defense mechanisms.",
      "tr": "Makale Başlığı: Large Language Models Üzerinde Knowledge-Driven Çok Dönemli Jailbreaking Saldırıları\n\nÖzet:\nLarge Language Models (LLMs), çok dönemli jailbreak saldırıları karşısında önemli bir tehdit altındadır; bu saldırılarda saldırganlar, zararlı çıktılar elde etmek amacıyla konuşmaları aşamalı olarak yönlendirirler. Ancak, mevcut saldırıların pratik etkinliği, çeşitli kritik sınırlamalarla zayıflamaktadır: uzun etkileşimler boyunca tutarlı bir ilerlemeyi sürdürmekte zorlanırlar, neyin başarıldığını ve neyin yapılması gerektiğini sıklıkla kaçırırlar; katı veya önceden tanımlanmış desenlere güvenirler ve LLM'nin dinamik ve öngörülemeyen konuşma durumuna uyum sağlayamazlar. Bu yetersizlikleri gidermek için, dinamik ve kendi kendini geliştiren bir yaklaşım benimseyen çok dönemli bir jailbreak çerçevesi olan Mastermind'ı sunuyoruz. Mastermind, otonom olarak etkileşim yoluyla modelin zayıflıkları hakkındaki bilgisini oluşturup iyileştirmesini sağlayan planlama, yürütme ve yansıtma kapalı döngüsünde çalışır. Yüksek seviyeli saldırı hedeflerini düşük seviyeli taktiksel yürütmeden ayıran hiyerarşik bir planlama mimarisi kullanır, bu da uzun vadeli odaklanma ve tutarlılığı sağlar. Bu planlama, etkileşimli deneyimler üzerine yansıtarak etkili saldırı desenlerini otonom olarak keşfeden ve iyileştiren bir knowledge repository tarafından yönlendirilir. Mastermind, hem etkinliği hem de dayanıklılığı önemli ölçüde artırarak saldırı vektörlerini dinamik olarak yeniden birleştirmek ve uyarlamak için bu birikmiş bilgiyi kullanır. GPT-5 ve Claude 3.7 Sonnet dahil olmak üzere son teknoloji modellere karşı kapsamlı deneyler yürütüyoruz. Sonuçlar, Mastermind'ın mevcut temel performans göstergelerinden önemli ölçüde daha iyi performans gösterdiğini, önemli ölçüde daha yüksek saldırı başarı oranları ve zararlılık dereceleri elde ettiğini göstermektedir. Dahası, çerçevemiz birden fazla gelişmiş savunma mekanizmasına karşı dikkate değer bir dayanıklılık sergilemektedir."
    }
  }
]