[
  {
    "id": "2601.00559v1",
    "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?",
    "authors": [
      "Jason Quantrill",
      "Noura Khajehnouri",
      "Zihan Guo",
      "Manar H. Alalfi"
    ],
    "published_date": "2026-01-02",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.00559v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00559v1",
    "content": {
      "en": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.",
      "tr": "**Makale Başlığı:** IoT Güvenliğinin Kilidini Açmak: LLM'ler Statik Analiz Araçlarını Alt Edebilir mi?\n\n**Özet:**\n\nopenHAB gibi akıllı ev IoT platformları, cihaz davranışlarını otomatikleştirmek için Tetikleyici Eylem Koşul (TAC) kurallarına güvenmektedir. Ancak bu kurallar arasındaki etkileşim, örtük bağımlılıklardan, çelişkili tetikleyicilerden veya çakışan koşullardan kaynaklanan etkileşim tehditlerine, istenmeyen veya güvensiz davranışlara yol açabilir. Bu tehditlerin tanımlanması, geleneksel olarak sembolik, kısıtlama güdümlü static analysis'e dayanan anlamsal bir anlayış ve structural reasoning gerektirir. Bu çalışma, Large Language Models'ın (LLMs) çok kategorili bir etkileşim tehdidi taksonomisi üzerinden kapsamlı bir değerlendirmesini sunmakta olup, hem orijinal openHAB (oHC/IoTB) veri kümesinde hem de kural dönüşümleri altındaki dayanıklılığı test etmek üzere tasarlanmış yapısal olarak zorlayıcı bir Mutation veri kümesindeki performanslarını değerlendirmektedir. Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro ve DeepSeek-R1 modellerini zero-, one- ve two-shot ayarlarında kıyaslıyoruz ve sonuçlarını oHIT'in manuel olarak doğrulanmış ground truth'u ile karşılaştırıyoruz. Bulgularımız, LLM'lerin umut vadeden anlamsal bir anlayış sergilediğini, özellikle eylem ve koşulla ilgili tehditlerde başarılı olduğunu göstermektedir. Ancak, özellikle mutated rule formları altında, çapraz kural structural reasoning gerektiren tehditler için doğrulukları önemli ölçüde düşmektedir. Model performansı, tehdit kategorileri ve prompt ayarları arasında geniş ölçüde değişiklik göstermekte olup, hiçbir model tutarlı bir güvenilirlik sağlamamaktadır. Buna karşılık, sembolik reasoning baseline'ı, kural yeniden yazımlarından veya yapısal pertürbasyonlardan etkilenmeden her iki veri kümesinde de kararlı bir tespit sağlamaktadır. Bu sonuçlar, LLM'lerin tek başlarına IoT ortamlarında safety critical etkileşim tehdidi tespiti için henüz güvenilir olmadığının altını çizmektedir. Araç tasarımı üzerindeki etkilerini tartışıyor ve false positive'leri azaltırken yapısal titizliği korumak için sembolik analysis ile LLM tabanlı anlamsal yorumlamayı birleştiren hybrid architectures potansiyelini vurguluyoruz."
    }
  },
  {
    "id": "2601.00509v1",
    "title": "Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback",
    "authors": [
      "Vidyut Sriram",
      "Sawan Pandita",
      "Achintya Lakshmanan",
      "Aneesh Shamraj",
      "Suman Saha"
    ],
    "published_date": "2026-01-01",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.00509v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00509v1",
    "content": {
      "en": "Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on \"stubborn\" models.",
      "tr": "**Makale Başlığı:** Retrieval-Augmented-Generation ve Çoklu Araç Geri Bildirimi ile LLM Destekli Güvenli Kod Üretiminin İyileştirilmesi\n\n**Özet:**\n\nBüyük Dil Modelleri (LLM'ler) kod üretebilmekte ancak sıklıkla güvenlik açıkları, mantıksal tutarsızlıklar ve derleme hataları ortaya çıkarabilmektedir. Önceki çalışmalar, LLM'lerin yapılandırılmış geri bildirimlerden, statik analizden, retrieval augmentation'dan ve yürütme tabanlı iyileştirmeden önemli ölçüde fayda sağladığını göstermektedir. Bu çalışmada, tek bir kod üreten LLM'nin derleyici tanıları, CodeQL güvenlik taraması ve KLEE sembolik yürütme kullanarak çıktılarını yinelemeli olarak iyileştirdiği bir retrieval-augmented, multi-tool repair workflow'u önermekteyiz. Önceki başarılı onarımların anlamsal retrieval'ı için hafif bir embedding modeli kullanılmakta, bu da üretimi yönlendiren güvenlik odaklı örnekler sunmaktadır. DeepSeek-Coder-1.3B ve CodeLlama-7B tarafından üretilen 3.242 programdan oluşan birleşik bir veri kümesi üzerinde değerlendirilen sistem, sağlamlıkta önemli iyileştirmeler göstermektedir. DeepSeek için güvenlik açıkları %96 oranında azaltılmıştır. Daha büyük olan CodeLlama modeli için ise kritik güvenlik kusuru oranı %58,55'ten %22,19'a düşürülmüştür, bu da \"inatçı\" modellerde bile araç destekli self-repair'in etkinliğini vurgulamaktadır."
    }
  },
  {
    "id": "2601.00418v1",
    "title": "Secure, Verifiable, and Scalable Multi-Client Data Sharing via Consensus-Based Privacy-Preserving Data Distribution",
    "authors": [
      "Prajwal Panth",
      "Sahaj Raj Malla"
    ],
    "published_date": "2026-01-01",
    "tags": [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.00418v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00418v1",
    "content": {
      "en": "We propose the Consensus-Based Privacy-Preserving Data Distribution (CPPDD) framework, a lightweight and post-setup autonomous protocol for secure multi-client data aggregation. The framework enforces unanimous-release confidentiality through a dual-layer protection mechanism that combines per-client affine masking with priority-driven sequential consensus locking. Decentralized integrity is verified via step (sigma_S) and data (sigma_D) checksums, facilitating autonomous malicious deviation detection and atomic abort without requiring persistent coordination. The design supports scalar, vector, and matrix payloads with O(N*D) computation and communication complexity, optional edge-server offloading, and resistance to collusion under N-1 corruptions. Formal analysis proves correctness, Consensus-Dependent Integrity and Fairness (CDIF) with overwhelming-probability abort on deviation, and IND-CPA security assuming a pseudorandom function family. Empirical evaluations on MNIST-derived vectors demonstrate linear scalability up to N = 500 with sub-millisecond per-client computation times. The framework achieves 100% malicious deviation detection, exact data recovery, and three-to-four orders of magnitude lower FLOPs compared to MPC and HE baselines. CPPDD enables atomic collaboration in secure voting, consortium federated learning, blockchain escrows, and geo-information capacity building, addressing critical gaps in scalability, trust minimization, and verifiable multi-party computation for regulated and resource-constrained environments.",
      "tr": "**Makale Başlığı:** Konsensüs Tabanlı Gizlilik Korumalı Veri Dağıtımı ile Güvenli, Doğrulanabilir ve Ölçeklenebilir Çok İstemcili Veri Paylaşımı\n\n**Özet:**\nBu çalışmada, güvenli çok istemcili veri toplama için hafif ve kurulum sonrası otonom bir protokol olan Consensus-Based Privacy-Preserving Data Distribution (CPPDD) çerçevesini öneriyoruz. Çerçeve, istemci başına affine masking ile öncelik güdümlü ardışık konsensüs kilitlemeyi birleştiren çift katmanlı bir koruma mekanizması aracılığıyla oybirliğiyle serbest bırakma gizliliğini zorunlu kılar. Decentralized integrity, step (sigma_S) ve data (sigma_D) checksum'ları aracılığıyla doğrulanır, bu da persistent coordination gerektirmeksizin otonom kötü niyetli sapma tespiti ve atomic abort'u kolaylaştırır. Tasarım, O(N*D) hesaplama ve iletişim karmaşıklığına sahip scalar, vector ve matrix payload'ları, isteğe bağlı edge-server offloading'i ve N-1 bozulmaya karşı dayanıklılığı destekler. Formal analysis, correctness, Consensus-Dependent Integrity and Fairness (CDIF) ile sapma durumunda overwhelming-probability abort ve pseudorandom function family varsayımı altında IND-CPA security'yi kanıtlar. MNIST'ten türetilmiş vektörler üzerinde yapılan ampirik değerlendirmeler, N = 500'e kadar doğrusal ölçeklenebilirlik ve istemci başına milisaniye altı hesaplama süreleri göstermiştir. Çerçeve, %100 kötü niyetli sapma tespiti, exact data recovery ve MPC ve HE baseline'larına kıyasla üç ila dört büyüklük mertebesi daha düşük FLOPs başarır. CPPDD, güvenli oylama, consortium federated learning, blockchain escrows ve geo-information capacity building alanlarında atomic collaboration'ı mümkün kılarak, regüle edilmiş ve kaynak kısıtlı ortamlar için ölçeklenebilirlik, trust minimization ve verifiable multi-party computation alanlarındaki kritik boşlukları doldurmaktadır."
    }
  },
  {
    "id": "2601.00389v1",
    "title": "NOS-Gate: Queue-Aware Streaming IDS for Consumer Gateways under Timing-Controlled Evasion",
    "authors": [
      "Muhammad Bilal",
      "Omer Tariq",
      "Hasan Ahmed"
    ],
    "published_date": "2026-01-01",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2601.00389v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00389v1",
    "content": {
      "en": "Timing and burst patterns can leak through encryption, and an adaptive adversary can exploit them. This undermines metadata-only detection in a stand-alone consumer gateway. Therefore, consumer gateways need streaming intrusion detection on encrypted traffic using metadata only, under tight CPU and latency budgets. We present a streaming IDS for stand-alone gateways that instantiates a lightweight two-state unit derived from Network-Optimised Spiking (NOS) dynamics per flow, named NOS-Gate. NOS-Gate scores fixed-length windows of metadata features and, under a $K$-of-$M$ persistence rule, triggers a reversible mitigation that temporarily reduces the flow's weight under weighted fair queueing (WFQ). We evaluate NOS-Gate under timing-controlled evasion using an executable 'worlds' benchmark that specifies benign device processes, auditable attacker budgets, contention structure, and packet-level WFQ replay to quantify queue impact. All methods are calibrated label-free via burn-in quantile thresholding. Across multiple reproducible worlds and malicious episodes, at an achieved $0.1%$ false-positive operating point, NOS-Gate attains 0.952 incident recall versus 0.857 for the best baseline in these runs. Under gating, it reduces p99.9 queueing delay and p99.9 collateral delay with a mean scoring cost of ~ 2.09 μs per flow-window on CPU.",
      "tr": "İşte akademik makale başlığının ve özetinin istenen şekilde Türkçe çevirisi:\n\n**Makale Başlığı:** NOS-Gate: Zamana Bağlı Kontrollü Kaçınma Altındaki Tüketici Ağ Geçitleri İçin Kuyruk Farkındalığına Sahip Akış Halindeki Saldırı Tespit Sistemi\n\n**Özet:**\n\nZamanlama ve ani veri paketleri şifreleme yoluyla sızdırılabilir ve adaptif bir saldırgan bunları istismar edebilir. Bu durum, bağımsız bir tüketici ağ geçidinde yalnızca meta verilere dayalı tespitin etkinliğini azaltır. Dolayısıyla, tüketici ağ geçitlerinin, sıkı CPU ve gecikme bütçeleri dahilinde, yalnızca meta verileri kullanarak şifreli trafik üzerinde akış halindeki saldırı tespitine ihtiyacı vardır. Biz, her akış için Network-Optimised Spiking (NOS) dinamiklerinden türetilen hafif bir iki durumlu birim oluşturan, NOS-Gate olarak adlandırılan, bağımsız ağ geçitleri için akış halindeki bir saldırı tespit sistemi sunuyoruz. NOS-Gate, meta veri özelliklerinin sabit uzunluklu pencerelerini puanlar ve bir $K$-of-$M$ persistence rule altında, weighted fair queueing (WFQ) altında akışın ağırlığını geçici olarak azaltan geri döndürülebilir bir azaltma (mitigation) tetikler. NOS-Gate'i, zararsız cihaz süreçlerini, denetlenebilir saldırgan bütçelerini, rekabet yapısını ve kuyruk etkisini ölçmek için paket düzeyinde WFQ tekrar oynatmasını belirten yürütülebilir bir 'worlds' benchmarkı kullanarak zamana bağlı kontrollü kaçınma altında değerlendiriyoruz. Tüm yöntemler, burn-in quantile thresholding aracılığıyla label-free olarak kalibre edilmiştir. Birden çok tekrarlanabilir 'world' ve kötü amaçlı epizotta, elde edilen %0.1 yanlış pozitif çalışma noktasında, NOS-Gate, bu çalıştırmalardaki en iyi baseline için %0.857 iken, 0.952 incident recall elde etmektedir. Gating altında, CPU üzerinde akış-penceresi başına ortalama ~2.09 μs puanlama maliyeti ile p99.9 kuyruklama gecikmesini ve p99.9 dolaylı gecikmeyi azaltır."
    }
  },
  {
    "id": "2601.00384v1",
    "title": "Engineering Attack Vectors and Detecting Anomalies in Additive Manufacturing",
    "authors": [
      "Md Mahbub Hasan",
      "Marcus Sternhagen",
      "Krishna Chandra Roy"
    ],
    "published_date": "2026-01-01",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.00384v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00384v1",
    "content": {
      "en": "Additive manufacturing (AM) is rapidly integrating into critical sectors such as aerospace, automotive, and healthcare. However, this cyber-physical convergence introduces new attack surfaces, especially at the interface between computer-aided design (CAD) and machine execution layers. In this work, we investigate targeted cyberattacks on two widely used fused deposition modeling (FDM) systems, Creality's flagship model K1 Max, and Ender 3. Our threat model is a multi-layered Man-in-the-Middle (MitM) intrusion, where the adversary intercepts and manipulates G-code files during upload from the user interface to the printer firmware. The MitM intrusion chain enables several stealthy sabotage scenarios. These attacks remain undetectable by conventional slicer software or runtime interfaces, resulting in structurally defective yet externally plausible printed parts. To counter these stealthy threats, we propose an unsupervised Intrusion Detection System (IDS) that analyzes structured machine logs generated during live printing. Our defense mechanism uses a frozen Transformer-based encoder (a BERT variant) to extract semantic representations of system behavior, followed by a contrastively trained projection head that learns anomaly-sensitive embeddings. Later, a clustering-based approach and a self-attention autoencoder are used for classification. Experimental results demonstrate that our approach effectively distinguishes between benign and compromised executions.",
      "tr": "**Makale Başlığı:** Katmanlı İmalatta Saldırı Vektörlerinin Mühendisliği ve Anormalliklerin Tespiti\n\n**Özet:**\n\nKatmanlı imalat (AM), havacılık, otomotiv ve sağlık hizmetleri gibi kritik sektörlere hızla entegre olmaktadır. Ancak bu siber-fiziksel yakınsama, özellikle bilgisayar destekli tasarım (CAD) ve makine yürütme katmanları arasındaki arayüzde yeni saldırı yüzeyleri oluşturmaktadır. Bu çalışmada, yaygın olarak kullanılan iki birleşik eriyik modelleme (FDM) sistemi olan Creality'nin amiral gemisi modeli K1 Max ve Ender 3 üzerinde hedeflenmiş siber saldırıları incelemekteyiz. Tehdit modelimiz, zararlının kullanıcı arayüzünden yazıcı firmware'ine yükleme sırasında G-code dosyalarını durdurup manipüle ettiği çok katmanlı bir Man-in-the-Middle (MitM) saldırısıdır. MitM saldırı zinciri, birkaç gizli sabotaj senaryosuna olanak tanır. Bu saldırılar, geleneksel dilimleme yazılımları veya çalışma zamanı arayüzleri tarafından tespit edilemez ve sonuç olarak yapısal olarak kusurlu ancak dışarıdan makul görünen basılı parçalar ortaya çıkarır. Bu gizli tehditlere karşı koymak için, canlı baskı sırasında üretilen yapılandırılmış makine günlüklerini analiz eden denetimsiz bir Saldırı Tespit Sistemi (IDS) önermekteyiz. Savunma mekanizmamız, sistem davranışının anlamsal temsillerini çıkarmak için dondurulmuş bir Transformer tabanlı kodlayıcı (bir BERT varyantı) kullanır, ardından anormallik duyarlı embedding'ler öğrenen karşıt olarak eğitilmiş bir projeksiyon başlığı gelir. Daha sonra sınıflandırma için kümeleme tabanlı bir yaklaşım ve bir self-attention autoencoder kullanılır. Deneysel sonuçlar, yaklaşımımızın iyi niyetli ve tehlikeye atılmış yürütmeleri etkili bir şekilde ayırt ettiğini göstermektedir."
    }
  },
  {
    "id": "2601.00367v1",
    "title": "PatchBlock: A Lightweight Defense Against Adversarial Patches for Embedded EdgeAI Devices",
    "authors": [
      "Nandish Chattopadhyay",
      "Abdul Basit",
      "Amira Guesmi",
      "Muhammad Abdullah Hanif",
      "Bassem Ouni"
    ],
    "published_date": "2026-01-01",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.00367v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00367v1",
    "content": {
      "en": "Adversarial attacks pose a significant challenge to the reliable deployment of machine learning models in EdgeAI applications, such as autonomous driving and surveillance, which rely on resource-constrained devices for real-time inference. Among these, patch-based adversarial attacks, where small malicious patches (e.g., stickers) are applied to objects, can deceive neural networks into making incorrect predictions with potentially severe consequences. In this paper, we present PatchBlock, a lightweight framework designed to detect and neutralize adversarial patches in images. Leveraging outlier detection and dimensionality reduction, PatchBlock identifies regions affected by adversarial noise and suppresses their impact. It operates as a pre-processing module at the sensor level, efficiently running on CPUs in parallel with GPU inference, thus preserving system throughput while avoiding additional GPU overhead. The framework follows a three-stage pipeline: splitting the input into chunks (Chunking), detecting anomalous regions via a redesigned isolation forest with targeted cuts for faster convergence (Separating), and applying dimensionality reduction on the identified outliers (Mitigating). PatchBlock is both model- and patch-agnostic, can be retrofitted to existing pipelines, and integrates seamlessly between sensor inputs and downstream models. Evaluations across multiple neural architectures, benchmark datasets, attack types, and diverse edge devices demonstrate that PatchBlock consistently improves robustness, recovering up to 77% of model accuracy under strong patch attacks such as the Google Adversarial Patch, while maintaining high portability and minimal clean accuracy loss. Additionally, PatchBlock outperforms the state-of-the-art defenses in efficiency, in terms of computation time and energy consumption per sample, making it suitable for EdgeAI applications.",
      "tr": "Makale Başlığı: PatchBlock: Gömülü EdgeAI Cihazlarına Yönelik Adversarial Patch'lere Karşı Hafif Bir Savunma\n\nÖzet:\nAdversarial saldırılar, otonom sürüş ve gözetim gibi EdgeAI uygulamalarında makine öğrenmesi modellerinin güvenilir dağıtımına önemli bir zorluk teşkil etmektedir ve bu uygulamalar gerçek zamanlı çıkarım için kaynak kısıtlı cihazlara dayanır. Bunlar arasında, nesnelere küçük kötü niyetli yamaların (örneğin, çıkartmalar) uygulandığı yama tabanlı adversarial saldırılar, sinir ağlarını potansiyel olarak ciddi sonuçlar doğurabilecek yanlış tahminler yapmaya ikna edebilir. Bu çalışmada, görüntülerdeki adversarial yamaları tespit etmek ve etkisiz hale getirmek için tasarlanmış hafif bir framework olan PatchBlock'u sunuyoruz. Outlier detection ve dimensionality reduction'dan yararlanarak, PatchBlock adversarial gürültüden etkilenen bölgeleri belirler ve etkilerini bastırır. Sensör seviyesinde bir ön işleme modülü olarak çalışır, GPU çıkarımı ile paralel olarak CPU'larda verimli bir şekilde çalışır, böylece ek GPU yükünden kaçınarak sistem verimini korur. Framework, üç aşamalı bir pipeline izler: girdiyi parçalara ayırma (Chunking), daha hızlı yakınsama için hedeflenmiş kesimlere sahip yeniden tasarlanmış bir isolation forest aracılığıyla anormal bölgeleri tespit etme (Separating) ve belirlenen outlier'lar üzerinde dimensionality reduction uygulama (Mitigating). PatchBlock hem model- hem de yama-agnostic'tir, mevcut pipeline'lara sonradan entegre edilebilir ve sensör girişleri ile aşağı akış modelleri arasında sorunsuz bir şekilde entegre olur. Birden fazla sinir mimarisi, benchmark veri setleri, saldırı türleri ve çeşitli edge cihazları üzerinde yapılan değerlendirmeler, PatchBlock'un tutarlı bir şekilde dayanıklılığı artırdığını, Google Adversarial Patch gibi güçlü yama saldırıları altında model doğruluğunun %77'sine kadar geri kazanım sağladığını ve aynı zamanda yüksek taşınabilirlik ve minimum temiz doğruluk kaybını koruduğunu göstermektedir. Ek olarak, PatchBlock, hesaplama süresi ve örnek başına enerji tüketimi açısından en gelişmiş savunmaları geride bırakarak EdgeAI uygulamaları için uygun hale gelmektedir."
    }
  },
  {
    "id": "2601.00270v1",
    "title": "Rectifying Adversarial Examples Using Their Vulnerabilities",
    "authors": [
      "Fumiya Morimoto",
      "Ryuto Morita",
      "Satoshi Ono"
    ],
    "published_date": "2026-01-01",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.NE"
    ],
    "link": "http://arxiv.org/abs/2601.00270v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00270v1",
    "content": {
      "en": "Deep neural network-based classifiers are prone to errors when processing adversarial examples (AEs). AEs are minimally perturbed input data undetectable to humans posing significant risks to security-dependent applications. Hence, extensive research has been undertaken to develop defense mechanisms that mitigate their threats. Most existing methods primarily focus on discriminating AEs based on the input sample features, emphasizing AE detection without addressing the correct sample categorization before an attack. While some tasks may only require mere rejection on detected AEs, others necessitate identifying the correct original input category such as traffic sign recognition in autonomous driving. The objective of this study is to propose a method for rectifying AEs to estimate the correct labels of their original inputs. Our method is based on re-attacking AEs to move them beyond the decision boundary for accurate label prediction, effectively addressing the issue of rectifying minimally perceptible AEs created using white-box attack methods. However, challenge remains with respect to effectively rectifying AEs produced by black-box attacks at a distance from the boundary, or those misclassified into low-confidence categories by targeted attacks. By adopting a straightforward approach of only considering AEs as inputs, the proposed method can address diverse attacks while avoiding the requirement of parameter adjustments or preliminary training. Results demonstrate that the proposed method exhibits consistent performance in rectifying AEs generated via various attack methods, including targeted and black-box attacks. Moreover, it outperforms conventional rectification and input transformation methods in terms of stability against various attacks.",
      "tr": "Elbette, akademik makale başlığını ve özetini istenen şekilde Türkçeye çevireyim:\n\n**Makale Başlığı:** Rectifying Adversarial Examples Using Their Vulnerabilities\n\n**Özet:**\n\nDerin öğrenme tabanlı sınıflandırıcılar, adversarial examples (AEs) işlenirken hatalara yatkındır. AEs, insanlar tarafından tespit edilemeyen ve güvenlik gerektiren uygulamalar için önemli riskler oluşturan, minimum düzeyde bozulmuş girdi verileridir. Bu nedenle, tehditlerini azaltacak savunma mekanizmaları geliştirmek için kapsamlı araştırmalar yapılmıştır. Mevcut yöntemlerin çoğu öncelikli olarak girdi örneklerinin özelliklerine dayanarak AEs'yi ayırt etmeye odaklanmakta, bir saldırıdan önce doğru örnek kategorizasyonunu ele almak yerine AE tespitini vurgulamaktadır. Bazı görevler yalnızca tespit edilen AEs'de basit bir reddetme gerektirebilirken, diğerleri otonom sürüşte trafik işareti tanıma gibi doğru orijinal girdi kategorisini tanımlamayı gerektirir. Bu çalışmanın amacı, orijinal girdilerinin doğru etiketlerini tahmin etmek için AEs'yi düzeltmeye yönelik bir yöntem önermektir. Yöntemimiz, doğru etiket tahmini için onları karar sınırının dışına çıkarmak amacıyla AEs'ye yeniden saldırmaya dayanmaktadır. Bu, white-box attack yöntemleri kullanılarak oluşturulan minimum algılanabilir AEs'yi düzeltme sorununu etkili bir şekilde ele almaktadır. Ancak, karar sınırından uzakta black-box attacks tarafından üretilen AEs'yi veya hedefli saldırılar tarafından düşük güven seviyesine sahip kategorilere yanlış sınıflandırılanları etkili bir şekilde düzeltme konusunda zorluklar devam etmektedir. Yalnızca AEs'yi girdi olarak kabul eden basit bir yaklaşım benimseyerek, önerilen yöntem parametre ayarlamaları veya ön eğitim gereksiniminden kaçınarak çeşitli saldırıları ele alabilir. Sonuçlar, önerilen yöntemin, hedefli ve black-box attacks dahil olmak üzere çeşitli saldırı yöntemleriyle üretilen AEs'yi düzeltmede tutarlı performans sergilediğini göstermektedir. Ayrıca, çeşitli saldırılara karşı kararlılık açısından geleneksel düzeltme ve girdi dönüşüm yöntemlerinden daha üstündür."
    }
  },
  {
    "id": "2601.00065v1",
    "title": "The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition",
    "authors": [
      "Xiaoze Liu",
      "Weichen Yu",
      "Matt Fredrikson",
      "Xiaoqian Wang",
      "Jing Gao"
    ],
    "published_date": "2025-12-31",
    "tags": [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.00065v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00065v1",
    "content": {
      "en": "The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single \"breaker token\" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge",
      "tr": "Elbette, işte makale başlığının ve özetinin akademik ve resmi bir dille Türkçeye çevirisi:\n\n**Makale Başlığı:** Kelime Hazinesindeki Truva Atı: LLM Kompozisyonunun Gizli Sabotajı\n\n**Özet:**\n\nAçık kaynaklı LLM ekosistemi, çeşitlilik gösteren kaynaklardan gelen yetenekleri yeniden karıştıran model kompozisyon teknikleriyle (örneğin: weight merging, speculative decoding ve vocabulary expansion) giderek daha fazla tanımlanmaktadır. Bu yöntemlerin farklı model aileleri arasında uygulanabilmesi için kritik bir ön koşul, uyumsuz kelime hazinelerini ortak bir embedding space'e hizalayan tokenizer transplant'tır. Bu temel birlikte çalışabilirlik adımının bir supply-chain vulnerability'si yarattığını gösteriyoruz: fonksiyonel olarak bir donor model'de etkisiz olan, ancak bir base model'e transplant edildikten sonra güvenilir bir şekilde yüksek önemlilikte zararlı bir özelliğe yeniden yapılandırılan tek bir \"breaker token\" tasarlıyoruz. Coefficient reuse'un geometrisini sömürerek, saldırımız base model'in üretimini sabote eden, ancak donor'un faydasını nominal davranıştan istatistiksel olarak ayırt edilemez bırakan asimetrik bir realizability gap oluşturur. Bunu dual-objective optimization problem olarak formüle ediyor ve saldırıyı sparse solver kullanarak örneklendiriyoruz. Ampirik olarak, saldırı training-free olup outlier detection'dan kaçınmak için spectral mimicry elde ederken, fine-tuning ve weight merging'e karşı structural persistence göstererek, modular AI composition'ın pipeline'ındaki gizli bir riski vurgulamaktadır. Kod https://github.com/xz-liu/tokenforge adresinde mevcuttur."
    }
  },
  {
    "id": "2601.00042v1",
    "title": "Large Empirical Case Study: Go-Explore adapted for AI Red Team Testing",
    "authors": [
      "Manish Bhatt",
      "Adrian Wood",
      "Idan Habler",
      "Ammar Al-Kahfah"
    ],
    "published_date": "2025-12-31",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.00042v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00042v1",
    "content": {
      "en": "Production LLM agents with tool-using capabilities require security testing despite their safety training. We adapt Go-Explore to evaluate GPT-4o-mini across 28 experimental runs spanning six research questions. We find that random-seed variance dominates algorithmic parameters, yielding an 8x spread in outcomes; single-seed comparisons are unreliable, while multi-seed averaging materially reduces variance in our setup. Reward shaping consistently harms performance, causing exploration collapse in 94% of runs or producing 18 false positives with zero verified attacks. In our environment, simple state signatures outperform complex ones. For comprehensive security testing, ensembles provide attack-type diversity, whereas single agents optimize coverage within a given attack type. Overall, these results suggest that seed variance and targeted domain knowledge can outweigh algorithmic sophistication when testing safety-trained models.",
      "tr": "Elbette, işte makale başlığının ve özetinin istenen şekilde çevrilmiş hali:\n\n**Makale Başlığı:** Büyük Ampirik Vaka Çalışması: Yapay Zeka Kırmızı Takım Testi İçin Uyarlanmış Go-Explore\n\n**Özet:**\nAraç kullanma yeteneklerine sahip üretim LLM ajanları, güvenlik eğitimlerine rağmen güvenlik testi gerektirir. GPT-4o-mini'yi altı araştırma sorusunu kapsayan 28 deneysel çalıştırma boyunca değerlendirmek için Go-Explore'u uyarlıyoruz. Rastgele seed varyansının algoritmik parametreleri domine ettiğini ve sonuçlarda 8 katlık bir yayılma sağladığını buluyoruz; tek seed karşılaştırmaları güvenilmezken, çoklu seed ortalaması kurulumumuzda varyansı maddi olarak azaltır. Reward shaping tutarlı bir şekilde performansa zarar verir, çalıştırmaların %94'ünde exploration collapse'e neden olur veya sıfır doğrulanmış saldırıyla 18 yanlış pozitif üretir. Ortamımızda, basit state signature'lar karmaşık olanlardan daha iyi performans gösterir. Kapsamlı güvenlik testi için ensemble'lar saldırı tipi çeşitliliği sağlarken, tek ajanlar belirli bir saldırı tipi dahilinde coverage'ı optimize eder. Genel olarak, bu sonuçlar, seed varyansı ve hedeflenmiş domain knowledge'ın, güvenlik eğitimi almış modelleri test ederken algoritmik sofistikeliğin önüne geçebileceğini düşündürmektedir."
    }
  },
  {
    "id": "2512.24571v1",
    "title": "SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System",
    "authors": [
      "Md Hasan Saju",
      "Austin Page",
      "Akramul Azim",
      "Jeff Gardiner",
      "Farzaneh Abazari"
    ],
    "published_date": "2025-12-31",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.24571v1",
    "pdf_link": "https://arxiv.org/pdf/2512.24571v1",
    "content": {
      "en": "Security Information and Event Management (SIEM) systems are essential for large enterprises to monitor their IT infrastructure by ingesting and analyzing millions of logs and events daily. Security Operations Center (SOC) analysts are tasked with monitoring and analyzing this vast data to identify potential threats and take preventive actions to protect enterprise assets. However, the diversity among SIEM platforms, such as Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel and the Elastic Stack, poses significant challenges. As these systems differ in attributes, architecture, and query languages, making it difficult for analysts to effectively monitor multiple platforms without undergoing extensive training or forcing enterprises to expand their workforce. To address this issue, we introduce SynRAG, a unified framework that automatically generates threat detection or incident investigation queries for multiple SIEM platforms from a platform-agnostic specification. SynRAG can generate platformspecific queries from a single high-level specification written by analysts. Without SynRAG, analysts would need to manually write separate queries for each SIEM platform, since query languages vary significantly across systems. This framework enables seamless threat detection and incident investigation across heterogeneous SIEM environments, reducing the need for specialized training and manual query translation. We evaluate SynRAG against state-of-the-art language models, including GPT, Llama, DeepSeek, Gemma, and Claude, using Qradar and SecOps as representative SIEM systems. Our results demonstrate that SynRAG generates significantly better queries for crossSIEM threat detection and incident investigation compared to the state-of-the-art base models.",
      "tr": "Makale Başlığı: SynRAG: Heterojen SIEM Sistemleri için Yürütülebilir Sorgu Üretimi Amaçlı Bir Büyük Dil Modeli Çerçevesi\n\nÖzet:\nSecurity Information and Event Management (SIEM) sistemleri, büyük kuruluşların BT altyapılarını günlük milyonlarca log ve olayı alıp analiz ederek izlemeleri için elzemdir. Security Operations Center (SOC) analistleri, potansiyel tehditleri belirlemek ve kurumsal varlıkları korumak için önleyici eylemler almak amacıyla bu devasa verileri izlemek ve analiz etmekle görevlidir. Ancak, Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel ve Elastic Stack gibi SIEM platformları arasındaki çeşitlilik önemli zorluklar teşkil etmektedir. Bu sistemler özellikler, mimari ve query languages açısından farklılık gösterdiği için, analistlerin kapsamlı eğitimden geçmeden birden fazla platformu etkili bir şekilde izlemelerini veya kuruluşların iş gücünü genişletmelerini zorlaştırmaktadır. Bu sorunu çözmek için, platformdan bağımsız bir spesifikasyondan birden fazla SIEM platformu için otomatik olarak tehdit tespiti veya olay incelemesi sorguları üreten birleşik bir çerçeve olan SynRAG'ı sunuyoruz. SynRAG, analistler tarafından yazılan tek bir üst düzey spesifikasyondan platforma özgü sorgular üretebilir. SynRAG olmadan, query languages sistemler arasında önemli ölçüde farklılık gösterdiği için analistlerin her SIEM platformu için ayrı sorgular yazması gerekecektir. Bu çerçeve, uzmanlaşmış eğitim ve manuel sorgu çevirisi ihtiyacını azaltarak, heterojen SIEM ortamlarında sorunsuz tehdit tespiti ve olay incelemesi sağlamaktadır. SynRAG'ı, Qradar ve SecOps'u temsili SIEM sistemleri olarak kullanarak, GPT, Llama, DeepSeek, Gemma ve Claude dahil olmak üzere en gelişmiş dil modellerine karşı değerlendiriyoruz. Sonuçlarımız, SynRAG'ın çapraz-SIEM tehdit tespiti ve olay incelemesi için en gelişmiş temel modellere kıyasla önemli ölçüde daha iyi sorgular ürettiğini göstermektedir."
    }
  }
]