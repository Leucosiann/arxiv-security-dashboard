[
  {
    "id": "2512.23557v1",
    "title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks",
    "authors": [
      "Toqeer Ali Syed",
      "Mishal Ateeq Almutairi",
      "Mahmoud Abdel Moaty"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.23557v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23557v1",
    "content": {
      "en": "Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.",
      "tr": "İşte akademik makale başlığının ve özetinin çevirisi:\n\n**Makale Başlığı:** Güvenilir Ajan Tabanlı Yapay Zeka'ya Doğru: Prompt Injection Saldırılarını Önlemek İçin Çok Modlu Bir Çerçeve\n\n**Özet:**\nBüyük Dil Modelleri (LLMs), Vision-Language Modelleri (VLMs) ve LangChain ve GraphChain gibi yeni ajan tabanlı yapay zeka sistemleri sayesinde, çok sayıda araç ve ajan kullanarak ve aralarında akıl yürüten (reasoning), planlayan ve sohbet eden güçlü otonom sistemler mümkün olmaktadır. Bununla birlikte, bu ajan tabanlı ortam, gizlenmiş veya kötü niyetli talimatların metin, resim, metadata veya ajanlar arası mesajlar aracılığıyla yayılarak grafikte yayılmasına ve istenmeyen davranışlara, politika ihlaline veya durumun bozulmasına yol açabilen çok modlu prompt injection (PI) saldırılarının meydana gelme olasılığını artırmaktadır. Bu riskleri azaltmak amacıyla, bu makale, kullanıcı tarafından oluşturulan veya üst düzey ajanlar tarafından üretilen tüm prompt'ların temizlendiği ve LLM tarafından üretilen tüm çıktıların alt düzey düğümlere gönderilmeden önce bağımsız olarak doğrulandığı bir Cross-Agent Multimodal Provenanc-Aware Defense Framework önermektedir. Bu çerçeve, tüm ajan ağı boyunca modalite, kaynak ve trust level metaverisini tutan bir provenance ledger tarafından koordine edilen bir Text sanitizer agent, visual sanitizer agent ve output validator agent içermektedir. Bu mimari, ajanlar arası iletişimin açık trust frames'e uyulmasını sağlayarak, injected instructions'ın LangChain veya GraphChain tarzı workflow'lar aşağı doğru yayılmamasını temin eder. Deneysel değerlendirmeler, multimodal injection detection accuracy'nin önemli ölçüde arttığını, cross-agent trust leakage'ın minimize edildiğini ve ajan tabanlı yürütme yollarının kararlı hale geldiğini göstermektedir. Provenance tracking ve validation kavramını çoklu ajan orkestrasyonuna genişleten bu çerçeve, güvenli, anlaşılabilir ve güvenilir ajan tabanlı yapay zeka sistemlerinin kurulmasını güçlendirir."
    }
  },
  {
    "id": "2512.23480v1",
    "title": "Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation",
    "authors": [
      "Toqeer Ali Syed",
      "Mohammad Riyaz Belgaum",
      "Salman Jan",
      "Asadullah Abdullah Khan",
      "Saad Said Alqahtani"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.23480v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23480v1",
    "content": {
      "en": "The software supply chain attacks are becoming more and more focused on trusted development and delivery procedures, so the conventional post-build integrity mechanisms cannot be used anymore. The available frameworks like SLSA, SBOM and in toto are majorly used to offer provenance and traceability but do not have the capabilities of actively identifying and removing vulnerabilities in software production. The current paper includes an example of agentic artificial intelligence (AI) based on autonomous software supply chain security that combines large language model (LLM)-based reasoning, reinforcement learning (RL), and multi-agent coordination. The suggested system utilizes specialized security agents coordinated with the help of LangChain and LangGraph, communicates with actual CI/CD environments with the Model Context Protocol (MCP), and documents all the observations and actions in a blockchain security ledger to ensure integrity and auditing. Reinforcement learning can be used to achieve adaptive mitigation strategies that consider the balance between security effectiveness and the operational overhead, and LLMs can be used to achieve semantic vulnerability analysis, as well as explainable decisions. This framework is tested based on simulated pipelines, as well as, actual world CI/CD integrations on GitHub Actions and Jenkins, including injection attacks, insecure deserialization, access control violations, and configuration errors. Experimental outcomes indicate better detection accuracy, shorter mitigation latency and reasonable build-time overhead than rule-based, provenance only and RL only baselines. These results show that agentic AI can facilitate the transition to self defending, proactive software supply chains rather than reactive verification ones.",
      "tr": "**Makale Başlığı:** Yazılım Tedarik Zinciri Güvenliğinde Otonom Savunma İçin Agentic AI: Provenance'ın Ötesinde Güvenlik Açığı Azaltma\n\n**Özet:**\n\nYazılım tedarik zinciri saldırıları, güvenilir geliştirme ve teslimat prosedürlerine giderek daha fazla odaklanmakta, bu nedenle geleneksel inşa sonrası bütünlük mekanizmaları artık kullanılamamaktadır. SLSA, SBOM ve in toto gibi mevcut çerçeveler büyük ölçüde provenance ve izlenebilirlik sunmak için kullanılmaktadır, ancak yazılım üretiminde güvenlik açıklarını aktif olarak tanımlama ve giderme yeteneklerine sahip değildir. Mevcut makale, large language model (LLM)-based reasoning, reinforcement learning (RL) ve multi-agent coordination'ı birleştiren otonom yazılım tedarik zinciri güvenliğine dayalı bir agentic artificial intelligence (AI) örneği içermektedir. Önerilen sistem, LangChain ve LangGraph yardımıyla koordine edilen özel güvenlik ajanlarından yararlanır, Model Context Protocol (MCP) ile gerçek CI/CD ortamlarıyla iletişim kurar ve bütünlük ve denetim sağlamak için tüm gözlemleri ve eylemleri bir blockchain security ledger'a kaydeder. Reinforcement learning, güvenlik etkinliği ile operasyonel maliyet arasındaki dengeyi gözeten uyarlanabilir azaltma stratejileri elde etmek için kullanılabilir ve LLM'ler anlamsal güvenlik açığı analizi ve açıklanabilir kararlar elde etmek için kullanılabilir. Bu çerçeve, simüle edilmiş pipeline'lar ve GitHub Actions ve Jenkins üzerinde gerçek dünya CI/CD entegrasyonları ile enjekte saldırıları, güvensiz deserializasyon, erişim kontrol ihlalleri ve yapılandırma hataları dahil olmak üzere test edilmiştir. Deneysel sonuçlar, kural tabanlı, yalnızca provenance ve yalnızca RL tabanlı temel sistemlere göre daha iyi tespit doğruluğu, daha kısa azaltma gecikmesi ve makul inşa süresi maliyeti göstermektedir. Bu sonuçlar, agentic AI'nın reaktif doğrulama sistemleri yerine kendi kendini savunan, proaktif yazılım tedarik zincirlerine geçişi kolaylaştırabileceğini göstermektedir."
    }
  },
  {
    "id": "2512.23385v1",
    "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?",
    "authors": [
      "The Anh Nguyen",
      "Triet Huynh Minh Le",
      "M. Ali Babar"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.HC"
    ],
    "link": "http://arxiv.org/abs/2512.23385v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23385v1",
    "content": {
      "en": "The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.",
      "tr": "**Makale Başlığı:** Yapay Zeka Tedarik Zincirini Güven altına Almak: Yapay Zeka Projelerindeki Geliştirici Raporlu Güvenlik Sorunları ve Çözümlerinden Neler Öğrenebiliriz?\n\n**Özet:**\n\nYapay zeka (AI) modellerinin ve uygulamalarının hızlı büyümesi, giderek karmaşıklaşan bir güvenlik ortamına yol açmıştır. Yapay zeka projeleri geliştirenlerin, geleneksel yazılım tedarik zinciri sorunlarıyla olduğu kadar, yenilikçi ve yapay zekaya özgü güvenlik tehditleriyle de başa çıkmaları gerekmektedir. Ancak, yaygın olarak karşılaşılan güvenlik sorunları ve bunların pratikte nasıl çözüldüğü hakkında çok az bilgi bulunmaktadır. Bu bilgi boşluğu, yapay zeka tedarik zincirinin her bileşeni için etkili güvenlik önlemlerinin geliştirilmesini engellemektedir. Hugging Face ve GitHub üzerindeki tartışmalara dayanarak geliştirici raporlu sorunlar ve çözümler üzerine ampirik bir inceleme yaparak bu bilgi boşluğunu dolduruyoruz. Güvenlikle ilgili tartışmaları belirlemek amacıyla, anahtar kelime eşleştirmesini çeşitli derin öğrenme ve large language modelleri arasındaki kapsamlı karşılaştırmamızda en iyi performansı gösteren optimal fine-tuned distilBERT sınıflandırıcı ile birleştiren bir pipeline geliştiriyoruz. Bu pipeline, yapay zeka uygulamaları ve projelerinin güvenlik raporlama pratikleri hakkında bilgiler sunan 312.868 güvenlik tartışmasından oluşan bir veri seti üretmektedir. Veri setimizden örneklenmiş 753 gönderi üzerinde tematik analiz yaparak, dört tema altında 32 güvenlik sorunu ve 24 çözümden oluşan ince taneli bir taksonomi ortaya koyuyoruz: (1) Sistem ve Yazılım, (2) Harici Araçlar ve Ekosistem, (3) Model ve (4) Veri. Yapay zeka bileşenlerinin karmaşık bağımlılıklarından ve black-box doğasından kaynaklanan pek çok güvenlik sorununun ortaya çıktığını ortaya koyuyoruz. Özellikle, Model ve Veri ile ilgili zorluklar sıklıkla somut çözümlerden yoksundur. Elde ettiğimiz bu bilgiler, geliştiricilere ve araştırmacılara yapay zeka tedarik zinciri boyunca gerçek dünya güvenlik tehditlerini ele almaları için kanıta dayalı rehberlik sunabilir."
    }
  },
  {
    "id": "2512.23173v1",
    "title": "EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion",
    "authors": [
      "Zhen Liang",
      "Hai Huang",
      "Zhengkui Chen"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.23173v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23173v1",
    "content": {
      "en": "Large language models (LLMs), such as ChatGPT, have achieved remarkable success across a wide range of fields. However, their trustworthiness remains a significant concern, as they are still susceptible to jailbreak attacks aimed at eliciting inappropriate or harmful responses. However, existing jailbreak attacks mainly operate at the natural language level and rely on a single attack strategy, limiting their effectiveness in comprehensively assessing LLM robustness. In this paper, we propose Equacode, a novel multi-strategy jailbreak approach for large language models via equation-solving and code completion. This approach transforms malicious intent into a mathematical problem and then requires the LLM to solve it using code, leveraging the complexity of cross-domain tasks to divert the model's focus toward task completion rather than safety constraints. Experimental results show that Equacode achieves an average success rate of 91.19% on the GPT series and 98.65% across 3 state-of-the-art LLMs, all with only a single query. Further, ablation experiments demonstrate that EquaCode outperforms either the mathematical equation module or the code module alone. This suggests a strong synergistic effect, thereby demonstrating that multi-strategy approach yields results greater than the sum of its parts.",
      "tr": "İşte akademik makale başlığı ve özetinin istenen şekilde Türkçeye çevrilmiş hali:\n\n**Makale Başlığı:** EquaCode: Denklem Çözme ve Kod Tamamlama Yoluyla Büyük Dil Modelleri İçin Çoklu Strateji Saldırı Yöntemi\n\n**Özet:**\nChatGPT gibi büyük dil modelleri (LLM'ler), geniş bir alanda kayda değer başarılar elde etmiştir. Ancak, uygunsuz veya zararlı yanıtları ortaya çıkarmayı hedefleyen jailbreak saldırılarına hala açık olmaları nedeniyle güvenilirlikleri önemli bir endişe kaynağı olmaya devam etmektedir. Mevcut jailbreak saldırıları ise büyük ölçüde doğal dil seviyesinde işlev göstermekte ve tek bir saldırı stratejisine dayanmaktadır, bu da LLM sağlamlığının kapsamlı bir şekilde değerlendirilmesindeki etkinliklerini sınırlamaktadır. Bu makalede, denklem çözme ve kod tamamlama yoluyla büyük dil modelleri için yeni bir çoklu strateji jailbreak yaklaşımı olan Equacode'u sunmaktayız. Bu yaklaşım, kötü niyetli amacı matematiksel bir probleme dönüştürür ve ardından LLM'nin kodu kullanarak bu problemi çözmesini gerektirir. Bu yöntem, LLM'nin odağını güvenlik kısıtlamaları yerine görev tamamlama yönüne kaydırmak için alanlar arası görevlerin karmaşıklığından yararlanır. Deneysel sonuçlar, Equacode'un GPT serisinde ortalama %91,19 ve 3 adet son teknoloji LLM'de %98,65 başarı oranı elde ettiğini göstermektedir; tüm bu sonuçlar yalnızca tek bir sorgu ile elde edilmiştir. Ayrıca, ablasyon deneyleri Equacode'un, yalnızca matematiksel denklem modülü veya yalnızca kod modülüne kıyasla daha iyi performans gösterdiğini ortaya koymaktadır. Bu durum, \"multi-strategy approach\"ın parçaların toplamından daha büyük sonuçlar elde ettiğini gösteren güçlü bir sinerjik etkiyi ortaya koymaktadır."
    }
  },
  {
    "id": "2512.23171v1",
    "title": "Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning",
    "authors": [
      "Yu Jiang",
      "Xindi Tong",
      "Ziyao Liu",
      "Xiaoxi Zhang",
      "Kwok-Yan Lam"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.23171v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23171v1",
    "content": {
      "en": "Federated unlearning has become an attractive approach to address privacy concerns in collaborative machine learning, for situations when sensitive data is remembered by AI models during the machine learning process. It enables the removal of specific data influences from trained models, aligning with the growing emphasis on the \"right to be forgotten.\" While extensively studied in horizontal federated learning, unlearning in vertical federated learning (VFL) remains challenging due to the distributed feature architecture. VFL unlearning includes sample unlearning that removes specific data points' influence and label unlearning that removes entire classes. Since different parties hold complementary features of the same samples, unlearning tasks require cross-party coordination, creating computational overhead and complexities from feature interdependencies. To address such challenges, we propose FedORA (Federated Optimization for data Removal via primal-dual Algorithm), designed for sample and label unlearning in VFL. FedORA formulates the removal of certain samples or labels as a constrained optimization problem solved using a primal-dual framework. Our approach introduces a new unlearning loss function that promotes classification uncertainty rather than misclassification. An adaptive step size enhances stability, while an asymmetric batch design, considering the prior influence of the remaining data on the model, handles unlearning and retained data differently to efficiently reduce computational costs. We provide theoretical analysis proving that the model difference between FedORA and Train-from-scratch is bounded, establishing guarantees for unlearning effectiveness. Experiments on tabular and image datasets demonstrate that FedORA achieves unlearning effectiveness and utility preservation comparable to Train-from-scratch with reduced computation and communication overhead.",
      "tr": "**Makale Başlığı:** Unutulma Hakkının Sertifikalandırılması: Vertical Federated Learning'de Örnek ve Etiket Unlearning'i için Primal-Dual Optimizasyon\n\n**Özet:**\n\nFederated unlearning, makine öğrenimi süreci sırasında AI modelleri tarafından hatırlanan hassas veriler söz konusu olduğunda, işbirlikçi makine öğrenimindeki gizlilik endişelerini gidermek için cazip bir yaklaşım haline gelmiştir. Bu, eğitilmiş modellerden belirli veri etkilerinin kaldırılmasını sağlayarak, giderek artan \"unutulma hakkı\" vurgusuyla uyum sağlar. Yatay federasyonlu öğrenmede (horizontal federated learning) kapsamlı bir şekilde incelenmiş olmasına rağmen, dikey federasyonlu öğrenmedeki (vertical federated learning - VFL) unlearning, dağıtık özellik mimarisi nedeniyle zorluğunu sürdürmektedir. VFL unlearning, belirli veri noktalarının etkisini kaldıran sample unlearning ve tüm sınıfları kaldıran label unlearning'i içerir. Farklı taraflar aynı örneklerin tamamlayıcı özelliklerini elinde bulundurduğundan, unlearning görevleri partiler arası koordinasyon gerektirir, bu da özellikler arası bağımlılıklardan kaynaklanan hesaplama yükü ve karmaşıklıklar yaratır. Bu zorlukların üstesinden gelmek için, VFL'de sample ve label unlearning için tasarlanmış FedORA'yı (Federated Optimization for data Removal via primal-dual Algorithm) sunuyoruz. FedORA, belirli örneklerin veya etiketlerin kaldırılmasını primal-dual çerçevesi kullanılarak çözülen bir kısıtlı optimizasyon problemi olarak formüle eder. Yaklaşımımız, yanlış sınıflandırma yerine sınıflandırma belirsizliğini teşvik eden yeni bir unlearning kaybı fonksiyonu sunmaktadır. Adaptif bir adım boyutu istikrarı artırırken, asimetrik bir batch tasarımı, kalan verilerin model üzerindeki ön etkisini göz önünde bulundurarak, unlearning ve muhafaza edilen verileri farklı şekilde ele alır ve hesaplama maliyetlerini verimli bir şekilde azaltır. FedORA ve Train-from-scratch arasındaki model farkının sınırlı olduğunu kanıtlayan teorik analizler sunarak, unlearning etkinliği için garantiler sağlıyoruz. Tablosal ve görüntü veri kümeleri üzerinde yapılan deneyler, FedORA'nın, azaltılmış hesaplama ve iletişim yükü ile Train-from-scratch ile karşılaştırılabilir unlearning etkinliği ve fayda korunumu elde ettiğini göstermektedir."
    }
  },
  {
    "id": "2512.23132v1",
    "title": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems",
    "authors": [
      "Armstrong Foundjem",
      "Lionel Nganyewou Tidjon",
      "Leuson Da Silva",
      "Foutse Khomh"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2512.23132v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23132v1",
    "content": {
      "en": "Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.",
      "tr": "**Makale Başlığı:** Yapay Zeka Tabanlı Sistemlerde Tehdit Azaltma ve Dayanıklılık İçin Çoklu-Agen (Multi-Agent) Bir Çerçeve\n\n**Özet:**\n\nFinans, sağlık ve kritik altyapılarda temel modellerin (foundation models) altında yatan makine öğrenmesi (ML), onları data poisoning, model extraction, prompt injection, automated jailbreaking ve model karşılaştırmalarını sömüren preference-guided black-box attacks gibi saldırılara hedef haline getirmektedir. Daha büyük modeller, introspection-driven jailbreaks ve cross-modal manipulation’a karşı daha savunmasız olabilir. Geleneksel siber güvenlik, foundation, multimodal ve RAG sistemleri için ML'e özgü tehdit modellemesinden yoksundur. Amaç: Hakim TTP'leri, zafiyetleri ve hedeflenen lifecycle stages’leri belirleyerek ML güvenlik risklerini karakterize etmek. Yöntemler: MITRE ATLAS (26), AI Incident Database (12) ve literatürden (55) 93 tehdit çıkarılmış ve 854 GitHub/Python deposu analiz edilmiştir. Ontology güdümlü bir threat graph oluşturmak için TTP'leri, zafiyetleri ve aşamaları ilişkilendiren çoklu-agen RAG sistemi (ChatGPT-4o, temp 0.4), 300'den fazla makaleden bilgi toplamıştır. Sonuçlar: Ticari LLM API model stealing, parameter memorization leakage ve preference-guided text-only jailbreaks dahil olmak üzere raporlanmamış tehditler belirlenmiştir. Hakim TTP'ler arasında MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors ve preference optimization leakage bulunmaktadır ve bunlar çoğunlukla pre-training ve inference’ı etkilemektedir. Graph analizi, zayıf patch propagation’a sahip kütüphanelerde yoğun zafiyet kümeleri ortaya koymuştur. Sonuç: Bağımlılık hijyeni (dependency hygiene), tehdit istihbaratı (threat intelligence) ve izlemeyi (monitoring) birleştiren adaptif, ML'e özgü güvenlik çerçeveleri, ML lifecycle’ı boyunca tedarik zinciri (supply-chain) ve inference risklerini azaltmak için elzemdir."
    }
  },
  {
    "id": "2512.22894v1",
    "title": "DECEPTICON: How Dark Patterns Manipulate Web Agents",
    "authors": [
      "Phil Cuvin",
      "Hao Zhu",
      "Diyi Yang"
    ],
    "published_date": "2025-12-28",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.22894v1",
    "pdf_link": "https://arxiv.org/pdf/2512.22894v1",
    "content": {
      "en": "Deceptive UI designs, widely instantiated across the web and commonly known as dark patterns, manipulate users into performing actions misaligned with their goals. In this paper, we show that dark patterns are highly effective in steering agent trajectories, posing a significant risk to agent robustness. To quantify this risk, we introduce DECEPTICON, an environment for testing individual dark patterns in isolation. DECEPTICON includes 700 web navigation tasks with dark patterns -- 600 generated tasks and 100 real-world tasks, designed to measure instruction-following success and dark pattern effectiveness. Across state-of-the-art agents, we find dark patterns successfully steer agent trajectories towards malicious outcomes in over 70% of tested generated and real-world tasks -- compared to a human average of 31%. Moreover, we find that dark pattern effectiveness correlates positively with model size and test-time reasoning, making larger, more capable models more susceptible. Leading countermeasures against adversarial attacks, including in-context prompting and guardrail models, fail to consistently reduce the success rate of dark pattern interventions. Our findings reveal dark patterns as a latent and unmitigated risk to web agents, highlighting the urgent need for robust defenses against manipulative designs.",
      "tr": "**Makale Başlığı:** DECEPTICON: Karanlık Kalıplar Web Ajanlarını Nasıl Manipüle Ediyor\n\n**Özet:**\n\nWeb genelinde yaygın olarak görülen ve genellikle karanlık kalıplar olarak bilinen aldatıcı UI tasarımları, kullanıcıları hedefleriyle uyumsuz eylemler gerçekleştirmeye yönlendirir. Bu makalede, karanlık kalıpların ajan yörüngelerini yönlendirmede son derece etkili olduğunu ve ajan dayanıklılığı için önemli bir risk oluşturduğunu gösteriyoruz. Bu riski ölçmek için, bireysel karanlık kalıpları izole edilmiş bir şekilde test etmek üzere tasarlanmış bir ortam olan DECEPTICON'u sunuyoruz. DECEPTICON, karanlık kalıplar içeren 700 web navigasyon görevi içermektedir; bu görevlerden 600'ü üretilmiş, 100'ü ise gerçek dünya görevleridir ve talimat takibi başarısını ve karanlık kalıp etkinliğini ölçmek üzere tasarlanmıştır. En gelişmiş ajanlar üzerinde yapılan testlerde, karanlık kalıpların test edilen üretilmiş ve gerçek dünya görevlerinin %70'inden fazlasında ajan yörüngelerini kötü niyetli sonuçlara doğru başarıyla yönlendirdiğini tespit ettik; bu oran, insan ortalaması olan %31 ile karşılaştırıldığında oldukça yüksektir. Dahası, karanlık kalıp etkinliğinin model boyutu ve test-time reasoning ile pozitif korelasyon gösterdiğini, böylece daha büyük ve daha yetenekli modellerin daha hassas hale geldiğini bulduk. Kapsam içi prompting ve guardrail models dahil olmak üzere, adversarial attacks'e karşı önde gelen önlemler, karanlık kalıp müdahalelerinin başarı oranını tutarlı bir şekilde azaltamamaktadır. Bulgularımız, karanlık kalıpları web ajanları için gizli ve henüz ele alınmamış bir risk olarak ortaya koymakta ve manipülatif tasarımlara karşı sağlam savunmalara olan acil ihtiyacı vurgulamaktadır."
    }
  },
  {
    "id": "2512.22883v1",
    "title": "Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations",
    "authors": [
      "Tao Li",
      "Quanyan Zhu"
    ],
    "published_date": "2025-12-28",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.22883v1",
    "pdf_link": "https://arxiv.org/pdf/2512.22883v1",
    "content": {
      "en": "Cybersecurity is being fundamentally reshaped by foundation-model-based artificial intelligence. Large language models now enable autonomous planning, tool orchestration, and strategic adaptation at scale, challenging security architectures built on static rules, perimeter defenses, and human-centered workflows. This chapter argues for a shift from prevention-centric security toward agentic cyber resilience. Rather than seeking perfect protection, resilient systems must anticipate disruption, maintain critical functions under attack, recover efficiently, and learn continuously. We situate this shift within the historical evolution of cybersecurity paradigms, culminating in an AI-augmented paradigm where autonomous agents participate directly in sensing, reasoning, action, and adaptation across cyber and cyber-physical systems. We then develop a system-level framework for designing agentic AI workflows. A general agentic architecture is introduced, and attacker and defender workflows are analyzed as coupled adaptive processes, and game-theoretic formulations are shown to provide a unifying design language for autonomy allocation, information flow, and temporal composition. Case studies in automated penetration testing, remediation, and cyber deception illustrate how equilibrium-based design enables system-level resiliency design.",
      "tr": "Makale Başlığı: Siber Direnç için Agentic AI: Yeni Bir Güvenlik Paradigması ve Sistem-Teorik Temelleri\n\nÖzet:\nSiber güvenlik, foundation-model-based artificial intelligence tarafından temelden yeniden şekillenmektedir. Büyük dil modelleri artık ölçekte otonom planlama, tool orchestration ve stratejik adaptasyonu mümkün kılmakta, statik kurallar, perimeter defenses ve insan merkezli iş akışları üzerine kurulu güvenlik mimarilerine meydan okumaktadır. Bu bölüm, prevention-centric security'den agentic cyber resilience'a bir geçişi savunmaktadır. Mükemmel koruma arayışı yerine, resilient sistemler bozulmayı öngörmeli, saldırı altında kritik fonksiyonları sürdürmeli, verimli bir şekilde iyileşmeli ve sürekli öğrenmelidir. Bu geçişi siber güvenlik paradigmalarının tarihsel evrimi içine yerleştiriyoruz ve siber ve siber-fiziksel sistemler genelinde sensing, reasoning, action ve adaptasyona doğrudan katılan otonom ajanları içeren AI-augmented bir paradigmaya ulaşıyoruz. Ardından, agentic AI iş akışlarını tasarlamak için bir system-level framework geliştiriyoruz. Genel bir agentic architecture tanıtılıyor ve attacker ve defender iş akışları coupled adaptive processes olarak analiz ediliyor ve game-theoretic formulations'ın otonomi tahsisi, information flow ve temporal composition için birleştirici bir tasarım dili sağladığı gösteriliyor. Otomatik penetrasyon testleri, remediation ve cyber deception'daki case studies, equilibrium-based design'ın system-level resiliency design'ı nasıl sağladığını göstermektedir."
    }
  },
  {
    "id": "2512.22860v1",
    "title": "Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks",
    "authors": [
      "Soham Padia",
      "Dhananjay Vaidya",
      "Ramchandra Mangrulkar"
    ],
    "published_date": "2025-12-28",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2512.22860v1",
    "pdf_link": "https://arxiv.org/pdf/2512.22860v1",
    "content": {
      "en": "Securing blockchain-enabled IoT networks against sophisticated adversarial attacks remains a critical challenge. This paper presents a trust-based delegated consensus framework integrating Fully Homomorphic Encryption (FHE) with Attribute-Based Access Control (ABAC) for privacy-preserving policy evaluation, combined with learning-based defense mechanisms. We systematically compare three reinforcement learning approaches -- tabular Q-learning (RL), Deep RL with Dueling Double DQN (DRL), and Multi-Agent RL (MARL) -- against five distinct attack families: Naive Malicious Attack (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine Fault Injection (BFI), and Time-Delayed Poisoning (TDP). Experimental results on a 16-node simulated IoT network reveal significant performance variations: MARL achieves superior detection under collusive attacks (F1=0.85 vs. DRL's 0.68 and RL's 0.50), while DRL and MARL both attain perfect detection (F1=1.00) against adaptive attacks where RL fails (F1=0.50). All agents successfully defend against Byzantine attacks (F1=1.00). Most critically, the Time-Delayed Poisoning attack proves catastrophic for all agents, with F1 scores dropping to 0.11-0.16 after sleeper activation, demonstrating the severe threat posed by trust-building adversaries. Our findings indicate that coordinated multi-agent learning provides measurable advantages for defending against sophisticated trust manipulation attacks in blockchain IoT environments.",
      "tr": "**Makale Başlığı:** Blockchain IoT için Adaptif Güven Uzlaşması: RL, DRL ve MARL'ın Naive, Collusive, Adaptive, Byzantine ve Sleeper Saldırılarına Karşı Karşılaştırılması\n\n**Özet:**\n\nBlockchain tabanlı IoT ağlarını gelişmiş düşmanca saldırılara karşı güvence altına almak kritik bir zorluk olmaya devam etmektedir. Bu makale, gizlilik korumalı politika değerlendirmesi için Attribute-Based Access Control (ABAC) ile Fully Homomorphic Encryption (FHE) entegre eden ve öğrenme tabanlı savunma mekanizmaları ile birleştiren bir güvene dayalı yetkilendirilmiş uzlaşma çerçevesi sunmaktadır. Üç pekiştirmeli öğrenme yaklaşımını -- tabular Q-learning (RL), Deep RL with Dueling Double DQN (DRL) ve Multi-Agent RL (MARL) -- beş farklı saldırı ailesine karşı sistematik olarak karşılaştırıyoruz: Naive Malicious Attack (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine Fault Injection (BFI) ve Time-Delayed Poisoning (TDP). 16 düğümlü simüle edilmiş bir IoT ağı üzerinde yapılan deneysel sonuçlar, önemli performans farklılıkları ortaya koymaktadır: MARL, collusive saldırılar altında üstün tespit sağlamaktadır (F1=0.85, DRL'nin 0.68 ve RL'nin 0.50'sine karşı), oysa DRL ve MARL, RL'nin başarısız olduğu (F1=0.50) adaptive saldırılara karşı mükemmel tespit (F1=1.00) elde etmektedir. Tüm ajanlar Byzantine saldırılara karşı başarıyla savunma yapmaktadır (F1=1.00). En önemlisi, Time-Delayed Poisoning saldırısı tüm ajanlar için yıkıcı olduğunu kanıtlamaktadır; sleeper aktivasyonundan sonra F1 skorları 0.11-0.16'ya düşerek, güven oluşturan düşmanların oluşturduğu ciddi tehdidi göstermektedir. Bulgularımız, koordineli multi-agent learning'in blockchain IoT ortamlarındaki gelişmiş güven manipülasyon saldırılarına karşı savunmada ölçülebilir avantajlar sağladığını göstermektedir."
    }
  },
  {
    "id": "2512.22307v1",
    "title": "LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators",
    "authors": [
      "You Li",
      "Guannan Zhao",
      "Yuhao Ju",
      "Yunqi He",
      "Jie Gu"
    ],
    "published_date": "2025-12-26",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.22307v1",
    "pdf_link": "https://arxiv.org/pdf/2512.22307v1",
    "content": {
      "en": "We introduce LLA, an effective intellectual property (IP) protection scheme for generative AI models. LLA leverages the synergy between hardware and software to defend against various supply chain threats, including model theft, model corruption, and information leakage. On the software side, it embeds key bits into neurons that can trigger outliers to degrade performance and applies invariance transformations to obscure the key values. On the hardware side, it integrates a lightweight locking module into the AI accelerator while maintaining compatibility with various dataflow patterns and toolchains. An accelerator with a pre-stored secret key acts as a license to access the model services provided by the IP owner. The evaluation results show that LLA can withstand a broad range of oracle-guided key optimization attacks, while incurring a minimal computational overhead of less than 0.1% for 7,168 key bits.",
      "tr": "Makale Başlığı: LLA: Mantık Kilitli Hızlandırıcılar ile Üretken Modeller İçin Güvenliği ve Gizliliği Artırma\n\nÖzet:\nÜretken yapay zeka modelleri için etkili bir fikri mülkiyet (IP) koruma şeması olan LLA'yı tanıtıyoruz. LLA, model hırsızlığı, model bozulması ve bilgi sızması dahil olmak üzere çeşitli tedarik zinciri tehditlerine karşı savunma yapmak için donanım ve yazılım arasındaki sinerjiden yararlanır. Yazılım tarafında, performansı düşürmek için aykırı değerleri tetikleyebilen anahtar bitlerini nöronlara gömer ve anahtar değerlerini gizlemek için değişmezlik dönüşümleri uygular. Donanım tarafında, çeşitli veri akışı modelleri ve toolchain'leriyle uyumluluğu korurken, yapay zeka hızlandırıcısına hafif bir kilitleme modülü entegre eder. Önceden depolanmış gizli bir anahtara sahip bir hızlandırıcı, IP sahibinin sağladığı model hizmetlerine erişim için bir lisans görevi görür. Değerlendirme sonuçları, LLA'nın geniş bir oracle-guided key optimization attacks yelpazesine dayanabileceğini, aynı zamanda 7.168 key bit için %0.1'den az minimal bir hesaplama yükü oluşturduğunu göstermektedir."
    }
  }
]