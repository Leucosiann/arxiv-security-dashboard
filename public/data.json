[
  {
    "id": "2601.10440v1",
    "title": "AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior",
    "authors": [
      "Nadya Abaev",
      "Denis Klimov",
      "Gerard Levinov",
      "David Mimran",
      "Yuval Elovici"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.10440v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10440v1",
    "content": {
      "en": "Artificial intelligence (AI) agents are increasingly used in a variety of domains to automate tasks, interact with users, and make decisions based on data inputs. Ensuring that AI agents perform only authorized actions and handle inputs appropriately is essential for maintaining system integrity and preventing misuse. In this study, we introduce the AgentGuardian, a novel security framework that governs and protects AI agent operations by enforcing context-aware access-control policies. During a controlled staging phase, the framework monitors execution traces to learn legitimate agent behaviors and input patterns. From this phase, it derives adaptive policies that regulate tool calls made by the agent, guided by both real-time input context and the control flow dependencies of multi-step agent actions. Evaluation across two real-world AI agent applications demonstrates that AgentGuardian effectively detects malicious or misleading inputs while preserving normal agent functionality. Moreover, its control-flow-based governance mechanism mitigates hallucination-driven errors and other orchestration-level malfunctions.",
      "tr": "Makale Başlığı: AgentGuardian: AI Agent Davranışlarını Yönetmek İçin Erişim Kontrol Politikaları Öğrenme\n\nÖzet:\nYapay zeka (AI) ajanları, görevleri otomatikleştirmek, kullanıcılarla etkileşim kurmak ve veri girdilerine dayalı kararlar almak için çeşitli alanlarda giderek daha fazla kullanılmaktadır. AI ajanlarının yalnızca yetkilendirilmiş eylemleri gerçekleştirmesini ve girdileri uygun şekilde işlemesini sağlamak, sistem bütünlüğünü korumak ve kötüye kullanımı önlemek için esastır. Bu çalışmada, bağlam-farkındalığı olan erişim kontrol politikalarını uygulayarak AI ajan operasyonlarını yöneten ve koruyan yeni bir güvenlik çerçevesi olan AgentGuardian'ı tanıtıyoruz. Kontrollü bir hazırlık aşaması sırasında çerçeve, meşru ajan davranışlarını ve girdi kalıplarını öğrenmek için yürütme izlerini izler. Bu aşamadan, hem gerçek zamanlı girdi bağlamı hem de çok adımlı ajan eylemlerinin kontrol akışı bağımlılıkları tarafından yönlendirilen, ajan tarafından yapılan tool call'ları düzenleyen uyarlanabilir politikalar türetir. İki gerçek dünya AI ajanı uygulaması üzerinden yapılan değerlendirme, AgentGuardian'ın normal ajan işlevselliğini korurken zararlı veya yanıltıcı girdileri etkili bir şekilde tespit ettiğini göstermektedir. Dahası, kontrol akışı tabanlı yönetim mekanizması, halüsinasyon kaynaklı hataları ve diğer orkestrasyon seviyesi arızalarını azaltır."
    }
  },
  {
    "id": "2601.10413v1",
    "title": "LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies",
    "authors": [
      "Haiyue Yuan",
      "Nikolay Matyunin",
      "Ali Raza",
      "Shujun Li"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.10413v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10413v1",
    "content": {
      "en": "Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.",
      "tr": "Makale Başlığı: LADFA: Gizlilik Politikalarında Kişisel Veri Akışı Analizi İçin Large Language Models ve Retrieval-Augmented Generation Kullanımına Yönelik Bir Framework\n\nÖzet:\nGizlilik politikaları, kurumların kişisel verileri işleme uygulamaları hakkında insanları bilgilendirmeye yardımcı olur ve veri toplama, veri depolama ve kişisel verilerin üçüncü taraflarla paylaşılması gibi farklı yönleri kapsar. Kullanılan uzun ve karmaşık yasal dil ile farklı sektörler ve kurumlar arasındaki tutarsız uygulamalar nedeniyle, gizlilik politikalarının insanlar tarafından tam olarak anlaşılması genellikle zordur. Gizlilik politikalarının otomatik ve büyük ölçekli analizlerini gerçekleştirmeye yardımcı olmak amacıyla birçok araştırmacı, large language models (LLMs) dahil olmak üzere makine öğrenmesi ve doğal dil işleme tekniklerinin uygulamalarını incelemiştir. LLM'lerin gizlilik politikalarından kişisel veri akışlarını çıkarmak için kullanıldığı sınırlı sayıda önceki çalışma bulunurken, bizim yaklaşımımız, LLM'leri retrieval-augmented generation (RAG) ve mevcut çalışmalardan türetilmiş özel bir knowledge base ile birleştirerek bu çalışma alanını genişletmektedir. Bu makale, LADFA'nın, yani yapılandırılmamış metinleri verilen bir gizlilik politikasında işleyebilen, kişisel veri akışlarını çıkarıp bir personal data flow graph oluşturabilen ve içgörü keşfini kolaylaştırmak amacıyla data flow graph analizini gerçekleştirebilen uçtan uca bir computational framework'ün geliştirilmesini sunmaktadır. Framework, bir pre-processor, bir LLM-based processor ve bir data flow post-processor'dan oluşmaktadır. Otomotiv endüstrisinden seçilen on gizlilik politikasının incelenmesini içeren bir vaka çalışması yürüterek önerilen yaklaşımın etkinliğini ve doğruluğunu gösterdik ve doğruladık. Dahası, LADFA'nın esnek ve özelleştirilebilir olacak şekilde tasarlandığı ve bu durumun gizlilik politikası analizinin ötesindeki çeşitli metin tabanlı analiz görevleri için uygun olmasını sağladığı belirtilmelidir."
    }
  },
  {
    "id": "2601.10338v1",
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "authors": [
      "Yi Liu",
      "Weizhe Wang",
      "Ruitao Feng",
      "Yao Zhang",
      "Guangquan Xu"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2601.10338v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10338v1",
    "content": {
      "en": "The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.",
      "tr": "**Makale Başlığı:** Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale\n\n**Özet:**\n\nYapay zeka agent framework'lerinin yükselişi, agent yeteneklerini dinamik olarak genişleten talimatlar ve yürütülebilir kod içeren modüler paketler olan agent skills'i ortaya çıkarmıştır. Bu mimari güçlü bir özelleştirme sağlarken, skills örtük güven ve minimum inceleme ile yürütülür, bu da önemli ancak henüz karakterize edilmemiş bir attack surface oluşturur. Bu gelişmekte olan ekosistemin ilk büyük ölçekli ampirik güvenlik analizini gerçekleştiriyoruz; iki büyük marketplace'ten 42.447 skill topladık ve SkillScan'ı kullanarak 31.132'sini sistematik olarak analiz ettik. SkillScan, static analysis'i LLM-based semantic classification ile entegre eden çok aşamalı bir detection framework'üdür. Bulgularımız yaygın güvenlik risklerini ortaya koymaktadır: skills'lerin %26,1'i dört kategori altında 14 farklı pattern'ı kapsayan en az bir vulnerability içermektedir: prompt injection, data exfiltration, privilege escalation ve supply chain risks. Data exfiltration (%13,3) ve privilege escalation (%11,8) en yaygın olanlardır, bu arada skills'lerin %5,2'si kötü niyetli niyeti güçlü bir şekilde gösteren yüksek-severity pattern'lar sergilemektedir. Yürütülebilir script'leri paketleyen skills'lerin, sadece talimat içeren skills'lere göre vulnerability içerme olasılığının 2,12 kat daha fazla olduğunu bulduk (OR=2,12, p<0,001). Katkılarımız şunları içermektedir: (1) 8.126 vulnerable skill'den türetilmiş bir grounded vulnerability taxonomy, (2) %86,7 precision ve %82,5 recall'a ulaşan doğrulanmış bir detection methodology, ve (3) gelecekteki araştırmaları desteklemek için açık bir dataset ve detection toolkit. Bu sonuçlar, bu attack vector'ünün daha fazla istismar edilmesinden önce capability-based permission system'lerine ve zorunlu güvenlik incelemesine acil bir ihtiyaç olduğunu göstermektedir."
    }
  },
  {
    "id": "2601.10237v1",
    "title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
    "authors": [
      "Murat Bilgehan Ertan",
      "Marten van Dijk"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.10237v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10237v1",
    "content": {
      "en": "Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy   $σ\\ge \\frac{1}{\\sqrt{2\\ln M}}$ $\\quad\\text{or}\\quad$ $κ\\ge\\ \\frac{1}{\\sqrt{8}}\\!\\left(1-\\frac{1}{\\sqrt{4π\\ln M}}\\right)$,   and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \\to \\infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.",
      "tr": "Elbette, makale başlığı ve özetinin çevirisi aşağıdadır:\n\n**Makale Başlığı:** DP-SGD için Uyumlu Gizlilik-Fayda Garantilerinin Temel Sınırlılıkları\n\n**Özet:**\nDifferentially Private Stochastic Gradient Descent (DP-SGD), özel eğitim için baskın paradigmalarından biri olmasına rağmen, en kötü durum saldırılarına karşı türevsel gizlilik tanımları altındaki temel sınırlılıkları henüz tam olarak anlaşılmamıştır. DP-SGD'yi, gizliliği hipotez testleri arasındaki değişim eğrileri aracılığıyla karakterize eden $f$-differential privacy çerçevesinde analiz ediyoruz ve $M$ gradyan güncellemesi ile tek bir epoch boyunca karıştırılmış örneklemeyi (shuffled sampling) inceliyoruz. Elde edilebilecek değişim eğrisi üzerinde açıkça suboptimal bir üst sınır türetiyoruz. Bu sonuç, mekanizmanın değişim eğrisi ile ideal rastgele tahmin çizgisi arasındaki maksimum mesafe olan ayrım $κ$ üzerinde geometrik bir alt sınır oluşturur. Büyük bir ayrımın önemli bir saldırı avantajı anlamına gelmesi nedeniyle, anlamlı gizlilik küçük $κ$ gerektirir. Ancak, küçük bir ayrımın uygulanmasının Gaussian gürültü çarpanı $σ$ üzerinde sıkı bir alt sınır dayattığını kanıtlıyoruz, bu da elde edilebilir faydayı doğrudan sınırlar. Özellikle, standart en kötü durum saldırı modeli altında, karıştırılmış DP-SGD'nin şu şartları sağlaması gerekir: $σ\\ge \\frac{1}{\\sqrt{2\\ln M}}$ veya $κ\\ge\\ \\frac{1}{\\sqrt{8}}\\!\\left(1-\\frac{1}{\\sqrt{4π\\ln M}}\\right)$, ve dolayısıyla aynı anda hem güçlü gizliliği hem de yüksek faydayı elde edemez. Bu sınır $M \\to \\infty$ olarak asimptotik olarak sıfıra yaklaşsa da, yakınsama son derece yavaştır: pratik olarak ilgili sayıda güncelleme için bile gereken gürültü büyüklüğü önemli kalmaktadır. Aynı sınırlılığın sabit faktörlere kadar Poisson alt örneklemeye (Poisson subsampling) de uzandığını daha ileri sürüyoruz. Deneylerimiz, bu sınırın ima ettiği gürültü seviyelerinin gerçekçi eğitim ayarlarında önemli bir doğruluk düşüşüne yol açtığını doğrulamaktadır, böylece standart en kötü durum saldırı varsayımları altında DP-SGD'de kritik bir darboğaz göstermektedir."
    }
  },
  {
    "id": "2601.10212v1",
    "title": "PADER: Paillier-based Secure Decentralized Social Recommendation",
    "authors": [
      "Chaochao Chen",
      "Jiaming Qian",
      "Fei Zheng",
      "Yachuan Liu"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.10212v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10212v1",
    "content": {
      "en": "The prevalence of recommendation systems also brings privacy concerns to both the users and the sellers, as centralized platforms collect as much data as possible from them. To keep the data private, we propose PADER: a Paillier-based secure decentralized social recommendation system. In this system, the users and the sellers are nodes in a decentralized network. The training and inference of the recommendation model are carried out securely in a decentralized manner, without the involvement of a centralized platform. To this end, we apply the Paillier cryptosystem to the SoReg (Social Regularization) model, which exploits both user's ratings and social relations. We view the SoReg model as a two-party secure polynomial evaluation problem and observe that the simple bipartite computation may result in poor efficiency. To improve efficiency, we design secure addition and multiplication protocols to support secure computation on any arithmetic circuit, along with an optimal data packing scheme that is suitable for the polynomial computations of real values. Experiment results show that our method only takes about one second to iterate through one user with hundreds of ratings, and training with ~500K ratings for one epoch only takes <3 hours, which shows that the method is practical in real applications. The code is available at https://github.com/GarminQ/PADER.",
      "tr": "Makale Başlığı: PADER: Paillier Tabanlı Güvenli Merkeziyetsiz Sosyal Tavsiye Sistemi\n\nÖzet:\nTavsiye sistemlerinin yaygınlaşması, hem kullanıcılar hem de satıcılar için gizlilik endişelerini de beraberinde getirmektedir, zira merkezi platformlar onlardan mümkün olduğunca çok veri toplar. Veri gizliliğini korumak amacıyla, Paillier tabanlı güvenli merkeziyetsiz sosyal tavsiye sistemi olan PADER'i öneriyoruz. Bu sistemde, kullanıcılar ve satıcılar merkeziyetsiz bir ağdaki düğümlerdir. Tavsiye modelinin eğitimi ve çıkarımı, merkezi bir platformun müdahalesi olmadan, güvenli bir şekilde ve merkeziyetsiz bir biçimde gerçekleştirilir. Bu amaçla, hem kullanıcının derecelendirmelerini hem de sosyal ilişkilerini kullanan SoReg (Social Regularization) modeline Paillier kriptosistemini uyguluyoruz. SoReg modelini, iki taraflı güvenli polinom değerlendirme problemi olarak ele alıyoruz ve basit iki taraflı hesaplamanın yetersiz verimliliğe yol açabileceğini gözlemliyoruz. Verimliliği artırmak için, herhangi bir aritmetik devreyi destekleyen güvenli toplama ve çarpma protokolleri tasarlıyoruz ve gerçek değerlerin polinom hesaplamalarına uygun optimal bir veri paketleme şeması geliştiriyoruz. Deney sonuçları, yöntemimizin yüzlerce derecelendirmeye sahip bir kullanıcı üzerinde yalnızca yaklaşık bir saniye sürdüğünü ve ~500 bin derecelendirme ile bir epoch'luk eğitimin yalnızca 3 saatten az sürdüğünü göstermektedir. Bu da yöntemin gerçek uygulamalarda pratik olduğunu ortaya koymaktadır. Kod, https://github.com/GarminQ/PADER adresinden edinilebilir."
    }
  },
  {
    "id": "2601.10173v1",
    "title": "ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack",
    "authors": [
      "Hao Li",
      "Yankai Yang",
      "G. Edward Suh",
      "Ning Zhang",
      "Chaowei Xiao"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2601.10173v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10173v1",
    "content": {
      "en": "Large Language Models (LLMs) have enabled the development of powerful agentic systems capable of automating complex workflows across various fields. However, these systems are highly vulnerable to indirect prompt injection attacks, where malicious instructions embedded in external data can hijack agent behavior. In this work, we present ReasAlign, a model-level solution to improve safety alignment against indirect prompt injection attacks. The core idea of ReasAlign is to incorporate structured reasoning steps to analyze user queries, detect conflicting instructions, and preserve the continuity of the user's intended tasks to defend against indirect injection attacks. To further ensure reasoning logic and accuracy, we introduce a test-time scaling mechanism with a preference-optimized judge model that scores reasoning steps and selects the best trajectory. Comprehensive evaluations across various benchmarks show that ReasAlign maintains utility comparable to an undefended model while consistently outperforming Meta SecAlign, the strongest prior guardrail. On the representative open-ended CyberSecEval2 benchmark, which includes multiple prompt-injected tasks, ReasAlign achieves 94.6% utility and only 3.6% ASR, far surpassing the state-of-the-art defensive model of Meta SecAlign (56.4% utility and 74.4% ASR). These results demonstrate that ReasAlign achieves the best trade-off between security and utility, establishing a robust and practical defense against prompt injection attacks in real-world agentic systems. Our code and experimental results could be found at https://github.com/leolee99/ReasAlign.",
      "tr": "**Makale Başlığı:** ReasAlign: Prompt Injection Saldırılarına Karşı Muhakeme ile Güçlendirilmiş Güvenlik Uyumluluğu\n\n**Özet:**\n\nBüyük Dil Modelleri (LLM), çeşitli alanlarda karmaşık iş akışlarını otomatikleştirebilen güçlü agentic sistemlerin geliştirilmesine olanak tanımıştır. Ancak, bu sistemler, harici verilere gömülü kötü niyetli talimatların agent davranışını ele geçirebildiği dolaylı prompt injection saldırılarına karşı oldukça savunmasızdır. Bu çalışmada, dolaylı prompt injection saldırılarına karşı güvenlik uyumluluğunu iyileştirmek için model düzeyinde bir çözüm olan ReasAlign'ı sunuyoruz. ReasAlign'ın temel fikri, kullanıcı sorgularını analiz etmek, çelişkili talimatları tespit etmek ve dolaylı injection saldırılarına karşı savunma yapmak için kullanıcının amaçladığı görevlerin sürekliliğini korumak amacıyla yapılandırılmış reasoning adımlarını dahil etmektir. Reasoning mantığını ve doğruluğunu daha da sağlamak için, reasoning adımlarını puanlayan ve en iyi trajectory'yi seçen bir preference-optimized judge model ile bir test-time scaling mekanizması sunuyoruz. Çeşitli benchmark'larda yapılan kapsamlı değerlendirmeler, ReasAlign'ın savunmasız bir modelle karşılaştırılabilir bir utility sürdürdüğünü ve en güçlü önceki guardrail olan Meta SecAlign'ı tutarlı bir şekilde geride bıraktığını göstermektedir. Birden fazla prompt-injected görevi içeren temsili açık uçlu CyberSecEval2 benchmark'ında ReasAlign, %94,6 utility ve sadece %3,6 ASR elde ederek, Meta SecAlign'ın state-of-the-art savunma modelini (56,4% utility ve 74,4% ASR) önemli ölçüde aşmaktadır. Bu sonuçlar, ReasAlign'ın güvenlik ve utility arasında en iyi dengeyi sağladığını ve gerçek dünya agentic sistemlerinde prompt injection saldırılarına karşı sağlam ve pratik bir savunma oluşturduğunu göstermektedir. Kodumuz ve deneysel sonuçlarımız https://github.com/leolee99/ReasAlign adresinde bulunabilir."
    }
  },
  {
    "id": "2601.10004v1",
    "title": "SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations",
    "authors": [
      "Mohoshin Ara Tahera",
      "Karamveer Singh Sidhu",
      "Shuvalaxmi Dass",
      "Sajal Saha"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.10004v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10004v1",
    "content": {
      "en": "Large Language Models (LLMs) are increasingly adopted in healthcare to support clinical decision-making, summarize electronic health records (EHRs), and enhance patient care. However, this integration introduces significant privacy and security challenges, driven by the sensitivity of clinical data and the high-stakes nature of medical workflows. These risks become even more pronounced across heterogeneous deployment environments, ranging from small on-premise hospital systems to regional health networks, each with unique resource limitations and regulatory demands. This Systematization of Knowledge (SoK) examines the evolving threat landscape across the three core LLM phases: Data preprocessing, Fine-tuning, and Inference within realistic healthcare settings. We present a detailed threat model that characterizes adversaries, capabilities, and attack surfaces at each phase, and we systematize how existing privacy-preserving techniques (PPTs) attempt to mitigate these vulnerabilities. While existing defenses show promise, our analysis identifies persistent limitations in securing sensitive clinical data across diverse operational tiers. We conclude with phase-aware recommendations and future research directions aimed at strengthening privacy guarantees for LLMs in regulated environments. This work provides a foundation for understanding the intersection of LLMs, threats, and privacy in healthcare, offering a roadmap toward more robust and clinically trustworthy AI systems.",
      "tr": "Makale Başlığı: SoK: Sağlık Hizmetlerinde Gizlilik Farkındalığına Sahip LLM: Tehdit Modeli, Gizlilik Teknikleri, Zorluklar ve Öneriler\n\nÖzet:\nLarge Language Models (LLM'ler), klinik karar destekleme, elektronik sağlık kayıtlarını (EHR) özetleme ve hasta bakımını iyileştirme amacıyla sağlık hizmetlerinde giderek daha fazla benimsenmektedir. Ancak bu entegrasyon, klinik verilerin hassasiyeti ve tıbbi iş akışlarının yüksek riskli doğası nedeniyle önemli gizlilik ve güvenlik zorlukları ortaya çıkarmaktadır. Bu riskler, her biri kendine özgü kaynak sınırlamalarına ve düzenleyici taleplere sahip olan küçük yerel hastane sistemlerinden bölgesel sağlık ağlarına kadar çeşitlilik gösteren heterojen dağıtım ortamlarında daha da belirgin hale gelmektedir. Bu Systematization of Knowledge (SoK), gerçekçi sağlık hizmetleri ortamlarındaki üç temel LLM aşaması boyunca gelişen tehdit manzarasını incelemektedir: Veri ön işleme, Fine-tuning ve Inference. Her aşamadaki saldırganları, yetenekleri ve saldırı yüzeylerini karakterize eden ayrıntılı bir tehdit modeli sunuyoruz ve mevcut gizlilik koruyucu tekniklerin (PPTs) bu savunmasızlıkları nasıl azaltmaya çalıştığını sistematize ediyoruz. Mevcut savunmalar umut verici olsa da, analizimiz çeşitli operasyonel katmanlarda hassas klinik verilerin güvenliğini sağlama konusunda kalıcı sınırlamaları tespit etmektedir. Düzenlenmiş ortamlarda LLM'ler için gizlilik garantilerini güçlendirmeyi amaçlayan aşamaya duyarlı öneriler ve gelecek araştırma yönleri ile sonuca ulaşıyoruz. Bu çalışma, LLM'lerin, tehditlerin ve gizliliğin sağlık hizmetlerindeki kesişimini anlama konusunda bir temel sağlamakta, daha sağlam ve klinik olarak güvenilir yapay zeka sistemlerine yönelik bir yol haritası sunmaktadır."
    }
  },
  {
    "id": "2601.09946v1",
    "title": "Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains",
    "authors": [
      "Chenxi Qiu"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.09946v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09946v1",
    "content": {
      "en": "Metric Differential Privacy (mDP) generalizes Local Differential Privacy (LDP) by adapting privacy guarantees based on pairwise distances, enabling context-aware protection and improved utility. While existing optimization-based methods reduce utility loss effectively in coarse-grained domains, optimizing mDP in fine-grained or continuous settings remains challenging due to the computational cost of constructing dense perterubation matrices and satisfying pointwise constraints.   In this paper, we propose an interpolation-based framework for optimizing lp-norm mDP in such domains. Our approach optimizes perturbation distributions at a sparse set of anchor points and interpolates distributions at non-anchor locations via log-convex combinations, which provably preserve mDP. To address privacy violations caused by naive interpolation in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms. in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms.",
      "tr": "İşte istenen çeviri:\n\n**Makale Başlığı:** Sürekli ve İnce Taneli Alanlarda Lp-Norm Metrik Hassasiyetinin Uygulanması için İnterpolasyon Tabanlı Optimizasyon\n\n**Özet:**\n\nMetric Differential Privacy (mDP), pairwise mesafeler temelinde gizlilik güvencelerini adapte ederek, bağlam-farkındalığına sahip koruma ve geliştirilmiş fayda sağlayan Local Differential Privacy'nin (LDP) genelleştirilmesidir. Mevcut optimizasyon tabanlı yöntemler, kaba taneli alanlarda fayda kaybını etkili bir şekilde azaltırken, ince taneli veya sürekli ortamlarda mDP'yi optimize etmek, yoğun pertürbasyon matrisleri oluşturmanın ve pointwise kısıtlamaları yerine getirmenin hesaplama maliyeti nedeniyle zorluğunu korumaktadır. Bu çalışmada, bu tür alanlarda lp-norm mDP'yi optimize etmek için interpolasyon tabanlı bir çerçeve öneriyoruz. Yaklaşımımız, seyrek bir anchor point kümesi üzerindeki pertürbasyon dağılımlarını optimize eder ve mDP'yi ispatlanabilir bir şekilde koruyan log-konveks kombinasyonlar aracılığıyla anchor olmayan konumlardaki dağılımları enterpole eder. Yüksek boyutlu uzaylarda saf enterpolasyondan kaynaklanan gizlilik ihlallerini ele almak için, enterpolasyon sürecini bir dizi tek boyutlu adıma ayırırız ve tasarıma göre lp-norm mDP'yi uygulayan düzeltilmiş bir formülasyon türetiriz. Ayrıca, pertürbasyon dağılımları ve boyutlar arası gizlilik bütçesi tahsisi üzerinden ortak optimizasyonu da araştırıyoruz. Gerçek dünya konum veri kümeleri üzerindeki deneyler, yöntemimizin ince taneli alanlarda titiz gizlilik güvenceleri ve rekabetçi fayda sunduğunu ve temel mekanizmalardan daha iyi performans gösterdiğini göstermektedir."
    }
  },
  {
    "id": "2601.09933v1",
    "title": "Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method",
    "authors": [
      "Ashish Anand",
      "Bhupendra Singh",
      "Sunil Khemka",
      "Bireswar Banerjee",
      "Vishi Singh Bhatia"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.09933v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09933v1",
    "content": {
      "en": "Android malware has become an increasingly critical threat to organizations, society and individuals, posing significant risks to privacy, data security and infrastructure. As malware continues to evolve in terms of complexity and sophistication, the mitigation and detection of these malicious software instances have become more time consuming and challenging particularly due to the requirement of large number of features to identify potential malware. To address these challenges, this research proposes Fast Gradient Sign Method with Diluted Convolutional Neural Network (FGSM DICNN) method for malware classification. DICNN contains diluted convolutions which increases receptive field, enabling the model to capture dispersed malware patterns across long ranges using fewer features without adding parameters. Additionally, the FGSM strategy enhance the accuracy by using one-step perturbations during training that provides more defensive advantage of lower computational cost. This integration helps to manage high classification accuracy while reducing the dependence on extensive feature sets. The proposed FGSM DICNN model attains 99.44% accuracy while outperforming other existing approaches such as Custom Deep Neural Network (DCNN).",
      "tr": "Elbette, akademik makale başlığını ve özetini istenen şekilde Türkçeye çevirdim:\n\n**Makale Başlığı:** Fast Gradient Sign Method ile Seyreltilmiş Evrişimsel Sinir Ağı Kullanılarak Zararlı Yazılım Sınıflandırması\n\n**Özet:**\nAndroid zararlı yazılımları, organizasyonlar, toplum ve bireyler için giderek artan kritik bir tehdit haline gelmiş olup gizlilik, veri güvenliği ve altyapı için önemli riskler barındırmaktadır. Zararlı yazılımların karmaşıklık ve sofistike olma açısından evrilmeye devam etmesiyle birlikte, bu kötü amaçlı yazılım örneklerinin azaltılması ve tespiti, potansiyel zararlı yazılımları tanımlamak için büyük sayıda özelliğe duyulan ihtiyaç nedeniyle özellikle daha zaman alıcı ve zorlu hale gelmiştir. Bu zorlukların üstesinden gelmek için bu araştırma, zararlı yazılım sınıflandırması için Fast Gradient Sign Method ile Seyreltilmiş Evrişimsel Sinir Ağı (FGSM DICNN) yöntemini önermektedir. DICNN, alıcı alanını artıran seyreltilmiş evrişimler içerir ve bu da modelin daha az özellik kullanarak, parametre eklemeden uzun menzillerdeki dağılmış zararlı yazılım örüntülerini yakalamasını sağlar. Ek olarak, FGSM stratejisi, düşük hesaplama maliyetinin daha fazla savunma avantajı sağladığı eğitim sırasında tek adımlı pertürbasyonlar kullanarak doğruluğu artırır. Bu entegrasyon, kapsamlı özellik kümelerine olan bağımlılığı azaltırken yüksek sınıflandırma doğruluğunu yönetmeye yardımcı olur. Önerilen FGSM DICNN modeli, Custom Deep Neural Network (DCNN) gibi diğer mevcut yaklaşımları geride bırakarak %99,44 doğruluk elde etmektedir."
    }
  },
  {
    "id": "2601.09902v1",
    "title": "A Novel Contrastive Loss for Zero-Day Network Intrusion Detection",
    "authors": [
      "Jack Wilkie",
      "Hanan Hindy",
      "Craig Michie",
      "Christos Tachtatzis",
      "James Irvine"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2601.09902v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09902v1",
    "content": {
      "en": "Machine learning has achieved state-of-the-art results in network intrusion detection; however, its performance significantly degrades when confronted by a new attack class -- a zero-day attack. In simple terms, classical machine learning-based approaches are adept at identifying attack classes on which they have been previously trained, but struggle with those not included in their training data. One approach to addressing this shortcoming is to utilise anomaly detectors which train exclusively on benign data with the goal of generalising to all attack classes -- both known and zero-day. However, this comes at the expense of a prohibitively high false positive rate. This work proposes a novel contrastive loss function which is able to maintain the advantages of other contrastive learning-based approaches (robustness to imbalanced data) but can also generalise to zero-day attacks. Unlike anomaly detectors, this model learns the distributions of benign traffic using both benign and known malign samples, i.e. other well-known attack classes (not including the zero-day class), and consequently, achieves significant performance improvements. The proposed approach is experimentally verified on the Lycos2017 dataset where it achieves an AUROC improvement of .000065 and .060883 over previous models in known and zero-day attack detection, respectively. Finally, the proposed method is extended to open-set recognition achieving OpenAUC improvements of .170883 over existing approaches.",
      "tr": "**Makale Başlığı:** A Novel Contrastive Loss for Zero-Day Network Intrusion Detection\n\n**Özet:**\n\nMakine öğrenimi, ağ saldırı tespiti alanında son teknoloji sonuçlar elde etmiştir; ancak, yeni bir saldırı sınıfı olan sıfır gün (zero-day) saldırılarla karşılaştığında performansı önemli ölçüde düşmektedir. Basit bir ifadeyle, klasik makine öğrenimi tabanlı yaklaşımlar, daha önce eğitildikleri saldırı sınıflarını tanımlamakta ustadır, ancak eğitim verilerine dahil edilmeyenlerle mücadele ederler. Bu eksikliğin üstesinden gelmek için bir yaklaşım, tüm saldırı sınıflarına – hem bilinen hem de zero-day – genelleme amacıyla yalnızca iyi huylu (benign) veriler üzerinde eğitilen anomali dedektörlerinin kullanılmasıdır. Ancak bu durum, kabul edilemez derecede yüksek bir yanlış pozitif oranı pahasına gelmektedir. Bu çalışma, diğer contrastive learning tabanlı yaklaşımların avantajlarını (dengesiz verilere karşı dayanıklılık) koruyabilen ancak zero-day saldırılara da genelleme yapabilen yeni bir contrastive loss fonksiyonu önermektedir. Anomali dedektörlerinin aksine, bu model iyi huylu trafik dağılımlarını hem iyi huylu hem de bilinen kötü niyetli örnekleri (yani zero-day sınıfı hariç diğer iyi bilinen saldırı sınıflarını) kullanarak öğrenir ve dolayısıyla önemli performans artışları elde eder. Önerilen yaklaşım, Lycos2017 veri kümesi üzerinde deneysel olarak doğrulanmış olup, bilinen ve zero-day saldırı tespitinde sırasıyla önceki modellere göre .000065 ve .060883 AUROC iyileşmesi sağlamıştır. Son olarak, önerilen yöntem açık set tanıma (open-set recognition) alanına genişletilerek mevcut yaklaşımlara göre .170883 OpenAUC iyileşmesi elde etmiştir."
    }
  }
]