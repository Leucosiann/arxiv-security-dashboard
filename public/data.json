[
  {
    "id": "2512.21241v1",
    "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
    "authors": [
      "Xinjie Xu",
      "Shuyu Cheng",
      "Dongwei Xu",
      "Qi Xuan",
      "Chen Ma"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2512.21241v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21241v1",
    "content": {
      "en": "In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum $\\ell_2$-norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, which proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum. We provide a theoretical analysis of its convergence behavior, showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization. To further accelerate convergence, we incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance. The superiority of our approach is supported by theoretical guarantees under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that our method surpasses 13 state-of-the-art approaches in query efficiency.",
      "tr": "Makale Başlığı: Query-Efficient Hard-Label Saldırıları için Ray Search Optimizasyonunun Yakınsama Hızının İyileştirilmesi\n\nÖzet:\nHard-label black-box adversarial saldırılarında, yalnızca en yüksek tahmini etiketin erişilebilir olduğu durumlarda, fahiş query karmaşıklığı pratik dağıtım önünde büyük bir engel teşkil etmektedir. Bu makalede, zararsız bir görüntüyü adversarial bölgeye taşımak için gereken minimum $\\ell_2$-norm pertürbasyonunu sağlayan optimal ray yönünü arayan, temsilci bir saldırı sınıfını optimize etmeye odaklanıyoruz. Nesterov's Accelerated Gradient (NAG)'den esinlenerek, birikmiş momentumdan çıkarılan gelecekteki bir ray yönüne göre gradyanı proaktif olarak tahmin eden momentum tabanlı bir algoritma olan ARS-OPT'yi öneriyoruz. Yakınsama davranışının teorik analizini sunarak, ARS-OPT'nin daha doğru yönsel güncellemeler sağladığını ve daha hızlı, daha kararlı optimizasyon elde ettiğini gösteriyoruz. Yakınsamayı daha da hızlandırmak için, PARS-OPT ile sonuçlanan, gelişmiş performans sunan ARS-OPT'nin gradyan tahminine surrogate-model priors'ları entegre ediyoruz. Yaklaşımımızın üstünlüğü, standart varsayımlar altında teorik garantilerle desteklenmektedir. ImageNet ve CIFAR-10 üzerindeki kapsamlı deneyler, yöntemimizin query verimliliğinde 13 adet state-of-the-art yaklaşımı geride bıraktığını göstermektedir."
    }
  },
  {
    "id": "2512.21238v1",
    "title": "Assessing the Software Security Comprehension of Large Language Models",
    "authors": [
      "Mohammed Latif Siddiq",
      "Natalie Sekerak",
      "Antonio Karam",
      "Maria Leal",
      "Arvin Islam-Gomes"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.SE",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.21238v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21238v1",
    "content": {
      "en": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.",
      "tr": "Makale Başlığı: Büyük Dil Modellerinin Yazılım Güvenliği Anlayışının Değerlendirilmesi\n\nÖzet:\nBüyük dil modelleri (LLM'ler) yazılım geliştirmede giderek daha fazla kullanılmaktadır, ancak yazılım güvenliği uzmanlık düzeyleri belirsizliğini korumaktadır. Bu çalışma, Blooms Taksonomisi'ni bir çerçeve olarak kullanarak beş önde gelen LLM'nin (GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1 ve Qwen-2.5) güvenlik anlayışını sistematik olarak değerlendirmektedir. Altı bilişsel boyutu değerlendiriyoruz: hatırlama, anlama, uygulama, analiz etme, değerlendirme ve yaratma. Metodolojimiz, özenle seçilmiş çoktan seçmeli soruları, güvenlik açığı içeren kod parçacıklarını (SALLM), bir Yazılım Güvenliğine Giriş dersinden alınan kurs değerlendirmelerini, gerçek dünya vaka çalışmalarını (XBOW) ve Güvenli Yazılım Mühendisliği dersinden elde edilen proje tabanlı yaratım görevlerini içeren çeşitli veri setlerini entegre etmektedir. Sonuçlar, LLM'lerin bilgileri hatırlama ve bilinen güvenlik açıklarını belirleme gibi alt düzey bilişsel görevlerde iyi performans gösterdiğini, ancak reasoning, mimari değerlendirme ve güvenli sistem oluşturma gibi üst düzey görevlerde performanslarının önemli ölçüde düştüğünü göstermektedir. Toplam doğruluğu raporlamanın ötesinde, bir modelin güvenilir performansını tutarlı bir şekilde sürdürdüğü en yüksek bilişsel düzeyi belirleyen bir yazılım güvenliği knowledge boundary tanıtıyoruz. Ek olarak, Blooms düzeylerinde LLM'ler tarafından sergilenen 51 tekrarlayan yanlış anlama deseni tespit ettik."
    }
  },
  {
    "id": "2512.21236v1",
    "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
    "authors": [
      "Yifan Huang",
      "Xiaojun Jia",
      "Wenbo Guo",
      "Yuqiang Sun",
      "Yihao Huang"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2512.21236v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21236v1",
    "content": {
      "en": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",
      "tr": "Kesinlikle, işte akademik makalenin başlığı ve özetinin istenen özelliklere göre çevrilmiş hali:\n\n**Makale Başlığı:** SPELL'in Kullanımı: LLM Sınırlamalarını Kırmada Cümle Eşleştirme Keşfi\n\n**Özet:**\nBüyük dil modelleri (LLM'ler), yapay zeka destekli kodlama araçları aracılığıyla yazılım geliştirmede devrim yaratmış, sınırlı programlama uzmanlığına sahip geliştiricilerin sofistike uygulamalar oluşturmalarını sağlamıştır. Ancak bu erişilebilirlik, bu güçlü araçları zararlı yazılımlar üretmek için istismar edebilecek kötü niyetli aktörlere de uzanmaktadır. Mevcut jailbreaking araştırmaları öncelikli olarak LLM'lere karşı genel saldırı senaryolarına odaklanmakta, malicious code generation'ı bir jailbreak hedefi olarak keşfetme konusunda ise sınırlı kalmaktadır. Bu boşluğu gidermek amacıyla, malicious code generation'daki güvenlik hizalanmasının (security alignment) zayıflıklarını değerlendirmek üzere özel olarak tasarlanmış kapsamlı bir test çerçevesi olan SPELL'i öneriyoruz. Çerçevemiz, önceden var olan bir knowledge graph'ten gelen cümleleri akıllıca birleştirerek jailbreaking prompt'ları sistematik olarak oluşturan bir zaman bölümlemesi seçimi (time-division selection) stratejisi kullanır; bu strateji, yeni saldırı modellerinin keşfedilmesi ile başarılı tekniklerin istismar edilmesi arasında bir denge kurar. Üç gelişmiş kod modelinde (GPT-4.1, Claude-3.5 ve Qwen2.5-Coder) gerçekleştirilen kapsamlı değerlendirme, SPELL'in etkinliğini göstermekte ve sekiz malicious code kategorisi genelinde sırasıyla %83.75, %19.38 ve %68.12 saldırı başarı oranları elde etmektedir. Üretilen prompt'lar, Cursor gibi gerçek dünya yapay zeka geliştirme araçlarında başarıyla malicious code üretmekte ve çıktılar, %73'ün üzerinde oranlarla en gelişmiş tespit sistemleri tarafından zararlı olarak doğrulanmaktadır. Bu bulgular, mevcut LLM uygulamalarındaki önemli güvenlik açıklarını ortaya koymakta ve kod üretimi uygulamalarında AI safety alignment'ı iyileştirmek için değerli içgörüler sunmaktadır."
    }
  },
  {
    "id": "2512.21132v1",
    "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
    "authors": [
      "Tobias von Arx",
      "Niels Mündler",
      "Mark Vero",
      "Maximilian Baader",
      "Martin Vechev"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "link": "http://arxiv.org/abs/2512.21132v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21132v1",
    "content": {
      "en": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.",
      "tr": "**Makale Başlığı:** AutoBaxBuilder: Kod Güvenliği Kıyaslamasının Önyüklemesi\n\n**Özet:**\n\nYazılım mühendisliğinde LLM'lerin yaygın olarak benimsenmesiyle birlikte, LLM tarafından üretilen kodun doğruluğunun ve güvenliğinin güvenilir bir şekilde değerlendirilmesi kritik öneme sahiptir. Özellikle, önceki çalışmalar güvenliğin genellikle göz ardı edildiğini göstermiş, LLM'lerin güvenlik açıkları içeren kod üretmeye yatkın olduğunu ortaya koymuştur. Bu içgörüler, güvenlik uzmanları tarafından önemli ölçüde manuel çaba ile oluşturulan özel kıyaslamalar sayesinde mümkün olmuştur. Ancak, manuel olarak hazırlanmış kıyaslamalara güvenmek uzun vadede yetersizdir, çünkü kıyaslamalar (i) doğal olarak eğitim verilerini kirletir, (ii) daha eksiksiz bir resim sunmak için yeni görevlere genişletilmeli ve (iii) daha yetenekli LLM'leri zorlamak için zorluk seviyeleri artırılmalıdır. Bu çalışmada, bu zorlukları ele alıyoruz ve sıfırdan kod güvenliği kıyaslaması için görevler ve testler üreten bir framework olan AutoBaxBuilder'ı sunuyoruz. İşlevsellik testleri ve uçtan uca güvenlik sorgulama exploitleri oluşturmak için LLM'lerin kod anlama yeteneklerinden yararlanarak, ince taneli makuliyet kontrolleri ile sağlam bir pipeline tanıtıyoruz. Üretilen kıyaslamanın kalitesini doğrulamak için hem nitel bir analiz gerçekleştiriyoruz hem de insan uzmanlar tarafından oluşturulan görevlerle karşılaştırarak nicel deneyler yapıyoruz. AutoBaxBuilder'ı tamamen yeni görevler oluşturmak için kullanıyoruz ve bunları AutoBaxBench olarak kamuoyuna sunuyoruz, LLM'lerin bu görevlerdeki güvenlik yeteneklerinin kapsamlı bir değerlendirmesi ile birlikte. Yeni bir görevin 2 saatten kısa sürede ve 10 ABD Doları'ndan az bir maliyetle üretilebildiğini buluyoruz."
    }
  },
  {
    "id": "2512.21110v1",
    "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent",
    "authors": [
      "Ahmed M. Hussain",
      "Salahuddin Salahuddin",
      "Panos Papadimitratos"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ],
    "link": "http://arxiv.org/abs/2512.21110v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21110v1",
    "content": {
      "en": "Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.",
      "tr": "**Makale Başlığı:** Bağlamın Ötesinde: Büyük Dil Modellerinin Kullanıcı Niyetini Kavramadaki Başarısızlığı\n\n**Özet:**\n\nMevcut Büyük Dil Modelleri (LLMs) güvenlik yaklaşımları, açıkça zararlı içeriklere odaklanırken kritik bir zafiyeti göz ardı etmektedir: bağlamı anlama ve kullanıcı niyetini tanıma konusundaki yetersizlik. Bu durum, kötü niyetli kullanıcıların güvenlik mekanizmalarını sistematik olarak atlatmak için istismar edebileceği savunmasızlıklar yaratmaktadır. ChatGPT, Claude, Gemini ve DeepSeek dahil olmak üzere birden fazla state-of-the-art LLMs'yi ampirik olarak değerlendiriyoruz. Analizimiz, duygusal çerçeveleme, aşamalı ifşa ve akademik gerekçelendirme teknikleri aracılığıyla güvenilir güvenlik mekanizmalarının atlatılmasını göstermektedir. Özellikle, reasoning-enabled konfigürasyonlar, gerçek bilgilerin hassasiyetini artırırken altta yatan niyeti sorgulamada başarısız olarak, istismarın etkinliğini azaltmak yerine artırmıştır. İstisna, bazı kullanım durumlarında bilgi sağlamaya öncelik vermek yerine intent detection'a öncelik veren Claude Opus 4.1 olmuştur. Bu örüntü, mevcut mimari tasarımların sistematik zafiyetler yarattığını ortaya koymaktadır. Bu sınırlamalar, post-hoc koruyucu mekanizmalar yerine, bağlamsal anlayış ve intent recognition'a doğru paradigmatik kaymalar gerektirmektedir."
    }
  },
  {
    "id": "2512.21048v1",
    "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
    "authors": [
      "Savvy Sharma",
      "George Petrovic",
      "Sarthak Kaushik"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.21048v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21048v1",
    "content": {
      "en": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.",
      "tr": "İşte istenen akademik makale başlığı ve özetinin Türkçeye çevirisi:\n\n**Makale Başlığı:** zkFL-Health: Tıbbi Yapay Zeka Gizliliği için Blok Zinciri Destekli Sıfır Bilgi Birleşik Öğrenme\n\n**Özet:**\n\nSağlık alanındaki yapay zeka (AI), büyük ve çeşitli veri kümelerine ihtiyaç duyar; ancak katı gizlilik ve yönetim kısıtlamaları, kurumlar arasında ham veri paylaşımını engellemektedir. Birleşik öğrenme (FL), verilerin bulunduğu yerde eğitim yaparak ve yalnızca model güncellemelerini değiş tokuş ederek bu durumu hafifletir; ancak pratik uygulamalar hala iki temel riskle karşı karşıyadır: (1) gizlilik sızıntısı aracılığıyla gradyanlar veya güncellemeler (membership inference, gradient inversion) ve (2) tek hata noktası olan ve katkıları tespit edilmeden düşürebilen, değiştirebilen veya enjekte edebilen toplayıcıya (aggregator) duyulan güven. Biz, tıbbi yapay zeka için gizliliği koruyan, doğrulanabilir şekilde doğru işbirlikçi eğitimi sağlamak üzere FL'yi zero-knowledge proofs (ZKPs) ve Trusted Execution Environments (TEEs) ile birleştiren bir mimari olan zkFL-Health'i sunuyoruz. İstemciler (clients) yerel olarak modellerini eğitir ve güncellemelerini commiT ederler; toplayıcı, küresel güncellemeyi hesaplamak için bir TEE içinde çalışır ve herhangi bir istemci güncellemesini ana bilgisayara (host) açıklamadan, tam olarak commiT edilmiş girdileri ve doğru aggregation kuralını kullandığına dair özlü bir ZK proof (Halo2/Nova aracılığıyla) üretir. Doğrulayıcı (verifier) düğümler, proof'u doğrular ve değiştirilemez bir denetim izi sağlayan kriptografik commiT'ları on-chain'e kaydeder, böylece tek bir tarafa güvenme ihtiyacını ortadan kaldırır. Sağlık hizmetlerine yönelik sistem ve tehdit modellerini, zkFL-Health protokolünü, güvenlik/gizlilik garantilerini ve doğruluk, gizlilik riski, gecikme (latency) ve maliyeti kapsayan bir performans değerlendirme planını özetliyoruz. Bu çerçeve, klinik kabul ve düzenleyici uyumluluk için temel özellikler olan güçlü gizlilik (confidentiality), bütünlük (integrity) ve denetlenebilirlik (auditability) ile çok kurumlu tıbbi yapay zekayı mümkün kılar."
    }
  },
  {
    "id": "2512.20872v1",
    "title": "Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification",
    "authors": [
      "Jakir Hossain",
      "Gurvinder Singh",
      "Lukasz Ziarek",
      "Ahmet Erdem Sarıyüce"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.20872v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20872v1",
    "content": {
      "en": "Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.",
      "tr": "**Makale Başlığı:** Better Call Graphs: Kötü Amaçlı Yazılım Sınıflandırması İçin Yeni Bir Fonksiyon Çağrı Grafikleri Veri Kümesi\n\n**Özet:**\n\nFonksiyon çağrı grafikleri (FCGs), uygulamaların yüzeyel imzaların ötesine geçen davranışsal yapısını yakalayarak kötü amaçlı yazılım tespiti için güçlü bir soyutlama olarak ortaya çıkmıştır. Geleneksel program analizindeki kullanımları, zararlı yazılımların etkili sınıflandırılmasına ve analizine olanak tanıyarak iyi bir şekilde kanıtlanmıştır. Mobil alanda, özellikle Android ekosisteminde, platformun yaygın benimsenmesi ve Android uygulamalarının karmaşık, bileşen tabanlı yapısı nedeniyle FCG tabanlı kötü amaçlı yazılım sınıflandırması özellikle kritiktir. Ancak, bu yöndeki ilerleme, büyük ölçekli, yüksek kaliteli Android'e özgü FCG veri kümelerinin eksikliğinden dolayı engellenmektedir. Mevcut veri kümeleri genellikle güncelliğini yitirmiş, uygulama yeniden paketlenmesinden kaynaklanan küçük veya yinelenen grafiklerin hakim olduğu ve gerçek dünya kötü amaçlı yazılımlarının çeşitliliğini yansıtmada yetersiz kalmaktadır. Bu sınırlamalar, aşırı uyum ve grafik tabanlı sınıflandırma yöntemlerinin güvenilmez değerlendirilmesine yol açmaktadır. Bu açığı gidermek için, güncel Android uygulama paketlerinden (APK'lar) çıkarılmış büyük ve benzersiz FCG'lerden oluşan kapsamlı bir veri kümesi olan Better Call Graphs'ı (BCG) sunuyoruz. BCG, çeşitli aileleri ve türleri kapsayan hem zararsız hem de kötü amaçlı örnekleri ve her APK için grafik düzeyinde özellikleri içermektedir. Temel sınıflandırıcılar kullanılarak yapılan kapsamlı deneyler aracılığıyla, mevcut veri kümelerine kıyasla BCG'nin gerekliliğini ve değerini göstermekteyiz. BCG, https://erdemub.github.io/BCG-dataset adresinden kamuya açık olarak erişilebilir durumdadır."
    }
  },
  {
    "id": "2512.20712v1",
    "title": "Real-World Adversarial Attacks on RF-Based Drone Detectors",
    "authors": [
      "Omer Gazit",
      "Yael Itzhakev",
      "Yuval Elovici",
      "Asaf Shabtai"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.20712v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20712v1",
    "content": {
      "en": "Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.",
      "tr": "**Akademik Makale Başlığı:** Gerçek Dünya RF Tabanlı İHA Dedektörlerine Yönelik Adversarial Saldırılar\n\n**Özet:**\n\nRadyo frekansı (RF) tabanlı sistemler, İHA'ların RF sinyal modellerini analiz ederek, bunları nesne tespit modelleri tarafından işlenen spectrogram görüntülerine dönüştürerek giderek daha fazla kullanılmaktadır. Görüntü tabanlı modellere yönelik mevcut RF saldırıları, dijital özellikleri değiştirerek karasal (OTA) uygulamayı zorlaştırmaktadır. Bunun nedeni, dijital pertürbasyonların senkronizasyon hataları ve parazitlere neden olabilecek iletilebilir dalga formlarına dönüştürülmesindeki zorluk ve donanım sınırlamalarıyla karşılaşılmasıdır. RF görüntü tabanlı İHA dedektörlerine yönelik ilk fiziksel saldırıyı sunuyoruz. Bu saldırı, yasal iletişimlerle birlikte iletilen sınıf-spesifik universal complex baseband (I/Q) pertürbasyon dalga formlarını optimize etmektedir. Saldırıyı dört farklı İHA türüyle yapılan RF kayıtları ve OTA deneyleri kullanarak değerlendirdik. Sonuçlarımız, mütevazı, yapılandırılmış I/Q pertürbasyonlarının standart RF zincirleriyle uyumlu olduğunu ve yasal İHA'ların tespitini korurken hedef İHA tespitini güvenilir bir şekilde azalttığını göstermektedir."
    }
  },
  {
    "id": "2512.20423v1",
    "title": "Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit",
    "authors": [
      "Adam Elaoumari"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2512.20423v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20423v1",
    "content": {
      "en": "The purpose of this project is to assess how well defenders can detect DNS-over-HTTPS (DoH) file exfiltration, and which evasion strategies can be used by attackers. While providing a reproducible toolkit to generate, intercept and analyze DoH exfiltration, and comparing Machine Learning vs threshold-based detection under adversarial scenarios. The originality of this project is the introduction of an end-to-end, containerized pipeline that generates configurable file exfiltration over DoH using several parameters (e.g., chunking, encoding, padding, resolver rotation). It allows for file reconstruction at the resolver side, while extracting flow-level features using a fork of DoHLyzer. The pipeline contains a prediction side, which allows the training of machine learning models based on public labelled datasets and then evaluates them side-by-side with threshold-based detection methods against malicious and evasive DNS-Over-HTTPS traffic. We train Random Forest, Gradient Boosting and Logistic Regression classifiers on a public DoH dataset and benchmark them against evasive DoH exfiltration scenarios. The toolkit orchestrates traffic generation, file capture, feature extraction, model training and analysis. The toolkit is then encapsulated into several Docker containers for easy setup and full reproducibility regardless of the platform it is run on. Future research regarding this project is directed at validating the results on mixed enterprise traffic, extending the protocol coverage to HTTP/3/QUIC request, adding a benign traffic generation, and working on real-time traffic evaluation. A key objective is to quantify when stealth constraints make DoH exfiltration uneconomical and unworthy for the attacker.",
      "tr": "İşte akademik makale başlığının ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** DNS-over-HTTPS Veri Sızdırılmasının Kaçınmaya Dirençli Tespiti: Pratik Bir Değerlendirme ve Araç Seti\n\n**Özet:**\n\nBu projenin amacı, savunucuların DNS-over-HTTPS (DoH) dosya sızdırılmasını ne kadar iyi tespit edebildiğini ve saldırganların hangi kaçınma stratejilerini kullanabileceğini değerlendirmektir. DoH sızdırma işlemini üreten, kesen ve analiz eden tekrarlanabilir bir araç seti sağlarken, aynı zamanda Machine Learning ve threshold-based detection yöntemlerini düşmanca senaryolar altında karşılaştırmaktadır. Bu projenin özgünlüğü, çeşitli parametreler (örneğin, chunking, encoding, padding, resolver rotation) kullanarak DoH üzerinden yapılandırılabilir dosya sızdırma işlemi üreten uçtan uca, konteynerleştirilmiş bir pipeline'ın tanıtılmasıdır. Bu pipeline, resolver tarafında dosya yeniden yapılandırmasına izin verirken, DoHLyzer'ın bir fork'unu kullanarak flow-level features çıkarmaktadır. Pipeline, public labelled datasets'e dayanarak machine learning modellerinin eğitimini sağlayan ve ardından bunları malicious ve evasive DNS-Over-HTTPS trafiğine karşı threshold-based detection yöntemleriyle yan yana değerlendiren bir prediction side içermektedir. Random Forest, Gradient Boosting ve Logistic Regression sınıflandırıcılarını halka açık bir DoH veri kümesi üzerinde eğitiyor ve bunları evasive DoH sızdırma senaryolarına karşı kıyaslıyoruz. Araç seti, trafik üretimini, dosya yakalamayı, feature extraction'ı, model eğitimini ve analizi düzenlemektedir. Araç seti daha sonra üzerinde çalışıldığı platformdan bağımsız olarak kolay kurulum ve tam tekrarlanabilirlik için çeşitli Docker konteynerlerine kapsüllenmiştir. Bu proje ile ilgili gelecekteki araştırmalar, sonuçları karma kurumsal trafik üzerinde doğrulamaya, protokol kapsamını HTTP/3/QUIC isteklere genişletmeye, benign trafik üretimi eklemeye ve gerçek zamanlı trafik değerlendirmesi üzerinde çalışmaya yöneliktir. Temel bir hedef, stealth constraints'in DoH sızdırmayı saldırgan için ekonomik olmayan ve değersiz kıldığı zamanı ölçmektir."
    }
  },
  {
    "id": "2512.20396v1",
    "title": "Symmaries: Automatic Inference of Formal Security Summaries for Java Programs",
    "authors": [
      "Narges Khakpour",
      "Nicolas Berthier"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.FL",
      "cs.PL",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2512.20396v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20396v1",
    "content": {
      "en": "We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.",
      "tr": "**Makale Başlığı:** Symmaries: Java Programları İçin Formal Güvenlik Özetlerinin Otomatik Çıkarımı\n\n**Özet:**\n\nBu çalışmada, Java bytecode programları için, metot özetleri (method summaries) şeklinde, ölçeklenebilir, modüler ve sağlam bir yaklaşımla otomatik olarak formal güvenlik spesifikasyonları oluşturma yöntemini sunuyoruz. Bir özet, bir metotun güvenli bir şekilde çağrılabileceği koşullar ile bilgi akışları ve aliasing güncellemelerine ilişkin spesifikasyonları içeren, metotun güvenlik davranışının soyut bir temsilini sağlar. Bu tür özetler, statik kod analiz araçları tarafından kullanılabilir ve ayrıca geliştiricilerin, kütüphaneler gibi kod segmentlerinin güvenlik etkilerini değerlendirmek amacıyla davranışlarını anlamalarına yardımcı olur. Yaklaşımımız, güvenlik özetlerinin oluşturulmasını otomatikleştiren Symmaries adlı bir araçta uygulanmıştır. Symmaries'i, güvenlik spesifikasyonlarını çıkarmak için Java API kütüphanelerine ve ölçeklenebilirliğini değerlendirmek için büyük gerçek dünya uygulamalarına uyguladık. Sonuçlarımız, aracın yüz binlerce satır kod içeren uygulamaları analiz etmek için başarılı bir şekilde ölçeklendiğini ve Symmaries'in kullanılan heap modeline bağlı olarak umut verici bir precision elde ettiğini göstermektedir. Yaklaşımımızın termination-insensitive non-interference'ı garanti etmesi açısından sağlamlığını (soundness) kanıtladık."
    }
  }
]