[
  {
    "id": "2602.13156v1",
    "title": "In-Context Autonomous Network Incident Response: An End-to-End Large Language Model Agent Approach",
    "authors": [
      "Yiran Gao",
      "Kim Hammar",
      "Tao Li"
    ],
    "published_date": "2026-02-13",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.13156v1",
    "pdf_link": "https://arxiv.org/pdf/2602.13156v1",
    "content": {
      "en": "Rapidly evolving cyberattacks demand incident response systems that can autonomously learn and adapt to changing threats. Prior work has extensively explored the reinforcement learning approach, which involves learning response strategies through extensive simulation of the incident. While this approach can be effective, it requires handcrafted modeling of the simulator and suppresses useful semantics from raw system logs and alerts. To address these limitations, we propose to leverage large language models' (LLM) pre-trained security knowledge and in-context learning to create an end-to-end agentic solution for incident response planning. Specifically, our agent integrates four functionalities, perception, reasoning, planning, and action, into one lightweight LLM (14b model). Through fine-tuning and chain-of-thought reasoning, our LLM agent is capable of processing system logs and inferring the underlying network state (perception), updating its conjecture of attack models (reasoning), simulating consequences under different response strategies (planning), and generating an effective response (action). By comparing LLM-simulated outcomes with actual observations, the LLM agent repeatedly refines its attack conjecture and corresponding response, thereby demonstrating in-context adaptation. Our agentic approach is free of modeling and can run on commodity hardware. When evaluated on incident logs reported in the literature, our agent achieves recovery up to 23% faster than those of frontier LLMs.",
      "tr": "**Makale Başlığı:** Bağlam İçinde Otonom Ağ Olay Müdahalesi: Uçtan Uca Büyük Dil Modeli Ajan Yaklaşımı\n\n**Özet:**\nHızla gelişen siber saldırılar, değişen tehditlere otonom olarak öğrenebilen ve uyum sağlayabilen olay müdahale sistemleri gerektirmektedir. Önceki çalışmalar, olayın kapsamlı simülasyonu yoluyla müdahale stratejileri öğrenmeyi içeren pekiştirmeli öğrenme yaklaşımını kapsamlı bir şekilde incelemiştir. Bu yaklaşım etkili olabilse de, simülatörün elle modellenmesini gerektirir ve ham sistem günlüklerinden ve uyarılarından yararlı anlambilim bilgilerini bastırır. Bu sınırlamaları gidermek için, olay müdahale planlaması için uçtan uca bir ajan çözüm oluşturmak üzere büyük dil modellerinin (LLM) önceden eğitilmiş güvenlik bilgisi ve in-context learning'den yararlanmayı öneriyoruz. Spesifik olarak, ajanımız dört işlevi, algı (perception), çıkarım (reasoning), planlama (planning) ve eylem (action) yeteneklerini tek bir hafif LLM (14b model) içine entegre eder. Fine-tuning ve chain-of-thought reasoning aracılığıyla, LLM ajanımız sistem günlüklerini işleme ve altta yatan ağ durumunu çıkarma (perception), saldırı modellerinin tahminini güncelleme (reasoning), farklı müdahale stratejileri altında sonuçları simüle etme (planning) ve etkili bir müdahale üretme (action) yeteneğine sahiptir. LLM tarafından simüle edilen sonuçları gerçek gözlemlerle karşılaştırarak, LLM ajan sürekli olarak saldırı tahminini ve karşılık gelen müdahalesini iyileştirir ve böylece in-context adaptasyonunu gösterir. Ajan yaklaşımımız modellemeden muaftır ve standart donanımlarda çalışabilir. Literatürde bildirilen olay günlükleri üzerinde değerlendirildiğinde, ajanımız sınır LLM'lere kıyasla %23'e kadar daha hızlı kurtarma sağlamaktadır."
    }
  },
  {
    "id": "2602.13062v1",
    "title": "Backdoor Attacks on Contrastive Continual Learning for IoT Systems",
    "authors": [
      "Alfous Tim",
      "Kuniyilh Simi D"
    ],
    "published_date": "2026-02-13",
    "tags": [
      "cs.LG",
      "cs.CR",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2602.13062v1",
    "pdf_link": "https://arxiv.org/pdf/2602.13062v1",
    "content": {
      "en": "The Internet of Things (IoT) systems increasingly depend on continual learning to adapt to non-stationary environments. These environments can include factors such as sensor drift, changing user behavior, device aging, and adversarial dynamics. Contrastive continual learning (CCL) combines contrastive representation learning with incremental adaptation, enabling robust feature reuse across tasks and domains. However, the geometric nature of contrastive objectives, when paired with replay-based rehearsal and stability-preserving regularization, introduces new security vulnerabilities. Notably, backdoor attacks can exploit embedding alignment and replay reinforcement, enabling the implantation of persistent malicious behaviors that endure through updates and deployment cycles. This paper provides a comprehensive analysis of backdoor attacks on CCL within IoT systems. We formalize the objectives of embedding-level attacks, examine persistence mechanisms unique to IoT deployments, and develop a layered taxonomy tailored to IoT. Additionally, we compare vulnerabilities across various learning paradigms and evaluate defense strategies under IoT constraints, including limited memory, edge computing, and federated aggregation. Our findings indicate that while CCL is effective for enhancing adaptive IoT intelligence, it may also elevate long-lived representation-level threats if not adequately secured.",
      "tr": "**Makale Başlığı:** IoT Sistemleri için Kontrastif Sürekli Öğrenme Üzerine Arka Kapı Saldırıları\n\n**Özet:**\n\nNesnelerin İnterneti (IoT) sistemleri, durağan olmayan ortamlara uyum sağlamak için giderek artan bir şekilde sürekli öğrenmeye dayanmaktadır. Bu ortamlarda sensör kayması, değişen kullanıcı davranışları, cihaz yaşlanması ve düşmanca dinamikler gibi faktörler bulunabilir. Kontrastif sürekli öğrenme (CCL), kontrastif temsil öğrenmesini artımlı uyum ile birleştirerek görevler ve alanlar arasında sağlam özellik yeniden kullanımına olanak tanır. Ancak, kontrastif hedeflerin geometrik doğası, tekrara dayalı tekrar provaları (replay-based rehearsal) ve kararlılık koruyucu düzenlileştirme (stability-preserving regularization) ile eşleştirildiğinde yeni güvenlik açıkları ortaya çıkarır. Özellikle, arka kapı saldırıları (backdoor attacks) gömme hizalamasını (embedding alignment) ve tekrar takviyesini (replay reinforcement) kullanarak, güncellemeler ve dağıtım döngüleri boyunca kalıcı hale gelen ısrarlı kötü amaçlı davranışların yerleştirilmesine olanak tanır. Bu makale, IoT sistemlerindeki CCL üzerine arka kapı saldırılarının kapsamlı bir analizini sunmaktadır. Gömme seviyesi saldırıların (embedding-level attacks) hedeflerini biçimlendirir, IoT dağıtımlarına özgü kalıcılık mekanizmalarını inceler ve IoT'ye özel katmanlı bir taksonomi geliştiririz. Ayrıca, çeşitli öğrenme paradigmaları (learning paradigms) arasındaki açıkları karşılaştırır ve sınırlı bellek, kenar bilişim (edge computing) ve federasyonel toplama (federated aggregation) gibi IoT kısıtlamaları altındaki savunma stratejilerini değerlendiririz. Bulgularımız, CCL'nin uyarlanabilir IoT zekasını geliştirmede etkili olmasına rağmen, yeterince güvence altına alınmadığı takdirde uzun ömürlü temsil seviyesi tehditlerini (representation-level threats) de yükseltebileceğini göstermektedir."
    }
  },
  {
    "id": "2602.12851v1",
    "title": "Chimera: Neuro-Symbolic Attention Primitives for Trustworthy Dataplane Intelligence",
    "authors": [
      "Rong Fu",
      "Wenxin Zhang",
      "Xiaowen Ma",
      "Kun Liu",
      "Wangyu Wu"
    ],
    "published_date": "2026-02-13",
    "tags": [
      "cs.NI",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.12851v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12851v1",
    "content": {
      "en": "Deploying expressive learning models directly on programmable dataplanes promises line-rate, low-latency traffic analysis but remains hindered by strict hardware constraints and the need for predictable, auditable behavior. Chimera introduces a principled framework that maps attention-oriented neural computations and symbolic constraints onto dataplane primitives, enabling trustworthy inference within the match-action pipeline. Chimera combines a kernelized, linearized attention approximation with a two-layer key-selection hierarchy and a cascade fusion mechanism that enforces hard symbolic guarantees while preserving neural expressivity. The design includes a hardware-aware mapping protocol and a two-timescale update scheme that together permit stable, line-rate operation under realistic dataplane budgets. The paper presents the Chimera architecture, a hardware mapping strategy, and empirical evidence showing that neuro-symbolic attention primitives can achieve high-fidelity inference within the resource envelope of commodity programmable switches.",
      "tr": "Elbette, akademik makale başlığını ve özetini istenen teknik terimleri İngilizce bırakarak, resmi ve akademik bir dilde Türkçeye çeviriyorum:\n\n**Makale Başlığı:** Chimera: Güvenilir Veri Düzlemi İstihbaratı İçin Nöro-Sembolik Dikkat Temelleri\n\n**Özet:**\nProgramlanabilir veri düzlemlerine doğrudan ifade edici öğrenme modellerinin dağıtılması, hat hızında, düşük gecikmeli trafik analizi vaat etmektedir ancak katı donanım kısıtlamaları ve öngörülebilir, denetlenebilir davranış ihtiyacı nedeniyle hala engellenmektedir. Chimera, dikkat odaklı nöral hesaplamaları ve sembolik kısıtlamaları veri düzlemi temellerine eşleyen prensipli bir çerçeve sunarak, eşleştirme-eylem (match-action) işlem hattı içinde güvenilir çıkarıma olanak tanır. Chimera, çekirdeklenmiş (kernelized), doğrusal hale getirilmiş bir dikkat yaklaşımını, iki katmanlı bir anahtar seçimi (key-selection) hiyerarşisi ve nöral ifade gücünü korurken sert sembolik garantileri zorlayan bir kaskat füzyon mekanizması ile birleştirir. Tasarım, donanım farkındalığına sahip bir eşleme protokolü ve birlikte çalışarak, gerçekçi veri düzlemi bütçeleri altında kararlı, hat hızında çalışmaya izin veren iki zaman ölçekli bir güncelleme şeması içermektedir. Bu makale, Chimera mimarisini, bir donanım eşleme stratejisini ve nöro-sembolik dikkat temellerinin, yaygın programlanabilir anahtarların kaynak zarfı içinde yüksek doğrulukta çıkarım elde edebildiğini gösteren deneysel kanıtları sunmaktadır."
    }
  },
  {
    "id": "2602.12825v1",
    "title": "Reliable Hierarchical Operating System Fingerprinting via Conformal Prediction",
    "authors": [
      "Rubén Pérez-Jove",
      "Osvaldo Simeone",
      "Alejandro Pazos",
      "Jose Vázquez-Naya"
    ],
    "published_date": "2026-02-13",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2602.12825v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12825v1",
    "content": {
      "en": "Operating System (OS) fingerprinting is critical for network security, but conventional methods do not provide formal uncertainty quantification mechanisms. Conformal Prediction (CP) could be directly wrapped around existing methods to obtain prediction sets with guaranteed coverage. However, a direct application of CP would treat OS identification as a flat classification problem, ignoring the natural taxonomic structure of OSs and providing brittle point predictions. This work addresses these limitations by introducing and evaluating two distinct structured CP strategies: level-wise CP (L-CP), which calibrates each hierarchy level independently, and projection-based CP (P-CP), which ensures structural consistency by projecting leaf-level sets upwards. Our results demonstrate that, while both methods satisfy validity guarantees, they expose a fundamental trade-off between level-wise efficiency and structural consistency. L-CP yields tighter prediction sets suitable for human forensic analysis but suffers from taxonomic inconsistencies. Conversely, P-CP guarantees hierarchically consistent, nested sets ideal for automated policy enforcement, albeit at the cost of reduced efficiency at coarser levels.",
      "tr": "Makale Başlığı: Güvenilir Hiyerarşik İşletim Sistemi Parmak İzleme için Conformal Prediction\n\nÖzet:\n\nİşletim Sistemi (OS) fingerprinting, ağ güvenliği için kritik öneme sahiptir, ancak geleneksel yöntemler resmi bir belirsizlik ölçüm mekanizması sağlamamaktadır. Conformal Prediction (CP), garanti edilen kapsama sahip tahmin setleri elde etmek için mevcut yöntemlerin etrafına doğrudan sarılabilir. Ancak, CP'nin doğrudan uygulanması, OS tanımlamasını düz bir sınıflandırma problemi olarak ele alır, OS'lerin doğal taksonomik yapısını göz ardı eder ve kırılgan nokta tahminleri sunar. Bu çalışma, iki farklı yapılandırılmış CP stratejisi sunarak ve değerlendirerek bu sınırlamalara değinmektedir: seviye bazlı CP (L-CP), hiyerarşinin her seviyesini bağımsız olarak kalibre eder ve projeksiyon tabanlı CP (P-CP), yaprak seviyesi kümelerini yukarı doğru yansıtarak yapısal tutarlılığı sağlar. Sonuçlarımız, her iki yöntemin de geçerlilik garantilerini karşılamasına rağmen, seviye bazlı verimlilik ve yapısal tutarlılık arasında temel bir ödünleşme ortaya koyduğunu göstermektedir. L-CP, insan adli analizleri için uygun daha sıkı tahmin kümeleri üretir ancak taksonomik tutarsızlıklardan muzdariptir. Buna karşılık, P-CP, daha kaba seviyelerde azalan verimlilik maliyetiyle, otomatik politika uygulaması için ideal olan hiyerarşik olarak tutarlı, iç içe geçmiş kümeleri garanti eder."
    }
  },
  {
    "id": "2602.12806v1",
    "title": "RAT-Bench: A Comprehensive Benchmark for Text Anonymization",
    "authors": [
      "Nataša Krčo",
      "Zexi Yao",
      "Matthieu Meeus",
      "Yves-Alexandre de Montjoye"
    ],
    "published_date": "2026-02-13",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.12806v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12806v1",
    "content": {
      "en": "Data containing personal information is increasingly used to train, fine-tune, or query Large Language Models (LLMs). Text is typically scrubbed of identifying information prior to use, often with tools such as Microsoft's Presidio or Anthropic's PII purifier. These tools have traditionally been evaluated on their ability to remove specific identifiers (e.g., names), yet their effectiveness at preventing re-identification remains unclear. We introduce RAT-Bench, a comprehensive benchmark for text anonymization tools based on re-identification risk. Using U.S. demographic statistics, we generate synthetic text containing various direct and indirect identifiers across domains, languages, and difficulty levels. We evaluate a range of NER- and LLM-based text anonymization tools and, based on the attributes an LLM-based attacker is able to correctly infer from the anonymized text, we report the risk of re-identification in the U.S. population, while properly accounting for the disparate impact of identifiers. We find that, while capabilities vary widely, even the best tools are far from perfect in particular when direct identifiers are not written in standard ways and when indirect identifiers enable re-identification. Overall we find LLM-based anonymizers, including new iterative anonymizers, to provide a better privacy-utility trade-off albeit at a higher computational cost. Importantly, we also find them to work well across languages. We conclude with recommendations for future anonymization tools and will release the benchmark and encourage community efforts to expand it, in particular to other geographies.",
      "tr": "**Makale Başlığı:** RAT-Bench: Metin Anonimleştirme için Kapsamlı Bir Benchmark\n\n**Özet:**\n\nBüyük Dil Modellerini (LLM) eğitmek, ince ayar yapmak veya sorgulamak için kişisel bilgileri içeren verilere artan bir şekilde başvurulmaktadır. Metinler genellikle kullanımdan önce tanımlayıcı bilgilerden arındırılır; bu işlem sıklıkla Microsoft'un Presidio veya Anthropic'in PII purifier gibi araçlarla gerçekleştirilir. Bu araçlar geleneksel olarak belirli tanımlayıcıları (örn. isimler) kaldırma yetenekleri açısından değerlendirilmiştir; ancak yeniden tanımlamayı önlemedeki etkinlikleri hala belirsizliğini korumaktadır. Bu çalışmada, yeniden tanımlama riskine dayalı olarak metin anonimleştirme araçları için kapsamlı bir benchmark olan RAT-Bench'i sunmaktayız. ABD demografik istatistiklerini kullanarak, çeşitli alanlarda, dillerde ve zorluk seviyelerinde doğrudan ve dolaylı tanımlayıcılar içeren sentetik metinler üretiyoruz. Bir dizi NER ve LLM tabanlı metin anonimleştirme aracını değerlendiriyoruz ve LLM tabanlı bir saldırganın anonimleştirilmiş metinden doğru bir şekilde çıkarabildiği niteliklere dayanarak, ABD nüfusundaki yeniden tanımlama riskini, tanımlayıcıların farklı etkilerini de doğru bir şekilde hesaba katarak raporluyoruz. Yeteneklerin büyük ölçüde farklılık göstermekle birlikte, en iyi araçların bile, özellikle doğrudan tanımlayıcılar standart yollarla yazılmadığında ve dolaylı tanımlayıcılar yeniden tanımlamayı mümkün kıldığında, mükemmel olmaktan çok uzak olduğunu buluyoruz. Genel olarak, yeni yinelemeli anonimleştiriciler dahil olmak üzere LLM tabanlı anonimleştiricilerin, daha yüksek hesaplama maliyetine rağmen daha iyi bir gizlilik-fayda dengesi sağladığına inanıyoruz. Önemlisi, diller arasında iyi çalıştıklarını da tespit ettik. Gelecekteki anonimleştirme araçları için önerilerle sonuçlanıyor ve benchmark'ı yayımlayarak, özellikle diğer coğrafyalara yönelik genişletme çabalarını topluluğu teşvik edeceğiz."
    }
  },
  {
    "id": "2602.12681v1",
    "title": "Fool Me If You Can: On the Robustness of Binary Code Similarity Detection Models against Semantics-preserving Transformations",
    "authors": [
      "Jiyong Uhm",
      "Minseok Kim",
      "Michalis Polychronakis",
      "Hyungjoon Koo"
    ],
    "published_date": "2026-02-13",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.12681v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12681v1",
    "content": {
      "en": "Binary code analysis plays an essential role in cybersecurity, facilitating reverse engineering to reveal the inner workings of programs in the absence of source code. Traditional approaches, such as static and dynamic analysis, extract valuable insights from stripped binaries, but often demand substantial expertise and manual effort. Recent advances in deep learning have opened promising opportunities to enhance binary analysis by capturing latent features and disclosing underlying code semantics. Despite the growing number of binary analysis models based on machine learning, their robustness to adversarial code transformations at the binary level remains underexplored. We evaluate the robustness of deep learning models for the task of binary code similarity detection (BCSD) under semantics-preserving transformations. The unique nature of machine instructions presents distinct challenges compared to the typical input perturbations found in other domains. We introduce asmFooler, a system that evaluates the resilience of BCSD models using a diverse set of adversarial code transformations that preserve functional semantics. We construct a dataset of 9,565 binary variants from 620 baseline samples by applying eight semantics-preserving transformations across six representative BCSD models. Our major findings highlight several key insights: i) model robustness relies on the processing pipeline, including code pre-processing, architecture, and feature selection; ii) adversarial transformation effectiveness is bounded by a budget shaped by model-specific constraints like input size and instruction expressive capacity; iii) well-crafted transformations can be highly effective with minimal perturbations; and iv) such transformations efficiently disrupt model decisions (e.g., misleading to false positives or false negatives) by focusing on semantically significant instructions.",
      "tr": "Elbette, istenen çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** Aldat Beni Eğer Yapabilirsen: Anlamı Koruyan Dönüşümlere Karşı İkili Kod Benzerliği Tespit Modellerinin Dayanıklılığı Üzerine\n\n**Özet:**\n\nİkili kod analizi, siber güvenlik alanında temel bir rol oynamakta olup, kaynak kodun mevcut olmadığı durumlarda programların iç işleyişini ortaya çıkarmak için tersine mühendisliği kolaylaştırmaktadır. Geleneksel yaklaşımlar, statik ve dinamik analiz gibi, kaldırılmış ikili kodlardan değerli bilgiler çıkarmaktadır ancak bu yaklaşımlar genellikle önemli düzeyde uzmanlık ve manuel çaba gerektirmektedir. Derin öğrenmedeki son gelişmeler, gizli özellikleri yakalayarak ve kodun altında yatan anlamı açıklayarak ikili analizini iyileştirmek için umut verici fırsatlar sunmuştur. Makine öğrenimine dayalı ikili analiz modellerinin sayısındaki artışa rağmen, ikili düzeydeki düşmanca kod dönüşümlerine karşı dayanıklılıkları yeterince araştırılmamıştır. Bu çalışmada, anlamı koruyan dönüşümler altında ikili kod benzerliği tespiti (BCSD) görevi için derin öğrenme modellerinin dayanıklılığını değerlendiriyoruz. Makine komutlarının özgün doğası, diğer alanlarda bulunan tipik girdi pertürbasyonlarına kıyasla farklı zorluklar sunmaktadır. Fonksiyonel anlamı koruyan çeşitli düşmanca kod dönüşümlerini kullanarak BCSD modellerinin direncini değerlendiren bir sistem olan asmFooler'ı tanıtıyoruz. Altı temsili BCSD modeli üzerinde sekiz anlamı koruyan dönüşüm uygulayarak 620 temel örnekten oluşan 9.565 ikili varyanttan oluşan bir veri seti oluşturduk. Elde ettiğimiz temel bulgular birkaç önemli içgörüyü vurgulamaktadır: i) model dayanıklılığı, kod ön işleme, mimari ve özellik seçimi gibi işleme hattına bağlıdır; ii) düşmanca dönüşüm etkinliği, girdi boyutu ve komut ifade kapasitesi gibi modele özgü kısıtlamalar tarafından şekillendirilen bir bütçe ile sınırlıdır; iii) iyi hazırlanmış dönüşümler, minimum pertürbasyonlarla oldukça etkili olabilir; ve iv) bu tür dönüşümler, anlamsal olarak önemli komutlara odaklanarak model kararlarını (örneğin, yanlış pozitiflere veya yanlış negatiflere yönlendirme) verimli bir şekilde bozmaktadır."
    }
  },
  {
    "id": "2602.12630v1",
    "title": "TensorCommitments: A Lightweight Verifiable Inference for Language Models",
    "authors": [
      "Oguzhan Baser",
      "Elahe Sadeghi",
      "Eric Wang",
      "David Ribeiro Alves",
      "Sam Kazemian"
    ],
    "published_date": "2026-02-13",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.12630v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12630v1",
    "content": {
      "en": "Most large language models (LLMs) run on external clouds: users send a prompt, pay for inference, and must trust that the remote GPU executes the LLM without any adversarial tampering. We critically ask how to achieve verifiable LLM inference, where a prover (the service) must convince a verifier (the client) that an inference was run correctly without rerunning the LLM. Existing cryptographic works are too slow at the LLM scale, while non-cryptographic ones require a strong verifier GPU. We propose TensorCommitments (TCs), a tensor-native proof-of-inference scheme. TC binds the LLM inference to a commitment, an irreversible tag that breaks under tampering, organized in our multivariate Terkle Trees. For LLaMA2, TC adds only 0.97% prover and 0.12% verifier time over inference while improving robustness to tailored LLM attacks by up to 48% over the best prior work requiring a verifier GPU.",
      "tr": "**Makale Başlığı:** TensorCommitments: Dil Modelleri İçin Hafif Doğrulanabilir Çıkarım\n\n**Özet:**\n\nBüyük dil modellerinin (LLM) çoğu harici bulutlarda çalışır: kullanıcılar bir istem gönderir, çıkarım için ödeme yapar ve uzaktaki GPU'nun LLM'yi herhangi bir kötü niyetli müdahale olmadan çalıştırdığına güvenmek zorundadır. Doğrulanabilir LLM çıkarımının nasıl sağlanabileceği sorusunu kritik olarak ele alıyoruz, bu durumda bir ispatçı (servis) bir doğrulayıcıyı (istemci) LLM'yi yeniden çalıştırmadan bir çıkarımın doğru bir şekilde çalıştığına ikna etmek zorundadır. Mevcut kriptografik çalışmalar LLM ölçeğinde çok yavaştır, oysa kriptografik olmayanlar güçlü bir doğrulayıcı GPU gerektirir. Biz, çıkarım için bir tensör-tabanlı ispat şeması olan TensorCommitments'ı (TCs) öneriyoruz. TC, LLM çıkarımını, müdahale altında bozulan geri döndürülemez bir etiket olan bir commitment'a bağlar; bu commitment, bizim çok değişkenli Terkle Trees yapımızda organize edilmiştir. LLaMA2 için TC, çıkarım süresine ek olarak sadece %0.97 ispatçı ve %0.12 doğrulayıcı süresi eklerken, doğrulayıcı GPU gerektiren en iyi önceki çalışmalara kıyasla LLM'ye özel saldırılara karşı dayanıklılığı %48'e kadar artırmaktadır."
    }
  },
  {
    "id": "2602.12500v1",
    "title": "Favia: Forensic Agent for Vulnerability-fix Identification and Analysis",
    "authors": [
      "André Storhaug",
      "Jiamou Sun",
      "Jingyue Li"
    ],
    "published_date": "2026-02-13",
    "tags": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.12500v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12500v1",
    "content": {
      "en": "Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only a small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs. Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, a forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning. Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using a ReAct-based LLM agent. By providing the agent with a pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude single-pass or similarity-based methods. We evaluate Favia on CVEVC, a large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores.",
      "tr": "İşte akademik makale başlığı ve özetinin çevirisi:\n\n**Makale Başlığı:** Favia: Sızma-Düzeltme Tanımlama ve Analizi İçin Adli Ajan\n\n**Özet:**\n\nAçıklanmış CVE'lere karşılık gelen sızma-düzeltme commit'lerini tanımlamak, güvenli yazılım bakımı için esastır ancak büyük ölçekte zorluğunu korumaktadır, zira büyük depolar yalnızca küçük bir kısmının güvenlik sorunlarını ele aldığı milyonlarca commit içermektedir. Geleneksel makine öğrenmesi teknikleri ve son büyük dil modeli (LLM) tabanlı yöntemler de dahil olmak üzere mevcut otomatik yaklaşımlar, genellikle zayıf precision-recall trade-off'larından muzdariptir. Rastgele örneklenmiş commit'ler üzerinde sıklıkla değerlendirilen bu yöntemlerin, aday commit'lerin zaten güvenlik açısından ilgili olduğu ve yüksek derecede benzer olduğu gerçek dünya zorluğunu önemli ölçüde küçümsediğini ortaya koyuyoruz. Ölçeklenebilir aday sıralamayı derin ve iteratif anlamsal reasoning ile birleştiren sızma-düzeltme tanımlaması için adli, agent-tabanlı bir framework olan Favia'yı öneriyoruz. Favia, öncelikle commit'lerin arama alanını daraltmak için verimli bir sıralama aşaması kullanır. Her commit daha sonra bir ReAct-based LLM agent kullanılarak titizlikle değerlendirilir. Agent'a bir pre-commit repository'yi çevre olarak ve ayrıca özel araçlar sağlayarak, agent sızma yapan bileşenleri lokalize etmeye çalışır, kod tabanında gezinir ve kod değişiklikleri ile sızma kök nedenleri arasında nedensel bir hizalama kurar. Bu kanıt-tabanlı süreç, tek geçişli veya benzerlik tabanlı yöntemlerden kaçan dolaylı, çoklu dosya ve önemsiz olmayan düzeltmelerin sağlam bir şekilde tanımlanmasını sağlar. Favia'yı, 3.708 gerçek dünya deposundan 8 milyondan fazla commit'i içeren, bizim tarafımızdan oluşturulmuş büyük ölçekli bir veri kümesi olan CVEVC üzerinde değerlendiriyoruz ve gerçekçi aday seçimi altında sürekli olarak en gelişmiş geleneksel ve LLM tabanlı temel yöntemlerden daha iyi performans gösterdiğini, en güçlü precision-recall trade-off'larını ve en yüksek F1-score'ları elde ettiğini gösteriyoruz."
    }
  },
  {
    "id": "2602.12418v1",
    "title": "Sparse Autoencoders are Capable LLM Jailbreak Mitigators",
    "authors": [
      "Yannick Assogba",
      "Jacopo Cortellazzi",
      "Javier Abad",
      "Pau Rodriguez",
      "Xavier Suau"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.12418v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12418v1",
    "content": {
      "en": "Jailbreak attacks remain a persistent threat to large language model safety. We propose Context-Conditioned Delta Steering (CC-Delta), an SAE-based defense that identifies jailbreak-relevant sparse features by comparing token-level representations of the same harmful request with and without jailbreak context. Using paired harmful/jailbreak prompts, CC-Delta selects features via statistical testing and applies inference-time mean-shift steering in SAE latent space. Across four aligned instruction-tuned models and twelve jailbreak attacks, CC-Delta achieves comparable or better safety-utility tradeoffs than baseline defenses operating in dense latent space. In particular, our method clearly outperforms dense mean-shift steering on all four models, and particularly against out-of-distribution attacks, showing that steering in sparse SAE feature space offers advantages over steering in dense activation space for jailbreak mitigation. Our results suggest off-the-shelf SAEs trained for interpretability can be repurposed as practical jailbreak defenses without task-specific training.",
      "tr": "Makale Başlığı: Seyrek Otomatik Kodlayıcılar (Sparse Autoencoders) Yetenekli LLM Jailbreak Azaltıcılarıdır\n\nÖzet:\nJailbreak saldırıları, büyük dil model güvenliği için kalıcı bir tehdit olmaya devam etmektedir. Biz, aynı zararlı isteğin jailbreak bağlamı ile ve olmadan token-düzeyindeki temsillerini karşılaştırarak jailbreak ile ilgili sparse özellikleri tanımlayan bir SAE tabanlı savunma olan Context-Conditioned Delta Steering (CC-Delta)'yı öneriyoruz. Eşleştirilmiş zararlı/jailbreak istemlerini kullanarak, CC-Delta istatistiksel testler yoluyla özellikleri seçer ve SAE latent space'de inference-time mean-shift steering uygular. Dört adet hizalanmış instruction-tuned model ve on iki adet jailbreak saldırısı boyunca, CC-Delta, yoğun latent space'de çalışan temel savunmalara kıyasla benzer veya daha iyi güvenlik-fayda dengeleri elde eder. Özellikle, yöntemimiz tüm dört modelde yoğun mean-shift steering'den ve özellikle out-of-distribution saldırılarına karşı daha iyi performans gösterir, bu da sparse SAE özellik alanında steering'in jailbreak azaltımı için yoğun aktivasyon alanında steering'e göre avantajlar sunduğunu göstermektedir. Sonuçlarımız, yorumlanabilirlik için eğitilmiş hazır SAE'lerin, göreve özgü eğitim gerektirmeden pratik jailbreak savunmaları olarak yeniden kullanılabileceğini önermektedir."
    }
  },
  {
    "id": "2602.12250v1",
    "title": "Community Concealment from Unsupervised Graph Learning-Based Clustering",
    "authors": [
      "Dalyapraz Manatova",
      "Pablo Moriano",
      "L. Jean Camp"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.LG",
      "cs.CR",
      "cs.SI"
    ],
    "link": "http://arxiv.org/abs/2602.12250v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12250v1",
    "content": {
      "en": "Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.",
      "tr": "Makale Başlığı: Gözetimsiz Graf Öğrenmeye Dayalı Kümeleme ile Topluluk Gizliliği\n\nÖzet:\nGraph neural networks (GNNs), temsilleri öğrenmek için atributlu grafikleri kullanmak üzere tasarlanmıştır. Bu tür temsiller, gözetimsiz kümeleme ve topluluk tespiti için faydalıdır. Bununla birlikte, bu tür çıkarımlar hassas grupları, kümelenmiş sistemleri veya kolektif davranışları ortaya çıkarabilir ve grup düzeyinde gizlilik endişelerine yol açabilir. Örneğin, sosyal ve kritik altyapı ağlarındaki topluluk ataması, profilleme veya istihbarat toplama için kullanılabilecek koordineli varlık gruplarını, operasyonel hiyerarşileri ve sistem bağımlılıklarını açığa çıkarabilir. Biz, bir veri yayıncısının (savunmacı) ilgilenilen bir topluluğu gizlemek ve aynı zamanda ağda sınırlı, faydayı gözeten değişiklikler yapmak istediği savunmacı bir ortamı inceliyoruz. Analizimiz, topluluk gizliliğinin iki ölçülebilir faktörden güçlü bir şekilde etkilendiğini göstermektedir: topluluk sınırındaki bağlantı ve korunan topluluk ile bitişik topluluklar arasındaki özellik benzerliği. Bu bulgular doğrultusunda, GNN mesaj geçişinin kullandığı ayırt ediciliği azaltmak için seçilmiş bir dizi kenarı yeniden bağlayan ve düğüm özelliklerini değiştiren bir pertürbasyon stratejisi sunuyoruz. Önerilen yöntem, sentetik benchmark'lar ve gerçek ağ grafikleri üzerindeki deneylerimizde, aynı pertürbasyon bütçeleri altında DICE'dan daha iyi performans gösterir. Genel olarak, değerlendirilen ortamlarda yaklaşık %20-45 arasında medyan göreceli gizlilik iyileştirmeleri elde eder. Bu bulgular, GNN tabanlı topluluk öğrenmesine karşı bir azaltma stratejisi sergiler ve graf öğrenmesinde içkin olan grup düzeyinde gizlilik risklerini vurgular."
    }
  }
]