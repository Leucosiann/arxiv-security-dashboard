[
  {
    "id": "2602.05838v1",
    "title": "FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation",
    "authors": [
      "Mayank Kumar",
      "Qian Lou",
      "Paulo Barreto",
      "Martine De Cock",
      "Sikha Pentyala"
    ],
    "published_date": "2026-02-05",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.05838v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05838v1",
    "content": {
      "en": "Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.",
      "tr": "**Makale Başlığı:** FHAIM: Özel Sentetik Veri Üretimi İçin Tam Homomorfik AIM\n\n**Özet:**\n\nYapay zeka (AI) için veri hayati önem taşımaktadır, ancak en değerli verilerin büyük bir kısmı gizlilik ve düzenlemeler nedeniyle silolar içinde kilitli kalmaktadır. Bunun sonucunda, sağlık, eğitim ve finans gibi en önemli alanların çoğunda AI'nın kullanımı büyük ölçüde sınırlı kalmaktadır. Sentetik veri üretimi (SDG), yani gerçek veriler üzerinde eğitilmiş bir sentetik veri üretici ile yapay verilerin üretilmesi, veri gizliliği endişelerini gidererek verileri kullanılabilir hale getirmek için çekici bir çözüm sunmaktadır. Ancak, mevcut SDG-as-a-service iş akışları, veri sahiplerinin özel verilere erişim konusunda sağlayıcılara güvenmesini gerektirmektedir. Biz, şifrelenmiş tablo verileri üzerinde marjinal tabanlı bir sentetik veri üreticisini eğitmek için ilk tam homomorfik şifreleme (FHE) çerçevesi olan FHAIM'i öneriyoruz. FHAIM, yaygın olarak kullanılan AIM algoritmasını yeni FHE protokolleri kullanarak FHE ortamına uyarlar, böylece özel verilerin şifrelenmiş kalmasını ve yalnızca differential privacy güvenceleriyle serbest bırakılmasını sağlar. Ampirik analizlerimiz, FHAIM'in AIM'in performansını korurken uygulanabilir çalışma sürelerini sürdürdüğünü göstermektedir."
    }
  },
  {
    "id": "2602.05817v1",
    "title": "Interpreting Manifolds and Graph Neural Embeddings from Internet of Things Traffic Flows",
    "authors": [
      "Enrique Feito-Casares",
      "Francisco M. Melgarejo-Meseguer",
      "Elena Casiraghi",
      "Giorgio Valentini",
      "José-Luis Rojo-Álvarez"
    ],
    "published_date": "2026-02-05",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2602.05817v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05817v1",
    "content": {
      "en": "The rapid expansion of Internet of Things (IoT) ecosystems has led to increasingly complex and heterogeneous network topologies. Traditional network monitoring and visualization tools rely on aggregated metrics or static representations, which fail to capture the evolving relationships and structural dependencies between devices. Although Graph Neural Networks (GNNs) offer a powerful way to learn from relational data, their internal representations often remain opaque and difficult to interpret for security-critical operations. Consequently, this work introduces an interpretable pipeline that generates directly visualizable low-dimensional representations by mapping high-dimensional embeddings onto a latent manifold. This projection enables the interpretable monitoring and interoperability of evolving network states, while integrated feature attribution techniques decode the specific characteristics shaping the manifold structure. The framework achieves a classification F1-score of 0.830 for intrusion detection while also highlighting phenomena such as concept drift. Ultimately, the presented approach bridges the gap between high-dimensional GNN embeddings and human-understandable network behavior, offering new insights for network administrators and security analysts.",
      "tr": "İşte akademik makale başlığının ve özetinin çevirisi:\n\n**Makale Başlığı:** Nesnelerin İnterneti Trafik Akışlarından Mıknatısların ve Grafik Sinir Ağları Yerleştirmelerinin Yorumlanması\n\n**Özet:**\nNesnelerin İnterneti (IoT) ekosistemlerinin hızla genişlemesi, giderek karmaşıklaşan ve heterojen ağ topolojilerine yol açmıştır. Geleneksel ağ izleme ve görselleştirme araçları, cihazlar arasındaki gelişen ilişkileri ve yapısal bağımlılıkları yakalamakta yetersiz kalan toplu metrikler veya statik temsiller üzerine kuruludur. Grafik Sinir Ağları (GNNs), ilişkisel verilerden öğrenmek için güçlü bir yol sunsa da, iç temsilleri genellikle opak kalmakta ve güvenlik açısından kritik operasyonlar için yorumlanması zor olmaktadır. Sonuç olarak, bu çalışma, yüksek boyutlu embeddings'i bir latent manifold üzerine haritalayarak doğrudan görselleştirilebilir düşük boyutlu temsiller üreten yorumlanabilir bir pipeline sunmaktadır. Bu projeksiyon, gelişen ağ durumlarının yorumlanabilir izlenmesini ve birlikte çalışabilirliğini sağlarken, entegre feature attribution teknikleri manifold yapısını şekillendiren özel özellikleri çözmektedir. Bu çerçeve, saldırı tespiti için 0.830 sınıflandırma F1-score'u elde etmekte ve aynı zamanda concept drift gibi olguları da vurgulamaktadır. Nihayetinde, sunulan yaklaşım, yüksek boyutlu GNN embeddings ile insan tarafından anlaşılabilir ağ davranışı arasındaki boşluğu kapatarak ağ yöneticileri ve güvenlik analistleri için yeni içgörüler sunmaktadır."
    }
  },
  {
    "id": "2602.05486v1",
    "title": "Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems",
    "authors": [
      "Matteo Esposito",
      "Lodovica Marchesi",
      "Roberto Tonelli",
      "Valentina Lenarduzzi"
    ],
    "published_date": "2026-02-05",
    "tags": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "link": "http://arxiv.org/abs/2602.05486v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05486v1",
    "content": {
      "en": "Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.",
      "tr": "**Makale Başlığı:** Tasarım Gereği Egemenlik: Yapay Zeka ve Blockchain Etkin Sistemler İçin Bir Referans Mimari\n\n**Özet:**\n\nDijital egemenlik, egemen olmayan bulut altyapılarının hakimiyeti, Üretken Yapay Zeka'nın hızlı benimsenmesi ve giderek sıkılaşan düzenleyici gereksinimler nedeniyle modern yazılım yoğun sistemler için merkezi bir endişe kaynağı olarak öne çıkmaktadır. Mevcut girişimler yönetişim, uyumluluk ve güvenliği izole bir şekilde ele alırken, egemenliğin mimari düzeyde nasıl operasyonelleştirilebileceği konusunda sınırlı rehberlik sunmaktadırlar. Bu makalede, egemenliğin yalnızca düzenleyici bir hedef olmaktan ziyade birinci sınıf bir mimari özellik olarak ele alınması gerektiğini savunuyoruz. Kendinden egemen kimlik, blockchain tabanlı güven ve denetlenebilirlik, egemen veri yönetişimi ve açık mimari kontrolü altında dağıtılmış Üretken Yapay Zeka'yı entegre eden bir Egemen Referans Mimari sunmaktayız. Mimari, Üretken Yapay Zeka'nın hem yönetişim riskinin bir kaynağı hem de uygun şekilde kısıtlandığında uyumluluk, hesap verebilirlik ve sürekli güvencenin bir kolaylaştırıcısı olarak ikili rolünü açıkça yakalamaktadır. Egemenliği bir mimari kalite niteliği olarak çerçeveleyerek, çalışmamız düzenleyici niyet ile somut sistem tasarımı arasında bir köprü kurmakta, denetlenebilir, evrilebilir ve yargı alanına duyarlı Yapay Zeka etkin sistemler inşa etmek için tutarlı bir temel sunmaktadır. Önerilen referans mimari, yazılım mimarisi, Üretken Yapay Zeka ve dijital egemenlik kesişiminde gelecekteki araştırma ve uygulamalar için prensipli bir başlangıç ​​noktası sağlamaktadır."
    }
  },
  {
    "id": "2602.05386v1",
    "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening",
    "authors": [
      "Zhenxiong Yu",
      "Zhi Yang",
      "Zhiheng Jin",
      "Shuhe Wang",
      "Heng Zhang"
    ],
    "published_date": "2026-02-05",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.05386v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05386v1",
    "content": {
      "en": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.",
      "tr": "**Makale Başlığı:** Spider-Sense: Hiyerarşik Adaptif Tarama ile Etkin Ajan Savunması İçin İçsel Risk Algılama\n\n**Özet:**\n\nBüyük dil modelleri (LLM'ler) otonom ajanlar haline geldikçe, gerçek dünya uygulamaları önemli ölçüde genişlemiş ve bununla birlikte yeni güvenlik zorlukları da ortaya çıkmıştır. Mevcut ajan savunma mekanizmalarının çoğu, güvenlik doğrulamalarının zorunlu olarak ajanın yaşam döngüsünün önceden tanımlanmış aşamalarında tetiklendiği zorunlu bir kontrol paradigmasını benimsemektedir. Bu çalışmada, etkili ajan güvenliğinin mimari olarak ayrılmış ve zorunlu olmaktan ziyade içsel ve seçici olması gerektiğini savunuyoruz. Spider-Sense framework'ünü öneriyoruz; bu, Intrinsic Risk Sensing (IRS) tabanlı olay güdümlü bir savunma framework'üdür ve ajanların gizli bir tetikte bulunmalarını ve yalnızca risk algılandığında savunmaları tetiklemelerini sağlar. Tetiklendiğinde, Spider-Sense verimlilik ve hassasiyet arasında denge kuran hiyerarşik bir savunma mekanizmasını devreye sokar: hafif benzerlik eşleştirmesi yoluyla bilinen desenleri çözerken, belirsiz durumları derin internal reasoning'e yükseltir ve böylece dış modellere olan bağımlılığı ortadan kaldırır. Titiz bir değerlendirmeyi kolaylaştırmak için, gerçekçi araç yürütme ve çok aşamalı saldırılar içeren, yaşam döngüsüne duyarlı bir benchmark olan S$^2$Bench'i sunuyoruz. Kapsamlı deneyler, Spider-Sense'in rekabetçi veya üstün savunma performansı elde ettiğini, en düşük Attack Success Rate (ASR) ve False Positive Rate (FPR)'ı yakaladığını ve yalnızca %8.3'lük marjinal bir gecikme maliyetine sahip olduğunu göstermektedir."
    }
  },
  {
    "id": "2602.05279v1",
    "title": "Hallucination-Resistant Security Planning with a Large Language Model",
    "authors": [
      "Kim Hammar",
      "Tansu Alpcan",
      "Emil Lupu"
    ],
    "published_date": "2026-02-05",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.05279v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05279v1",
    "content": {
      "en": "Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.",
      "tr": "**Makale Başlığı:** Hallucination-Resistant Security Planning with a Large Language Model\n\n**Özet:**\n\nLarge language models (LLMs), olay müdahale planlaması gibi güvenlik yönetimi görevlerini desteklemek için umut vadeden araçlardır. Bununla birlikte, güvenilirliklerinin düşük olması ve hallucination eğilimleri önemli zorluklar teşkil etmeye devam etmektedir. Bu makalede, LLM'leri güvenlik yönetiminde karar destek sistemi olarak kullanmak için prensipli bir framework sunarak bu zorlukların üstesinden geliyoruz. Framework'ümüz, LLM'yi tekrarlı bir döngüye entegre eder; bu döngüde LLM, sistem kısıtlamaları ve lookahead predictions ile tutarlılığı kontrol edilen aday eylemler üretir. Tutarlılık düşük olduğunda, üretilen eylemlerden kaçınılır ve bunun yerine harici geri bildirimler toplanır; örneğin, dijital bir ikizde (digital twin) eylemlerin değerlendirilmesi yoluyla. Bu geri bildirimler, in-context learning (ICL) yoluyla aday eylemleri iyileştirmek için kullanılır. Bu tasarımın, consistency threshold ayarlanarak hallucination riskini kontrol etmemizi sağladığını kanıtlıyoruz. Dahası, belirli varsayımlar altında ICL'nin regret'i için bir sınır belirliyoruz. Framework'ümüzü değerlendirmek için, sistem loglarına dayalı bir müdahale ve kurtarma planı oluşturmayı hedefleyen bir olay müdahale kullanım senaryosuna uyguluyoruz. Dört adet halka açık veri kümesi üzerinde yapılan deneyler, framework'ümüzün frontier LLM'lere kıyasla kurtarma sürelerini %30'a kadar azalttığını göstermektedir."
    }
  },
  {
    "id": "2602.05089v1",
    "title": "Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning",
    "authors": [
      "Ethan Rathbun",
      "Wo Wei Lin",
      "Alina Oprea",
      "Christopher Amato"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.RO"
    ],
    "link": "http://arxiv.org/abs/2602.05089v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05089v1",
    "content": {
      "en": "Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger'', leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent's training pipeline, enabling them to both alter and observe agent's rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze'' which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze's effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks.",
      "tr": "İşte makale başlığı ve özetinin istenen şekilde Türkçeye çevrilmiş hali:\n\n**Makale Başlığı:** Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning\n\n**Özet:**\n\nTaklit ortamları, Reinforcement Learning (RL) alanındaki başarının temel bir parçasıdır ve uygulayıcılar ile araştırmacıların pahalı deneyleri gerçek donanım üzerinde çalıştırmadan karar verme ajanlarını eğitmelerine olanak tanır. Bununla birlikte, taklit ortamları bir güvenlik kör noktası olmaya devam etmektedir ve bu durum, kötü niyetli geliştiricilerin, yayımladıkları taklit ortamlarının dinamiklerini kötü amaçlı hedefler için değiştirmelerine imkan tanımaktadır. Bu nedenle, bu çalışmada, ajanların içine gizlice action-level backdoors yerleştirmek için taklit ortamı dinamiklerinin nasıl istismar edilebileceğini göstererek yeni bir tehdidi vurguluyoruz. Ardından, bu backdoor, bir saldırganın önceden tanımlanmış bir \"trigger\" gözlemlediğinde bir ajanda hedeflenen eylemleri güvenilir bir şekilde etkinleştirmesine olanak tanır, bu da potansiyel olarak tehlikeli sonuçlara yol açabilir. Geleneksel backdoor saldırıları, saldırganın bir ajanın eğitim pipeline'ı üzerinde neredeyse tam kontrole sahip olduğunu varsayan güçlü tehdit modelleriyle sınırlıdır, bu da onlara hem ajanların ödüllerini hem de değiştirmelerine hem de gözlemlemelerine izin verir. Bu varsayımların bir taklit ortamı içinde uygulanması mümkün olmadığından, gerçek dünya görevleri için eğitilen RL ajanlarına, ödüllerini değiştirmeden veya hatta gözlemlemeden güvenilir ve gizlice backdoors yerleştirebilen yeni bir saldırı olan \"Daze\"yi öneriyoruz. Genel RL görevlerinde saldırı başarısını garanti etme konusunda Daze'nin etkinliğine dair resmi bir kanıtı, hem ayrık hem de sürekli eylem alanı domainlerinde kapsamlı ampirik değerlendirmelerle birlikte sunuyoruz. Ayrıca, RL backdoor saldırılarının gerçek robotik donanıma aktarılmasının ilk örneğini de sağlıyoruz. Bu gelişmeler, kötü niyetli saldırıları önlemek için RL eğitim pipeline'ının tüm bileşenlerini güvenli hale getirme konusunda daha fazla araştırmayı teşvik etmektedir."
    }
  },
  {
    "id": "2602.05066v1",
    "title": "Bypassing AI Control Protocols via Agent-as-a-Proxy Attacks",
    "authors": [
      "Jafar Isbarov",
      "Murat Kantarcioglu"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.05066v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05066v1",
    "content": {
      "en": "As AI agents automate critical workloads, they remain vulnerable to indirect prompt injection (IPI) attacks. Current defenses rely on monitoring protocols that jointly evaluate an agent's Chain-of-Thought (CoT) and tool-use actions to ensure alignment with user intent. We demonstrate that these monitoring-based defenses can be bypassed via a novel Agent-as-a-Proxy attack, where prompt injection attacks treat the agent as a delivery mechanism, bypassing both agent and monitor simultaneously. While prior work on scalable oversight has focused on whether small monitors can supervise large agents, we show that even frontier-scale monitors are vulnerable. Large-scale monitoring models like Qwen2.5-72B can be bypassed by agents with similar capabilities, such as GPT-4o mini and Llama-3.1-70B. On the AgentDojo benchmark, we achieve a high attack success rate against AlignmentCheck and Extract-and-Evaluate monitors under diverse monitoring LLMs. Our findings suggest current monitoring-based agentic defenses are fundamentally fragile regardless of model scale.",
      "tr": "**Makale Başlığı:** Agent-as-a-Proxy Saldırıları Aracılığıyla AI Kontrol Protokollerinin Aşılması\n\n**Özet:**\n\nYapay zeka (AI) ajanlarının kritik iş yüklerini otomatikleştirmesiyle birlikte, bu ajanlar dolaylı prompt injection (IPI) saldırılarına karşı savunmasız kalmaya devam etmektedir. Mevcut savunmalar, bir ajanın Chain-of-Thought (CoT) ve tool-use eylemlerini kullanıcı niyetiyle uyumunu sağlamak üzere ortaklaşa değerlendiren izleme protokollerine dayanmaktadır. Bu izleme tabanlı savunmaların, ajan ve monitörün her ikisini aynı anda atlatarak, prompt injection saldırılarının ajanı bir teslim mekanizması olarak kullandığı yeni bir Agent-as-a-Proxy saldırısı yoluyla aşılabileceğini göstermekteyiz. Ölçeklenebilir denetim üzerine yapılan önceki çalışmalar, küçük monitörlerin büyük ajanları denetleyip denetleyemeyeceğine odaklanmışken, biz en gelişmiş ölçekteki monitörlerin bile savunmasız olduğunu ortaya koymaktayız. Qwen2.5-72B gibi büyük ölçekli izleme modelleri, GPT-4o mini ve Llama-3.1-70B gibi benzer yeteneklere sahip ajanlar tarafından aşılabilecektir. AgentDojo benchmark'ında, çeşitli izleme LLM'leri altında AlignmentCheck ve Extract-and-Evaluate monitörlerine karşı yüksek bir saldırı başarı oranı elde etmekteyiz. Bulgularımız, mevcut izleme tabanlı ajansal savunmaların model ölçeğinden bağımsız olarak temelden kırılgan olduğunu düşündürmektedir."
    }
  },
  {
    "id": "2602.05056v1",
    "title": "VEXA: Evidence-Grounded and Persona-Adaptive Explanations for Scam Risk Sensemaking",
    "authors": [
      "Heajun An",
      "Connor Ng",
      "Sandesh Sharma Dulal",
      "Junghwan Kim",
      "Jin-Hee Cho"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.05056v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05056v1",
    "content": {
      "en": "Online scams across email, short message services, and social media increasingly challenge everyday risk assessment, particularly as generative AI enables more fluent and context-aware deception. Although transformer-based detectors achieve strong predictive performance, their explanations are often opaque to non-experts or misaligned with model decisions. We propose VEXA, an evidence-grounded and persona-adaptive framework for generating learner-facing scam explanations by integrating GradientSHAP-based attribution with theory-informed vulnerability personas. Evaluation across multi-channel datasets shows that grounding explanations in detector-derived evidence improves semantic reliability without increasing linguistic complexity, while persona conditioning introduces interpretable stylistic variation without disrupting evidential alignment. These results reveal a key design insight: evidential grounding governs semantic correctness, whereas persona-based adaptation operates at the level of presentation under constraints of faithfulness. Together, VEXA demonstrates the feasibility of persona-adaptive, evidence-grounded explanations and provides design guidance for trustworthy, learner-facing security explanations in non-formal contexts.",
      "tr": "**Makale Başlığı:** VEXA: Dolandırıcılık Riski Anlama İçin Kanıta Dayalı ve Kişiliğe Uyarlanabilir Açıklamalar\n\n**Özet:**\n\nE-posta, kısa mesaj servisleri ve sosyal medya üzerinden yaşanan çevrimiçi dolandırıcılıklar, özellikle üretken yapay zekanın daha akıcı ve bağlama duyarlı aldatmacalara olanak sağlamasıyla, günlük risk değerlendirmesini giderek daha fazla zorlamaktadır. Transformer tabanlı tespit sistemleri güçlü tahmin performansı sergilese de, açıklamaları genellikle uzman olmayanlar için opak kalmakta veya model kararlarıyla uyumsuz olmaktadır. Biz, GradientSHAP tabanlı atıf ile teori temelli kırılganlık kişiliklerini (vulnerability personas) entegre ederek, öğrenci odaklı dolandırıcılık açıklamaları üretmek için kanıta dayalı ve kişiliğe uyarlanabilir bir framework olan VEXA'yı öneriyoruz. Çok kanallı veri kümeleri üzerinden yapılan değerlendirmeler, açıklamaları tespit sisteminden türetilen kanıtlara dayandırmanın, dilsel karmaşıklığı artırmadan anlamsal güvenilirliği artırdığını göstermektedir. Persona koşullandırması ise, kanıtsal uyumu bozmadan yorumlanabilir stilistik varyasyonlar sunmaktadır. Bu sonuçlar, önemli bir tasarım içgörüsünü ortaya koymaktadır: kanıtsal temellendirme anlamsal doğruluğu yönetirken, kişiliğe dayalı uyarlama sadakat kısıtlamaları altında sunum seviyesinde işlev görür. Birlikte ele alındığında, VEXA kişiliğe uyarlanabilir, kanıta dayalı açıklamaların fizibilitesini göstermekte ve gayri resmi bağlamlarda güvenilir, öğrenci odaklı güvenlik açıklamaları için tasarım rehberliği sağlamaktadır."
    }
  },
  {
    "id": "2602.05023v1",
    "title": "Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?",
    "authors": [
      "Ruixin Yang",
      "Ethan Mendes",
      "Arthur Wang",
      "James Hays",
      "Sauvik Das"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.05023v1",
    "pdf_link": "https://arxiv.org/pdf/2602.05023v1",
    "content": {
      "en": "Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity, we introduce VLM-GEOPRIVACY, a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.",
      "tr": "**Makale Başlığı:** Vizyon-Dil Modelleri, Konum Açıklamasında Bağlamsal Bütünlüğe Saygı Duyuyor mu?\n\n**Özet:**\n\nVision-language models (VLMs), görüntü jeolokalizasyonunda güçlü performans sergilemiştir ve bu yetenek frontier multimodal large reasoning models (MLRMs) ile daha da keskinleştirilmiştir. Bu durum, yaygın olarak erişilebilir bu modellerin rastgele paylaşılan fotoğraflardan, genellikle sokak seviyesinde hassasiyetle, hatta paylaşan kişinin rıza gösterdiği veya ifşa etmeyi amaçladığı detay seviyesini aşabilecek şekilde hassas konumları çıkarmak için kullanılabileceği göz önüne alındığında, önemli bir gizlilik riski oluşturmaktadır. Yakın zamandaki çalışmalar, bu riske karşı koymak için jeolokalizasyon açıklamasında genel bir kısıtlama getirmeyi önerse de, bu önlemler geçerli jeolokalizasyon kullanımlarını kötü niyetli davranışlardan ayırt edememektedir. Bunun yerine, VLMs gizlilik ve fayda dengesini sağlayarak, bilgi açıklamasının uygun seviyesini belirlemek için bir görüntüdeki unsurlar hakkında reasoning yaparak bağlamsal bütünlüğü korumalıdır. Modellerin bağlamsal bütünlüğe ne kadar saygı gösterdiğini değerlendirmek için, VLMs'yi gerçek dünya görüntülerindeki örtük sosyal normları ve bağlamsal ipuçlarını yorumlamaya ve konum açıklamasının uygun seviyesini belirlemeye zorlayan bir benchmark olan VLM-GEOPRIVACY'yi tanıtıyoruz. 14 önde gelen VLM üzerindeki değerlendirmemiz, görüntüleri hassas bir şekilde jeolokalize etme yeteneklerine rağmen, modellerin insan gizlilik beklentileriyle zayıf bir şekilde uyumlu olduğunu göstermektedir. Hassas bağlamlarda sıklıkla aşırı açıklama yapmakta ve prompt-based attacks'lere karşı savunmasızdırlar. Sonuçlarımız, bağlam odaklı gizlilik reasoning'ini kapsayan multimodal sistemlerde yeni tasarım prensipleri gerektirmektedir."
    }
  },
  {
    "id": "2602.04753v1",
    "title": "Comparative Insights on Adversarial Machine Learning from Industry and Academia: A User-Study Approach",
    "authors": [
      "Vishruti Kakkad",
      "Paul Chung",
      "Hanan Hibshi",
      "Maverick Woo"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.04753v1",
    "pdf_link": "https://arxiv.org/pdf/2602.04753v1",
    "content": {
      "en": "An exponential growth of Machine Learning and its Generative AI applications brings with it significant security challenges, often referred to as Adversarial Machine Learning (AML). In this paper, we conducted two comprehensive studies to explore the perspectives of industry professionals and students on different AML vulnerabilities and their educational strategies. In our first study, we conducted an online survey with professionals revealing a notable correlation between cybersecurity education and concern for AML threats. For our second study, we developed two CTF challenges that implement Natural Language Processing and Generative AI concepts and demonstrate a poisoning attack on the training data set. The effectiveness of these challenges was evaluated by surveying undergraduate and graduate students at Carnegie Mellon University, finding that a CTF-based approach effectively engages interest in AML threats. Based on the responses of the participants in our research, we provide detailed recommendations emphasizing the critical need for integrated security education within the ML curriculum.",
      "tr": "**Makale Başlığı:** Endüstri ve Akademiden Çekişmeli Makine Öğrenimi Üzerine Karşılaştırmalı İçgörüler: Bir Kullanıcı-Çalışması Yaklaşımı\n\n**Özet:**\n\nMakine Öğrenimi ve Üretken Yapay Zeka uygulamalarındaki üstel artış, Çekişmeli Makine Öğrenimi (Adversarial Machine Learning - AML) olarak adlandırılan önemli güvenlik zorluklarını da beraberinde getirmektedir. Bu çalışmada, endüstri profesyonellerinin ve öğrencilerin çeşitli AML güvenlik açıkları ile eğitim stratejilerine ilişkin bakış açılarını keşfetmek amacıyla iki kapsamlı çalışma yürütülmüştür. İlk çalışmamızda, siber güvenlik eğitimi ile AML tehditlerine yönelik endişe arasında dikkate değer bir korelasyon ortaya koyan profesyonellerle çevrimiçi bir anket gerçekleştirilmiştir. İkinci çalışmamızda ise, Natural Language Processing ve Generative AI kavramlarını uygulayan ve eğitim veri setine yönelik bir poisoning attack'ı sergileyen iki CTF challenge geliştirilmiştir. Carnegie Mellon Üniversitesi'ndeki lisans ve lisansüstü öğrencileriyle yapılan anketle bu challenge'ların etkinliği değerlendirilmiş ve CTF tabanlı bir yaklaşımın AML tehditlerine olan ilgiyi etkili bir şekilde artırdığı bulunmuştur. Araştırmamızdaki katılımcıların yanıtlarına dayanarak, ML müfredatına entegre edilmiş güvenlik eğitiminin kritik gerekliliğini vurgulayan ayrıntılı öneriler sunulmaktadır."
    }
  }
]