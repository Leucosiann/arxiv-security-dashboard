[
  {
    "id": "2512.20423v1",
    "title": "Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit",
    "authors": [
      "Adam Elaoumari"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2512.20423v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20423v1",
    "content": {
      "en": "The purpose of this project is to assess how well defenders can detect DNS-over-HTTPS (DoH) file exfiltration, and which evasion strategies can be used by attackers. While providing a reproducible toolkit to generate, intercept and analyze DoH exfiltration, and comparing Machine Learning vs threshold-based detection under adversarial scenarios. The originality of this project is the introduction of an end-to-end, containerized pipeline that generates configurable file exfiltration over DoH using several parameters (e.g., chunking, encoding, padding, resolver rotation). It allows for file reconstruction at the resolver side, while extracting flow-level features using a fork of DoHLyzer. The pipeline contains a prediction side, which allows the training of machine learning models based on public labelled datasets and then evaluates them side-by-side with threshold-based detection methods against malicious and evasive DNS-Over-HTTPS traffic. We train Random Forest, Gradient Boosting and Logistic Regression classifiers on a public DoH dataset and benchmark them against evasive DoH exfiltration scenarios. The toolkit orchestrates traffic generation, file capture, feature extraction, model training and analysis. The toolkit is then encapsulated into several Docker containers for easy setup and full reproducibility regardless of the platform it is run on. Future research regarding this project is directed at validating the results on mixed enterprise traffic, extending the protocol coverage to HTTP/3/QUIC request, adding a benign traffic generation, and working on real-time traffic evaluation. A key objective is to quantify when stealth constraints make DoH exfiltration uneconomical and unworthy for the attacker.",
      "tr": "**Makale Başlığı:** DNS-over-HTTPS Veri Sızdırmanın Kaçınmaya Dayanıklı Tespiti: Pratik Bir Değerlendirme ve Araç Seti\n\n**Özet:**\n\nBu projenin amacı, savunmacıların DNS-over-HTTPS (DoH) dosya sızdırmayı ne kadar iyi tespit edebileceğini ve saldırganların hangi kaçınma stratejilerini kullanabileceğini değerlendirmektir. DoH sızdırmayı üreten, kesen ve analiz eden tekrarlanabilir bir araç seti sunarken, makine öğrenmesi ile eşik tabanlı algılamayı düşmanca senaryolar altında karşılaştırmaktadır. Bu projenin özgünlüğü, çeşitli parametreler (örneğin, chunking, encoding, padding, resolver rotation) kullanarak DoH üzerinden yapılandırılabilir dosya sızdırmayı üreten uçtan uca, kapsüllenmiş bir pipeline'ın tanıtılmasıdır. Bu pipeline, DoHLyzer'ın bir fork'unu kullanarak flow-level features'ı çıkarırken, resolver tarafında dosya yeniden yapılandırmasına olanak tanır. Pipeline, halka açık etiketlenmiş veri kümelerine dayalı makine öğrenmesi modellerinin eğitilmesine ve ardından bunları kötü niyetli ve kaçıngan DNS-Over-HTTPS trafiğine karşı eşik tabanlı algılama yöntemleriyle yan yana değerlendirmeye olanak tanıyan bir prediction side içerir. Random Forest, Gradient Boosting ve Logistic Regression sınıflandırıcılarını halka açık bir DoH veri kümesi üzerinde eğitiyor ve bunları kaçıngan DoH sızdırma senaryolarına karşı kıyaslıyoruz. Araç seti, trafik üretimi, dosya yakalama, feature extraction, model eğitimi ve analizi koordine eder. Araç seti daha sonra kurulumunun kolaylığı ve üzerinde çalıştığı platformdan bağımsız tam tekrarlanabilirlik için çeşitli Docker container'larına kapsüllenir. Bu projeyle ilgili gelecekteki araştırmalar, sonuçları karma kurumsal trafik üzerinde doğrulamaya, protokol kapsamını HTTP/3/QUIC isteklere genişletmeye, iyi huylu trafik üretimi eklemeye ve gerçek zamanlı trafik değerlendirmesi üzerinde çalışmaya yöneliktir. Temel bir hedef, gizlilik kısıtlamalarının DoH sızdırmayı saldırgan için ekonomiyi boşa çıkarıp değersiz hale getirdiği noktayı ölçmektir."
    }
  },
  {
    "id": "2512.20396v1",
    "title": "Symmaries: Automatic Inference of Formal Security Summaries for Java Programs",
    "authors": [
      "Narges Khakpour",
      "Nicolas Berthier"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.FL",
      "cs.PL",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2512.20396v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20396v1",
    "content": {
      "en": "We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.",
      "tr": "Makale Başlığı: Symmaries: Java Programları İçin Resmi Güvenlik Özetlerinin Otomatik Çıkarımı\n\nÖzet:\nJava bytecode programları için method özetleri biçiminde resmi güvenlik spesifikasyonlarının otomatik olarak oluşturulmasına yönelik ölçeklenebilir, modüler ve tutarlı bir yaklaşım sunuyoruz. Bir özet, yöntemin güvenli bir şekilde çağrılabileceği koşulları ve bilgi akışları ile aliasing güncellemelerinin spesifikasyonlarını içeren, bir yöntemin güvenlik davranışının soyut bir temsilini sağlar. Bu tür özetler, statik kod analizi araçları tarafından tüketilebilir ve aynı zamanda geliştiricilerin, uygulamalarda yeniden kullanıldıklarında güvenlik etkilerini değerlendirmek amacıyla kod parçalarının (kütüphaneler gibi) davranışlarını anlamalarına yardımcı olur. Yaklaşımımız, güvenlik özetlerinin üretimini otomatikleştiren Symmaries adlı bir araçta uygulanmıştır. Symmaries'i, güvenlik spesifikasyonlarını çıkarmak için Java API kütüphanelerine ve ölçeklenebilirliğini değerlendirmek için büyük gerçek dünya uygulamalarına uyguladık. Sonuçlarımız, aracın yüz binlerce satır kod içeren uygulamaları analiz etmeye başarılı bir şekilde ölçeklendiğini ve Symmaries'in kullanılan heap modeline bağlı olarak umut verici bir kesinlik elde ettiğini göstermektedir. Yaklaşımımızın, termination-insensitive non-interference'ı garanti etme açısından tutarlılığını kanıtlıyoruz."
    }
  },
  {
    "id": "2512.20168v1",
    "title": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography",
    "authors": [
      "Songze Li",
      "Jiameng Cheng",
      "Yiming Li",
      "Xiaojun Jia",
      "Dacheng Tao"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.20168v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20168v1",
    "content": {
      "en": "By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.",
      "tr": "**Makale Başlığı:** Odysseus: Dual Steganography Yöntemiyle Ticari Multimodal LLM-Entegre Sistemlerin Jailbreak'i\n\n**Özet:**\n\nDil anlayışını görüntü gibi algısal modalitelerle entegre eden multimodal large language models (MLLMs), özellikle açık ve etkileşimli ortamlarda çalışan akıllı ajanlar gibi modern yapay zeka sistemleri için kritik bir altyapı oluşturmaktadır. Ancak, artan erişilebilirliği, zararlı veya güvensiz içerik üretme gibi kötüye kullanım risklerini de beraberinde getirmektedir. Bu riskleri azaltmak için, model davranışını insan değerleriyle uyumlu hale getirmek amacıyla alignment teknikleri yaygın olarak uygulanmaktadır. Bu çabalara rağmen, son çalışmalar jailbreak saldırılarının alignment'ı atlatarak güvensiz çıktılar üretebildiğini göstermiştir. Şu anda, mevcut jailbreak yöntemlerinin çoğu açık kaynaklı modellere göre uyarlanmış olup, ek filtreler kullanan ticari MLLM-integrated sistemlere karşı sınırlı etkililik sergilemektedir. Bu filtreler, kötü niyetli girdi ve çıktı içeriklerini tespit edip engelleyerek jailbreak tehditlerini önemli ölçüde azaltabilir. Bu makalede, bu güvenlik filtrelerinin başarısının büyük ölçüde, kötü niyetli içeriğin girdi veya çıktıda açıkça görünmesi gerektiği yönündeki kritik bir varsayıma dayandığını ortaya koymaktayız. Geleneksel LLM-integrated sistemler için genellikle geçerli olan bu varsayım, saldırganların çoklu modaliteleri kullanarak kötü niyetli amacı gizleyebildiği ve mevcut MLLM-integrated sistemlerde yanlış bir güvenlik hissi yarattığı MLLM-integrated sistemlerde çöküyor. Bu varsayıma meydan okumak için, kötü niyetli sorguları ve yanıtları zararsız görünen görüntülere gizlice gömmek için dual steganography'yi tanıtan yeni bir jailbreak paradigması olan Odysseus'u öneriyoruz. Kıyaslama veri kümeleri üzerinde yapılan kapsamlı deneyler, Odysseus'umuzun birkaç öncü ve gerçekçi MLLM-integrated sistemi başarıyla jailbreak'lediğini ve %99'a varan saldırı başarı oranı elde ettiğini göstermektedir. Mevcut savunmalarda temel bir kör noktayı ortaya koymakta ve MLLM-integrated sistemlerde çapraz modal güvenlik konusunda yeniden düşünme çağrısı yapmaktadır."
    }
  },
  {
    "id": "2512.20062v1",
    "title": "On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities",
    "authors": [
      "Sangryu Park",
      "Gihyuk Ko",
      "Homook Cho"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.20062v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20062v1",
    "content": {
      "en": "Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows.",
      "tr": "**Makale Başlığı:** Yazılım Güvenlik Açıklarını Tespit Etmek İçin Lokal LLM'lerin Instruction-Tuning Etkinliği Üzerine\n\n**Özet:**\nBüyük Dil Modelleri (LLM'ler), modern yazılım sistemlerinin güvenlik arızalarının etkileri göz önüne alındığında kritik bir görev olan yazılım güvenlik açığı analizini otomatikleştirmede önemli vaatler sunmaktadır. Ancak, LLM'leri güvenlik açığı analizini otomatikleştirmek için kullanma konusundaki mevcut yaklaşımlar çoğunlukla çevrimiçi API tabanlı LLM hizmetlerine dayanmaktadır ve bu da kullanıcının geliştirme aşamasında kaynak kodunu ifşa etmesini gerektirmektedir. Dahası, bu yaklaşımlar görevi çoğunlukla ikili sınıflandırma (güvenli veya güvensiz) olarak çerçevelemektedir, bu da potansiyel pratik faydayı sınırlamaktadır. Bu makale, problemi Yazılım Güvenlik Açığı Tespiti (Software Vulnerability Identification - SVI) olarak yeniden formüle ederek bu sınırlamaları ele almaktadır; burada LLM'lerden bir güvenlik açığının varlığını veya yokluğunu basitçe belirtmek yerine Common Weakness Enumeration (CWE) ID'lerinde zayıflık türünü çıktı olarak vermeleri istenmektedir. Ayrıca, daha küçük, lokal olarak dağıtılabilir LLM'lerin instruction-tuning'inin üstün tespit performansı sağlayabileceğini göstererek büyük, API tabanlı LLM'lere olan bağımlılığı da ele alıyoruz. Analizimizde, bir lokal LLM'nin instruction-tuning'inin çevrimiçi API tabanlı LLM'lere kıyasla daha iyi genel performans ve maliyet dengesi gösterdiği ortaya konulmuştur. Bulgularımız, instruction-tuned lokal modellerin, gerçek dünya güvenlik açığı yönetimi iş akışlarında LLM'lerden yararlanmak için daha etkili, güvenli ve pratik bir yaklaşımı temsil ettiğini göstermektedir."
    }
  },
  {
    "id": "2512.20004v1",
    "title": "IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense",
    "authors": [
      "Rahul Yumlembam",
      "Biju Issac",
      "Seibu Mary Jacob",
      "Longzhi Yang"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.20004v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20004v1",
    "content": {
      "en": "Since the Internet of Things (IoT) is widely adopted using Android applications, detecting malicious Android apps is essential. In recent years, Android graph-based deep learning research has proposed many approaches to extract relationships from applications as graphs to generate graph embeddings. First, we demonstrate the effectiveness of graph-based classification using a Graph Neural Network (GNN)-based classifier to generate API graph embeddings. The graph embeddings are combined with Permission and Intent features to train multiple machine learning and deep learning models for Android malware detection. The proposed classification approach achieves an accuracy of 98.33 percent on the CICMaldroid dataset and 98.68 percent on the Drebin dataset. However, graph-based deep learning models are vulnerable, as attackers can add fake relationships to evade detection by the classifier. Second, we propose a Generative Adversarial Network (GAN)-based attack algorithm named VGAE-MalGAN targeting graph-based GNN Android malware classifiers. The VGAE-MalGAN generator produces adversarial malware API graphs, while the VGAE-MalGAN substitute detector attempts to mimic the target detector. Experimental results show that VGAE-MalGAN can significantly reduce the detection rate of GNN-based malware classifiers. Although the model initially fails to detect adversarial malware, retraining with generated adversarial samples improves robustness and helps mitigate adversarial attacks.",
      "tr": "**Makale Başlığı:** Graph Neural Network ile Yapay Savunma Kullanılarak IoT Tabanlı Android Kötü Amaçlı Yazılım Tespiti\n\n**Özet:**\n\nİnternet of Things (IoT)'nin Android uygulamaları aracılığıyla yaygın olarak benimsenmesi göz önüne alındığında, kötü amaçlı Android uygulamaların tespiti büyük önem taşımaktadır. Son yıllarda, Android'in grafik tabanlı derin öğrenme araştırmaları, uygulamalardan grafikler şeklinde ilişkileri çıkarmak ve bu grafiklerden graph embeddings üretmek için birçok yaklaşım sunmuştur. İlk olarak, API graph embeddings üretmek amacıyla bir Graph Neural Network (GNN)-tabanlı sınıflandırıcı kullanarak grafik tabanlı sınıflandırmanın etkinliğini gösteriyoruz. Bu graph embeddings, İzin (Permission) ve Niyet (Intent) özellikleri ile birleştirilerek Android kötü amaçlı yazılım tespiti için çoklu makine öğrenmesi ve derin öğrenme modellerini eğitmek üzere kullanılmıştır. Önerilen sınıflandırma yaklaşımı, CICMaldroid veri kümesinde %98.33 ve Drebin veri kümesinde %98.68 doğruluk elde etmiştir. Ancak, grafik tabanlı derin öğrenme modelleri savunmasızdır; saldırganlar, sınıflandırıcı tarafından tespitten kaçınmak için sahte ilişkiler ekleyebilirler. İkinci olarak, grafik tabanlı GNN Android kötü amaçlı yazılım sınıflandırıcılarını hedef alan VGAE-MalGAN adlı bir Generative Adversarial Network (GAN)-tabanlı saldırı algoritması öneriyoruz. VGAE-MalGAN jeneratörü, adversarial malware API graphs üretirken, VGAE-MalGAN substitute detector hedef dedektörü taklit etmeye çalışır. Deneysel sonuçlar, VGAE-MalGAN'ın GNN tabanlı kötü amaçlı yazılım sınıflandırıcılarının tespit oranını önemli ölçüde azaltabildiğini göstermektedir. Model başlangıçta adversarial kötü amaçlı yazılımları tespit edemese de, üretilen adversarial örneklerle yeniden eğitilmesi, dayanıklılığı artırır ve adversarial saldırıları azaltmaya yardımcı olur."
    }
  },
  {
    "id": "2512.19935v1",
    "title": "Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress",
    "authors": [
      "Samruddhi Baviskar"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.19935v1",
    "pdf_link": "https://arxiv.org/pdf/2512.19935v1",
    "content": {
      "en": "Machine learning models used in financial decision systems operate in nonstationary economic environments, yet adversarial robustness is typically evaluated under static assumptions. This work introduces Conditional Adversarial Fragility, a regime dependent phenomenon in which adversarial vulnerability is systematically amplified during periods of macroeconomic stress. We propose a regime aware evaluation framework for time indexed tabular financial classification tasks that conditions robustness assessment on external indicators of economic stress. Using volatility based regime segmentation as a proxy for macroeconomic conditions, we evaluate model behavior across calm and stress periods while holding model architecture, attack methodology, and evaluation protocols constant. Baseline predictive performance remains comparable across regimes, indicating that economic stress alone does not induce inherent performance degradation. Under adversarial perturbations, however, models operating during stress regimes exhibit substantially greater degradation across predictive accuracy, operational decision thresholds, and risk sensitive outcomes. We further demonstrate that this amplification propagates to increased false negative rates, elevating the risk of missed high risk cases during adverse conditions. To complement numerical robustness metrics, we introduce an interpretive governance layer based on semantic auditing of model explanations using large language models. Together, these results demonstrate that adversarial robustness in financial machine learning is a regime dependent property and motivate stress aware approaches to model risk assessment in high stakes financial deployments.",
      "tr": "Makale Başlığı: Makroekonomik Stres Altında Finansal Makine Öğreniminde Koşullu Çekişmeli Kırılganlık\n\nÖzet:\nFinansal karar sistemlerinde kullanılan makine öğrenimi modelleri durağan olmayan ekonomik ortamlarda çalışır; ancak çekişmeli sağlamlık (adversarial robustness) tipik olarak statik varsayımlar altında değerlendirilir. Bu çalışma, makroekonomik stres dönemlerinde çekişmeli savunmasızlığın sistematik olarak arttığı rejim bağımlı bir olgu olan Conditional Adversarial Fragility'yi sunmaktadır. Ekonomik stresin dışsal göstergelerine göre sağlamlık değerlendirmesini koşullandıran, zaman indeksli tablo finansal sınıflandırma görevleri için rejim farkındalığı olan bir değerlendirme çerçevesi öneriyoruz. Makroekonomik koşulların vekili olarak oynaklık tabanlı rejim segmentasyonunu kullanarak, model mimarisini, attack methodology'sini ve evaluation protocols'ü sabit tutarak sakin ve stres dönemleri boyunca model davranışını değerlendiriyoruz. Temel tahmin performansı rejimler arasında karşılaştırılabilir düzeyde kalır, bu da ekonomik stresin tek başına içsel performans düşüşüne neden olmadığını göstermektedir. Ancak, çekişmeli pertürbasyonlar altında, stres rejimleri sırasında çalışan modellerin tahmin doğruluğu, operasyonel karar eşikleri ve risk duyarlı çıktılar boyunca önemli ölçüde daha fazla bozulma sergilediği görülmektedir. Ayrıca, bu amplifikasyonun artan yanlış negatif oranlarına yayıldığını ve olumsuz koşullar sırasında yüksek riskli vakaların kaçırılma riskini yükselttiğini gösteriyoruz. Sayısal sağlamlık metriklerini tamamlamak üzere, large language models kullanılarak model açıklamalarının anlamsal denetimine dayanan yorumlayıcı bir yönetişim katmanı sunuyoruz. Birlikte ele alındığında, bu sonuçlar finansal makine öğrenimindeki çekişmeli sağlamlığın rejim bağımlı bir özellik olduğunu göstermekte ve yüksek riskli finansal konuşlandırmalarda model risk değerlendirmesi için stres farkındalığı olan yaklaşımları teşvik etmektedir."
    }
  },
  {
    "id": "2512.19297v1",
    "title": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
    "authors": [
      "Linzhi Chen",
      "Yang Sun",
      "Hongru Wei",
      "Yuqi Chen"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.19297v1",
    "pdf_link": "https://arxiv.org/pdf/2512.19297v1",
    "content": {
      "en": "Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.",
      "tr": "**Makale Başlığı: Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models**\n\n**Özet:**\n\nLow-Rank Adaptation (LoRA), büyük dil modellerini (LLM) ince ayar yapmak için verimli bir yöntem olarak öne çıkmış ve açık kaynak topluluğu içinde yaygın olarak benimsenmiştir. Ancak, Hugging Face gibi platformlar aracılığıyla LoRA adaptörlerinin merkezi olmayan dağıtımı, yeni güvenlik açıkları getirmektedir: kötü niyetli adaptörler kolayca dağıtılabilir ve geleneksel denetim mekanizmalarından kaçabilir. Bu risklere rağmen, LoRA tabanlı ince ayarı hedefleyen backdoor saldırıları nispeten yeterince araştırılmamıştır. Mevcut backdoor saldırı stratejileri bu ortam için uygun değildir, çünkü genellikle erişilemeyen eğitim verilerine dayanır, LoRA'ya özgü yapısal özellikleri dikkate alamaz veya yüksek false trigger rates (FTR) ile sonuçlanır, bu da gizliliklerini tehlikeye atar. Bu zorlukları ele almak için, açık ağırlıklı LoRA modelleri için özel olarak tasarlanmış yeni bir backdoor saldırı çerçevesi olan Causal-Guided Detoxify Backdoor Attack (CBA) öneriyoruz. CBA, orijinal eğitim verilerine erişim olmadan çalışır ve iki temel yenilikle yüksek gizlilik sağlar: (1) davranışsal keşif yoluyla göreve uyumlu girdileri sentezleyen bir coverage-guided data generation pipeline ve (2) göreve kritik nöronları koruyarak zehirlenmiş ve temiz adaptörleri birleştiren bir causal-guided detoxification strategy. Önceki yaklaşımların aksine, CBA, causal influence-based weight allocation aracılığıyla saldırı yoğunluğu üzerinde eğitim sonrası kontrolü mümkün kılar, bu da tekrarlanan yeniden eğitim ihtiyacını ortadan kaldırır. Altı LoRA modeli üzerinde değerlendirilen CBA, yüksek saldırı başarı oranları elde ederken, temel yöntemlere kıyasla FTR'yi %50-70 oranında azaltır. Ayrıca, state-of-the-art backdoor savunmalarına karşı gelişmiş direnç göstererek gizliliğini ve sağlamlığını vurgular."
    }
  },
  {
    "id": "2512.19286v1",
    "title": "GShield: Mitigating Poisoning Attacks in Federated Learning",
    "authors": [
      "Sameera K. M.",
      "Serena Nicolazzo",
      "Antonino Nocera",
      "Vinod P.",
      "Rafidha Rehiman K. A"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.19286v1",
    "pdf_link": "https://arxiv.org/pdf/2512.19286v1",
    "content": {
      "en": "Federated Learning (FL) has recently emerged as a revolutionary approach to collaborative training Machine Learning models. In particular, it enables decentralized model training while preserving data privacy, but its distributed nature makes it highly vulnerable to a severe attack known as Data Poisoning. In such scenarios, malicious clients inject manipulated data into the training process, thereby degrading global model performance or causing targeted misclassification. In this paper, we present a novel defense mechanism called GShield, designed to detect and mitigate malicious and low-quality updates, especially under non-independent and identically distributed (non-IID) data scenarios. GShield operates by learning the distribution of benign gradients through clustering and Gaussian modeling during an initial round, enabling it to establish a reliable baseline of trusted client behavior. With this benign profile, GShield selectively aggregates only those updates that align with the expected gradient patterns, effectively isolating adversarial clients and preserving the integrity of the global model. An extensive experimental campaign demonstrates that our proposed defense significantly improves model robustness compared to the state-of-the-art methods while maintaining a high accuracy of performance across both tabular and image datasets. Furthermore, GShield improves the accuracy of the targeted class by 43\\% to 65\\% after detecting malicious and low-quality clients.",
      "tr": "İşte akademik makale başlığı ve özetinin istenen dilde çevirisi:\n\n**Makale Başlığı:** GShield: Federated Learning'de Zehirlenme Saldırılarını Azaltma\n\n**Özet:**\nFederated Learning (FL), Makine Öğrenmesi modellerinin işbirlikçi eğitimi için devrim niteliğinde bir yaklaşım olarak son zamanlarda ortaya çıkmıştır. Özellikle, veri gizliliğini korurken merkezi olmayan model eğitimini mümkün kılar, ancak dağıtık doğası onu Veri Zehirlenmesi olarak bilinen ciddi bir saldırıya karşı son derece savunmasız hale getirir. Bu tür senaryolarda, kötü niyetli istemciler eğitim sürecine manipüle edilmiş veriler enjekte ederek global model performansını düşürür veya hedeflenen yanlış sınıflandırmalara neden olur. Bu çalışmada, özellikle non-independent and identically distributed (non-IID) veri senaryolarında kötü niyetli ve düşük kaliteli güncellemeleri tespit etmek ve azaltmak için tasarlanmış GShield adlı yeni bir savunma mekanizması sunuyoruz. GShield, ilk tur sırasında kümeleme ve Gaussian modeling yoluyla benign gradientlerin dağılımını öğrenerek çalışır, bu da güvenilir bir trusted client behavior temeli oluşturmasına olanak tanır. Bu benign profile sahip GShield, yalnızca beklenen gradient patterns ile uyumlu olan güncellemeleri seçici olarak toplar, böylece adversarial clients'ı etkili bir şekilde izole eder ve global modelin bütünlüğünü korur. Kapsamlı bir deneysel kampanya, önerdiğimiz savunmanın state-of-the-art yöntemlere kıyasla model sağlamlığını önemli ölçüde artırdığını ve hem tabular hem de görüntü veri kümelerinde yüksek bir performans doğruluğunu koruduğunu göstermektedir. Dahası, GShield, kötü niyetli ve düşük kaliteli istemcileri tespit ettikten sonra hedeflenen sınıfın doğruluğunu %43 ila %65 oranında iyileştirmektedir."
    }
  },
  {
    "id": "2512.19037v1",
    "title": "Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms",
    "authors": [
      "Md Minhazul Islam Munna",
      "Md Mahbubur Rahman",
      "Jaroslav Frnda",
      "Muhammad Shahid Anwar",
      "Alpamis Kutlimuratov"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.19037v1",
    "pdf_link": "https://arxiv.org/pdf/2512.19037v1",
    "content": {
      "en": "The proliferation of IoT devices and their reliance on Wi-Fi networks have introduced significant security vulnerabilities, particularly the KRACK and Kr00k attacks, which exploit weaknesses in WPA2 encryption to intercept and manipulate sensitive data. Traditional IDS using classifiers face challenges such as model overfitting, incomplete feature extraction, and high false positive rates, limiting their effectiveness in real-world deployments. To address these challenges, this study proposes a robust multiclass machine learning based intrusion detection framework. The methodology integrates advanced feature selection techniques to identify critical attributes, mitigating redundancy and enhancing detection accuracy. Two distinct ML architectures are implemented: a baseline classifier pipeline and a stacked ensemble model combining noise injection, Principal Component Analysis (PCA), and meta learning to improve generalization and reduce false positives. Evaluated on the AWID3 data set, the proposed ensemble architecture achieves superior performance, with an accuracy of 98%, precision of 98%, recall of 98%, and a false positive rate of just 2%, outperforming existing state-of-the-art methods. This work demonstrates the efficacy of combining preprocessing strategies with ensemble learning to fortify network security against sophisticated Wi-Fi attacks, offering a scalable and reliable solution for IoT environments. Future directions include real-time deployment and adversarial resilience testing to further enhance the model's adaptability.",
      "tr": "Makale Başlığı: Akıllı Ağlarda Siber Güvenlik ve Sistem Analizi Konularında Keskin Makine Öğrenmesi Paradigmaları ile Saldırı Tespitinin ve Güvenlik Güçlendirmesinin Yükseltilmesi\n\nÖzet:\nIoT cihazlarının yaygınlaşması ve Wi-Fi ağlarına olan bağımlılıkları, özellikle KRACK ve Kr00k saldırıları gibi önemli güvenlik zafiyetlerini ortaya çıkarmıştır. Bu saldırılar, WPA2 şifrelemesindeki zayıflıkları istismar ederek hassas verilerin kesilmesine ve manipüle edilmesine neden olmaktadır. Sınıflandırıcılar kullanan geleneksel IDS'ler, model aşırı uyumu (model overfitting), eksik özellik çıkarımı (incomplete feature extraction) ve yüksek yanlış pozitif oranları gibi zorluklarla karşı karşıya kalmakta, bu da gerçek dünya uygulamalarındaki etkinliklerini sınırlamaktadır. Bu zorlukların üstesinden gelmek için bu çalışma, sağlam bir multiclass machine learning tabanlı saldırı tespit çerçevesi önermektedir. Metodoloji, kritik nitelikleri belirlemek, gereksizliği azaltmak ve tespit doğruluğunu artırmak için gelişmiş özellik seçimi (feature selection) tekniklerini entegre etmektedir. İki farklı ML mimarisi uygulanmaktadır: bir baseline classifier pipeline ve gürültü enjeksiyonu (noise injection), Principal Component Analysis (PCA) ve meta öğrenmeyi (meta learning) birleştiren ve genelleştirmeyi iyileştiren ve yanlış pozitifleri azaltan bir stacked ensemble model. AWID3 veri kümesi üzerinde değerlendirilen önerilen ensemble mimarisi, %98 doğruluk (accuracy), %98 kesinlik (precision), %98 geri çağırma (recall) ve yalnızca %2 yanlış pozitif oranı (false positive rate) ile üstün performans sergileyerek mevcut en gelişmiş yöntemleri geride bırakmaktadır. Bu çalışma, sofistike Wi-Fi saldırılarına karşı ağ güvenliğini güçlendirmek için ön işleme stratejilerinin (preprocessing strategies) ensemble öğrenme ile birleştirilmesinin etkinliğini göstermekte ve IoT ortamları için ölçeklenebilir ve güvenilir bir çözüm sunmaktadır. Gelecek yönelimleri arasında modelin adaptasyonunu daha da geliştirmek için gerçek zamanlı dağıtım (real-time deployment) ve adversarial resilience testing yer almaktadır."
    }
  },
  {
    "id": "2512.19025v2",
    "title": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
    "authors": [
      "Hengrui Jia",
      "Taoran Li",
      "Jonas Guan",
      "Varun Chandrasekaran"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.19025v2",
    "pdf_link": "https://arxiv.org/pdf/2512.19025v2",
    "content": {
      "en": "Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have \"forgotten\" the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose Proximal Surrogate Generation (PSG), an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$β$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.",
      "tr": "Elbette, akademik makalenin başlığını ve özetini talep ettiğiniz şekilde çevirdim:\n\n**Makale Başlığı:** The Erasure Illusion: LLM Forgetting Evaluation'ın Generalization'ını Stres Testinden Geçirme\n\n**Özet:**\nMachine unlearning, eğitilmiş modellerden belirli veri etkilerini kaldırmayı hedefler; bu yetenek, telif hakkı yasalarına uyumluluk ve AI safety'nin sağlanması için temeldir. Mevcut unlearning metrikleri, genellikle unlearning dataset'i ($D_u$) üzerindeki model performansındaki düşüşü izleyerek başarıyı ölçer. Büyük Dil Modelleri (LLM'ler) için bu değerlendirme paradigmasının yetersiz ve potansiyel olarak yanıltıcı olduğunu savunuyoruz. Unlearning'in telif hakkı veya safety tarafından motive edilen birçok gerçek dünya kullanımı, yalnızca $D_u$'daki kelimesi kelimesine içeriği değil, aynı zamanda ondan türetilen daha geniş genellemelere dayanan davranışları da örtük olarak hedefler. Standart unlearning değerlendirmesini geçen ve hedef bilgiyi \"unutmuş\" gibi görünen LLM'lerin, aynı zamanda $D_u$'ya anlamsal olarak bitişik içeriğe yönelik güçlü yeteneklerini koruyabildiğini gösteriyoruz. Bu olgu, tam cümleleri silmenin mutlaka altında yatan bilgiyi kaldırmakla eşdeğer olmadığını göstermektedir. Bu boşluğu gidermek için, bir surrogate dataset, $\\tilde{D}_u$ üreten otomatik bir stres-testing framework'ü olan Proximal Surrogate Generation'ı (PSG) öneriyoruz. Bu surrogate set, $D_u$'dan anlamsal olarak türetilmiş ancak embedding space'te yeterince farklı olacak şekilde oluşturulmuştur. $D_u$ ve $\\tilde{D}_u$ arasındaki unlearning metrik puanlarını karşılaştırarak, metriğin kendisinin güvenilirliğini stres testinden geçirebiliriz. Üç LLM ailesi (Llama-3-8B, Qwen2.5-7B ve Zephyr-7B-$β$), üç farklı dataset ve yedi standart metrik üzerindeki kapsamlı değerlendirmemiz, yaygın tutarsızlıkları ortaya koymaktadır. Mevcut metriklerin, stres-testi dataset'lerimiz tarafından ortaya konan saklanan bilgiyi tespit edemeyerek, unlearning başarısını sıklıkla aşırı tahmin ettiğini bulduk."
    }
  }
]