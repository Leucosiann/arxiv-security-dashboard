[
  {
    "id": "2602.12250v1",
    "title": "Community Concealment from Unsupervised Graph Learning-Based Clustering",
    "authors": [
      "Dalyapraz Manatova",
      "Pablo Moriano",
      "L. Jean Camp"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.LG",
      "cs.CR",
      "cs.SI"
    ],
    "link": "http://arxiv.org/abs/2602.12250v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12250v1",
    "content": {
      "en": "Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.",
      "tr": "**Makale Başlığı:** Gözetimsiz Grafik Öğrenme Tabanlı Kümeleme Yoluyla Topluluk Gizliliği\n\n**Özet:**\n\nGraph neural networks (GNNs), atıflandırılmış grafları temsil öğrenmek için tasarlanmıştır. Bu tür temsiller, gözetimsiz kümeleme ve topluluk tespiti öğreniminde faydalıdır. Bununla birlikte, bu tür çıkarımlar hassas grupları, kümelenmiş sistemleri veya kolektif davranışları ortaya çıkarabilir ve grup düzeyinde gizlilik endişelerine yol açabilir. Örneğin, sosyal ve kritik altyapı ağlarındaki topluluk atıfları, profil oluşturma veya istihbarat toplama amacıyla kullanılabilecek koordineli varlık gruplarını, operasyonel hiyerarşileri ve sistem bağımlılıklarını açığa çıkarabilir. Veri yayıncısının (savunmacı) ilgilenilen bir topluluğu gizlemeye çalıştığı ve ağda sınırlı, fayda-farkında değişiklikler yaptığı savunmacı bir ortamı inceliyoruz. Analizimiz, topluluk gizliliğinin iki ölçülebilir faktörden güçlü bir şekilde etkilendiğini göstermektedir: topluluk sınırındaki bağlantı ve korunan topluluk ile komşu topluluklar arasındaki özellik benzerliği. Bu bulgulardan yola çıkarak, seçilmiş kenarların bir kümesini yeniden yapılandıran ve GNN mesaj geçişinden yararlanılan belirginliği azaltmak için düğüm özelliklerini değiştiren bir pertürbasyon stratejisi sunuyoruz. Önerilen yöntem, sentetik kıyaslamalar ve gerçek ağ grafları üzerinde yapılan deneylerimizde, aynı pertürbasyon bütçeleri altında DICE'ı geride bırakmaktadır. Genel olarak, değerlendirilen ayarlarda yaklaşık %20-45 oranında ortalama göreceli gizlilik iyileştirmeleri elde eder. Bu bulgular, GNN tabanlı topluluk öğrenimine karşı bir azaltma stratejisi sergilemekte ve grafik öğrenmeye özgü grup düzeyinde gizlilik risklerini vurgulamaktadır."
    }
  },
  {
    "id": "2602.12092v1",
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "authors": [
      "Bo Zhang",
      "Jiaxuan Guo",
      "Lijun Li",
      "Dongrui Liu",
      "Sujin Chen"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2602.12092v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12092v1",
    "content": {
      "en": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.",
      "tr": "**Makale Başlığı:** DeepSight: Kapsamlı Bir LM Güvenliği Araç Seti\n\n**Özet:**\n\nBüyük Modellerin (LM'ler) geliştirilmesi hızla ilerlerken, güvenlikleri de öncelik haline gelmektedir. Mevcut Büyük Dil Modelleri (LLM'ler) ve Çok Modlu Büyük Dil Modelleri (MLLM'ler) güvenlik iş akışında değerlendirme, teşhis ve uyumlandırma genellikle ayrı araçlarla ele alınmaktadır. Özellikle, güvenlik değerlendirmesi yalnızca harici davranışsal riskleri belirleyebilmekte ancak içsel kök nedenleri ortaya çıkaramamaktadır. Bu esnada, güvenlik teşhisi genellikle somut risk senaryolarından uzaklaşmakta ve açıklanabilir düzeyde kalmaktadır. Bu şekilde, güvenlik uyumlandırması içsel mekanizmalardaki değişikliklere ilişkin özel açıklamalar eksikliği çekmekte ve potansiyel olarak genel yetenekleri bozabilmektedir. Bu sorunları sistematik olarak ele almak için, yeni bir güvenlik değerlendirme-teşhis entegre paradigmasını uygulamak üzere açık kaynaklı bir proje olan DeepSight'ı öneriyoruz. DeepSight, değerlendirme araç seti DeepSafe ve teşhis araç seti DeepScan'den oluşan, düşük maliyetli, tekrarlanabilir, verimli ve yüksek ölçeklenebilirliğe sahip büyük ölçekli model güvenlik değerlendirme projesidir. Görev ve veri protokollerini birleştirerek, iki aşama arasında bir bağlantı kuruyor ve güvenlik değerlendirmesini siyah kutudan beyaz kutu içgörüsüne dönüştürüyoruz. Bunun yanı sıra DeepSight, öncü yapay zeka riski değerlendirmesini ve ortak güvenlik değerlendirme ve teşhisini destekleyen ilk açık kaynaklı araç settidir."
    }
  },
  {
    "id": "2602.11897v1",
    "title": "Agentic AI for Cybersecurity: A Meta-Cognitive Architecture for Governable Autonomy",
    "authors": [
      "Andrei Kojukhov",
      "Arkady Bovshover"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11897v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11897v1",
    "content": {
      "en": "Contemporary AI-driven cybersecurity systems are predominantly architected as model-centric detection and automation pipelines optimized for task-level performance metrics such as accuracy and response latency. While effective for bounded classification tasks, these architectures struggle to support accountable decision-making under adversarial uncertainty, where actions must be justified, governed, and aligned with organizational and regulatory constraints. This paper argues that cybersecurity orchestration should be reconceptualized as an agentic, multi-agent cognitive system, rather than a linear sequence of detection and response components. We introduce a conceptual architectural framework in which heterogeneous AI agents responsible for detection, hypothesis formation, contextual interpretation, explanation, and governance are coordinated through an explicit meta-cognitive judgement function. This function governs decision readiness and dynamically calibrates system autonomy when evidence is incomplete, conflicting, or operationally risky. By synthesizing distributed cognition theory, multi-agent systems research, and responsible AI governance frameworks, we demonstrate that modern security operations already function as distributed cognitive systems, albeit without an explicit organizing principle. Our contribution is to make this cognitive structure architecturally explicit and governable by embedding meta-cognitive judgement as a first-class system function. We discuss implications for security operations centers, accountable autonomy, and the design of next-generation AI-enabled cyber defence architectures. The proposed framework shifts the focus of AI in cybersecurity from optimizing isolated predictions to governing autonomy under uncertainty.",
      "tr": "Makale Başlığı: Siber Güvenlik İçin Agentic AI: Yönetilebilir Özerklik İçin Meta-Bilişsel Bir Mimari\n\nÖzet:\nGünümüzdeki yapay zeka odaklı siber güvenlik sistemleri ağırlıklı olarak, doğruluk ve yanıt gecikmesi gibi görev seviyesi performans metrikleri için optimize edilmiş, model merkezli tespit ve otomasyon hatları olarak tasarlanmaktadır. Sınırlı sınıflandırma görevleri için etkili olsalar da, bu mimariler, eylemlerin gerekçelendirilmesi, yönetilmesi ve organizasyonel ve düzenleyici kısıtlamalarla uyumlu olması gereken düşmanca belirsizlik altında hesap verebilir karar verme süreçlerini desteklemekte zorlanmaktadır. Bu makale, siber güvenlik orkestrasyonunun, doğrusal bir tespit ve müdahale bileşeni dizisi yerine, agentic, multi-agent bilişsel bir sistem olarak yeniden kavramsallaştırılması gerektiğini savunmaktadır. Tespit, hipotez oluşturma, bağlamsal yorumlama, açıklama ve yönetişimden sorumlu heterojen AI ajanlarının açık bir meta-cognitive judgement function aracılığıyla koordine edildiği kavramsal bir mimari çerçeve sunuyoruz. Bu fonksiyon, karar hazırlığını yönetir ve kanıtların eksik, çelişkili veya operasyonel olarak riskli olduğu durumlarda sistem özerkliğini dinamik olarak kalibre eder. Dağıtılmış biliş teorisi, multi-agent sistemler araştırması ve sorumlu yapay zeka yönetişimi çerçevelerini sentezleyerek, modern güvenlik operasyonlarının, açık bir organize edici ilke olmasa da, dağıtılmış bilişsel sistemler olarak zaten işlev gördüğünü göstermekteyiz. Katkımız, meta-cognitive judgement'ı birinci sınıf bir sistem fonksiyonu olarak gömerek bu bilişsel yapıyı mimari olarak açık ve yönetilebilir hale getirmektir. Güvenlik operasyon merkezleri, hesap verebilir özerklik ve yeni nesil yapay zeka destekli siber savunma mimarilerinin tasarımı için çıkarımları tartışıyoruz. Önerilen çerçeve, siber güvenlikte yapay zekanın odağını izole edilmiş tahminleri optimize etmekten, belirsizlik altında özerkliği yönetmeye kaydırır."
    }
  },
  {
    "id": "2602.11851v1",
    "title": "Resource-Aware Deployment Optimization for Collaborative Intrusion Detection in Layered Networks",
    "authors": [
      "André García Gómez",
      "Ines Rieger",
      "Wolfgang Hotwagner",
      "Max Landauer",
      "Markus Wurzenberger"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11851v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11851v1",
    "content": {
      "en": "Collaborative Intrusion Detection Systems (CIDS) are increasingly adopted to counter cyberattacks, as their collaborative nature enables them to adapt to diverse scenarios across heterogeneous environments. As distributed critical infrastructure operates in rapidly evolving environments, such as drones in both civil and military domains, there is a growing need for CIDS architectures that can flexibly accommodate these dynamic changes. In this study, we propose a novel CIDS framework designed for easy deployment across diverse distributed environments. The framework dynamically optimizes detector allocation per node based on available resources and data types, enabling rapid adaptation to new operational scenarios with minimal computational overhead. We first conducted a comprehensive literature review to identify key characteristics of existing CIDS architectures. Based on these insights and real-world use cases, we developed our CIDS framework, which we evaluated using several distributed datasets that feature different attack chains and network topologies. Notably, we introduce a public dataset based on a realistic cyberattack targeting a ground drone aimed at sabotaging critical infrastructure. Experimental results demonstrate that the proposed CIDS framework can achieve adaptive, efficient intrusion detection in distributed settings, automatically reconfiguring detectors to maintain an optimal configuration, without requiring heavy computation, since all experiments were conducted on edge devices.",
      "tr": "Makale Başlığı: Katmanlı Ağlarda İşbirlikçi Saldırı Tespitine Yönelik Kaynak-Duyarlı Dağıtım Optimizasyonu\n\nÖzet:\nİşbirlikçi Saldırı Tespit Sistemleri (CIDS), heterojen ortamlarda çeşitli senaryolara uyum sağlama yetenekleri sayesinde siber saldırılara karşı koymak için giderek daha fazla benimsenmektedir. Dağıtılmış kritik altyapılar, sivil ve askeri alanlardaki dronlar gibi hızla gelişen ortamlarda faaliyet gösterdiğinden, bu dinamik değişiklikleri esnek bir şekilde karşılayabilen CIDS mimarilerine olan ihtiyaç artmaktadır. Bu çalışmada, çeşitli dağıtılmış ortamlarda kolay dağıtım için tasarlanmış yeni bir CIDS çerçevesi önermekteyiz. Çerçeve, mevcut kaynaklara ve veri türlerine göre düğüm başına dedektör tahsisini dinamik olarak optimize ederek, minimum hesaplama yükü ile yeni operasyonel senaryolara hızlı adaptasyon sağlar. İlk olarak, mevcut CIDS mimarilerinin temel özelliklerini belirlemek için kapsamlı bir literatür taraması gerçekleştirdik. Bu içgörüler ve gerçek dünya kullanım senaryolarına dayanarak, CIDS çerçevemizi geliştirdik ve farklı saldırı zincirleri ve ağ topolojileri içeren birkaç dağıtılmış veri kümesi kullanarak değerlendirdik. Özellikle, kritik altyapıyı sabote etmeyi amaçlayan ve bir kara dronuna yönelik gerçekçi bir siber saldırıya dayanan halka açık bir veri kümesi sunuyoruz. Deneysel sonuçlar, önerilen CIDS çerçevesinin dağıtılmış ortamlarda uyarlanabilir, verimli saldırı tespiti sağlayabildiğini, tüm deneyler edge device'larda gerçekleştirildiği için ağır hesaplama gerektirmeden dedektörleri otomatik olarak yeniden yapılandırarak optimum bir yapılandırmayı sürdürebildiğini göstermektedir."
    }
  },
  {
    "id": "2602.11655v1",
    "title": "LoRA-based Parameter-Efficient LLMs for Continuous Learning in Edge-based Malware Detection",
    "authors": [
      "Christian Rondanini",
      "Barbara Carminati",
      "Elena Ferrari",
      "Niccolò Lardo",
      "Ashish Kundu"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "link": "http://arxiv.org/abs/2602.11655v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11655v1",
    "content": {
      "en": "The proliferation of edge devices has created an urgent need for security solutions capable of detecting malware in real time while operating under strict computational and memory constraints. Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in recognizing complex patterns, yet their deployment on edge devices remains impractical due to their resource demands. However, in edge malware detection, static or centrally retrained models degrade under evolving threats and heterogeneous traffic; locally trained models become siloed and fail to transfer across domains. To overcome these limitations, in this paper, we present a continuous learning architecture for edge-based malware detection that combines local adaptation on each device with global knowledge sharing through parameter-efficient LoRA adapters. Lightweight transformer models (DistilBERT, DistilGPT-2, TinyT5) run on edge nodes and are incrementally fine-tuned on device-specific traffic; only the resulting LoRA modules are aggregated by a lightweight coordinator and redistributed, enabling cross-device generalization without exchanging raw data. We evaluate on two public IoT security datasets, Edge-IIoTset and TON-IoT, under multi-round learning to simulate evolving threats. Compared to isolated fine-tuning, the LoRA-based exchange yields up to 20-25% accuracy gains when models encounter previously unseen attacks from another domain, while maintaining stable loss and F1 across rounds. LoRA adds less than 1% to model size (~0.6-1.8 MB), making updates practical for constrained edge hardware.",
      "tr": "Makale Başlığı: Kenar Tabanlı Zararlı Yazılım Tespiti için Sürekli Öğrenmede LoRA Tabanlı Parametre Verimli LLM'ler\n\nÖzet:\nKenar cihazlarının yaygınlaşması, kısıtlı hesaplama ve bellek kaynakları altında gerçek zamanlı zararlı yazılım tespiti yapabilen güvenlik çözümlerine acil bir ihtiyaç doğurmuştur. Son zamanlarda, Large Language Models (LLM'ler) karmaşık örüntüleri tanıma konusunda dikkate değer yetenekler sergilemiştir, ancak kaynak talepleri nedeniyle kenar cihazlarında kullanımları pratik olmaktan uzaktır. Bununla birlikte, kenar zararlı yazılım tespitinde, statik veya merkezi olarak yeniden eğitilmiş modeller gelişen tehditler ve heterojen trafik altında bozulur; yerel olarak eğitilmiş modeller ise izole hale gelir ve alanlar arası transferde başarısız olur. Bu sınırlamaların üstesinden gelmek için bu makalede, her cihazda yerel adaptasyonu, parametre verimli LoRA adapter'ları aracılığıyla küresel bilgi paylaşımı ile birleştiren kenar tabanlı zararlı yazılım tespiti için bir sürekli öğrenme mimarisi sunuyoruz. Hafif transformer modelleri (DistilBERT, DistilGPT-2, TinyT5) kenar düğümlerinde çalışır ve cihaza özgü trafik üzerinde artımlı olarak ince ayarlanır; yalnızca sonuçta ortaya çıkan LoRA modülleri hafif bir koordinatör tarafından toplanır ve yeniden dağıtılır, bu da ham verileri değiştirmeden cihazlar arası genelleme yapılmasına olanak tanır. Gelişen tehditleri simüle etmek için çok turlu öğrenme altında iki kamuya açık IoT güvenlik veri seti olan Edge-IIoTset ve TON-IoT üzerinde değerlendirme yapıyoruz. İzole ince ayar ile karşılaştırıldığında, LoRA tabanlı değişim, modeller daha önce görülmemiş saldırılarla başka bir alandan karşılaştığında, turlar boyunca istikrarlı loss ve F1 değerlerini korurken, %20-25'e varan doğruluk artışı sağlamaktadır. LoRA, model boyutuna %1'den az ekleme yapar (~0.6-1.8 MB), bu da güncellemeleri kısıtlı kenar donanımları için pratik hale getirir."
    }
  },
  {
    "id": "2602.11651v1",
    "title": "DMind-3: A Sovereign Edge--Local--Cloud AI System with Controlled Deliberation and Correction-Based Tuning for Safe, Low-Latency Transaction Execution",
    "authors": [
      "Enhao Huang",
      "Frank Li",
      "Tony Lin",
      "Lowes Yang"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11651v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11651v1",
    "content": {
      "en": "This paper introduces DMind-3, a sovereign Edge-Local-Cloud intelligence stack designed to secure irreversible financial execution in Web3 environments against adversarial risks and strict latency constraints. While existing cloud-centric assistants compromise privacy and fail under network congestion, and purely local solutions lack global ecosystem context, DMind-3 resolves these tensions by decomposing capability into three cooperating layers: a deterministic signing-time intent firewall at the edge, a private high-fidelity reasoning engine on user hardware, and a policy-governed global context synthesizer in the cloud. We propose policy-driven selective offloading to route computation based on privacy sensitivity and uncertainty, supported by two novel training objectives: Hierarchical Predictive Synthesis (HPS) for fusing time-varying macro signals, and Contrastive Chain-of-Correction Supervised Fine-Tuning (C$^3$-SFT) to enhance local verification reliability. Extensive evaluations demonstrate that DMind-3 achieves a 93.7% multi-turn success rate in protocol-constrained tasks and superior domain reasoning compared to general-purpose baselines, providing a scalable framework where safety is bound to the edge execution primitive while maintaining sovereignty over sensitive user intent.",
      "tr": "Makale Başlığı: DMind-3: Güvenli, Düşük Gecikmeli İşlem Yürütme için Kontrollü Müzakere ve Düzeltme Tabanlı Ayarlamaya Sahip Egemen Bir Kenar-Yerel-Bulut Yapay Zeka Sistemi\n\nÖzet:\nBu makale, DMind-3'ü tanıtmaktadır. DMind-3, Web3 ortamlarında geri döndürülemez finansal işlemleri, saldırgan risklerine ve katı gecikme kısıtlamalarına karşı güvence altına almak üzere tasarlanmış egemen bir Edge-Local-Cloud zeka yığınıdır. Mevcut bulut odaklı asistanlar gizliliği tehlikeye atarken ve ağ tıkanıklığında başarısız olurken, tamamen yerel çözümler global ekosistem bağlamından yoksundur. DMind-3, bu gerilimleri, yeteneği üç işbirliği yapan katmana ayırarak çözer: kenarda deterministik bir signing-time intent firewall, kullanıcı donanımında özel, yüksek doğruluklu bir reasoning engine ve bulutta politikalarla yönetilen global bir bağlam sentezleyicisi. Gizlilik hassasiyetine ve belirsizliğe göre hesaplamayı yönlendirmek için politikalarla yönlendirilen seçici offloading'i öneriyoruz. Bu, iki yeni eğitim hedefi ile desteklenmektedir: zamanla değişen makro sinyalleri birleştirmek için Hierarchical Predictive Synthesis (HPS) ve yerel doğrulama güvenilirliğini artırmak için Contrastive Chain-of-Correction Supervised Fine-Tuning (C$^3$-SFT). Kapsamlı değerlendirmeler, DMind-3'ün protokol kısıtlamalı görevlerde %93,7'lik çoklu tur başarı oranına ulaştığını ve genel amaçlı temel sistemlere kıyasla üstün alan reasoning yeteneği sağladığını göstermektedir. Bu sistem, güvenliğin kenar yürütme ilkelere bağlı olduğu, hassas kullanıcı niyeti üzerindeki egemenliği korurken ölçeklenebilir bir çerçeve sunmaktadır."
    }
  },
  {
    "id": "2602.11528v1",
    "title": "Stop Tracking Me! Proactive Defense Against Attribute Inference Attack in LLMs",
    "authors": [
      "Dong Yan",
      "Jian Liang",
      "Ran He",
      "Tieniu Tan"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2602.11528v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11528v1",
    "content": {
      "en": "Recent studies have shown that large language models (LLMs) can infer private user attributes (e.g., age, location, gender) from user-generated text shared online, enabling rapid and large-scale privacy breaches. Existing anonymization-based defenses are coarse-grained, lacking word-level precision in anonymizing privacy-leaking elements. Moreover, they are inherently limited as altering user text to hide sensitive cues still allows attribute inference to occur through models' reasoning capabilities. To address these limitations, we propose a unified defense framework that combines fine-grained anonymization (TRACE) with inference-preventing optimization (RPS). TRACE leverages attention mechanisms and inference chain generation to identify and anonymize privacy-leaking textual elements, while RPS employs a lightweight two-stage optimization strategy to induce model rejection behaviors, thereby preventing attribute inference. Evaluations across diverse LLMs show that TRACE-RPS reduces attribute inference accuracy from around 50\\% to below 5\\% on open-source models. In addition, our approach offers strong cross-model generalization, prompt-variation robustness, and utility-privacy tradeoffs. Our code is available at https://github.com/Jasper-Yan/TRACE-RPS.",
      "tr": "Elbette, işte akademik makale başlığı ve özetinin istenen kriterlere göre çevirisi:\n\n**Makale Başlığı:** Beni İzlemeyi Bırakın! LLM'lerde Atıf Çıkarım Saldırılarına Karşı Proaktif Savunma\n\n**Özet:**\nSon çalışmalar, büyük dil modellerinin (LLM'ler) çevrimiçi paylaşılan kullanıcı tarafından üretilen metinlerden özel kullanıcı özniteliklerini (örneğin, yaş, konum, cinsiyet) çıkarabildiğini ve bu durumun hızlı ve büyük ölçekli gizlilik ihlallerine yol açabildiğini göstermiştir. Mevcut anonimleştirme tabanlı savunmalar kaba tanelidir ve gizlilik sızdıran öğeleri anonimleştirirken kelime düzeyinde hassasiyetten yoksundurlar. Dahası, hassas ipuçlarını gizlemek için kullanıcı metnini değiştirmek, modellerin **reasoning** kabiliyetleri aracılığıyla öznitelik çıkarımının gerçekleşmesine hala izin verdiğinden, doğası gereği sınırlıdırlar. Bu sınırlamaları gidermek için, ince taneli anonimleştirmeyi (TRACE) çıkarım önleyici optimizasyon (RPS) ile birleştiren birleşik bir savunma çerçevesi öneriyoruz. TRACE, gizlilik sızdıran metinsel öğeleri tanımlamak ve anonimleştirmek için dikkat mekanizmalarını ve çıkarım zinciri üretimini kullanırken, RPS öznitelik çıkarımını engelleyerek model reddetme davranışları oluşturmak için hafif bir iki aşamalı optimizasyon stratejisi kullanır. Çeşitli LLM'ler üzerinde yapılan değerlendirmeler, TRACE-RPS'nin açık kaynaklı modellerde öznitelik çıkarım doğruluğunu yaklaşık %50'den %5'in altına düşürdüğünü göstermektedir. Ek olarak, yaklaşımımız güçlü modelden-modele genelleme, prompt-varyasyon sağlamlığı ve kullanışlılık-gizlilik ödünleşmeleri sunmaktadır. Kodumuz https://github.com/Jasper-Yan/TRACE-RPS adresinde mevcuttur."
    }
  },
  {
    "id": "2602.11513v1",
    "title": "Differentially Private and Communication Efficient Large Language Model Split Inference via Stochastic Quantization and Soft Prompt",
    "authors": [
      "Yujie Gu",
      "Richeng Jin",
      "Xiaoyu Ji",
      "Yier Jin",
      "Wenyuan Xu"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11513v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11513v1",
    "content": {
      "en": "Large Language Models (LLMs) have achieved remarkable performance and received significant research interest. The enormous computational demands, however, hinder the local deployment on devices with limited resources. The current prevalent LLM inference paradigms require users to send queries to the service providers for processing, which raises critical privacy concerns. Existing approaches propose to allow the users to obfuscate the token embeddings before transmission and utilize local models for denoising. Nonetheless, transmitting the token embeddings and deploying local models may result in excessive communication and computation overhead, preventing practical implementation. In this work, we propose \\textbf{DEL}, a framework for \\textbf{D}ifferentially private and communication \\textbf{E}fficient \\textbf{L}LM split inference. More specifically, an embedding projection module and a differentially private stochastic quantization mechanism are proposed to reduce the communication overhead in a privacy-preserving manner. To eliminate the need for local models, we adapt soft prompt at the server side to compensate for the utility degradation caused by privacy. To the best of our knowledge, this is the first work that utilizes soft prompt to improve the trade-off between privacy and utility in LLM inference, and extensive experiments on text generation and natural language understanding benchmarks demonstrate the effectiveness of the proposed method.",
      "tr": "İşte istenen çeviri:\n\n**Makale Başlığı:** Differentially Private ve Communication Efficient Large Language Model Split Inference, Stochastic Quantization ve Soft Prompt Üzerinden\n\n**Özet:**\nLarge Language Models (LLMs), dikkate değer performanslar sergilemiş ve önemli araştırma ilgisi görmüştür. Bununla birlikte, devasa hesaplama gereksinimleri, sınırlı kaynaklara sahip cihazlarda yerel dağıtımı engellemektedir. Mevcut yaygın LLM inference paradigmaları, kullanıcıların işlem için servis sağlayıcılara sorgu göndermesini gerektirmekte, bu da kritik gizlilik endişelerine yol açmaktadır. Mevcut yaklaşımlar, kullanıcıların iletimden önce token embeddings'leri karartmasına ve gürültüyü gidermek için yerel modelleri kullanmasına izin vermeyi önermektedir. Yine de, token embeddings'lerinin iletilmesi ve yerel modellerin dağıtılması, aşırı iletişim ve hesaplama yüküne yol açarak pratik uygulamayı engelleyebilir. Bu çalışmada, **D**ifferentially private ve communication **E**fficient **L**LM split inference için bir çerçeve olan **DEL**'i öneriyoruz. Daha spesifik olarak, gizliliği koruyan bir şekilde iletişim yükünü azaltmak için bir embedding projection modülü ve differentially private stochastic quantization mekanizması önerilmektedir. Yerel modellere olan ihtiyacı ortadan kaldırmak için, gizlilik nedeniyle oluşan fayda (utility) kaybını telafi etmek üzere sunucu tarafında soft prompt'u adapte ediyoruz. Bildiğimiz kadarıyla bu çalışma, LLM inference'da gizlilik ve fayda (utility) arasındaki dengeyi (trade-off) iyileştirmek için soft prompt'u kullanan ilk çalışmadır ve metin üretimi ile doğal dil anlama benchmark'ları üzerindeki kapsamlı deneyler, önerilen yöntemin etkinliğini göstermektedir."
    }
  },
  {
    "id": "2602.11416v1",
    "title": "Optimizing Agent Planning for Security and Autonomy",
    "authors": [
      "Aashish Kolluri",
      "Rishi Sharma",
      "Manuel Costa",
      "Boris Köpf",
      "Tobias Nießen"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.11416v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11416v1",
    "content": {
      "en": "Indirect prompt injection attacks threaten AI agents that execute consequential actions, motivating deterministic system-level defenses. Such defenses can provably block unsafe actions by enforcing confidentiality and integrity policies, but currently appear costly: they reduce task completion rates and increase token usage compared to probabilistic defenses. We argue that existing evaluations miss a key benefit of system-level defenses: reduced reliance on human oversight. We introduce autonomy metrics to quantify this benefit: the fraction of consequential actions an agent can execute without human-in-the-loop (HITL) approval while preserving security. To increase autonomy, we design a security-aware agent that (i) introduces richer HITL interactions, and (ii) explicitly plans for both task progress and policy compliance. We implement this agent design atop an existing information-flow control defense against prompt injection and evaluate it on the AgentDojo and WASP benchmarks. Experiments show that this approach yields higher autonomy without sacrificing utility.",
      "tr": "Makale Başlığı: Güvenlik ve Otonomi için Ajan Planlamasının Optimize Edilmesi\n\nÖzet:\nDolaylı prompt injection saldırıları, önemli eylemleri yürüten yapay zeka ajanlarını tehdit ederek deterministik sistem düzeyinde savunmaları gerektirmektedir. Bu tür savunmalar, gizlilik ve bütünlük politikalarını uygulayarak güvensiz eylemleri kanıtlanabilir bir şekilde engelleyebilir, ancak mevcut durumda maliyetli görünmektedir: olasılıksal savunmalara kıyasla görev tamamlama oranlarını düşürmekte ve token kullanımını artırmaktadır. Sistem düzeyinde savunmaların önemli bir faydasını mevcut değerlendirmelerin gözden kaçırdığını savunmaktayız: insan denetimine olan bağımlılığın azalması. Bu faydayı ölçmek için otonomi metrikleri sunuyoruz: güvenliği korurken bir ajanın insan-döngüde (HITL) onay olmadan yürütebileceği önemli eylemlerin oranı. Otonomiyi artırmak için, (i) daha zengin HITL etkileşimleri sunan ve (ii) hem görev ilerlemesi hem de politika uyumu için açıkça planlama yapan güvenliğe duyarlı bir ajan tasarlıyoruz. Bu ajan tasarımını, prompt injection'a karşı mevcut bir bilgi akışı kontrol savunmasının üzerine uyguluyoruz ve AgentDojo ve WASP benchmark'larında değerlendiriyoruz. Deneyler, bu yaklaşımın faydadan ödün vermeden daha yüksek otonomi sağladığını göstermektedir."
    }
  },
  {
    "id": "2602.11327v1",
    "title": "Security Threat Modeling for Emerging AI-Agent Protocols: A Comparative Analysis of MCP, A2A, Agora, and ANP",
    "authors": [
      "Zeynab Anbiaee",
      "Mahdi Rabbani",
      "Mansur Mirani",
      "Gunjan Piya",
      "Igor Opushnyev"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11327v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11327v1",
    "content": {
      "en": "The rapid development of the AI agent communication protocols, including the Model Context Protocol (MCP), Agent2Agent (A2A), Agora, and Agent Network Protocol (ANP), is reshaping how AI agents communicate with tools, services, and each other. While these protocols support scalable multi-agent interaction and cross-organizational interoperability, their security principles remain understudied, and standardized threat modeling is limited; no protocol-centric risk assessment framework has been established yet. This paper presents a systematic security analysis of four emerging AI agent communication protocols. First, we develop a structured threat modeling analysis that examines protocol architectures, trust assumptions, interaction patterns, and lifecycle behaviors to identify protocol-specific and cross-protocol risk surfaces. Second, we introduce a qualitative risk assessment framework that identifies twelve protocol-level risks and evaluates security posture across the creation, operation, and update phases through systematic assessment of likelihood, impact, and overall protocol risk, with implications for secure deployment and future standardization. Third, we provide a measurement-driven case study on MCP that formalizes the risk of missing mandatory validation/attestation for executable components as a falsifiable security claim by quantifying wrong-provider tool execution under multi-server composition across representative resolver policies. Collectively, our results highlight key design-induced risk surfaces and provide actionable guidance for secure deployment and future standardization of agent communication ecosystems.",
      "tr": "The rapid development of the AI agent communication protocols, including the Model Context Protocol (MCP), Agent2Agent (A2A), Agora, and Agent Network Protocol (ANP), is reshaping how AI agents communicate with tools, services, and each other. While these protocols support scalable multi-agent interaction and cross-organizational interoperability, their security principles remain understudied, and standardized threat modeling is limited; no protocol-centric risk assessment framework has been established yet. This paper presents a systematic security analysis of four emerging AI agent communication protocols. First, we develop a structured threat modeling analysis that examines protocol architectures, trust assumptions, interaction patterns, and lifecycle behaviors to identify protocol-specific and cross-protocol risk surfaces. Second, we introduce a qualitative risk assessment framework that identifies twelve protocol-level risks and evaluates security posture across the creation, operation, and update phases through systematic assessment of likelihood, impact, and overall protocol risk, with implications for secure deployment and future standardization. Third, we provide a measurement-driven case study on MCP that formalizes the risk of missing mandatory validation/attestation for executable components as a falsifiable security claim by quantifying wrong-provider tool execution under multi-server composition across representative resolver policies. Collectively, our results highlight key design-induced risk surfaces and provide actionable guidance for secure deployment and future standardization of agent communication ecosystems."
    }
  }
]