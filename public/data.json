[
  {
    "id": "2601.04940v1",
    "title": "CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs",
    "authors": [
      "Arthur Nijdam",
      "Harri Kähkönen",
      "Valtteri Niemi",
      "Paul Stankovski Wagner",
      "Sara Ramezanian"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.04940v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04940v1",
    "content": {
      "en": "The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development.   CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies.   We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity.",
      "tr": "**Makale Başlığı:** CurricuLLM: Fine-Tuned LLM'ler Kullanılarak Kişiselleştirilmiş ve İşgücüyle Uyumlu Siber Güvenlik Müfredatlarının Tasarlanması\n\n**Özet:**\nDijitalleşmenin artması ve yeni siber güvenlik tehditlerinin ortaya çıkmasıyla birlikte siber güvenlik alanı sürekli gelişmektedir. Siber güvenlik programları, özellikle siber güvenlikteki son gelişmelerle ilgili olarak, mezunları işgücünün talep ettiği becerilerle donatmakta sıklıkla başarısız olmaktadır, zira müfredat tasarımı maliyetli ve yoğun emek gerektiren bir süreçtir. Bu uyumsuzluğu gidermek için, CurricuLLM olarak adlandırılan, siber güvenlik müfredatlarının otomatik tasarımı ve analizi için yeni bir Large Language Model (LLM)-tabanlı çerçeve sunuyoruz. Yaklaşımımız üç temel katkı sağlamaktadır: (1) kişiselleştirilmiş müfredat tasarımının otomasyonu, (2) endüstri talepleriyle uyumlu veri odaklı bir pipeline ve (3) müfredat geliştirmede fine-tuned LLM'lerden yararlanmak için kapsamlı bir metodoloji. CurricuLLM, girdi verilerini standartlaştıran PreprocessLM ve kurs içeriğini siber güvenliğin dokuz Knowledge Area'sına atayan ClassifyLM'den oluşan iki katmanlı bir yaklaşım kullanır. Bidirectional Encoder Representations from Transformers (BERT) modelini, temel siber güvenlik kavramları ve işgücü yetkinlikleri üzerinde fine-tuned edilmiş ClassifyLM olarak seçmeden önce çeşitli Natural Language Processing (NLP) mimarilerini ve fine-tuning stratejilerini sistematik olarak değerlendirdik. Yöntemimizi, gerçek dünya siber güvenlik müfredatlarını ve çerçevelerini analiz eden insan uzmanlarla ilk kez doğruladık; bu da CurricuLLM'nin yoğun emek gerektiren müfredat analizini değiştirmek için verimli bir çözüm olduğunu göstermektedir. Dahası, kurs içeriği sınıflandırıldıktan sonra, bunu belirli iş rolleri, işgücü kategorileri veya genel pazar ihtiyaçlarıyla eğitim programının uyumunu sağlayarak, yerleşik siber güvenlik rol tabanlı ağırlıklarla entegre edilebilir. Bu, öğrencileri siber güvenlikteki gelişen taleplere hazırlayan, kişiselleştirilmiş, işgücüyle uyumlu siber güvenlik müfredatları için temel oluşturmaktadır."
    }
  },
  {
    "id": "2601.04795v1",
    "title": "Defense Against Indirect Prompt Injection via Tool Result Parsing",
    "authors": [
      "Qiang Yu",
      "Xinran Cheng",
      "Chuanyi Liu"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2601.04795v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04795v1",
    "content": {
      "en": "As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.",
      "tr": "**Makale Başlığı:** Araç Sonuç Ayrıştırması Yoluyla Dolaylı Komut Enjeksiyonuna Karşı Savunma\n\n**Özet:**\n\nLLM ajanları, dijital asistanlardan otonom sistemler ve robotikte fiziksel kontrolcülere doğru geçiş yaparken, dolaylı komut enjeksiyonundan kaynaklanan artan bir tehditle karşı karşıya kalmaktadır. Saldırganlar, araç çağrılarının sonuçlarına kötü niyetli talimatlar gömerek, ajanın karar verme sürecini ele geçirip yetkisiz eylemleri gerçekleştirebilir. Bu güvenlik açığı, ajanlar fiziksel ortamlar üzerinde daha doğrudan kontrol kazandıkça önemli bir risk oluşturmaktadır. Dolaylı Komut Enjeksiyonu (IPI) için mevcut savunma mekanizmaları genellikle iki kategoriye ayrılır. Birincisi, özel tespit modelleri eğitmeyi içerir; ancak bu yaklaşım, hem eğitim hem de çıkarım için yüksek hesaplama yükü gerektirir ve gelişen saldırı vektörlerine ayak uydurmak için sık sık güncellemeler yapılmasını zorunlu kılar. Alternatif olarak, komut tabanlı yöntemler, komut mühendisliği yoluyla kötü niyetli talimatları tespit etmek veya göz ardı etmek için LLM'lerin doğasında var olan yeteneklerden yararlanır. Esnekliklerine rağmen, mevcut komut tabanlı savunmaların çoğu, yüksek Saldırı Başarı Oranlarına (ASR) sahiptir ve gelişmiş enjeksiyon saldırılarına karşı sınırlı sağlamlık göstermektedir. Bu makalede, araç sonuç ayrıştırması yoluyla LLM'lere kesin veri sağlayan ve enjekte edilen kötü niyetli kodu etkili bir şekilde filtreleyen yeni bir yöntem sunuyoruz. Yaklaşımımız, günümüze kadarki en düşük Saldırı Başarı Oranını (ASR) korurken rekabetçi Saldırı Altında Fayda (UA) elde etmekte ve mevcut yöntemlerden önemli ölçüde daha iyi performans göstermektedir. Kod GitHub'da mevcuttur."
    }
  },
  {
    "id": "2601.04666v1",
    "title": "Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning",
    "authors": [
      "Zhiyuan Chang",
      "Mingyang Li",
      "Yuekai Huang",
      "Ziyou Jiang",
      "Xiaojun Jia"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.04666v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04666v1",
    "content": {
      "en": "Large language model (LLM)-integrated applications have become increasingly prevalent, yet face critical security vulnerabilities from prompt injection (PI) attacks. Defending against PI attacks faces two major issues: malicious instructions can be injected through diverse vectors, and injected instructions often lack clear semantic boundaries from the surrounding context, making them difficult to identify. To address these issues, we propose InstruCoT, a model enhancement method for PI defense that synthesizes diverse training data and employs instruction-level chain-of-thought fine-tuning, enabling LLMs to effectively identify and reject malicious instructions regardless of their source or position in the context. We evaluate InstruCoT across three critical dimensions: Behavior Deviation, Privacy Leakage, and Harmful Output. Experimental results across four LLMs demonstrate that InstruCoT significantly outperforms baselines in all dimensions while maintaining utility performance without degradation",
      "tr": "Makale Başlığı: Bil Düşmanını: Çeşitli Veri Sentezi ve Instruction-Level Chain-of-Thought Öğrenimi ile LLM'leri Prompt Injection'a Karşı Güvenli Hale Getirmek\n\nÖzet:\nBüyük dil modeli (LLM) entegre edilmiş uygulamalar giderek daha yaygın hale gelmekle birlikte, prompt injection (PI) saldırılarından kaynaklanan kritik güvenlik açıklarıyla karşı karşıyadır. PI saldırılarına karşı savunma yapmak iki temel sorunla karşı karşıyadır: kötü niyetli komutlar çeşitli vektörler aracılığıyla enjekte edilebilir ve enjekte edilen komutlar genellikle çevreleyen bağlamdan net semantik sınırlara sahip olmadığından, tanımlanmaları zordur. Bu sorunları ele almak için, PI savunması için bir model geliştirme yöntemi olan InstruCoT'u öneriyoruz. Bu yöntem, çeşitli eğitim verilerini sentezler ve instruction-level chain-of-thought fine-tuning kullanarak LLM'lerin, kaynakları veya bağlamdaki konumları ne olursa olsun kötü niyetli komutları etkili bir şekilde tanımlamasını ve reddetmesini sağlar. InstruCoT'u üç kritik boyutta değerlendiriyoruz: Behavior Deviation, Privacy Leakage ve Harmful Output. Dört LLM üzerindeki deneysel sonuçlar, InstruCoT'un, fayda performansını düşürmeden tüm boyutlarda temel yöntemlerden önemli ölçüde daha iyi performans gösterdiğini ortaya koymaktadır."
    }
  },
  {
    "id": "2601.04641v1",
    "title": "DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization",
    "authors": [
      "Lionel Z. Wang",
      "Yusheng Zhao",
      "Jiabin Luo",
      "Xinfeng Li",
      "Lixu Wang"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.04641v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04641v1",
    "content": {
      "en": "The deployment of Machine-Generated Text (MGT) detection systems necessitates processing sensitive user data, creating a fundamental conflict between authorship verification and privacy preservation. Standard anonymization techniques often disrupt linguistic fluency, while rigorous Differential Privacy (DP) mechanisms typically degrade the statistical signals required for accurate detection. To resolve this dilemma, we propose \\textbf{DP-MGTD}, a framework incorporating an Adaptive Differentially Private Entity Sanitization algorithm. Our approach utilizes a two-stage mechanism that performs noisy frequency estimation and dynamically calibrates privacy budgets, applying Laplace and Exponential mechanisms to numerical and textual entities respectively. Crucially, we identify a counter-intuitive phenomenon where the application of DP noise amplifies the distinguishability between human and machine text by exposing distinct sensitivity patterns to perturbation. Extensive experiments on the MGTBench-2.0 dataset show that our method achieves near-perfect detection accuracy, significantly outperforming non-private baselines while satisfying strict privacy guarantees.",
      "tr": "**Makale Başlığı:** DP-MGTD: Adaptif Diferansiyel Olarak Özel Varlık Sanitizasyonu Yoluyla Gizlilik Korumalı Makine Tarafından Üretilen Metin Tespiti\n\n**Özet:**\n\nMakine Tarafından Üretilen Metin (MGT) tespit sistemlerinin devreye alınması, hassas kullanıcı verilerinin işlenmesini gerektirir, bu da yazarlık doğrulama ile gizliliğin korunması arasında temel bir çatışma yaratır. Standart anonimleştirme teknikleri genellikle dilsel akıcılığı bozar; buna karşılık, titiz Differential Privacy (DP) mekanizmaları genellikle doğru tespit için gerekli olan istatistiksel sinyalleri düşürür. Bu ikilemi çözmek için, Adaptif Differentially Private Entity Sanitization algoritmasını içeren bir çerçeve olan **DP-MGTD**'yi öneriyoruz. Yaklaşımımız, gürültülü frekans tahmini yapan ve gizlilik bütçelerini dinamik olarak kalibre eden iki aşamalı bir mekanizma kullanır; bu mekanizma sayısal ve metinsel varlıklara sırasıyla Laplace ve Exponential mekanizmalarını uygular. Kritik olarak, DP gürültüsünün uygulanmasının, pertürbasyona karşı farklı hassasiyet modelleri açığa çıkararak insan ve makine metni arasındaki ayırt edilebilirliği artırdığı sezgilere aykırı bir olgu tespit ettik. MGTBench-2.0 veri kümesi üzerinde yapılan kapsamlı deneyler, yöntemimizin katı gizlilik garantilerini karşılarken, özel olmayan temel yöntemlere göre önemli ölçüde daha iyi performans göstererek neredeyse mükemmel bir tespit doğruluğu elde ettiğini göstermektedir."
    }
  },
  {
    "id": "2601.04603v1",
    "title": "Constitutional Classifiers++: Efficient Production-Grade Defenses against Universal Jailbreaks",
    "authors": [
      "Hoagy Cunningham",
      "Jerry Wei",
      "Zihan Wang",
      "Andrew Persic",
      "Alwin Peng"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.04603v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04603v1",
    "content": {
      "en": "We introduce enhanced Constitutional Classifiers that deliver production-grade jailbreak robustness with dramatically reduced computational costs and refusal rates compared to previous-generation defenses. Our system combines several key insights. First, we develop exchange classifiers that evaluate model responses in their full conversational context, which addresses vulnerabilities in last-generation systems that examine outputs in isolation. Second, we implement a two-stage classifier cascade where lightweight classifiers screen all traffic and escalate only suspicious exchanges to more expensive classifiers. Third, we train efficient linear probe classifiers and ensemble them with external classifiers to simultaneously improve robustness and reduce computational costs. Together, these techniques yield a production-grade system achieving a 40x computational cost reduction compared to our baseline exchange classifier, while maintaining a 0.05% refusal rate on production traffic. Through extensive red-teaming comprising over 1,700 hours, we demonstrate strong protection against universal jailbreaks -- no attack on this system successfully elicited responses to all eight target queries comparable in detail to an undefended model. Our work establishes Constitutional Classifiers as practical and efficient safeguards for large language models.",
      "tr": "Makale Başlığı: Constitutional Classifiers++: Evrensel Jailbreak'lere Karşı Verimli Üretim Sınıfı Savunmalar\n\nÖzet:\nÖnceki nesil savunmalarla karşılaştırıldığında dramatik şekilde azaltılmış hesaplama maliyetleri ve reddetme oranları ile üretim sınıfı jailbreak dayanıklılığı sunan geliştirilmiş Constitutional Classifiers'ı tanıtıyoruz. Sistemimiz birkaç önemli içgörüyü birleştirmektedir. İlk olarak, model yanıtlarını tam konuşma bağlamlarında değerlendiren exchange classifiers geliştiriyoruz, bu da çıktıları izole olarak inceleyen son nesil sistemlerdeki güvenlik açıklarını ele almaktadır. İkinci olarak, hafif sınıflandırıcıların tüm trafiği taradığı ve yalnızca şüpheli alışverişleri daha pahalı sınıflandırıcılara yükselttiği iki aşamalı bir sınıflandırıcı kaskadı uyguluyoruz. Üçüncü olarak, verimli linear probe classifiers eğitiyor ve bunları harici sınıflandırıcılarla birlikte kullanarak dayanıklılığı eş zamanlı olarak artırıyor ve hesaplama maliyetlerini azaltıyoruz. Birlikte ele alındığında, bu teknikler, reddetme oranını üretim trafiğinde %0,05 düzeyinde tutarken, temel exchange classifier'ımıza kıyasla 40 kat hesaplama maliyeti azaltımı sağlayan üretim sınıfı bir sistem sunmaktadır. 1.700 saati aşan kapsamlı red-teaming çalışmaları aracılığıyla, evrensel jailbreak'lere karşı güçlü bir koruma gösteriyoruz; bu sisteme yönelik hiçbir saldırı, sekiz hedef sorgunun tamamına, savunmasız bir modelle karşılaştırılabilir ayrıntıda yanıtlar üretmeyi başaramamıştır. Çalışmamız, Constitutional Classifiers'ı büyük dil modelleri için pratik ve verimli korumalar olarak ortaya koymaktadır."
    }
  },
  {
    "id": "2601.04486v1",
    "title": "Decision-Aware Trust Signal Alignment for SOC Alert Triage",
    "authors": [
      "Israt Jahan Chowdhury",
      "Md Abu Yousuf Tanvir"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "link": "http://arxiv.org/abs/2601.04486v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04486v1",
    "content": {
      "en": "Detection systems that utilize machine learning are progressively implemented at Security Operations Centers (SOCs) to help an analyst to filter through high volumes of security alerts. Practically, such systems tend to reveal probabilistic results or confidence scores which are ill-calibrated and hard to read when under pressure. Qualitative and survey based studies of SOC practice done before reveal that poor alert quality and alert overload greatly augment the burden on the analyst, especially when tool outputs are not coherent with decision requirements, or signal noise. One of the most significant limitations is that model confidence is usually shown without expressing that there are asymmetric costs in decision making where false alarms are much less harmful than missed attacks. The present paper presents a decision-sensitive trust signal correspondence scheme of SOC alert triage. The framework combines confidence that has been calibrated, lightweight uncertainty cues, and cost-sensitive decision thresholds into coherent decision-support layer, instead of making changes to detection models. To enhance probabilistic consistency, the calibration is done using the known post-hoc methods and the uncertainty cues give conservative protection in situations where model certainty is low. To measure the model-independent performance of the suggested model, we apply the Logistic Regression and the Random Forest classifiers to the UNSW-NB15 intrusion detection benchmark. According to simulation findings, false negatives are greatly amplified by the presence of misaligned displays of confidence, whereas cost weighted loss decreases by orders of magnitude between models with decision aligned trust signals. Lastly, we describe a human-in-the-loop study plan that would allow empirically assessing the decision-making of the analysts with aligned and misaligned trust interfaces.",
      "tr": "**Makale Başlığı:** SOC Alert Triage İçin Karar-Farkındalığı Olan Güven Sinyali Hizalaması\n\n**Özet:**\n\nGüvenlik Operasyon Merkezleri'nde (SOC) makine öğrenimi kullanan algılama sistemleri, analistlerin yüksek hacimli güvenlik uyarılarını filtrelemesine yardımcı olmak amacıyla giderek daha fazla uygulanmaktadır. Uygulamada, bu tür sistemler, baskı altındayken ayarlanamamış ve okunması zor olasılıksal sonuçlar veya güven skorları sunma eğilimindedir. SOC uygulamalarına yönelik daha önce yapılmış niteliksel ve anket temelli çalışmalar, düşük uyarı kalitesinin ve uyarı yükünün analist üzerindeki yükünü büyük ölçüde artırdığını ortaya koymuştur; özellikle araç çıktıları karar gereksinimleriyle veya sinyal gürültüsüyle tutarlı olmadığında bu durum daha da kötüleşmektedir. En önemli sınırlamalardan biri, model güveninin genellikle karar vermedeki asimetrik maliyetleri ifade etmeden gösterilmesidir; bu durumda yanlış alarmlar, kaçırılan saldırılardan çok daha az zararlıdır. Bu çalışma, SOC uyarı sınıflandırması için karar-hassasiyetli bir güven sinyali karşılık şeması sunmaktadır. Çerçeve, tespit modellerinde değişiklik yapmak yerine, kalibre edilmiş güveni, hafif belirsizlik ipuçlarını ve maliyet-hassas karar eşiklerini tutarlı bir karar destek katmanına entegre eder. Olasılıksal tutarlılığı artırmak için kalibrasyon, bilinen post-hoc yöntemler kullanılarak yapılır ve belirsizlik ipuçları, model kesinliğinin düşük olduğu durumlarda muhafazakâr bir koruma sağlar. Önerilen modelin modele bağımsız performansını ölçmek için, UNSW-NB15 izinsiz giriş tespit referans veri kümesine Logistic Regression ve Random Forest sınıflandırıcılarını uyguluyoruz. Simülasyon bulgularına göre, yanlış negatifler, tutarsız güven gösterimlerinin varlığıyla büyük ölçüde artarken, maliyet ağırlıklı kayıp, karar hizalı güven sinyallerine sahip modeller arasında büyüklük mertebelerinde azalmaktadır. Son olarak, analistlerin hizalanmış ve hizalanmamış güven arayüzleriyle karar verme süreçlerini ampirik olarak değerlendirmeyi sağlayacak bir insan-merkezli çalışma planını detaylandırıyoruz."
    }
  },
  {
    "id": "2601.04443v1",
    "title": "Large Language Models for Detecting Cyberattacks on Smart Grid Protective Relays",
    "authors": [
      "Ahmad Mohammad Saber",
      "Saeed Jafari",
      "Zhengmao Ouyang",
      "Paul Budnarain",
      "Amr Youssef"
    ],
    "published_date": "2026-01-07",
    "tags": [
      "cs.CR",
      "cs.LG",
      "eess.SP"
    ],
    "link": "http://arxiv.org/abs/2601.04443v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04443v1",
    "content": {
      "en": "This paper presents a large language model (LLM)-based framework for detecting cyberattacks on transformer current differential relays (TCDRs), which, if undetected, may trigger false tripping of critical transformers. The proposed approach adapts and fine-tunes compact LLMs such as DistilBERT to distinguish cyberattacks from actual faults using textualized multidimensional TCDR current measurements recorded before and after tripping. Our results demonstrate that DistilBERT detects 97.6% of cyberattacks without compromising TCDR dependability and achieves inference latency below 6 ms on a commercial workstation. Additional evaluations confirm the framework's robustness under combined time-synchronization and false-data-injection attacks, resilience to measurement noise, and stability across prompt formulation variants. Furthermore, GPT-2 and DistilBERT+LoRA achieve comparable performance, highlighting the potential of LLMs for enhancing smart grid cybersecurity. We provide the full dataset used in this study for reproducibility.",
      "tr": "**Makale Başlığı:** Akıllı Şebeke Koruyucu Rölelerindeki Siber Saldırıları Tespit Etmek İçin Büyük Dil Modelleri\n\n**Özet:**\n\nBu çalışma, trafo akım diferansiyel röleleri (TCDR) üzerindeki siber saldırıları tespit etmek için büyük dil modeli (LLM) tabanlı bir çerçeve sunmaktadır. Tespit edilmeyen bu saldırılar, kritik trafoların yanlış açılmasına (false tripping) neden olabilir. Önerilen yaklaşım, DistilBERT gibi kompakt LLM'leri adapte ederek ve fine-tune ederek, rölenin açılmasından önce ve sonra kaydedilen metinleştirilmiş çok boyutlu TCDR akım ölçümlerini kullanarak siber saldırıları gerçek arızalardan ayırt etmektedir. Sonuçlarımız, DistilBERT'in TCDR'nin güvenilirliğini (dependability) tehlikeye atmadan siber saldırıların %97,6'sını tespit ettiğini ve ticari bir iş istasyonunda 6 ms'nin altında çıkarım gecikmesi (inference latency) elde ettiğini göstermektedir. Ek değerlendirmeler, çerçevenin birleşik zaman senkronizasyonu (time-synchronization) ve yanlış veri enjeksiyonu (false-data-injection) saldırıları altındaki sağlamlığını (robustness), ölçüm gürültüsüne karşı dayanıklılığını (resilience) ve prompt formülasyonu varyantları genelindeki istikrarını (stability) doğrulamaktadır. Ayrıca, GPT-2 ve DistilBERT+LoRA'nın karşılaştırılabilir performans elde etmesi, LLM'lerin akıllı şebeke siber güvenliğini artırma potansiyelini vurgulamaktadır. Tekrarlanabilirlik (reproducibility) için bu çalışmada kullanılan tam veri setini sunmaktayız."
    }
  },
  {
    "id": "2601.04034v1",
    "title": "HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense",
    "authors": [
      "Siyuan Li",
      "Xi Lin",
      "Jun Wu",
      "Zehao Liu",
      "Haoyu Li"
    ],
    "published_date": "2026-01-07",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.04034v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04034v1",
    "content": {
      "en": "Jailbreak attacks pose significant threats to large language models (LLMs), enabling attackers to bypass safeguards. However, existing reactive defense approaches struggle to keep up with the rapidly evolving multi-turn jailbreaks, where attackers continuously deepen their attacks to exploit vulnerabilities. To address this critical challenge, we propose HoneyTrap, a novel deceptive LLM defense framework leveraging collaborative defenders to counter jailbreak attacks. It integrates four defensive agents, Threat Interceptor, Misdirection Controller, Forensic Tracker, and System Harmonizer, each performing a specialized security role and collaborating to complete a deceptive defense. To ensure a comprehensive evaluation, we introduce MTJ-Pro, a challenging multi-turn progressive jailbreak dataset that combines seven advanced jailbreak strategies designed to gradually deepen attack strategies across multi-turn attacks. Besides, we present two novel metrics: Mislead Success Rate (MSR) and Attack Resource Consumption (ARC), which provide more nuanced assessments of deceptive defense beyond conventional measures. Experimental results on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1 demonstrate that HoneyTrap achieves an average reduction of 68.77% in attack success rates compared to state-of-the-art baselines. Notably, even in a dedicated adaptive attacker setting with intensified conditions, HoneyTrap remains resilient, leveraging deceptive engagement to prolong interactions, significantly increasing the time and computational costs required for successful exploitation. Unlike simple rejection, HoneyTrap strategically wastes attacker resources without impacting benign queries, improving MSR and ARC by 118.11% and 149.16%, respectively.",
      "tr": "Makale Başlığı: HoneyTrap: Large Language Model Saldırganlarını Honeypot Tuzaklarına Çekerek Aldatmak ve Dayanıklı Çoklu Ajan Savunması\n\nÖzet:\nJailbreak saldırıları, büyük dil modellerinin (LLM) güvenlik önlemlerini aşmalarına olanak tanıyarak önemli tehditler oluşturmaktadır. Ancak, mevcut reaktif savunma yaklaşımları, saldırganların güvenlik açıklarından yararlanmak için saldırılarını sürekli derinleştirdiği hızla gelişen çok turlu jailbreak'lerle başa çıkmakta zorlanmaktadır. Bu kritik zorluğun üstesinden gelmek için, jailbreak saldırılarına karşı işbirlikçi savunuculardan yararlanan yenilikçi bir aldatıcı LLM savunma çerçevesi olan HoneyTrap'ı öneriyoruz. Her biri özel bir güvenlik rolü üstlenen ve aldatıcı bir savunmayı tamamlamak üzere işbirliği yapan dört savunma ajanını entegre eder: Threat Interceptor, Misdirection Controller, Forensic Tracker ve System Harmonizer. Kapsamlı bir değerlendirme sağlamak için, çok turlu saldırılar boyunca saldırı stratejilerini aşamalı olarak derinleştirmek üzere tasarlanmış yedi gelişmiş jailbreak stratejisini birleştiren zorlu bir çok turlu aşamalı jailbreak veri kümesi olan MTJ-Pro'yu sunuyoruz. Ayrıca, yanıltıcı savunmanın geleneksel ölçütlerin ötesinde daha incelikli değerlendirmeler sağlayan iki yeni metrik sunuyoruz: Mislead Success Rate (MSR) ve Attack Resource Consumption (ARC). GPT-4, GPT-3.5-turbo, Gemini-1.5-pro ve LLaMa-3.1 üzerindeki deneysel sonuçlar, HoneyTrap'in mevcut en gelişmiş temel sistemlere kıyasla saldırı başarı oranlarında ortalama %68,77'lik bir azalma sağladığını göstermektedir. Özellikle, yoğunlaştırılmış koşullara sahip özel bir uyarlanabilir saldırgan ortamında bile HoneyTrap, etkileşimleri uzatmak için aldatıcı etkileşimden yararlanarak dayanıklı kalır ve başarılı istismar için gereken süre ve hesaplama maliyetlerini önemli ölçüde artırır. Basit reddetmenin aksine, HoneyTrap zararsız sorguları etkilemeden saldırgan kaynaklarını stratejik olarak boşa harcar, MSR ve ARC'yi sırasıyla %118,11 ve %149,16 oranında iyileştirir."
    }
  },
  {
    "id": "2601.04278v1",
    "title": "From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning",
    "authors": [
      "Xiaoyu Xu",
      "Minxin Du",
      "Zitong Li",
      "Zi Liang",
      "Zhibiao Guo"
    ],
    "published_date": "2026-01-07",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.04278v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04278v1",
    "content": {
      "en": "Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true \"forgetting scope\" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\\sim}20$ and diversity by ${\\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.",
      "tr": "**Makale Başlığı:** Alanlardan Örneklemelere: LLM Unlearning için Çift Kademeli Veri Sentezi\n\n**Özet:**\n\nMakine unlearning, LLM'lerden özel, zararlı veya telif hakkıyla korunan içeriği kaldırmak için esastır, ancak mevcut benchmarklar, modelin öğrendiği gerçek \"unutma kapsamını\" doğru bir şekilde temsil etmekte sıklıkla başarısız olmaktadır. İki farklı unlearning granülaritesi olan alan-seviyesi (domain-level) ve örneklem-seviyesi (instance-level) olgularını formüle ediyor ve yüksek kaliteli unutma kümeleri (forget sets) sentezlemek için otomatik bir çerçeve olan BiForget'i öneriyoruz. Dış üreticilere (external generators) dayanan önceki çalışmalardan farklı olarak BiForget, hedef modeli (target model) kendisi kullanarak, seed-guided ve adversarial prompting yoluyla içsel bilgi dağılımıyla (internal knowledge distribution) eşleşen verileri ortaya çıkarmaktadır. Çeşitli benchmarklar üzerinde yapılan deneylerimiz, BiForget'in alaka düzeyi (relevance), çeşitlilik (diversity) ve verimlilik (efficiency) açısından üstün bir denge sağladığını göstermektedir. Nicel olarak, Harry Potter alanında, SOTA'lara kıyasla toplam veri boyutunu yarıya indirirken alaka düzeyini yaklaşık %20 ve çeşitliliği yaklaşık 0.05 oranında artırmaktadır. Nihayetinde, daha sağlam bir unutma ve daha iyi bir fayda korunumu (utility preservation) sağlayarak LLM unlearning'in değerlendirilmesi için daha titiz bir temel sunmaktadır."
    }
  },
  {
    "id": "2601.03868v1",
    "title": "What Matters For Safety Alignment?",
    "authors": [
      "Xing Li",
      "Hui-Ling Zhen",
      "Lihao Yin",
      "Xianzhi Yu",
      "Zhenhua Dong"
    ],
    "published_date": "2026-01-07",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.03868v1",
    "pdf_link": "https://arxiv.org/pdf/2601.03868v1",
    "content": {
      "en": "This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spanning a parameter scale from 3B to 235B. The assessment leverages five established safety datasets and probes model vulnerabilities with 56 jailbreak techniques and four CoT attack strategies, resulting in 4.6M API calls. Our key empirical findings are fourfold. First, we identify the LRMs GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, and GPT-OSS-120B as the top-three safest models, which substantiates the significant advantage of integrated reasoning and self-reflection mechanisms for robust safety alignment. Second, post-training and knowledge distillation may lead to a systematic degradation of safety alignment. We thus argue that safety must be treated as an explicit constraint or a core optimization objective during these stages, not merely subordinated to the pursuit of general capability. Third, we reveal a pronounced vulnerability: employing a CoT attack via a response prefix can elevate the attack success rate by 3.34x on average and from 0.6% to 96.3% for Seed-OSS-36B-Instruct. This critical finding underscores the safety risks inherent in text-completion interfaces and features that allow user-defined response prefixes in LLM services, highlighting an urgent need for architectural and deployment safeguards. Fourth, roleplay, prompt injection, and gradient-based search for adversarial prompts are the predominant methodologies for eliciting unaligned behaviors in modern models.",
      "tr": "Makale Başlığı: Güvenlik Uyumunda Neler Önemlidir?\n\nÖzet:\nBu makale, safety alignment yetenekleri üzerine kapsamlı bir ampirik çalışma sunmaktadır. Daha güvenli ve güvenilir yapay zeka sistemleri geliştirmek için temel içgörüler sağlamak amacıyla LLM'ler ve LRM'lerde safety alignment için nelerin önemli olduğunu değerlendiriyoruz. Altı kritik intrinsic model özelliği ve üç harici saldırı tekniğinin etkisini sistematik olarak araştırıyor ve karşılaştırıyoruz. Büyük ölçekli değerlendirmemiz, 3B ila 235B arası parametre ölçeğini kapsayan on üç farklı model ailesindeki 32 güncel, popüler LLM ve LRM kullanılarak gerçekleştirilmiştir. Değerlendirme, beş yerleşik güvenlik veri kümesinden yararlanmakta ve 56 jailbreak tekniği ile dört CoT attack strategy kullanarak model zayıflıklarını incelemekte, bu da 4.6M API call'a yol açmaktadır. Ana ampirik bulgularımız dört katlıdır. İlk olarak, LRMs GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking ve GPT-OSS-120B'yi en güvenli üç model olarak belirledik; bu, sağlam safety alignment için entegre reasoning ve self-reflection mekanizmalarının önemli avantajını desteklemektedir. İkinci olarak, post-training ve knowledge distillation, safety alignment'ın sistematik olarak bozulmasına yol açabilir. Bu nedenle, bu aşamalarda safety'nin, sadece genel yetenek arayışına tabi kılınmak yerine, açık bir kısıtlama veya temel bir optimization objective olarak ele alınması gerektiğini savunuyoruz. Üçüncü olarak, belirgin bir zayıflık ortaya koyuyoruz: bir response prefix aracılığıyla CoT attack kullanmak, Seed-OSS-36B-Instruct için ortalama 3.34 kat saldırı başarı oranını ve %0.6'dan %96.3'e kadar bir artış sağlayabilir. Bu kritik bulgu, text-completion arayüzlerinde ve LLM hizmetlerinde kullanıcı tanımlı response prefix'lere izin veren özelliklerde yerleşik güvenlik risklerini vurgulamakta ve mimari ve deployment safeguards'e acil bir ihtiyaç olduğunu göstermektedir. Dördüncü olarak, roleplay, prompt injection ve adversarial prompts için gradient-based search, modern modellerde uyumsuz davranışları ortaya çıkarmak için baskın metodolojilerdir."
    }
  }
]