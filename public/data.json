[
  {
    "id": "2601.03005v1",
    "title": "JPU: Bridging Jailbreak Defense and Unlearning via On-Policy Path Rectification",
    "authors": [
      "Xi Wang",
      "Songlei Jian",
      "Shasha Li",
      "Xiaopeng Li",
      "Zhaoye Li"
    ],
    "published_date": "2026-01-06",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.03005v1",
    "pdf_link": "https://arxiv.org/pdf/2601.03005v1",
    "content": {
      "en": "Despite extensive safety alignment, Large Language Models (LLMs) often fail against jailbreak attacks. While machine unlearning has emerged as a promising defense by erasing specific harmful parameters, current methods remain vulnerable to diverse jailbreaks. We first conduct an empirical study and discover that this failure mechanism is caused by jailbreaks primarily activating non-erased parameters in the intermediate layers. Further, by probing the underlying mechanism through which these circumvented parameters reassemble into the prohibited output, we verify the persistent existence of dynamic $\\textbf{jailbreak paths}$ and show that the inability to rectify them constitutes the fundamental gap in existing unlearning defenses. To bridge this gap, we propose $\\textbf{J}$ailbreak $\\textbf{P}$ath $\\textbf{U}$nlearning (JPU), which is the first to rectify dynamic jailbreak paths towards safety anchors by dynamically mining on-policy adversarial samples to expose vulnerabilities and identify jailbreak paths. Extensive experiments demonstrate that JPU significantly enhances jailbreak resistance against dynamic attacks while preserving the model's utility.",
      "tr": "Makale Başlığı: JPU: Jailbreak Savunması ve Unlearning Arasındaki Köprüyü On-Policy Path Rectification Üzerinden Kurmak\n\nÖzet:\nKapsamlı güvenlik hizalamasına rağmen, Büyük Dil Modelleri (LLM'ler) genellikle jailbreak saldırılarına karşı başarısız olmaktadır. Makine unlearning'i, belirli zararlı parametreleri silerek umut vadeden bir savunma olarak ortaya çıkmış olsa da, mevcut yöntemler çeşitli jailbreak'lere karşı hala savunmasızdır. İlk olarak, ampirik bir çalışma yürüterek bu başarısızlık mekanizmasının, jailbreak'lerin öncelikli olarak ara katmanlardaki silinmemiş parametreleri aktive etmesinden kaynaklandığını keşfediyoruz. Dahası, bu aşılmış parametrelerin yasaklanmış çıktıya yeniden nasıl birleştiği alttaki mekanizmayı inceleyerek, dinamik **jailbreak paths**'in kalıcı varlığını doğruluyoruz ve bunları düzeltmedeki yetersizliğin, mevcut unlearning savunmalarındaki temel boşluğu oluşturduğunu gösteriyoruz. Bu boşluğu kapatmak için, savunmasızlıkları ortaya çıkarmak ve **jailbreak paths**'i belirlemek üzere on-policy adversarial sample'ları dinamik olarak madencilik yaparak, dinamik **jailbreak paths**'i güvenlik çapalarına doğru düzelten ilk yöntem olan **J**ailbreak **P**ath **U**nlearning'i (JPU) öneriyoruz. Kapsamlı deneyler, JPU'nun modelin kullanışlılığını korurken dinamik saldırılara karşı jailbreak direncini önemli ölçüde artırdığını göstermektedir."
    }
  },
  {
    "id": "2601.02941v1",
    "title": "SastBench: A Benchmark for Testing Agentic SAST Triage",
    "authors": [
      "Jake Feiglin",
      "Guy Dar"
    ],
    "published_date": "2026-01-06",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2601.02941v1",
    "pdf_link": "https://arxiv.org/pdf/2601.02941v1",
    "content": {
      "en": "SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity, employed by commercial and non-commercial organizations to identify potential vulnerabilities in software. Despite their great utility, they generate numerous false positives, requiring costly manual filtering (aka triage). While LLM-powered agents show promise for automating cybersecurity tasks, existing benchmarks fail to emulate real-world SAST finding distributions. We introduce SastBench, a benchmark for evaluating SAST triage agents that combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SastBench features an agent-agnostic design. We evaluate different agents on the benchmark and present a comparative analysis of their performance, provide a detailed analysis of the dataset, and discuss the implications for future development.",
      "tr": "**Makale Başlığı:** SastBench: A Benchmark for Testing Agentic SAST Triage\n\n**Özet:**\n\nSAST (Static Application Security Testing) araçları, yazılımlardaki potansiyel güvenlik açıklarını tespit etmek amacıyla ticari ve ticari olmayan kuruluşlar tarafından kullanılan, savunma siber güvenliğinde en yaygın kullanılan teknikler arasındadır. Sunduğu büyük faydalara rağmen, bu araçlar çok sayıda false positive üretmekte olup, bu da maliyetli manuel filtreleme (yani triage) gerektirmektedir. LLM destekli ajanlar siber güvenlik görevlerini otomatikleştirmede umut vaat etse de, mevcut benchmark'lar gerçek dünya SAST bulgu dağılımlarını tam olarak taklit edememektedir. Gerçek CVE'leri true positive olarak ve filtrelenmiş SAST araç bulgularını approximate false positive olarak birleştiren SAST triage ajanlarını değerlendirmek için bir benchmark olan SastBench'i sunuyoruz. SastBench, agent-agnostic bir tasarıma sahiptir. Benchmark üzerinde farklı ajanları değerlendiriyor ve performanslarının karşılaştırmalı bir analizini sunuyor, veri setinin detaylı bir analizini yapıyor ve gelecekteki geliştirmeler için çıkarımları tartışıyoruz."
    }
  },
  {
    "id": "2601.02751v1",
    "title": "Window-based Membership Inference Attacks Against Fine-tuned Large Language Models",
    "authors": [
      "Yuetian Chen",
      "Yuntao Du",
      "Kaiyuan Zhang",
      "Ashish Kundu",
      "Charles Fleming"
    ],
    "published_date": "2026-01-06",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.02751v1",
    "pdf_link": "https://arxiv.org/pdf/2601.02751v1",
    "content": {
      "en": "Most membership inference attacks (MIAs) against Large Language Models (LLMs) rely on global signals, like average loss, to identify training data. This approach, however, dilutes the subtle, localized signals of memorization, reducing attack effectiveness. We challenge this global-averaging paradigm, positing that membership signals are more pronounced within localized contexts. We introduce WBC (Window-Based Comparison), which exploits this insight through a sliding window approach with sign-based aggregation. Our method slides windows of varying sizes across text sequences, with each window casting a binary vote on membership based on loss comparisons between target and reference models. By ensembling votes across geometrically spaced window sizes, we capture memorization patterns from token-level artifacts to phrase-level structures. Extensive experiments across eleven datasets demonstrate that WBC substantially outperforms established baselines, achieving higher AUC scores and 2-3 times improvements in detection rates at low false positive thresholds. Our findings reveal that aggregating localized evidence is fundamentally more effective than global averaging, exposing critical privacy vulnerabilities in fine-tuned LLMs.",
      "tr": "**Makale Başlığı:** İnce Ayarlanmış Büyük Dil Modellerine Karşı Pencere Tabanlı Üyelik Çıkarım Saldırıları\n\n**Özet:**\n\nBüyük Dil Modelleri (LLM'ler) üzerindeki çoğu üyelik çıkarım saldırısı (MIA), eğitim verilerini tanımlamak için ortalama kayıp gibi global sinyallere dayanmaktadır. Ancak bu yaklaşım, ezberlemenin incelikli, lokalize sinyallerini seyreltmekte ve saldırı etkinliğini azaltmaktadır. Biz bu global-ortalama paradigmasını sorgulayarak, üyelik sinyallerinin lokalize bağlamlarda daha belirgin olduğu tezini ileri sürüyoruz. İşaret tabanlı agregasyon ile kayan pencere yaklaşımını kullanan WBC (Window-Based Comparison) yöntemini tanıtıyoruz. Yöntemimiz, hedef ve referans modeller arasındaki kayıp karşılaştırmalarına dayalı olarak üyelik hakkında ikili bir oy kullanan, farklı boyutlarda pencereleri metin dizileri boyunca kaydırır. Geometrik olarak aralıklı pencere boyutları boyunca oyları toplamak suretiyle, token düzeyindeki artefaktlardan ifade düzeyindeki yapılara kadar ezberleme örüntülerini yakalarız. On bir veri kümesi üzerinde gerçekleştirilen kapsamlı deneyler, WBC'nin yerleşik temel çizgileri önemli ölçüde aştığını, daha yüksek AUC skorları elde ettiğini ve düşük yanlış pozitif eşiklerinde tespit oranlarında 2-3 kat iyileşme sağladığını göstermektedir. Bulgularımız, lokalize kanıtları toplamanın global ortalamadan temel olarak daha etkili olduğunu ortaya koyarak, ince ayarlanmış LLM'lerde kritik gizlilik savunmasızlıklarını açığa çıkarmaktadır."
    }
  },
  {
    "id": "2601.02720v1",
    "title": "Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System",
    "authors": [
      "Yuqiao Xu",
      "Mina Namazi",
      "Sahith Reddy Jalapally",
      "Osama Zafar",
      "Youngjin Yoo"
    ],
    "published_date": "2026-01-06",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.02720v1",
    "pdf_link": "https://arxiv.org/pdf/2601.02720v1",
    "content": {
      "en": "Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.",
      "tr": "**Makale Başlığı:** Gizliliği Koruyan Yapay Zeka Destekli Merkezi Olmayan Öğrenme ve İstihdam Kayıtları Sistemi\n\n**Özet:**\n\nÖğrenme ve İstihdam Kaydı (LER) sistemleri, eğitim ve iş başarılarını güvenli bir şekilde derleme ve paylaşma konusunda kritik altyapılar olarak öne çıkmaktadır. Mevcut blok zinciri tabanlı platformlar, doğrulanabilir kimlik bilgilerinden (verifiable credentials) yararlanmaktadır, ancak genellikle otomatik beceri-kimlik bilgisi üretimi ve yapılandırılmamış öğrenme kanıtlarını dahil etme yeteneğinden yoksundurlar. Bu çalışmada, bu eksiklikleri gidermek üzere gizliliği koruyan, yapay zeka destekli merkezi olmayan bir LER sistemi önerilmektedir. Eğitim kurumlarından gelen dijital olarak imzalanmış transkriptler kabul edilmekte ve güvenilir bir yürütme ortamında (trusted execution environment - TEE) doğal dil işleme (natural language processing - NLP) boru hattı tarafından, resmi kayıtlar (örneğin, transkriptler, ders içerikleri) ve gayri resmi belgeler analiz edilerek doğrulanabilir ve kendi kendine çıkarılan (self-issued) beceri kimlik bilgileri türetilmektedir. Tüm doğrulama ve iş-beceri eşleştirmeleri, seçici ifşa (selective disclosure) ile sığınak (enclave) içinde gerçekleştirilmekte, böylece ham kimlik bilgileri ve özel anahtarlar sığınak içinde kalmaktadır. İş eşleştirmesi tamamen doğrulanmış beceri vektörlerine (attested skill vectors) dayanmakta ve beceri dışı özgeçmiş alanlarından bağımsız olmakta, böylece tarama yanlılığı (screening bias) fırsatlarını azaltmaktadır. NLP bileşeni örnek öğrenci verileri üzerinde değerlendirilmiş olup, eşleştirme doğrulanmış Syllabus-to-O*NET metodolojisini takip etmekte ve tekrarlanan çalıştırmalar boyunca gözlemlenen <5% varyans ile en üst sıralardaki becerilerde kararlılık (stability) görülmüştür. Türetilen kimlik bilgilerinin sahtesinin yapılamayacağını ve hassas bilgilerin gizli kaldığını gösteren resmi güvenlik ifadeleri ve kanıt taslakları (proof sketches) sunulmaktadır. Dolayısıyla, önerilen sistem merkezi olmayan bir çerçevede güvenli eğitim ve istihdam kimlik bilgilerini, sağlam transkript doğrulamasını ve otomatik, gizliliği koruyan beceri çıkarımını desteklemektedir."
    }
  },
  {
    "id": "2601.02680v1",
    "title": "Adversarial Contrastive Learning for LLM Quantization Attacks",
    "authors": [
      "Dinghong Song",
      "Zhiwei Xu",
      "Hai Wan",
      "Xibin Zhao",
      "Pengfei Su"
    ],
    "published_date": "2026-01-06",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.02680v1",
    "pdf_link": "https://arxiv.org/pdf/2601.02680v1",
    "content": {
      "en": "Model quantization is critical for deploying large language models (LLMs) on resource-constrained hardware, yet recent work has revealed severe security risks that benign LLMs in full precision may exhibit malicious behaviors after quantization. In this paper, we propose Adversarial Contrastive Learning (ACL), a novel gradient-based quantization attack that achieves superior attack effectiveness by explicitly maximizing the gap between benign and harmful responses probabilities. ACL formulates the attack objective as a triplet-based contrastive loss, and integrates it with a projected gradient descent two-stage distributed fine-tuning strategy to ensure stable and efficient optimization. Extensive experiments demonstrate ACL's remarkable effectiveness, achieving attack success rates of 86.00% for over-refusal, 97.69% for jailbreak, and 92.40% for advertisement injection, substantially outperforming state-of-the-art methods by up to 44.67%, 18.84%, and 50.80%, respectively.",
      "tr": "**Makale Başlığı:** LLM Kuantizasyon Saldırıları İçin Adversarial Contrastive Learning\n\n**Özet:**\n\nModel quantization, large language models (LLMs) için resource-constrained hardware üzerinde deployment açısından kritik öneme sahiptir. Ancak, son çalışmalar, full precision durumundaki benign LLM'lerin quantization sonrasında malicious behaviors sergileyebileceğine dair ciddi güvenlik risklerini ortaya koymuştur. Bu çalışmada, benign ve harmful response olasılıkları arasındaki farkı açıkça maksimize ederek üstün attack effectiveness elde eden, gradient-based bir quantization attack olan Adversarial Contrastive Learning (ACL)'i öneriyoruz. ACL, attack objective'ini triplet-based bir contrastive loss olarak formüle eder ve stable ve efficient optimizasyonu sağlamak için projected gradient descent iki-aşamalı distributed fine-tuning stratejisi ile entegre eder. Kapsamlı deneyler, ACL'in olağanüstü etkinliğini göstermektedir; over-refusal için %86.00, jailbreak için %97.69 ve advertisement injection için %92.40'lık attack success rates elde ederek, sırasıyla %44.67, %18.84 ve %50.80'e kadar state-of-the-art yöntemleri önemli ölçüde geride bırakmaktadır."
    }
  },
  {
    "id": "2601.02624v1",
    "title": "LAsset: An LLM-assisted Security Asset Identification Framework for System-on-Chip (SoC) Verification",
    "authors": [
      "Md Ajoad Hasan",
      "Dipayan Saha",
      "Khan Thamid Hasan",
      "Nashmin Alam",
      "Azim Uddin"
    ],
    "published_date": "2026-01-06",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.02624v1",
    "pdf_link": "https://arxiv.org/pdf/2601.02624v1",
    "content": {
      "en": "The growing complexity of modern system-on-chip (SoC) and IP designs is making security assurance difficult day by day. One of the fundamental steps in the pre-silicon security verification of a hardware design is the identification of security assets, as it substantially influences downstream security verification tasks, such as threat modeling, security property generation, and vulnerability detection. Traditionally, assets are determined manually by security experts, requiring significant time and expertise. To address this challenge, we present LAsset, a novel automated framework that leverages large language models (LLMs) to identify security assets from both hardware design specifications and register-transfer level (RTL) descriptions. The framework performs structural and semantic analysis to identify intra-module primary and secondary assets and derives inter-module relationships to systematically characterize security dependencies at the design level. Experimental results show that the proposed framework achieves high classification accuracy, reaching up to 90% recall rate in SoC design, and 93% recall rate in IP designs. This automation in asset identification significantly reduces manual overhead and supports a scalable path forward for secure hardware development.",
      "tr": "Makale Başlığı: LAsset: Sistem-Üzerinde-Çip (SoC) Doğrulaması İçin Büyük Dil Modeli Destekli Güvenlik Varlığı Tanımlama Çerçevesi\n\nÖzet:\nModern Sistem-Üzerinde-Çip (SoC) ve IP tasarımlarının artan karmaşıklığı, güvenlik güvencesini her geçen gün zorlaştırmaktadır. Bir donanım tasarımının silikon öncesi güvenlik doğrulaması aşamasındaki temel adımlardan biri, güvenlik varlıklarının tanımlanmasıdır, çünkü bu, tehdit modellemesi, güvenlik özelliği üretimi ve zafiyet tespiti gibi sonraki güvenlik doğrulama görevlerini önemli ölçüde etkiler. Geleneksel olarak, varlıklar güvenlik uzmanları tarafından manuel olarak belirlenir ve bu da önemli ölçüde zaman ve uzmanlık gerektirir. Bu zorluğun üstesinden gelmek için, donanım tasarım spesifikasyonlarından ve register-transfer level (RTL) tanımlarından güvenlik varlıklarını tanımlamak üzere büyük dil modellerini (LLMs) kullanan yeni bir otomatik çerçeve olan LAsset'ı sunuyoruz. Çerçeve, modül içi birincil ve ikincil varlıkları tanımlamak için yapısal ve semantik analiz gerçekleştirir ve tasarım düzeyinde güvenlik bağımlılıklarını sistematik olarak karakterize etmek için modüller arası ilişkiler türetir. Deneysel sonuçlar, önerilen çerçevenin SoC tasarımında %90'a varan ve IP tasarımlarında %93'e varan geri çağırma (recall) oranı ile yüksek sınıflandırma doğruluğu elde ettiğini göstermektedir. Varlık tanımlamasındaki bu otomasyon, manuel ek yükü önemli ölçüde azaltır ve güvenli donanım geliştirme için ölçeklenebilir bir yol sunar."
    }
  },
  {
    "id": "2601.02602v1",
    "title": "SWaRL: Safeguard Code Watermarking via Reinforcement Learning",
    "authors": [
      "Neusha Javidnia",
      "Ruisi Zhang",
      "Ashish Kundu",
      "Farinaz Koushanfar"
    ],
    "published_date": "2026-01-05",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.02602v1",
    "pdf_link": "https://arxiv.org/pdf/2601.02602v1",
    "content": {
      "en": "We present SWaRL, a robust and fidelity-preserving watermarking framework designed to protect the intellectual property of code LLM owners by embedding unique and verifiable signatures in the generated output. Existing approaches rely on manually crafted transformation rules to preserve watermarked code functionality or manipulate token-generation probabilities at inference time, which are prone to compilation errors. To address these challenges, SWaRL employs a reinforcement learning-based co-training framework that uses compiler feedback for functional correctness and a jointly trained confidential verifier as a reward signal to maintain watermark detectability. Furthermore, SWaRL employs low-rank adaptation (LoRA) during fine-tuning, allowing the learned watermark information to be transferable across model updates. Extensive experiments show that SWaRL achieves higher watermark detection accuracy compared to prior methods while fully maintaining watermarked code functionality. The LoRA-based signature embedding steers the base model to generate and solve code in a watermark-specific manner without significant computational overhead. Moreover, SWaRL exhibits strong resilience against refactoring and adversarial transformation attacks.",
      "tr": "**Makale Başlığı:** SWaRL: Yapay Zeka ile Korunan Kodun İzini Sürme\n\n**Özet:**\n\nSWaRL'ı sunuyoruz; bu, üretilen çıktılara benzersiz ve doğrulanabilir imzalar gömerek kod LLM sahiplerinin fikri mülkiyetini korumak üzere tasarlanmış, sağlam ve doğruluk koruyucu bir iz sürme (watermarking) çerçevesidir. Mevcut yaklaşımlar, izlenmiş kodun işlevselliğini korumak için manuel olarak oluşturulmuş dönüşüm kurallarına dayanır veya çıkarım zamanında token-üretim olasılıklarını manipüle eder ki bu da derleme hatalarına yatkındır. Bu zorlukların üstesinden gelmek için SWaRL, işlevsel doğruluk için derleyici geri bildirimini ve watermark tespit edilebilirliğini korumak üzere bir ödül sinyali olarak ortak eğitilmiş gizli bir doğrulayıcı (verifier) kullanan, reinforcement learning tabanlı bir eş-eğitim (co-training) çerçevesi benimser. Ek olarak, SWaRL ince ayar sırasında low-rank adaptation (LoRA) kullanır, bu da öğrenilen watermark bilgisinin model güncellemeleri arasında aktarılabilir olmasını sağlar. Kapsamlı deneyler, SWaRL'ın izlenmiş kod işlevselliğini tam olarak korurken önceki yöntemlere kıyasla daha yüksek watermark tespit doğruluğu elde ettiğini göstermektedir. LoRA tabanlı imza gömme, taban modeli anlamlı bir hesaplama yükü olmadan watermark'a özgü bir şekilde kod üretmeye ve çözmeye yönlendirir. Dahası, SWaRL yeniden yapılandırma (refactoring) ve düşmanca dönüşüm saldırılarına (adversarial transformation attacks) karşı güçlü bir dayanıklılık sergiler."
    }
  },
  {
    "id": "2601.02257v1",
    "title": "Improved Accuracy for Private Continual Cardinality Estimation in Fully Dynamic Streams via Matrix Factorization",
    "authors": [
      "Joel Daniel Andersson",
      "Palak Jain",
      "Satchit Sivakumar"
    ],
    "published_date": "2026-01-05",
    "tags": [
      "cs.CR",
      "cs.DS",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.02257v1",
    "pdf_link": "https://arxiv.org/pdf/2601.02257v1",
    "content": {
      "en": "We study differentially-private statistics in the fully dynamic continual observation model, where many updates can arrive at each time step and updates to a stream can involve both insertions and deletions of an item. Earlier work (e.g., Jain et al., NeurIPS 2023 for counting distinct elements; Raskhodnikova & Steiner, PODS 2025 for triangle counting with edge updates) reduced the respective cardinality estimation problem to continual counting on the difference stream associated with the true function values on the input stream. In such reductions, a change in the original stream can cause many changes in the difference stream, this poses a challenge for applying private continual counting algorithms to obtain optimal error bounds. We improve the accuracy of several such reductions by studying the associated $\\ell_p$-sensitivity vectors of the resulting difference streams and isolating their properties.   We demonstrate that our framework gives improved bounds for counting distinct elements, estimating degree histograms, and estimating triangle counts (under a slightly relaxed privacy model), thus offering a general approach to private continual cardinality estimation in streaming settings. Our improved accuracy stems from tight analysis of known factorization mechanisms for the counting matrix in this setting; the key technical challenge is arguing that one can use state-of-the-art factorizations for sensitivity vector sets with the properties we isolate. Empirically and analytically, we demonstrate that our improved error bounds offer a substantial improvement in accuracy for cardinality estimation problems over a large range of parameters.",
      "tr": "**Makale Başlığı: Matris Ayrıştırma Yoluyla Tam Dinamik Akışlarda Özel Sürekli Kardinalite Tahmini İçin İyileştirilmiş Doğruluk**\n\n**Özet:**\n\nTam dinamik sürekli gözlem modelinde, her zaman adımında birçok güncellemenin gelebildiği ve bir akışa yapılan güncellemelerin bir öğenin hem eklenmesini hem de silinmesini içerebildiği durumlarda, diferansiyel olarak özel istatistikleri inceliyoruz. Daha önceki çalışmalar (örneğin, Jain vd., NeurIPS 2023, farklı öğelerin sayımı için; Raskhodnikova & Steiner, PODS 2025, kenar güncellemeleriyle üçgen sayımı için), ilgili kardinalite tahmini problemini, giriş akışındaki gerçek fonksiyon değerlerine ilişkin fark akışında sürekli sayım problemine indirgemiştir. Bu tür indirgemelerde, orijinal akıştaki bir değişiklik fark akışında birçok değişikliğe neden olabilir; bu durum, optimal hata sınırları elde etmek için özel sürekli sayım algoritmalarının uygulanmasında bir zorluk teşkil etmektedir. İlgili fark akışlarının $\\ell_p$-sensitivity vektörlerini inceleyerek ve özelliklerini izole ederek, bu tür birkaç indirgemenin doğruluğunu iyileştiriyoruz. Çerçevemizin, farklı öğelerin sayımı, derece histogramlarının tahmini ve üçgen sayılarının tahmini (hafif gevşetilmiş bir gizlilik modeli altında) için iyileştirilmiş sınırlar sağladığını gösteriyoruz, böylece akış ayarlarında özel sürekli kardinalite tahmini için genel bir yaklaşım sunuyoruz. İyileştirilmiş doğruluğumuz, bu ortamdaki sayım matrisi için bilinen faktörizasyon mekanizmalarının sıkı analizinden kaynaklanmaktadır; temel teknik zorluk, izole ettiğimiz özelliklere sahip hassasiyet vektör setleri için en son teknoloji faktörizasyonların kullanılabileceğini savunmaktır. Ampirik ve analitik olarak, iyileştirilmiş hata sınırlarımızın, geniş bir parametre aralığında kardinalite tahmini problemleri için doğrulukta önemli bir iyileşme sunduğunu gösteriyoruz."
    }
  },
  {
    "id": "2601.02444v1",
    "title": "VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses",
    "authors": [
      "Maryam Abbasihafshejani",
      "AHM Nazmus Sakib",
      "Murtuza Jadliwala"
    ],
    "published_date": "2026-01-05",
    "tags": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ],
    "link": "http://arxiv.org/abs/2601.02444v1",
    "pdf_link": "https://arxiv.org/pdf/2601.02444v1",
    "content": {
      "en": "The rapid advancement of speech synthesis technologies, including text-to-speech (TTS) and voice conversion (VC), has intensified security and privacy concerns related to voice cloning. Recent defenses attempt to prevent unauthorized cloning by embedding protective perturbations into speech to obscure speaker identity while maintaining intelligibility. However, adversaries can apply advanced purification techniques to remove these perturbations, recover authentic acoustic characteristics, and regenerate cloneable voices. Despite the growing realism of such attacks, the robustness of existing defenses under adaptive purification remains insufficiently studied.   Most existing purification methods are designed to counter adversarial noise in automatic speech recognition (ASR) systems rather than speaker verification or voice cloning pipelines. As a result, they fail to suppress the fine-grained acoustic cues that define speaker identity and are often ineffective against speaker verification attacks (SVA). To address these limitations, we propose Diffusion-Bridge (VocalBridge), a purification framework that learns a latent mapping from perturbed to clean speech in the EnCodec latent space. Using a time-conditioned 1D U-Net with a cosine noise schedule, the model enables efficient, transcript-free purification while preserving speaker-discriminative structure. We further introduce a Whisper-guided phoneme variant that incorporates lightweight temporal guidance without requiring ground-truth transcripts. Experimental results show that our approach consistently outperforms existing purification methods in recovering cloneable voices from protected speech. Our findings demonstrate the fragility of current perturbation-based defenses and highlight the need for more robust protection mechanisms against evolving voice-cloning and speaker verification threats.",
      "tr": "Makale Başlığı: VocalBridge: Pertürbasyon-Temelli Ses Kimliği Savunmalarını Yenmek İçin Gizil Yayılım-Köprü Arıtması\n\nÖzet:\nMetinden-sese (text-to-speech - TTS) ve ses dönüştürme (voice conversion - VC) gibi konuşma sentezi teknolojilerindeki hızlı ilerlemeler, ses klonlamasıyla ilgili güvenlik ve gizlilik endişelerini yoğunlaştırmıştır. Son dönemdeki savunmalar, konuşmacı kimliğini gizleyerek ancak anlaşılırlığı koruyarak yetkisiz klonlamayı önlemeye çalışmaktadır. Ancak, saldırganlar bu pertürbasyonları kaldırmak, özgün akustik özellikleri geri kazanmak ve klonlanabilir sesleri yeniden üretmek için gelişmiş purification teknikleri uygulayabilmektedir. Böyle saldırıların artan gerçekçiliğine rağmen, adaptif purification altındaki mevcut savunmaların sağlamlığı yetersiz incelenmiştir. Mevcut purification yöntemlerinin çoğu, konuşmacı doğrulama veya ses klonlama hatlarından ziyade otomatik konuşma tanıma (automatic speech recognition - ASR) sistemlerindeki düşmanca gürültüye karşı koymak için tasarlanmıştır. Sonuç olarak, konuşmacı kimliğini tanımlayan ince taneli akustik ipuçlarını bastırmakta başarısız olurlar ve konuşmacı doğrulama saldırılarına (speaker verification attacks - SVA) karşı genellikle etkisizdirler. Bu sınırlamaları ele almak için, EnCodec latent space'de pertürbe edilmiş konuşmadan temiz konuşmaya bir latent mapping öğrenen bir purification çerçevesi olan Diffusion-Bridge (VocalBridge)'i öneriyoruz. Kosinüs gürültü programına sahip zamana bağlı bir 1D U-Net kullanarak, model konuşmacı-ayırt edici yapıyı korurken verimli, transkript-gerektirmeyen purification sağlar. Ayrıca, gerçek transkript gerektirmeyen hafif zamansal rehberlik içeren bir Whisper-guided fonem varyantı sunmaktayız. Deneysel sonuçlar, yaklaşımımızın korumalı konuşmadan klonlanabilir sesleri geri kazanmada mevcut purification yöntemlerinden tutarlı bir şekilde daha iyi performans gösterdiğini göstermektedir. Bulgularımız, mevcut pertürbasyon-temelli savunmaların kırılganlığını ortaya koymakta ve gelişen ses klonlama ve konuşmacı doğrulama tehditlerine karşı daha sağlam koruma mekanizmalarına duyulan ihtiyacı vurgulamaktadır."
    }
  },
  {
    "id": "2601.02438v1",
    "title": "Focus on What Matters: Fisher-Guided Adaptive Multimodal Fusion for Vulnerability Detection",
    "authors": [
      "Yun Bian",
      "Yi Chen",
      "HaiQuan Wang",
      "ShiHao Li",
      "Zhe Cui"
    ],
    "published_date": "2026-01-05",
    "tags": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.02438v1",
    "pdf_link": "https://arxiv.org/pdf/2601.02438v1",
    "content": {
      "en": "Software vulnerability detection is a critical task for securing software systems and can be formulated as a binary classification problem: given a code snippet, determine whether it contains a vulnerability. Existing multimodal approaches typically fuse Natural Code Sequence (NCS) representations from pretrained language models with Code Property Graph (CPG) representations from graph neural networks, often under the implicit assumption that adding a modality necessarily yields extra information. In practice, sequence and graph representations can be redundant, and fluctuations in the quality of the graph modality can dilute the discriminative signal of the dominant modality. To address this, we propose TaCCS-DFA, a framework that introduces Fisher information as a geometric measure of how sensitive feature directions are to the classification decision, enabling task-oriented complementary fusion. TaCCS-DFA online estimates a low-rank principal Fisher subspace and restricts cross-modal attention to task-sensitive directions, thereby retrieving structural features from CPG that complement the sequence modality; meanwhile, an adaptive gating mechanism dynamically adjusts the contribution of the graph modality for each sample to suppress noise propagation. Our analysis shows that, under an isotropic perturbation assumption, the proposed mechanism admits a tighter risk bound than conventional full-spectrum attention. Experiments on BigVul, Devign, and ReVeal show that TaCCS-DFA achieves strong performance across multiple backbones. With CodeT5 as the backbone, TaCCS-DFA reaches an F1 score of 87.80\\% on the highly imbalanced BigVul dataset, improving over a strong baseline Vul-LMGNNs by 6.3 percentage points while maintaining low calibration error and computational overhead.",
      "tr": "**Makale Başlığı:** Önemli Olana Odaklanın: Güvenlik Açığı Tespiti İçin Fisher Rehberli Adaptif Multimodal Füzyon\n\n**Özet:**\n\nYazılım güvenlik açığı tespiti, yazılım sistemlerini güvence altına almak için kritik bir görevdir ve ikili sınıflandırma problemi olarak formüle edilebilir: bir kod parçacığı verildiğinde, bir güvenlik açığı içerip içermediğini belirlemek. Mevcut multimodal yaklaşımlar, genellikle eklenen bir modallitenin zorunlu olarak ek bilgi sağlayacağı örtük varsayımı altında, önceden eğitilmiş dil modellerinden elde edilen Natural Code Sequence (NCS) temsillerini graph neural network'lerden elde edilen Code Property Graph (CPG) temsilleriyle kaynaştırır. Pratikte, dizi ve grafik temsilleri gereksiz olabilir ve grafik modalliğinin kalitesindeki dalgalanmalar, baskın modalliğin ayırt edici sinyalini seyreltebilir. Bunu ele almak için, Fisher bilgisini, özellik yönlerinin sınıflandırma kararına ne kadar hassas olduğunun geometrik bir ölçüsü olarak sunan ve görev odaklı tamamlayıcı füzyonu sağlayan TaCCS-DFA framework'ünü öneriyoruz. TaCCS-DFA, düşük ranklı bir principal Fisher subspace'i çevrimiçi olarak tahmin eder ve cross-modal attention'ı görev-hassas yönlerle sınırlar, böylece dizi modalliğini tamamlayan CPG'den yapısal özellikler alır; bu sırada, adaptif bir gating mekanizması, gürültü yayılımını bastırmak için her örnek için grafik modalliğinin katkısını dinamik olarak ayarlar. Analizimiz, izotropik bir pertürbasyon varsayımı altında, önerilen mekanizmanın geleneksel tam spektrum attention'dan daha sıkı bir risk sınırı sağladığını göstermektedir. BigVul, Devign ve ReVeal üzerindeki deneyler, TaCCS-DFA'nın birden fazla backbone üzerinde güçlü performans sergilediğini göstermektedir. CodeT5'i backbone olarak kullanarak, TaCCS-DFA, yüksek derecede dengesiz BigVul veri kümesinde %87.80 F1 skoru elde ederek, güçlü bir baseline olan Vul-LMGNN'lere göre 6.3 yüzde puanı iyileşme sağlamış, düşük kalibrasyon hatası ve hesaplama maliyetini korumuştur."
    }
  }
]