[
  {
    "id": "2512.21871v1",
    "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?",
    "authors": [
      "Naen Xu",
      "Jinghuai Zhang",
      "Changjiang Li",
      "Hengyu An",
      "Chunyi Zhou"
    ],
    "published_date": "2025-12-26",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "link": "http://arxiv.org/abs/2512.21871v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21871v1",
    "content": {
      "en": "Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.",
      "tr": "**Makale Başlığı:** Telif Hakkı Boşluğunu Kapatmak: Büyük Görsel-Dil Modelleri Telif Hakkıyla Korunan İçeriği Tanıyor ve Saygı Duyuyor mu?\n\n**Özet:**\n\nBüyük görsel-dil modelleri (LVLMs), multimodal reasoning görevlerinde dikkate değer ilerlemeler kaydetmiştir. Bununla birlikte, geniş çapta erişilebilir olmaları potansiyel telif hakkı ihlallerine ilişkin kritik endişeleri beraberinde getirmektedir. LVLMs, telif hakkıyla korunan içerikle (örneğin, kullanıcı girdisi, erişilen belgeler) karşılaştığında telif hakkı düzenlemelerini doğru bir şekilde tanıyıp bunlara uyacak mı? Telif hakkı düzenlemelerine uyulmaması, özellikle LVLMs telif hakkıyla korunan materyallere (örneğin, erişilen kitap bölümleri, haber raporları) dayanarak yanıtlar ürettiğinde ciddi yasal ve etik sonuçlara yol açabilir. Bu makalede, çeşitli LVLMs'lerin, telif hakkıyla korunan içeriklerle – örneğin kitap alıntıları, haber makaleleri, müzik sözleri ve kod belgeleri gibi görsel girdiler olarak sunulduğunda – nasıl başa çıktıklarını inceleyen kapsamlı bir değerlendirme sunuyoruz. Telif hakkı uyumluluğunu sistematik olarak ölçmek amacıyla, telif hakkı ihlaline yol açabilecek sorguları LVLMs'lerin ne kadar etkili bir şekilde işlediğini değerlendirmek üzere tasarlanmış 50.000 multimodal sorgu-içerik çiftinden oluşan büyük ölçekli bir benchmark veri kümesi sunuyoruz. Gerçek dünyadaki telif hakkıyla korunan içeriğin telif hakkı bildirimi içerip içermeyebileceği göz önüne alındığında, veri kümesi iki farklı senaryoda sorgu-içerik çiftlerini içermektedir: telif hakkı bildirimiyle ve telif hakkı bildirimi olmadan. İlk senaryo için, farklı durumları hesaba katmak üzere dört tür telif hakkı bildirimini kapsamlı bir şekilde ele alıyoruz. Değerlendirmemiz, en gelişmiş kapalı kaynaklı LVLMs'lerin bile, telif hakkı bildirimi sunulduğunda bile telif hakkıyla korunan içeriği tanıma ve saygı duyma konusunda önemli eksiklikler sergilediğini ortaya koymaktadır. Bu sınırlamayı çözmek için, telif hakkı uyumluluğu için yeni bir tool-augmented defense framework'ü sunuyoruz; bu, tüm senaryolarda ihlal risklerini azaltmaktadır. Bulgularımız, telif hakkıyla korunan içeriğin sorumlu ve yasal kullanımını sağlamak için telif hakkı bilincine sahip LVLMs geliştirmenin önemini vurgulamaktadır."
    }
  },
  {
    "id": "2512.21762v1",
    "title": "Assessing the Effectiveness of Membership Inference on Generative Music",
    "authors": [
      "Kurtis Chow",
      "Omar Samiullah",
      "Vinesh Sridhar",
      "Hewen Zhang"
    ],
    "published_date": "2025-12-25",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.21762v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21762v1",
    "content": {
      "en": "Generative AI systems are quickly improving, now able to produce believable output in several modalities including images, text, and audio. However, this fast development has prompted increased scrutiny concerning user privacy and the use of copyrighted works in training. A recent attack on machine-learning models called membership inference lies at the crossroads of these two concerns. The attack is given as input a set of records and a trained model and seeks to identify which of those records may have been used to train the model. On one hand, this attack can be used to identify user data used to train a model, which may violate their privacy especially in sensitive applications such as models trained on medical data. On the other hand, this attack can be used by rights-holders as evidence that a company used their works without permission to train a model.   Remarkably, it appears that no work has studied the effect of membership inference attacks (MIA) on generative music. Given that the music industry is worth billions of dollars and artists would stand to gain from being able to determine if their works were being used without permission, we believe this is a pressing issue to study. As such, in this work we begin a preliminary study into whether MIAs are effective on generative music. We study the effect of several existing attacks on MuseGAN, a popular and influential generative music model. Similar to prior work on generative audio MIAs, our findings suggest that music data is fairly resilient to known membership inference techniques.",
      "tr": "**Makale Başlığı:** Üretken Müzikte Üyelik Çıkarımının Etkinliğinin Değerlendirilmesi\n\n**Özet:**\n\nÜretken yapay zeka sistemleri hızla gelişmekte ve artık görseller, metinler ve sesler dahil olmak üzere çeşitli modalitelerde inandırıcı çıktılar üretebilmektedir. Ancak bu hızlı gelişim, kullanıcı gizliliği ve eğitimde telif hakkıyla korunan eserlerin kullanımı konusunda artan bir incelemeyi beraberinde getirmiştir. Makine öğrenmesi modellerine yönelik yakın zamanda ortaya çıkan bir saldırı türü olan membership inference, bu iki endişenin kesişim noktasında yer almaktadır. Bu saldırıya girdi olarak bir dizi kayıt ve eğitilmiş bir model verilir ve bu kayıtlardan hangilerinin modeli eğitmek için kullanılmış olabileceğini belirlemeyi amaçlar. Bir yandan, bu saldırı bir modeli eğitmek için kullanılan kullanıcı verilerini tanımlamak için kullanılabilir; bu durum, özellikle tıbbi veriler üzerinde eğitilmiş modeller gibi hassas uygulamalarda kullanıcı gizliliğini ihlal edebilir. Öte yandan, bu saldırı hak sahipleri tarafından bir şirketin izni olmadan eserlerini bir modeli eğitmek için kullandığına dair kanıt olarak kullanılabilir. Kayda değerdir ki, üretken müzik üzerinde membership inference attacks (MIA) etkisini inceleyen herhangi bir çalışma bulunmamaktadır. Müzik endüstrisinin milyarlarca dolarlık bir değere sahip olduğu ve sanatçıların eserlerinin izinsiz kullanılıp kullanılmadığını belirleyebilmekten fayda sağlayacağı göz önüne alındığında, bunun incelenmesi acil bir konu olduğuna inanıyoruz. Dolayısıyla, bu çalışmada üretken müzikte MIAs'nin etkin olup olmadığına dair ön bir çalışma başlatıyoruz. Popüler ve etkili bir üretken müzik modeli olan MuseGAN üzerindeki çeşitli mevcut saldırıların etkisini inceliyoruz. Üretken ses MIAs üzerine yapılan önceki çalışmalarla benzer şekilde, bulgularımız müzik verilerinin bilinen membership inference tekniklerine karşı oldukça dirençli olduğunu göstermektedir."
    }
  },
  {
    "id": "2512.21404v1",
    "title": "LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors",
    "authors": [
      "Tianwei Lan",
      "Farid Naït-Abdesselam"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.21404v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21404v1",
    "content": {
      "en": "The rapid growth in both the scale and complexity of Android malware has driven the widespread adoption of machine learning (ML) techniques for scalable and accurate malware detection. Despite their effectiveness, these models remain vulnerable to adversarial attacks that introduce carefully crafted feature-level perturbations to evade detection while preserving malicious functionality. In this paper, we present LAMLAD, a novel adversarial attack framework that exploits the generative and reasoning capabilities of large language models (LLMs) to bypass ML-based Android malware classifiers. LAMLAD employs a dual-agent architecture composed of an LLM manipulator, which generates realistic and functionality-preserving feature perturbations, and an LLM analyzer, which guides the perturbation process toward successful evasion. To improve efficiency and contextual awareness, LAMLAD integrates retrieval-augmented generation (RAG) into the LLM pipeline. Focusing on Drebin-style feature representations, LAMLAD enables stealthy and high-confidence attacks against widely deployed Android malware detection systems. We evaluate LAMLAD against three representative ML-based Android malware detectors and compare its performance with two state-of-the-art adversarial attack methods. Experimental results demonstrate that LAMLAD achieves an attack success rate (ASR) of up to 97%, requiring on average only three attempts per adversarial sample, highlighting its effectiveness, efficiency, and adaptability in practical adversarial settings. Furthermore, we propose an adversarial training-based defense strategy that reduces the ASR by more than 30% on average, significantly enhancing model robustness against LAMLAD-style attacks.",
      "tr": "**Makale Başlığı:** LLM Güdümlü Android Kötü Amaçlı Yazılım Dedektörlerine Karşı Özellik Seviyesinde Saldırılar\n\n**Özet:**\n\nAndroid kötü amaçlı yazılımların hem ölçeğindeki hem de karmaşıklığındaki hızlı artış, ölçeklenebilir ve doğru kötü amaçlı yazılım tespiti için makine öğrenimi (ML) tekniklerinin yaygın olarak benimsenmesine yol açmıştır. Etkinliklerine rağmen, bu modeller, kötü amaçlı işlevselliği korurken tespit edilmekten kaçınmak için özenle hazırlanmış feature-level perturbations introduced eden adversarial attackslara karşı savunmasız kalmaktadır. Bu makalede, ML tabanlı Android kötü amaçlı yazılım sınıflandırıcılarını atlatmak için large language models (LLMs) üretken ve reasoning yeteneklerinden yararlanan yeni bir adversarial attack framework'ü olan LAMLAD'ı sunuyoruz. LAMLAD, gerçekçi ve işlevselliği koruyan feature perturbations üreten bir LLM manipulator ve perturbation sürecini başarılı bir şekilde kaçınmaya yönlendiren bir LLM analyzer'dan oluşan ikili bir ajan mimarisi kullanır. Verimliliği ve bağlamsal farkındalığı artırmak için LAMLAD, LLM pipeline'ına retrieval-augmented generation (RAG) entegre eder. Drebin-style feature representations odaklanan LAMLAD, yaygın olarak kullanılan Android kötü amaçlı yazılım tespit sistemlerine karşı gizli ve yüksek güvenilirlikli saldırılar gerçekleştirilmesini sağlar. LAMLAD'ı üç temsili ML tabanlı Android kötü amaçlı yazılım dedektörüne karşı değerlendiriyor ve performansını iki state-of-the-art adversarial attack yöntemiyle karşılaştırıyoruz. Deneysel sonuçlar, LAMLAD'ın %97'ye varan bir attack success rate (ASR) elde ettiğini ve her bir adversarial sample için ortalama yalnızca üç deneme gerektirdiğini göstermektedir; bu da pratik adversarial settings'deki etkinliğini, verimliliğini ve adaptasyonunu vurgulamaktadır. Dahası, ortalama olarak ASR'yi %30'un üzerinde azaltan bir adversarial training-based defense strategy öneriyoruz, bu da LAMLAD-style saldırılara karşı modelin sağlamlığını önemli ölçüde artırmaktadır."
    }
  },
  {
    "id": "2512.21241v1",
    "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
    "authors": [
      "Xinjie Xu",
      "Shuyu Cheng",
      "Dongwei Xu",
      "Qi Xuan",
      "Chen Ma"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2512.21241v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21241v1",
    "content": {
      "en": "In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum $\\ell_2$-norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, which proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum. We provide a theoretical analysis of its convergence behavior, showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization. To further accelerate convergence, we incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance. The superiority of our approach is supported by theoretical guarantees under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that our method surpasses 13 state-of-the-art approaches in query efficiency.",
      "tr": "**Makale Başlığı:** Sorgu-Verimli Zor Etiketli Saldırılar İçin Ray Arama Optimizasyonunun Yakınsama Hızını İyileştirme\n\n**Özet:**\n\nZor etiketli siyah kutu adversaral saldırılarda, yalnızca en yüksek olasılıklı ilk tahmin edilen etiketin erişilebilir olması durumunda, aşırı sorgu karmaşıklığı pratik uygulamaların önünde büyük bir engel teşkil etmektedir. Bu çalışmada, iyi huylu bir görüntüyü adversaral bölgeye taşımak için gereken minimum $\\ell_2$-norm pertürbasyonunu veren optimal ray yönünü arayan saldırıların temsili bir sınıfını optimize etmeye odaklanıyoruz. Nesterov's Accelerated Gradient (NAG)'den esinlenerek, birikmiş momentumdan çıkarılan gelecekteki bir ray yönüne göre gradyanı proaktif olarak tahmin eden momentum tabanlı bir algoritma olan ARS-OPT'yi öneriyoruz. Yakınsama davranışının teorik analizini sunarak, ARS-OPT'nin daha doğru yönel güncellemeleri sağladığını ve daha hızlı, daha kararlı optimizasyon elde ettiğini gösteriyoruz. Yakınsamayı daha da hızlandırmak için, ARS-OPT'nin gradyan tahminine surrogate-model priors'larını dahil ederek, gelişmiş performans gösteren PARS-OPT'yi elde ediyoruz. Yaklaşımımızın üstünlüğü, standart varsayımlar altındaki teorik garantilerle desteklenmektedir. ImageNet ve CIFAR-10 üzerindeki kapsamlı deneyler, yöntemimizin sorgu verimliliğinde 13 state-of-the-art yaklaşımı geride bıraktığını göstermektedir."
    }
  },
  {
    "id": "2512.21238v1",
    "title": "Assessing the Software Security Comprehension of Large Language Models",
    "authors": [
      "Mohammed Latif Siddiq",
      "Natalie Sekerak",
      "Antonio Karam",
      "Maria Leal",
      "Arvin Islam-Gomes"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.SE",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.21238v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21238v1",
    "content": {
      "en": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.",
      "tr": "**Makale Başlığı:** Büyük Dil Modellerinin Yazılım Güvenliği Kavrayışının Değerlendirilmesi\n\n**Özet:**\n\nBüyük dil modelleri (LLM'ler), yazılım geliştirmede giderek daha fazla kullanılmaktadır, ancak yazılım güvenliği uzmanlık düzeyleri henüz net değildir. Bu çalışma, Blooms Taksonomisi'ni bir çerçeve olarak kullanarak beş önde gelen LLM'nin: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1 ve Qwen-2.5'in güvenlik kavrayışını sistematik olarak değerlendirmektedir. Altı bilişsel boyutu değerlendiriyoruz: remembering, understanding, applying, analyzing, evaluating ve creating. Metodolojimiz, küratörlü çoktan seçmeli soruları, savunmasız kod parçacıklarını (SALLM), bir Introduction to Software Security dersinden alınan ders değerlendirmelerini, gerçek dünya vaka çalışmalarını (XBOW) ve bir Secure Software Engineering dersinden proje tabanlı creation görevlerini içeren çeşitli veri kümelerini entegre etmektedir. Sonuçlar, LLM'lerin recalling facts ve identifying known vulnerabilities gibi daha düşük düzey bilişsel görevlerde iyi performans gösterirken, reasoning, architectural evaluation ve secure system creation gerektiren daha üst düzey görevlerde performanslarının önemli ölçüde azaldığını göstermektedir. Toplu doğruluğu raporlamanın ötesinde, bir modelin güvenilir performansını tutarlı bir şekilde sürdürdüğü en yüksek bilişsel seviyeyi belirleyen bir software security knowledge boundary sunuyoruz. Ek olarak, Blooms seviyeleri boyunca LLM'ler tarafından sergilenen 51 tekrarlayan misconception pattern tanımlıyoruz."
    }
  },
  {
    "id": "2512.21236v1",
    "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
    "authors": [
      "Yifan Huang",
      "Xiaojun Jia",
      "Wenbo Guo",
      "Yuqiang Sun",
      "Yihao Huang"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2512.21236v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21236v1",
    "content": {
      "en": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",
      "tr": "**Makale Başlığı:** SPELL: LLM Sınırlamalarını Kırma İçin Cümle Eşleştirme Keşfi\n\n**Özet:**\n\nBüyük Dil Modelleri (LLMs), AI destekli kodlama araçları aracılığıyla yazılım geliştirme alanında devrim yaratmış, sınırlı programlama uzmanlığına sahip geliştiricilerin sofistike uygulamalar oluşturmalarını sağlamıştır. Ancak, bu erişilebilirlik, bu güçlü araçları zararlı yazılımlar üretmek için sömürebilecek kötü niyetli aktörlere kadar uzanmaktadır. Mevcut jailbreaking araştırmaları öncelikli olarak LLM'lere yönelik genel saldırı senaryolarına odaklanmakta olup, zararlı kod üretimini bir jailbreak hedefi olarak sınırlı bir şekilde araştırmaktadır. Bu boşluğu gidermek için, zararlı kod üretiminde güvenlik uyumunun (security alignment) zayıflıklarını değerlendirmek üzere özel olarak tasarlanmış kapsamlı bir test çerçevesi olan SPELL'i öneriyoruz. Çerçevemiz, önceden var olan bir knowledge graph veri kümesinden cümleleri akıllıca birleştirerek jailbreaking promptları sistematik olarak oluşturan bir zaman bölmeli seçim stratejisi (time-division selection strategy) kullanır; bu strateji, yeni saldırı kalıplarının keşfini başarılı tekniklerin sömürülmesiyle dengeler. Üç gelişmiş kod modeli (GPT-4.1, Claude-3.5 ve Qwen2.5-Coder) üzerinde yapılan kapsamlı değerlendirme, SPELL'in etkinliğini göstermekte, sekiz zararlı kod kategorisinde sırasıyla %83,75, %19,38 ve %68,12 saldırı başarı oranları elde etmektedir. Üretilen promptlar, Cursor gibi gerçek dünya AI geliştirme araçlarında başarılı bir şekilde zararlı kod üretmekte ve üretilen çıktılar, %73'ü aşan oranlarda state-of-the-art detection systems tarafından zararlı olarak doğrulanmaktadır. Bu bulgular, mevcut LLM uygulamalarındaki önemli güvenlik açıklarını ortaya koymakta ve kod üretimi uygulamalarında AI safety alignment'ı iyileştirmek için değerli bilgiler sunmaktadır."
    }
  },
  {
    "id": "2512.21132v1",
    "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
    "authors": [
      "Tobias von Arx",
      "Niels Mündler",
      "Mark Vero",
      "Maximilian Baader",
      "Martin Vechev"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "link": "http://arxiv.org/abs/2512.21132v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21132v1",
    "content": {
      "en": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.",
      "tr": "İşte akademik makale başlığı ve özetinin istenen şekilde Türkçe çevirisi:\n\n**Makale Başlığı:** AutoBaxBuilder: Kod Güvenliği Kıyaslamalarının Önyüklenmesi\n\n**Özet:**\n\nYazılım mühendisliğinde LLM'lerin yaygın olarak benimsenmesiyle birlikte, LLM tarafından üretilen kodun doğruluğunun ve güvenliğinin güvenilir bir şekilde değerlendirilmesi büyük önem taşımaktadır. Özellikle önceki çalışmalar, güvenliğin sıklıkla göz ardı edildiğini ve LLM'lerin güvenlik açıkları içeren kod üretmeye yatkın olduğunu ortaya koymuştur. Bu bulgular, güvenlik uzmanları tarafından önemli manuel çaba ile oluşturulan özel kıyaslamalar sayesinde elde edilmiştir. Ancak, manuel olarak hazırlanmış kıyaslamalara güvenmek uzun vadede yetersizdir, çünkü kıyaslamalar (i) doğal olarak eğitim verilerini kirletir, (ii) daha eksiksiz bir tablo sunmak için yeni görevlere genişletilmeli ve (iii) daha yetenekli LLM'leri zorlamak için zorluk seviyeleri artırılmalıdır. Bu çalışmada, bu zorlukları ele alıyor ve sıfırdan kod güvenliği kıyaslamaları için görevler ve testler üreten bir çerçeve olan AutoBaxBuilder'ı sunuyoruz. LLM'lerin kod anlama yeteneklerinden yararlanarak işlevsellik testleri ve uçtan uca güvenlik yoklama exploit'leri oluşturmak için ince taneli makuliyet kontrollerine sahip sağlam bir pipeline tanıtıyoruz. Üretilen kıyaslamanın kalitesini doğrulamak için hem nitel bir analiz gerçekleştiriyor hem de insan uzmanlar tarafından oluşturulan görevlerle karşılaştırarak nicel deneyler yapıyoruz. AutoBaxBuilder'ı tamamen yeni görevler oluşturmak için kullandık ve bu görevlerdeki LLM'lerin güvenlik yeteneklerinin kapsamlı bir değerlendirmesiyle birlikte AutoBaxBench olarak kamuya açık hale getirdik. Yeni bir görevin 2 saatten az sürede, 10 USD'den daha az bir maliyetle üretilebileceğini bulduk."
    }
  },
  {
    "id": "2512.21110v1",
    "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent",
    "authors": [
      "Ahmed M. Hussain",
      "Salahuddin Salahuddin",
      "Panos Papadimitratos"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ],
    "link": "http://arxiv.org/abs/2512.21110v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21110v1",
    "content": {
      "en": "Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.",
      "tr": "**Makale Başlığı:** Bağlamın Ötesinde: Büyük Dil Modellerinin Kullanıcı Niyetini Kavramadaki Başarısızlığı\n\n**Özet:**\n\nMevcut Büyük Dil Modelleri (LLM) güvenlik yaklaşımları, açıkça zararlı içeriğe odaklanırken kritik bir zafiyeti göz ardı etmektedir: bağlamı anlama ve kullanıcı niyetini tanıma konusundaki yeteneksizlik. Bu durum, kötü niyetli kullanıcıların güvenlik mekanizmalarını atlatmak için sistematik olarak kullanabilecekleri istismar edilebilir zafiyetler yaratmaktadır. ChatGPT, Claude, Gemini ve DeepSeek dahil olmak üzere birçok son teknoloji LLM'yi ampirik olarak değerlendiriyoruz. Analizimiz, duygusal çerçeveleme, aşamalı ifşa ve akademik gerekçelendirme teknikleri aracılığıyla güvenilir güvenlik mekanizmalarının atlatılmasını göstermektedir. Özellikle, reasoning-enabled konfigürasyonlar, altta yatan niyeti sorgulamaktan kaçınırken olgusal kesinliği artırarak istismarın etkinliğini hafifletmek yerine artırmıştır. İstisna, bazı kullanım durumlarında bilgi sağlamak yerine intent detection'ı önceliklendiren Claude Opus 4.1 olmuştur. Bu örüntü, mevcut mimari tasarımların sistematik zafiyetler yarattığını ortaya koymaktadır. Bu sınırlamalar, mevcut koruyucu mekanizmalar yerine, bağlamsal anlayış ve intent recognition'a doğru paradigmatik değişimler gerektirmektedir."
    }
  },
  {
    "id": "2512.21048v1",
    "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
    "authors": [
      "Savvy Sharma",
      "George Petrovic",
      "Sarthak Kaushik"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.21048v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21048v1",
    "content": {
      "en": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.",
      "tr": "Makale Başlığı: zkFL-Health: Tıbbi Yapay Zeka Gizliliği için Blok Zinciri Destekli Zero-Knowledge Federated Learning\n\nÖzet:\nSağlık alanında yapay zeka büyük ve çeşitli veri setlerine ihtiyaç duymaktadır, ancak katı gizlilik ve yönetim kısıtlamaları kurumlar arasında ham veri paylaşımını engellemektedir. Federated learning (FL) bu durumu, verilerin bulunduğu yerde eğitim yaparak ve yalnızca model güncellemelerini değiş tokuş ederek hafifletir, ancak pratik uygulamalar hala iki temel riskle karşı karşıyadır: (1) gizlilik sızıntısı (gradient'ler veya güncellemeler aracılığıyla) (membership inference, gradient inversion) ve (2) güvenilir olmayan aggregator, yani katkıları tespit edilmeden düşürebilen, değiştirebilen veya enjekte edebilen tek hata noktası. Biz zkFL-Health'i sunuyoruz; bu mimari, tıbbi yapay zeka için gizlilik korumalı, doğrulanabilir şekilde doğru işbirliğine dayalı eğitimi sağlamak amacıyla FL'yi zero-knowledge proofs (ZKPs) ve Trusted Execution Environments (TEEs) ile birleştirmektedir. İstemciler yerel olarak güncellemelerini eğitir ve taahhüt eder; aggregator bir TEE içinde çalışarak global güncellemeyi hesaplar ve Halo2/Nova aracılığıyla (zk proof) özet bir ZK proof üretir; bu proof, herhangi bir istemci güncellemesini host'a ifşa etmeden, yalnızca taahhüt edilen girdileri ve doğru aggregation rule'u kullandığını gösterir. Doğrulayıcı düğümler proof'u doğrular ve on-chain kriptografik taahhütleri kaydeder; bu, değişmez bir denetim izi sağlar ve herhangi bir tek partiye güvenme ihtiyacını ortadan kaldırır. Sağlık sektörüne özel sistem ve tehdit modellerini, zkFL-Health protokolünü, güvenlik/gizlilik garantilerini ve doğruluk, gizlilik riski, gecikme ve maliyetleri kapsayan bir performans değerlendirme planını ana hatlarıyla belirtiyoruz. Bu çerçeve, güçlü gizlilik, bütünlük ve denetlenebilirlik özellikleriyle kurumsal olmayan tıbbi yapay zekayı mümkün kılar; bu özellikler klinik benimseme ve düzenleyici uyumluluk için anahtar niteliklerdir."
    }
  },
  {
    "id": "2512.20872v1",
    "title": "Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification",
    "authors": [
      "Jakir Hossain",
      "Gurvinder Singh",
      "Lukasz Ziarek",
      "Ahmet Erdem Sarıyüce"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.20872v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20872v1",
    "content": {
      "en": "Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.",
      "tr": "**Makale Başlığı:** Better Call Graphs: Kötü Amaçlı Yazılım Sınıflandırması İçin İşlev Çağrı Grafikleri Yeni Bir Veri Kümesi\n\n**Özet:**\n\nFunction call graphs (FCGs), uygulamaların yüzey seviyesi imzaların ötesindeki davranışsal yapısını yakalayarak kötü amaçlı yazılım tespiti için güçlü bir soyutlama olarak ortaya çıkmıştır. Geleneksel program analizindeki faydası, kötü amaçlı yazılımların etkili sınıflandırma ve analizini mümkün kılarak iyi bir şekilde kabul görmüştür. Mobil alanda, özellikle Android ekosisteminde, FCG tabanlı kötü amaçlı yazılım sınıflandırması, platformun yaygın benimsenmesi ve Android uygulamalarının karmaşık, bileşen tabanlı yapısı nedeniyle özellikle kritiktir. Ancak, bu yöndeki ilerleme, büyük ölçekli, yüksek kaliteli Android'e özgü FCG veri kümelerinin eksikliği ile engellenmektedir. Mevcut veri kümeleri genellikle güncelliğini yitirmiş, uygulama yeniden paketlenmesinden kaynaklanan küçük veya gereksiz grafiklerden oluşmakta ve gerçek dünya kötü amaçlı yazılımlarının çeşitliliğini yansıtmada yetersiz kalmaktadır. Bu sınırlamalar, aşırı uyuma (overfitting) ve grafik tabanlı sınıflandırma yöntemlerinin güvenilmez değerlendirmesine yol açmaktadır. Bu boşluğu gidermek için, güncel Android uygulama paketlerinden (APKs) çıkarılan büyük ve benzersiz FCG'lerden oluşan kapsamlı bir veri kümesi olan Better Call Graphs'ı (BCG) sunuyoruz. BCG, çeşitli aileleri ve türleri kapsayan hem zararsız hem de zararlı örnekleri ve her APK için grafik seviyesi özellikleri içermektedir. Temel sınıflandırıcılar kullanılarak yapılan kapsamlı deneyler aracılığıyla, mevcut veri kümelerine kıyasla BCG'nin gerekliliğini ve değerini göstermekteyiz. BCG, https://erdemub.github.io/BCG-dataset adresinden kamuya açıktır."
    }
  }
]