[
  {
    "id": "2602.16708v1",
    "title": "Policy Compiler for Secure Agentic Systems",
    "authors": [
      "Nils Palumbo",
      "Sarthak Choudhary",
      "Jihye Choi",
      "Prasad Chalasani",
      "Mihai Christodorescu"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2602.16708v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16708v1",
    "content": {
      "en": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.   Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.   PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "tr": "Makale Başlığı: Secure Agentic Systems için Politika Derleyicisi\n\nÖzet:\nLLM-temelli agent'lar, giderek artan bir şekilde karmaşık yetkilendirme politikaları gerektiren bağlamlarda konuşlandırılmaktadır: müşteri hizmetleri protokolleri, onay iş akışları, veri erişim kısıtlamaları ve mevzuata uyumluluk. Bu politikaları prompt'lara gömmek, herhangi bir zorlama garantisi sağlamaz. PCAS'ı sunuyoruz, Agentic Systems için Politika Derleyicisi, deterministik politika zorlaması sağlar. Bu tür politikaları zorlamak, agent'lar arasındaki bilgi akışını izlemeyi gerektirir; doğrusal mesaj geçmişleri bunu yakalayamaz. Bunun yerine, PCAS, agentic sistem durumunu olaylar arasındaki nedensel ilişkileri yakalayan bir dependency graph olarak modeller; örneğin tool calls, tool results ve messages gibi. Politikalar, Datalog'dan türetilmiş bir dilde, geçişli bilgi akışını ve cross-agent provenance'ı dikkate alan deklaratif kurallar olarak ifade edilir. Bir reference monitor tüm eylemleri engeller ve yürütmeden önce ihlalleri engeller, model reasoning'den bağımsız deterministik zorlama sağlar. PCAS, mevcut bir agent implementasyonunu ve bir politika spesifikasyonunu alır ve bunları politika uyumlu olacak şekilde yapılandırılmış bir sisteme derler; bu yapılandırma sırasında herhangi bir güvenlik odaklı yeniden yapılandırma gerektirmez. PCAS'ı üç vaka incelemesi üzerinde değerlendiriyoruz: prompt injection defense için bilgi akış politikaları, multi-agent pharmacovigilance sisteminde onay iş akışları ve müşteri hizmetleri için organizasyonel politikalar. Müşteri hizmetleri görevlerinde, PCAS politika uyumluluğunu frontier modellerde %48'den %93'e iyileştirir ve yapılandırılmış çalışmalarda sıfır politika ihlali ile sonuçlanır."
    }
  },
  {
    "id": "2602.16596v1",
    "title": "Sequential Membership Inference Attacks",
    "authors": [
      "Thomas Michel",
      "Debabrota Basu",
      "Emilie Kaufmann"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.LG",
      "cs.CR",
      "math.ST",
      "stat.ML"
    ],
    "link": "http://arxiv.org/abs/2602.16596v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16596v1",
    "content": {
      "en": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.",
      "tr": "Makale Başlığı: Ardışık Üyelik Çıkarım Saldırıları\n\nÖzet:\nModern yapay zeka modelleri statik değildir. Yaşam döngüleri boyunca çoklu güncelleme aşamalarından geçerler. Bu nedenle, daha güçlü Üyelik Çıkarım (MI) saldırıları oluşturmak ve daha sıkı gizlilik denetimleri gerçekleştirmek için model dinamiklerinden yararlanmak zamanında sorulardır. Literatürde, bir dizi model güncellemesinin kullanılmasının MI saldırılarının gücünü artırabileceği ampirik olarak gösterilmesine rağmen, `optimal' MI saldırılarının titiz analizi, sonsuz örneğe sahip statik modellerle sınırlıdır. Bu nedenle, belirli bir güncelleme adımında yerleştirilen bir hedefin varlığını tespit etmek için model güncelleme dizisini kullanan `optimal' bir MI saldırısı olan SeMI*'ı geliştiriyoruz. Ampirik ortalama hesaplaması için, gizlilikli veya gizliliksiz sınırlı sayıda örneğe erişimle SeMI*'ın optimal gücünü türetiyoruz. Sonuçlarımız mevcut asimptotik analizi yeniden elde eder. Model dizisine erişimin, MI sinyalinin kaybolduğu son model üzerindeki mevcut saldırıların aksine, MI sinyalinin seyreltilmesini önlediğini gözlemliyoruz, zira eğitim verileri biriktikçe MI sinyali ortadan kalkmaktadır. Dahası, bir saldırgan, hem yerleştirme zamanını hem de kanaryayı ayarlamak için SeMI*'ı kullanarak daha sıkı gizlilik denetimleri elde edebilir. Son olarak, DP-SGD ile eğitilmiş veya ince ayarlanmış veri dağılımları ve modeller genelinde deneyler yürüterek, SeMI*'ın pratik varyantlarının taban çizgilerine göre daha sıkı gizlilik denetimleri sağladığını gösteriyoruz."
    }
  },
  {
    "id": "2602.16564v1",
    "title": "A Scalable Approach to Solving Simulation-Based Network Security Games",
    "authors": [
      "Michael Lanier",
      "Yevgeniy Vorobeychik"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.16564v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16564v1",
    "content": {
      "en": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.",
      "tr": "Makale Başlığı: **Ölçeklenebilir Bir Yaklaşım ile Simülasyon Tabanlı Ağ Güvenliği Oyunlarının Çözümü**\n\nÖzet:\nÇok büyük siber-ağ ortamlarında ölçeklenebilir çoklu-ajan pekiştirmeli öğrenmeyi (multi-agent reinforcement learning) sağlamak amacıyla, öğrenilmiş, bölüm-farkındalığı olan (partition-aware) bir filtreleme katmanı ve Q-değeri önbelleği (Q-value caching) ile Double Oracle / PSRO paradigmasını güçlendiren hafif bir meta-denetleyici olan MetaDOAR'ı tanıtıyoruz. MetaDOAR, her düğümdeki yapısal gömmelerden (structural embeddings) kompakt bir durum projeksiyonu (state projection) öğrenerek, geleneksel düşük seviyeli bir aktörün odaklanmış ışın araması (focused beam search) yaptığı küçük bir cihaz alt kümesini (bir top-k bölümü) hızla puanlar ve seçer, bu işlemde bir eleştirmen ajan (critic agent) kullanılır. Seçilen aday eylemler, yığıtlı eleştirmen ilerlemeleri (batched critic forwards) ile değerlendirilir ve nicelenmiş bir durum projeksiyonu ve yerel eylem tanımlayıcıları ile anahtarlanmış bir LRU önbelleğinde saklanır. Bu, karar kalitesini muhafazakar k-hop önbellek geçersizliği (conservative k-hop cache invalidation) yoluyla koruyarak, gereksiz eleştirmen hesaplamalarını dramatik bir şekilde azaltır. Ampirik olarak, MetaDOAR büyük ağ topolojilerinde bellekte yer kullanımı veya eğitim süresi açısından önemli ölçeklenme sorunları olmaksızın, en gelişmiş (SOTA) temel çizgilere göre daha yüksek oyuncu getirileri elde eder. Bu katkı, büyük ölçekli ağ tabanlı karar problemleri için verimli hiyerarşik politika öğrenimine (hierarchical policy learning) pratik, teorik olarak motive edilmiş bir yol sunar."
    }
  },
  {
    "id": "2602.16520v1",
    "title": "Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents",
    "authors": [
      "Doron Shavit"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.16520v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16520v1",
    "content": {
      "en": "Jailbreak prompts are a practical and evolving threat to large language models (LLMs), particularly in agentic systems that execute tools over untrusted content. Many attacks exploit long-context hiding, semantic camouflage, and lightweight obfuscations that can evade single-pass guardrails. We present RLM-JB, an end-to-end jailbreak detection framework built on Recursive Language Models (RLMs), in which a root model orchestrates a bounded analysis program that transforms the input, queries worker models over covered segments, and aggregates evidence into an auditable decision. RLM-JB treats detection as a procedure rather than a one-shot classification: it normalizes and de-obfuscates suspicious inputs, chunks text to reduce context dilution and guarantee coverage, performs parallel chunk screening, and composes cross-chunk signals to recover split-payload attacks. On AutoDAN-style adversarial inputs, RLM-JB achieves high detection effectiveness across three LLM backends (ASR/Recall 92.5-98.0%) while maintaining very high precision (98.99-100%) and low false positive rates (0.0-2.0%), highlighting a practical sensitivity-specificity trade-off as the screening backend changes.",
      "tr": "Elbette, istenen çeviri aşağıdadır:\n\n**Makale Başlığı:** Recursive language models for jailbreak detection: a procedural defense for tool-augmented agents\n\n**Özet:**\n\nLarge language models (LLMs) için jailbreak prompt'ları, özellikle araçları güvenilmeyen içerikler üzerinde çalıştıran agentic sistemlerde pratik ve gelişen bir tehdittir. Birçok saldırı, tek geçişli guardrail'ları atlatabilen long-context hiding, semantic camouflage ve hafif karmaşıklaştırmalardan faydalanır. Biz, Recursive Language Models (RLMs) üzerine kurulu uçtan uca bir jailbreak detection framework olan RLM-JB'yi sunuyoruz. Bu framework'te, bir root model, girdiyi dönüştüren, kapsanan segmentler üzerinde worker models'a sorgular yapan ve bulguları denetlenebilir bir karara dönüştüren sınırlı bir analysis programını yönetir. RLM-JB, detection'ı tek seferlik bir sınıflandırma yerine bir prosedür olarak ele alır: şüpheli girdileri normalize eder ve de-obfuscate eder, context dilution'ı azaltmak ve kapsama alanını garanti etmek için metni parçalara ayırır, paralel chunk screening gerçekleştirir ve split-payload saldırılarını kurtarmak için cross-chunk sinyallerini birleştirir. AutoDAN tarzı adversarial girdiler üzerinde, RLM-JB üç LLM backend'inde yüksek detection etkinliği (ASR/Recall 92.5-98.0%) elde ederken aynı zamanda çok yüksek precision (%98.99-100) ve düşük false positive oranları (%0.0-2.0) korur. Bu durum, screening backend'i değiştikçe pratik bir sensitivity-specificity trade-off'unu vurgular."
    }
  },
  {
    "id": "2602.16436v1",
    "title": "Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent",
    "authors": [
      "Jean Dufraiche",
      "Paul Mangold",
      "Michaël Perrot",
      "Marc Tommasi"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "link": "http://arxiv.org/abs/2602.16436v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16436v1",
    "content": {
      "en": "Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.",
      "tr": "**Makale Başlığı:** Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent\n\n**Özet:**\n\nNon-interactive Local Differential Privacy (LDP) altında verinin tek seferde serbest bırakılması, verinin tam yeniden kullanılabilirliğini sağlamaktadır, ancak ortaya çıkan gürültü sonraki analizlerde yanlılığa neden olabilir. Bu çalışmada, ikili sınıflandırmada bu yanlılığı karakterize etmek için Weierstrass transformundan yararlanmaktayız. Bu transformun tersini almak, LDP altında serbest bırakılan örnekler üzerindeki doğrusal olmayan fonksiyonların yanlısız tahminlerini hesaplamak için bir yanlılık düzeltme yöntemi sağladığını ispat etmekteyiz. Ardından, Inverse Weierstrass Private SGD (IWP-SGD) adını verdiğimiz yeni bir stochastic gradient descent algoritması geliştirmekteyiz. Bu algoritma, $n$ örneğin sayısına bağlı olarak $\\mathcal{O}(1/n)$ oranında, gerçek popülasyon risk minimizer'ına yakınsamaktadır. IWP-SGD'yi sentetik ve gerçek dünya veri setlerini kullanarak ikili sınıflandırma görevlerinde ampirik olarak doğrulamaktayız."
    }
  },
  {
    "id": "2602.16309v1",
    "title": "The Weight of a Bit: EMFI Sensitivity Analysis of Embedded Deep Learning Models",
    "authors": [
      "Jakub Breier",
      "Štefan Kučerák",
      "Xiaolu Hou"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.16309v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16309v1",
    "content": {
      "en": "Fault injection attacks on embedded neural network models have been shown as a potent threat. Numerous works studied resilience of models from various points of view. As of now, there is no comprehensive study that would evaluate the influence of number representations used for model parameters against electromagnetic fault injection (EMFI) attacks.   In this paper, we investigate how four different number representations influence the success of an EMFI attack on embedded neural network models. We chose two common floating-point representations (32-bit, and 16-bit), and two integer representations (8-bit, and 4-bit). We deployed four common image classifiers, ResNet-18, ResNet-34, ResNet-50, and VGG-11, on an embedded memory chip, and utilized a low-cost EMFI platform to trigger faults. Our results show that while floating-point representations exhibit almost a complete degradation in accuracy (Top-1 and Top-5) after a single fault injection, integer representations offer better resistance overall. Especially, when considering the the 8-bit representation on a relatively large network (VGG-11), the Top-1 accuracies stay at around 70% and the Top-5 at around 90%.",
      "tr": "Aşağıda akademik makale başlığı ve özetinin istenen şekilde Türkçeye çevrilmiş halini bulabilirsiniz:\n\n**Makale Başlığı:** The Weight of a Bit: EMFI Sensitivity Analysis of Embedded Deep Learning Models\n\n**Özet:**\n\nGömülü sinir ağı modellerine yönelik fault injection saldırıları, güçlü bir tehdit olarak kabul edilmektedir. Mevcut çalışmalar, modellerin dayanıklılığını çeşitli açılardan incelemiştir. Şu ana kadar, model parametreleri için kullanılan sayı temsillerinin elektromanyetik fault injection (EMFI) saldırılarına karşı etkisini değerlendiren kapsamlı bir çalışma bulunmamaktadır. Bu çalışmada, dört farklı sayı temsilinin gömülü sinir ağı modelleri üzerindeki bir EMFI saldırısının başarısını nasıl etkilediğini araştırmaktayız. İki yaygın kayan nokta temsilini (32-bit ve 16-bit) ve iki tam sayı temsilini (8-bit ve 4-bit) seçtik. Dört yaygın görüntü sınıflandırıcısını (ResNet-18, ResNet-34, ResNet-50 ve VGG-11) bir gömülü bellek çipine yerleştirdik ve arızaları tetiklemek için düşük maliyetli bir EMFI platformu kullandık. Sonuçlarımız, kayan nokta temsillerinin tek bir fault injection sonrasında doğrulukta (Top-1 ve Top-5) neredeyse tam bir düşüş sergilerken, tam sayı temsillerinin genel olarak daha iyi bir direnç sunduğunu göstermektedir. Özellikle, nispeten büyük bir ağda (VGG-11) 8-bit temsil ele alındığında, Top-1 doğruluğu yaklaşık %70 civarında, Top-5 doğruluğu ise yaklaşık %90 civarında kalmaktadır."
    }
  },
  {
    "id": "2602.16109v1",
    "title": "Federated Graph AGI for Cross-Border Insider Threat Intelligence in Government Financial Schemes",
    "authors": [
      "Srikumar Nayak",
      "James Walmesley"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CE"
    ],
    "link": "http://arxiv.org/abs/2602.16109v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16109v1",
    "content": {
      "en": "Cross-border insider threats pose a critical challenge to government financial schemes, particularly when dealing with distributed, privacy-sensitive data across multiple jurisdictions. Existing approaches face fundamental limitations: they cannot effectively share intelligence across borders due to privacy constraints, lack reasoning capabilities to understand complex multi-step attack patterns, and fail to capture intricate graph-structured relationships in financial networks. We introduce FedGraph-AGI, a novel federated learning framework integrating Artificial General Intelligence (AGI) reasoning with graph neural networks for privacy-preserving cross-border insider threat detection. Our approach combines: (1) federated graph neural networks preserving data sovereignty; (2) Mixture-of-Experts (MoE) aggregation for heterogeneous jurisdictions; and (3) AGI-powered reasoning via Large Action Models (LAM) performing causal inference over graph data. Through experiments on a 50,000-transaction dataset across 10 jurisdictions, FedGraph-AGI achieves 92.3% accuracy, significantly outperforming federated baselines (86.1%) and centralized approaches (84.7%). Our ablation studies reveal AGI reasoning contributes 6.8% improvement, while MoE adds 4.4%. The system maintains epsilon = 1.0 differential privacy while achieving near-optimal performance and scales efficiently to 50+ clients. This represents the first integration of AGI reasoning with federated graph learning for insider threat detection, opening new directions for privacy-preserving cross-border intelligence sharing.",
      "tr": "**Makale Başlığı:** Sınır Ötesi İç Tehdit İstihbaratı İçin Federated Graph AGI'nin Devlet Finansal Planlarında Kullanımı\n\n**Özet:**\n\nSınır ötesi iç tehditler, özellikle birden fazla yetki alanında dağıtılmış ve gizliliğe duyarlı verilerle uğraşılırken, devlet finansal planları için kritik bir zorluk teşkil etmektedir. Mevcut yaklaşımlar temel sınırlamalara sahiptir: gizlilik kısıtlamaları nedeniyle istihbaratı sınırlar ötesine etkili bir şekilde paylaşamazlar, karmaşık çok adımlı saldırı modellerini anlamak için reasoning yeteneklerinden yoksundurlar ve finansal ağlardaki karmaşık graph-structured ilişkileri yakalamada başarısız olurlar. Gizlilik korumalı sınır ötesi iç tehdit tespiti için Artificial General Intelligence (AGI) reasoning'i graph neural networks ile entegre eden yeni bir federated learning framework'ü olan FedGraph-AGI'yi sunuyoruz. Yaklaşımımız şunları birleştirir: (1) veri egemenliğini koruyan federated graph neural networks; (2) heterojen yetki alanları için Mixture-of-Experts (MoE) aggregation'ı; ve (3) graph verileri üzerinde causal inference gerçekleştiren Large Action Models (LAM) aracılığıyla AGI-powered reasoning. 10 yetki alanı üzerinden 50.000 işlemlik bir veri kümesinde yapılan deneyler aracılığıyla FedGraph-AGI, %92,3 doğruluk oranı elde ederek federated baseline'ları (%86,1) ve centralized yaklaşımları (%84,7) önemli ölçüde geride bırakmaktadır. Ablasyon çalışmalarımız, AGI reasoning'in %6,8 iyileştirme sağladığını, MoE'nin ise %4,4 eklediğini ortaya koymaktadır. Sistem, epsilon = 1.0 differential privacy'yi korurken neredeyse optimal bir performans elde etmekte ve 50'den fazla client'a verimli bir şekilde ölçeklenmektedir. Bu, iç tehdit tespiti için AGI reasoning'in federated graph learning ile ilk entegrasyonunu temsil etmekte ve gizlilik korumalı sınır ötesi istihbarat paylaşımı için yeni yönler açmaktadır."
    }
  },
  {
    "id": "2602.16098v1",
    "title": "Collaborative Zone-Adaptive Zero-Day Intrusion Detection for IoBT",
    "authors": [
      "Amirmohammad Pasdar",
      "Shabnam Kasra Kermanshahi",
      "Nour Moustafa",
      "Van-Thuan Pham"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.16098v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16098v1",
    "content": {
      "en": "The Internet of Battlefield Things (IoBT) relies on heterogeneous, bandwidth-constrained, and intermittently connected tactical networks that face rapidly evolving cyber threats. In this setting, intrusion detection cannot depend on continuous central collection of raw traffic due to disrupted links, latency, operational security limits, and non-IID traffic across zones. We present Zone-Adaptive Intrusion Detection (ZAID), a collaborative detection and model-improvement framework for unseen attack types, where \"zero-day\" refers to previously unobserved attack families and behaviours (not vulnerability disclosure timing). ZAID combines a universal convolutional model for generalisable traffic representations, an autoencoder-based reconstruction signal as an auxiliary anomaly score, and lightweight adapter modules for parameter-efficient zone adaptation. To support cross-zone generalisation under constrained connectivity, ZAID uses federated aggregation and pseudo-labelling to leverage locally observed, weakly labelled behaviours. We evaluate ZAID on ToN_IoT using a zero-day protocol that excludes MITM, DDoS, and DoS from supervised training and introduces them during zone-level deployment and adaptation. ZAID achieves up to 83.16% accuracy on unseen attack traffic and transfers to UNSW-NB15 under the same procedure, with a best accuracy of 71.64%. These results indicate that parameter-efficient, zone-personalised collaboration can improve the detection of previously unseen attacks in contested IoBT environments.",
      "tr": "Kesinlikle, istediğiniz çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** IoBT için İşbirlikçi Bölge-Adaptif Sıfır-Gün Saldırı Tespiti\n\n**Özet:**\nThe Internet of Battlefield Things (IoBT), heterogeneous, bandwidth-constrained, and intermittently connected tactical networks' üzerine kuruludur ve hızla gelişen siber tehditlerle karşı karşıyadır. Bu ortamda, kesintili bağlantılar, gecikme, operasyonel güvenlik sınırlamaları ve bölgeler arası non-IID traffic nedeniyle, saldırı tespiti ham trafiğin sürekli merkezi toplanmasına güvenemez. Biz, daha önce gözlemlenmemiş saldırı aileleri ve davranışları için \"sıfır-gün\" anlamına gelen, görünmeyen saldırı türleri için işbirlikçi bir tespit ve model iyileştirme çerçevesi olan Zone-Adaptive Intrusion Detection (ZAID)'i sunuyoruz. ZAID, genelleştirilebilir traffic temsilleri için evrensel bir convolutional model, yardımcı bir anomali skoru olarak autoencoder tabanlı bir reconstruction signal ve parametre-etkin bölge adaptasyonu için hafif adapter modüllerini birleştirir. Kısıtlı bağlantı altında bölge üstü genelleştirmeyi desteklemek için ZAID, yerel olarak gözlemlenen, zayıf etiketlenmiş davranışlardan yararlanmak üzere federated aggregation ve pseudo-labelling kullanır. Biz ZAID'i, supervised training'den MITM, DDoS ve DoS'u hariç tutan ve bunları bölge düzeyinde dağıtım ve adaptasyon sırasında tanıtan bir sıfır-gün protokolü kullanarak ToN_IoT üzerinde değerlendiriyoruz. ZAID, daha önce görülmemiş saldırı trafiğinde %83,16'ya kadar doğruluk elde eder ve aynı prosedür altında UNSW-NB15'e %71,64'lük en iyi doğruluk ile aktarılır. Bu sonuçlar, parametre-etkin, bölgeye özel kişiselleştirilmiş işbirliğinin, tartışmalı IoBT ortamlarında daha önce görülmemiş saldırıların tespitini iyileştirebileceğini göstermektedir."
    }
  },
  {
    "id": "2602.15945v1",
    "title": "From Tool Orchestration to Code Execution: A Study of MCP Design Choices",
    "authors": [
      "Yuval Felendler",
      "Parth A. Gandhi",
      "Idan Habler",
      "Yuval Elovici",
      "Asaf Shabtai"
    ],
    "published_date": "2026-02-17",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.15945v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15945v1",
    "content": {
      "en": "Model Context Protocols (MCPs) provide a unified platform for agent systems to discover, select, and orchestrate tools across heterogeneous execution environments. As MCP-based systems scale to incorporate larger tool catalogs and multiple concurrently connected MCP servers, traditional tool-by-tool invocation increases coordination overhead, fragments state management, and limits support for wide-context operations. To address these scalability challenges, recent MCP designs have incorporated code execution as a first-class capability, an approach called Code Execution MCP (CE-MCP). This enables agents to consolidate complex workflows, such as SQL querying, file analysis, and multi-step data transformations, into a single program that executes within an isolated runtime environment. In this work, we formalize the architectural distinction between context-coupled (traditional) and context-decoupled (CE-MCP) models, analyzing their fundamental scalability trade-offs. Using the MCP-Bench framework across 10 representative servers, we empirically evaluate task behavior, tool utilization patterns, execution latency, and protocol efficiency as the scale of connected MCP servers and available tools increases, demonstrating that while CE-MCP significantly reduces token usage and execution latency, it introduces a vastly expanded attack surface. We address this security gap by applying the MAESTRO framework, identifying sixteen attack classes across five execution phases-including specific code execution threats such as exception-mediated code injection and unsafe capability synthesis. We validate these vulnerabilities through adversarial scenarios across multiple LLMs and propose a layered defense architecture comprising containerized sandboxing and semantic gating. Our findings provide a rigorous roadmap for balancing scalability and security in production-ready executable agent workflows.",
      "tr": "**Makale Başlığı:** Araç Orkestrasyonundan Kod Yürütmeye: MCP Tasarım Seçimleri Üzerine Bir Çalışma\n\n**Özet:**\n\nModel Context Protocols (MCPs), aracıl sistemlerin heterojen yürütme ortamları genelinde araçları keşfetmesi, seçmesi ve orkestre etmesi için birleşik bir platform sunmaktadır. MCP tabanlı sistemler, daha büyük araç kataloglarını ve eşzamanlı olarak bağlı birden fazla MCP sunucusunu içerecek şekilde ölçeklendikçe, geleneksel araçtan araca çağırma, koordinasyon yükünü artırır, durum yönetimini parçalar ve geniş bağlam operasyonları için desteği sınırlar. Bu ölçeklenebilirlik zorluklarını ele almak için, yakın zamandaki MCP tasarımları, Kod Yürütme MCP (CE-MCP) olarak adlandırılan bir yaklaşım olan birinci sınıf bir yetenek olarak kod yürütmeyi içermiştir. Bu, aracılar tarafından SQL sorgulama, dosya analizi ve çok adımlı veri dönüşümleri gibi karmaşık iş akışlarının, izole bir runtime ortamında yürütülen tek bir programa konsolide edilmesini sağlar. Bu çalışmada, bağlam-bağlı (geleneksel) ve bağlam-bağımsız (CE-MCP) modeller arasındaki mimari ayrımı resmi olarak belirlemekte ve bunların temel ölçeklenebilirlik ödünleşimlerini analiz etmekteyiz. MCP-Bench framework'ünü 10 temsili sunucu üzerinde kullanarak, bağlı MCP sunucularının ve mevcut araçların ölçeği arttıkça görev davranışını, araç kullanım modellerini, yürütme gecikmesini ve protokol verimliliğini ampirik olarak değerlendirmekteyiz. CE-MCP'nin belirgin şekilde token kullanımını ve yürütme gecikmesini azaltırken, çok daha geniş bir saldırı yüzeyi sunduğunu göstermekteyiz. Bu güvenlik açığını, MAESTRO framework'ünü uygulayarak ele almakta, istisna aracılı kod enjeksiyonu ve güvensiz yetenek sentezi gibi belirli kod yürütme tehditleri de dahil olmak üzere beş yürütme aşaması boyunca on altı saldırı sınıfını belirlemekteyiz. Bu zafiyetleri, birden fazla LLM üzerinde düşmanca senaryolarla doğrulamakta ve konteynerize sandboxing ve semantic gating'den oluşan katmanlı bir savunma mimarisi önermekteyiz. Bulgularımız, üretim ortamına hazır yürütülebilir aracıl iş akışlarında ölçeklenebilirlik ve güvenliği dengelemek için titiz bir yol haritası sunmaktadır."
    }
  },
  {
    "id": "2602.15756v1",
    "title": "A Note on Non-Composability of Layerwise Approximate Verification for Neural Inference",
    "authors": [
      "Or Zamir"
    ],
    "published_date": "2026-02-17",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.15756v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15756v1",
    "content": {
      "en": "A natural and informal approach to verifiable (or zero-knowledge) ML inference over floating-point data is: ``prove that each layer was computed correctly up to tolerance $δ$; therefore the final output is a reasonable inference result''. This short note gives a simple counterexample showing that this inference is false in general: for any neural network, we can construct a functionally equivalent network for which adversarially chosen approximation-magnitude errors in individual layer computations suffice to steer the final output arbitrarily (within a prescribed bounded range).",
      "tr": "**Makale Başlığı:** Bir Sinirsel Çıkarım İçin Katman Bazlı Yaklaşık Doğrulamanın Kompoze Edilemezliğine Dair Bir Not\n\n**Özet:**\n\nFloat veri üzerinde doğrulanabilir (veya zero-knowledge) Makine Öğrenmesi çıkarımı için doğal ve gayri resmi bir yaklaşım şöyledir: \"Her bir katmanın belirli bir tolerans $δ$ 'ya kadar doğru hesaplandığını kanıtla; bu nedenle nihai çıktı makul bir çıkarım sonucudur.\" Bu kısa not, bu çıkarımın genel olarak yanlış olduğunu gösteren basit bir karşı örnek sunmaktadır: her hangi bir sinirsel ağ için, bireysel katman hesaplamalarındaki düşmanca seçilmiş yaklaşım büyüklüğü hatalarının nihai çıktıyı keyfi olarak yönlendirmeye (belirlenmiş sınırlı bir aralık içinde) yettiği işlevsel olarak eşdeğer bir ağ oluşturabiliriz."
    }
  }
]