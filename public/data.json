[
  {
    "id": "2602.00979v1",
    "title": "GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability",
    "authors": [
      "Xueyi Li",
      "Zhuoneng Zhou",
      "Zitao Liu",
      "Yongdong Wu",
      "Weiqi Luo"
    ],
    "published_date": "2026-02-01",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2602.00979v1",
    "pdf_link": "https://arxiv.org/pdf/2602.00979v1",
    "content": {
      "en": "Large language models (LLMs) have demonstrated remarkable potential for automatic short answer grading (ASAG), significantly boosting student assessment efficiency and scalability in educational scenarios. However, their vulnerability to adversarial manipulation raises critical concerns about automatic grading fairness and reliability. In this paper, we introduce GradingAttack, a fine-grained adversarial attack framework that systematically evaluates the vulnerability of LLM based ASAG models. Specifically, we align general-purpose attack methods with the specific objectives of ASAG by designing token-level and prompt-level strategies that manipulate grading outcomes while maintaining high camouflage. Furthermore, to quantify attack camouflage, we propose a novel evaluation metric that balances attack success and camouflage. Experiments on multiple datasets demonstrate that both attack strategies effectively mislead grading models, with prompt-level attacks achieving higher success rates and token-level attacks exhibiting superior camouflage capability. Our findings underscore the need for robust defenses to ensure fairness and reliability in ASAG. Our code and datasets are available at https://anonymous.4open.science/r/GradingAttack.",
      "tr": "Makale Başlığı: GradingAttack: Kısa Cevap Derecelendirme Yeteneğine Yönelik Büyük Dil Modellerine Saldırı\n\nÖzet:\nBüyük Dil Modelleri (LLMs), öğrenci değerlendirme verimliliğini ve eğitim senaryolarındaki ölçeklenebilirliği önemli ölçüde artırarak otomatik kısa cevap derecelendirme (ASAG) için dikkate değer bir potansiyel sergilemiştir. Ancak, düşmanca manipülasyona karşı olan kırılganlıkları, otomatik derecelendirme adaleti ve güvenilirliği hakkında kritik endişeler doğurmaktadır. Bu makalede, LLM tabanlı ASAG modellerinin kırılganlığını sistematik olarak değerlendiren ince taneli bir düşmanca saldırı çerçevesi olan GradingAttack'ı sunuyoruz. Spesifik olarak, yüksek kamuflajı korurken derecelendirme sonuçlarını manipüle eden token-level ve prompt-level stratejiler tasarlayarak genel amaçlı saldırı yöntemlerini ASAG'nin özel hedefleriyle uyumlu hale getiriyoruz. Dahası, saldırı kamuflajını ölçmek için saldırı başarısını ve kamuflajı dengeleyen yeni bir değerlendirme metriği öneriyoruz. Çoklu veri setleri üzerinde yapılan deneyler, her iki saldırı stratejisinin de derecelendirme modellerini etkili bir şekilde yanılttığını göstermektedir. Prompt-level saldırılar daha yüksek başarı oranları elde ederken, token-level saldırılar üstün kamuflaj yeteneği sergilemektedir. Bulgularımız, ASAG'de adaleti ve güvenilirliği sağlamak için sağlam savunma mekanizmalarının gerekliliğini vurgulamaktadır. Kodumuz ve veri setlerimiz https://anonymous.4open.science/r/GradingAttack adresinden temin edilebilir."
    }
  },
  {
    "id": "2602.00750v1",
    "title": "Bypassing Prompt Injection Detectors through Evasive Injections",
    "authors": [
      "Md Jahedur Rahman",
      "Ihsen Alouani"
    ],
    "published_date": "2026-01-31",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.00750v1",
    "pdf_link": "https://arxiv.org/pdf/2602.00750v1",
    "content": {
      "en": "Large language models (LLMs) are increasingly used in interactive and retrieval-augmented systems, but they remain vulnerable to task drift; deviations from a user's intended instruction due to injected secondary prompts. Recent work has shown that linear probes trained on activation deltas of LLMs' hidden layers can effectively detect such drift. In this paper, we evaluate the robustness of these detectors against adversarially optimised suffixes. We generate universal suffixes that cause poisoned inputs to evade detection across multiple probes simultaneously. Our experiments on Phi-3 3.8B and Llama-3 8B show that a single suffix can achieve high attack success rates; up to 93.91% and 99.63%, respectively, when all probes must be fooled, and nearly perfect success (>90%) under majority vote setting. These results demonstrate that activation delta-based task drift detectors are highly vulnerable to adversarial suffixes, highlighting the need for stronger defences against adaptive attacks. We also propose a defence technique where we generate multiple suffixes and randomly append one of them to the prompts while making forward passes of the LLM and train logistic regression models with these activations. We found this approach to be highly effective against such attacks.",
      "tr": "İşte makalenin başlığı ve özetinin Türkçe çevirisi, istenen teknik terimler İngilizce bırakılarak ve akademik bir dil kullanılarak:\n\n**Makale Başlığı:** Bypassing Prompt Injection Detectors through Evasive Injections\n\n**Özet:**\n\nBüyük dil modelleri (LLM'ler), interaktif ve retrieval-augmented sistemlerde giderek daha fazla kullanılmaktadır, ancak görev kayması (task drift); yani enjekte edilen ikincil komutlar (secondary prompts) nedeniyle kullanıcı tarafından amaçlanan talimatlardan sapmalar konusunda hala savunmasızdırlar. Son çalışmalar, LLM'lerin gizli katmanlarının (hidden layers) aktivasyon farklarına (activation deltas) dayalı eğitilen doğrusal problerin (linear probes) bu tür sapmaları etkili bir şekilde tespit edebildiğini göstermiştir. Bu makalede, bu tespit cihazlarının (detectors) düşmanca optimize edilmiş son eklerle (adversarially optimised suffixes) karşılaştırılan sağlamlığını değerlendiriyoruz. Birden fazla probu eş zamanlı olarak tespit edilmekten kurtulmayı sağlayan evrensel son ekler (universal suffixes) üretiyoruz. Phi-3 3.8B ve Llama-3 8B üzerinde gerçekleştirdiğimiz deneyler, tek bir son ekin yüksek saldırı başarı oranları elde edebildiğini göstermektedir; tüm probların kandırılması gerektiğinde sırasıyla %93,91 ve %99,63'e kadar, çoğunluk oyu (majority vote) ayarında ise neredeyse mükemmel bir başarı (%90'ın üzeri) elde etmektedir. Bu sonuçlar, aktivasyon farkına dayalı görev kayması tespit cihazlarının düşmanca son eklere karşı oldukça savunmasız olduğunu ve uyarlanabilir saldırılara (adaptive attacks) karşı daha güçlü savunmalara duyulan ihtiyacı vurgulamaktadır. Ayrıca, birden fazla son ek ürettiğimiz ve LLM'nin ileri geçişleri (forward passes) sırasında bunlardan birini rastgele komutlara eklediğimiz ve bu aktivasyonlarla lojistik regresyon modelleri (logistic regression models) eğittiğimiz bir savunma tekniği öneriyoruz. Bu yaklaşımın bu tür saldırılara karşı oldukça etkili olduğunu bulduk."
    }
  },
  {
    "id": "2602.00711v1",
    "title": "From Detection to Prevention: Explaining Security-Critical Code to Avoid Vulnerabilities",
    "authors": [
      "Ranjith Krishnamurthy",
      "Oshando Johnson",
      "Goran Piskachev",
      "Eric Bodden"
    ],
    "published_date": "2026-01-31",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2602.00711v1",
    "pdf_link": "https://arxiv.org/pdf/2602.00711v1",
    "content": {
      "en": "Security vulnerabilities often arise unintentionally during development due to a lack of security expertise and code complexity. Traditional tools, such as static and dynamic analysis, detect vulnerabilities only after they are introduced in code, leading to costly remediation. This work explores a proactive strategy to prevent vulnerabilities by highlighting code regions that implement security-critical functionality -- such as data access, authentication, and input handling -- and providing guidance for their secure implementation. We present an IntelliJ IDEA plugin prototype that uses code-level software metrics to identify potentially security-critical methods and large language models (LLMs) to generate prevention-oriented explanations. Our initial evaluation on the Spring-PetClinic application shows that the selected metrics identify most known security-critical methods, while an LLM provides actionable, prevention-focused insights. Although these metrics capture structural properties rather than semantic aspects of security, this work lays the foundation for code-level security-aware metrics and enhanced explanations.",
      "tr": "Elbette, işte makale başlığının ve özetinin istenen şekilde Türkçeye çevirisi:\n\n**Makale Başlığı:** Tespitten Önlemeye: Güvenlik Açıklarını Önlemek İçin Güvenlik-Kritik Kodları Açıklama\n\n**Özet:**\n\nGüvenlik açıkları, geliştirme sırasında güvenlik uzmanlığı eksikliği ve kod karmaşıklığı nedeniyle sıklıkla istemsizce ortaya çıkar. Statik ve dinamik analiz gibi geleneksel araçlar, güvenlik açıklarını yalnızca koda eklendikten sonra tespit eder, bu da maliyetli düzeltmelere yol açar. Bu çalışma, güvenlik-kritik işlevleri (veri erişimi, kimlik doğrulama ve girdi işleme gibi) uygulayan kod bölgelerini vurgulayarak ve bunların güvenli uygulaması için rehberlik sağlayarak güvenlik açıklarını proaktif olarak önlemeye yönelik bir stratejiyi incelemektedir. Potansiyel olarak güvenlik-kritik yöntemleri belirlemek için kod düzeyinde yazılım metriklerini ve önleme odaklı açıklamalar üretmek için large language models (LLMs) kullanan bir IntelliJ IDEA eklenti prototipi sunuyoruz. Spring-PetClinic uygulaması üzerindeki ilk değerlendirmemiz, seçilen metriklerin bilinen güvenlik-kritik yöntemlerin çoğunu tespit ettiğini, LLM'nin ise eyleme geçirilebilir, önleme odaklı içgörüler sağladığını göstermektedir. Bu metrikler, güvenlik açıklarının anlamsal yönlerinden ziyade yapısal özellikleri yakalasa da, bu çalışma kod düzeyinde güvenlik farkındalığı olan metrikler ve geliştirilmiş açıklamalar için temel oluşturmaktadır."
    }
  },
  {
    "id": "2602.00619v1",
    "title": "Jailbreaking LLMs via Calibration",
    "authors": [
      "Yuxuan Lu",
      "Yongkang Guo",
      "Yuqing Kong"
    ],
    "published_date": "2026-01-31",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.00619v1",
    "pdf_link": "https://arxiv.org/pdf/2602.00619v1",
    "content": {
      "en": "Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower \"Jailbreak Tax\" compared with existing methods, especially on the safety-hardened gpt-oss-120b.",
      "tr": "Makale Başlığı: Kalibrasyon Yoluyla LLM'lerin Jailbreaking'i\n\nÖzet:\nBüyük Dil Modellerinde (LLM'ler) güvenlik uyumu (safety alignment), genellikle modelin uyumlu çıktısı ile önceden uyumlanmış veri dağılımı arasında sistematik bir tutarsızlık yaratır. Bu çalışmada, güvenlik uyumunun bir sonraki-token tahminine olan etkisini, ön-uyum dağılımının sistematik bir çarpıtması olarak modelleyen bir framework öneriyoruz. Weak-to-Strong Jailbreaking'i bir tahmin toplama problemi (forecast aggregation problem) olarak ele alıyoruz ve kayıp fonksiyonundan kaynaklanan ikili uzayda (dual space) bir Gradient Shift ile karakterize edilen optimal bir toplama stratejisi türetiyoruz. Logit-aritmetik jailbreaking yöntemlerinin, çapraz-entropi kaybı (cross-entropy loss) altında bu framework'ün özel bir durumu olduğunu gösteriyor ve diğer uygun kayıp fonksiyonlarına (proper losses) karşılık gelen daha geniş bir toplama kuralları ailesi türetiyoruz. Ayrıca yeni bir hibrit toplama kuralı da öneriyoruz. Sınır modelleri (frontier models) kullanarak kırmızı-takım benchmark'ları (red-teaming benchmarks) ve matematiksel fayda görevleri (math utility tasks) üzerindeki değerlendirmeler, yaklaşımımızın mevcut yöntemlere kıyasla, özellikle güvenlik açısından güçlendirilmiş gpt-oss-120b üzerinde, üstün Saldırı Başarı Oranları (Attack Success Rates) ve daha düşük \"Jailbreak Tax\" elde ettiğini göstermektedir."
    }
  },
  {
    "id": "2602.00446v1",
    "title": "Towards Building Non-Fine-Tunable Foundation Models",
    "authors": [
      "Ziyao Wang",
      "Nizhang Li",
      "Pingzhi Li",
      "Guoheng Sun",
      "Tianlong Chen"
    ],
    "published_date": "2026-01-31",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.00446v1",
    "pdf_link": "https://arxiv.org/pdf/2602.00446v1",
    "content": {
      "en": "Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.",
      "tr": "**Makale Başlığı:** Non-Fine-Tunable Foundation Models Oluşturmaya Doğru\n\n**Özet:**\n\nFoundation models'un (FM'ler) açık kaynaklı olarak yayımlanması geniş çaplı yeniden kullanıma olanak tanırken, aynı zamanda model eğiticilerini kısıtlanmamış aşağı akış fine-tuning'inden kaynaklanan ekonomik ve güvenlik risklerine maruz bırakmaktadır. Bu problemi, non-fine-tunable foundation models oluşturarak ele alıyoruz; bu modeller, yayımlandıkları haliyle geniş ölçüde kullanılabilir kalırken, görev-bağımsız yetkisiz fine-tuning altında sınırlı adaptasyon kazançları sunmaktadır. Eğitim sürecinin erken aşamalarında tanımlanan seyrek bir alt ağa representation learning'i yoğunlaştıran Private Mask Pre-Training (PMP) adında bir pre-training framework'ü öneriyoruz. Bu alt ağı tanımlayan ikili maske gizli tutulmakta ve yalnızca nihai dense weights yayımlanmaktadır. Bu durum, maskeye erişim olmadan gerçekleştirilen yetkisiz fine-tuning'i, pre-training subspace ile uyumsuz parametreleri güncellemeye zorlamakta, böylece fine-tuning hedefi ile pre-training geometry arasında içsel bir uyumsuzluk yaratmaktadır. Bu uyumsuzluğun gradient-based adaptation'ı istikrarsızlaştırdığını ve fine-tuning kazançlarını sınırladığını gösteren teorik analizler sunuyoruz. Büyük dil modelleri üzerindeki ampirik sonuçlar, PMP'nin temel model performansını korurken, mask ratio tarafından kontrol edilen non-fine-tunability'nin gücüyle geniş bir yelpazedeki aşağı akış görevlerinde yetkisiz fine-tuning'i tutarlı bir şekilde düşürdüğünü göstermektedir."
    }
  },
  {
    "id": "2602.00428v1",
    "title": "When Agents \"Misremember\" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems",
    "authors": [
      "Naen Xu",
      "Hengyu An",
      "Shuo Shi",
      "Jinghuai Zhang",
      "Chunyi Zhou"
    ],
    "published_date": "2026-01-31",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.00428v1",
    "pdf_link": "https://arxiv.org/pdf/2602.00428v1",
    "content": {
      "en": "Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.",
      "tr": "**Makale Başlığı:** Ajanlar Kolektif Olarak \"Yanlış Hatırladığında\": LLM Tabanlı Çoklu-Ajan Sistemlerde Mandela Etkisini Keşfetmek\n\n**Özet:**\nBüyük dil modellerindeki (LLMs) son gelişmeler, işbirlikçi çoklu-ajan sistemlerinin karmaşık zorlukları ele alma yeteneklerini önemli ölçüde artırmıştır. Ancak, bu çoklu-ajan sistemleri içerisinde, ajanların kolektif bilişsel önyargılara karşı yatkınlığı henüz yeterince araştırılmamış bir konudur. Çarpıcı bir örnek, toplumsal etki ve içselleştirilmiş misinformation yoluyla pekiştirilen yanlış detaylar sonucunda grupların geçmiş olayları kolektif olarak yanlış hatırlaması olarak tanımlanan Mandela effect'tir. Bu hassasiyet, çoklu-ajan sistemlerindeki hafıza önyargısına ilişkin anlayışımızı sınırlamakta ve misinformation'ın potansiyel yayılması hakkında etik kaygılar doğurmaktadır. Bu çalışmada, LLM tabanlı çoklu-ajan sistemlerinde Mandela effect üzerine kapsamlı bir inceleme gerçekleştirerek, varlığını, neden olan faktörleri ve azaltma stratejilerini ele alıyoruz. Mandela effect'e yatkın dört yaygın görev türünde ajan davranışlarını değerlendirmek üzere tasarlanmış, ajan rollerini ve memory timescales'ları farklılaştıran beş etkileşim protokolü kullanan yeni bir benchmark olan MANBENCH'i öneriyoruz. MANBENCH üzerinde çeşitli LLM'ler tarafından desteklenen ajanları değerlendirerek Mandela effect'i ölçüyor ve farklı faktörlerin bunu nasıl etkilediğini analiz ediyoruz. Dahası, bu etkiyi azaltmaya yönelik, prompt-level defenses (örneğin, cognitive anchoring ve source scrutiny) ve model-level alignment-based defense dahil olmak üzere stratejiler öneriyoruz ve temel duruma kıyasla ortalama %74,40 oranında bir Mandela effect azalması sağlıyoruz. Bulgularımız, daha dirençli ve etik olarak uyumlu işbirlikçi çoklu-ajan sistemleri geliştirmek için değerli içgörüler sunmaktadır."
    }
  },
  {
    "id": "2602.00420v1",
    "title": "Text is All You Need for Vision-Language Model Jailbreaking",
    "authors": [
      "Yihang Chen",
      "Zhao Xu",
      "Youyuan Jiang",
      "Tianle Zheng",
      "Cho-Jui Hsieh"
    ],
    "published_date": "2026-01-31",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.00420v1",
    "pdf_link": "https://arxiv.org/pdf/2602.00420v1",
    "content": {
      "en": "Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.",
      "tr": "**Makale Başlığı:** Vision-Language Model Jailbreaking İçin Metin Her Şeydir\n\n**Özet:**\n\nBüyük Vision-Language Modelleri (LVLMs), zararlı veya izin verilmeyen komutlara yanıt vermeyi önlemek için giderek artan oranda sağlam güvenlik önlemleri ile donatılmaktadır. Ancak bu savunmalar genellikle açık metin girdilerini veya ilgili görsel sahneleri analiz etmeye odaklanır. Bu çalışmada, modelin Optical Character Recognition (OCR) yeteneğinden faydalanarak bu güvenlik önlemlerini aşan yeni bir jailbreak saldırısı olan Text-DJ'i tanıtıyoruz. Metodolojimiz üç aşamadan oluşmaktadır. İlk olarak, tek bir zararlı sorguyu, anlamsal olarak ilişkili ancak daha zararsız birden fazla alt sorguya ayırırız. İkinci olarak, zararlı sorguyla maksimum ilgisizliği olan bir dizi dikkat dağıtıcı sorgu seçeriz. Üçüncü olarak, tüm ayrıştırılmış alt sorguları ve dikkat dağıtıcı sorguları, alt sorguların konumunun ızgaranın ortasında olması koşuluyla, LVLM'ye eşzamanlı olarak bir resim ızgarası olarak sunarız. Bu yöntemin en gelişmiş LVLMs'lerin güvenlik hizalamasını başarıyla aştığını gösteriyoruz. Bu saldırının başarılı olduğunu, (1) metin tabanlı komutları görüntülere dönüştürerek standart metin tabanlı filtreleri atlatması ve (2) dikkat dağıtma yoluyla, modelin güvenlik protokollerinin yüksek sayıda ilgisiz sorgu içindeki dağılmış alt sorguları birbirine bağlamakta başarısız olmasıyla açıklıyoruz. Genel olarak, bulgularımız, LVLMs'lerin dağılmış, çoklu-görsel saldırı girdilerine karşı dayanıklı olmayan OCR yeteneklerinde kritik bir zafiyet ortaya koymakta ve parçalanmış multimodal girdilere yönelik savunma ihtiyacını vurgulamaktadır."
    }
  },
  {
    "id": "2602.00318v1",
    "title": "Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection",
    "authors": [
      "Kunal Mukherjee",
      "Zulfikar Alom",
      "Tran Gia Bao Ngo",
      "Cuneyt Gurcan Akcora",
      "Murat Kantarcioglu"
    ],
    "published_date": "2026-01-30",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.00318v1",
    "pdf_link": "https://arxiv.org/pdf/2602.00318v1",
    "content": {
      "en": "The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.   To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.",
      "tr": "**Akademik Makale Başlığı Çevirisi:**\n\n**Graph Neural Network Tabanlı Bot Tespiti Üzerine Optimal Transport Güdümlü Düşmanca Saldırılar**\n\n**Özet Çevirisi:**\n\nSosyal medyada bot hesapların yaygınlaşması, kamusal söylem için önemli riskler taşımaktadır. Bu tehdide karşı, modern bot tespit sistemleri giderek daha fazla Graph Neural Network (GNN) üzerine kurulmaktadır. Ancak, bu GNN tabanlı tespit sistemlerinin gerçek dünya ortamlarındaki etkinliği hakkında anlayışımız sınırlıdır. Pratikte, saldırganlar stratejilerini sürekli olarak adapte etmek zorundadır ve alan-özgü ve zamansal kısıtlamalar altında çalışmalıdır, bu da mevcut saldırı yöntemlerinin uygulanabilirliğini temelden sınırlayabilir. Sonuç olarak, gerçekçi, kısıt-farkındalığı olan saldırı senaryoları altında sağlam GNN tabanlı bot tespit yöntemlerine acil bir ihtiyaç duyulmaktadır. Bu boşluğu gidermek amacıyla, gerçekçi kısıtlamalar altında hem edge editing hem de node injection düşmanca saldırılar aracılığıyla GNN tabanlı sosyal bot tespitinin sağlamlığını sistematik olarak değerlendirmek için BOCLOAK'u tanıtıyoruz. BOCLOAK, uzamsal-zamansal komşu özellikleri üzerinde bir olasılık ölçüsü oluşturur ve insan ile bot davranışlarını ayıran bir optimal transport geometrisi öğrenir. Ardından, gerçek dünya kısıtlamalarına uyarken tespitten kaçan seyrek, makul edge editlerine transport planlarını deşifre eder. BOCLOAK'u üç sosyal bot veri kümesi, beş son teknoloji bot tespit sistemi, üç düşmanca savunma üzerinde değerlendiriyoruz ve dört önde gelen graf düşmanca saldırı temel çizgisine karşı karşılaştırıyoruz. BOCLOAK, gerçekçi gerçek dünya kısıtlamaları altında %99.80 daha az GPU belleği kullanırken, %80.13'e kadar daha yüksek saldırı başarı oranları elde etmektedir. En önemlisi, BOCLOAK, optimal transport'un düşmanca saldırılar ile gerçek dünya bot tespiti arasındaki boşluğu kapatmak için hafif, prensipli bir çerçeve sağladığını göstermektedir."
    }
  },
  {
    "id": "2602.00305v1",
    "title": "Semantics-Preserving Evasion of LLM Vulnerability Detectors",
    "authors": [
      "Luze Sun",
      "Alina Oprea",
      "Eric Wong"
    ],
    "published_date": "2026-01-30",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.00305v1",
    "pdf_link": "https://arxiv.org/pdf/2602.00305v1",
    "content": {
      "en": "LLM-based vulnerability detectors are increasingly deployed in security-critical code review, yet their resilience to evasion under behavior-preserving edits remains poorly understood. We evaluate detection-time integrity under a semantics-preserving threat model by instantiating diverse behavior-preserving code transformations on a unified C/C++ benchmark (N=5000), and introduce a metric of joint robustness across different attack methods/carriers. Across models, we observe a systemic failure of semantic invariant adversarial transformations: even state-of-the-art vulnerability detectors perform well on clean inputs while predictions flip under behavior-equivalent edits. Universal adversarial strings optimized on a single surrogate model remain effective when transferred to black-box APIs, and gradient access can further amplify evasion success. These results show that even high-performing detectors are vulnerable to low-cost, semantics-preserving evasion. Our carrier-based metrics provide practical diagnostics for evaluating LLM-based code detectors.",
      "tr": "**Makale Başlığı:** LLM Zafiyet Dedektörlerinin Semantiği Koruyarak Kaçınılması\n\n**Özet:**\n\nLLM tabanlı zafiyet dedektörleri, güvenlik açısından kritik kod incelemelerinde giderek daha fazla kullanılmaktadır, ancak davranışları koruyan düzenlemeler altında bu dedektörlerin kaçınılmazlığa karşı direnci yeterince anlaşılmamıştır. Semantiği koruyan bir tehdit modeli altında, birleşik bir C/C++ kıyaslamasında (N=5000) çeşitli davranışları koruyan kod dönüşümlerini örnekleyerek tespit anındaki bütünlüğü değerlendiriyor ve farklı saldırı yöntemleri/aktarıcıları boyunca ortak dayanıklılık ölçütünü sunuyoruz. Model genelinde, anlamsal değişmez adversarial dönüşümlerin sistematik bir başarısızlığını gözlemliyoruz: en gelişmiş zafiyet dedektörleri bile temiz girdilerde iyi performans gösterirken, davranışsal olarak eşdeğer düzenlemeler altında tahminler tersine dönmektedir. Tek bir vekil model üzerinde optimize edilmiş evrensel adversarial dizeler, black-box API'lere aktarıldığında etkili kalmakta ve gradient erişimi kaçınma başarısını daha da artırabilmektedir. Bu sonuçlar, yüksek performans gösteren dedektörlerin bile düşük maliyetli, semantiği koruyan kaçınılmalara karşı savunmasız olduğunu göstermektedir. Aktarıcı tabanlı ölçütlerimiz, LLM tabanlı kod dedektörlerini değerlendirmek için pratik teşhisler sunmaktadır."
    }
  },
  {
    "id": "2601.23255v1",
    "title": "Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models",
    "authors": [
      "Ye Yu",
      "Haibo Jin",
      "Yaoning Yu",
      "Jun Zhuang",
      "Haohan Wang"
    ],
    "published_date": "2026-01-30",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.23255v1",
    "pdf_link": "https://arxiv.org/pdf/2601.23255v1",
    "content": {
      "en": "Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.",
      "tr": "**Makale Başlığı:** Now You Hear Me: Large Audio-Language Modellerine Yönelik Sesli Anlatım Saldırıları\n\n**Özet:**\n\nLarge audio-language modeller, sesli asistanlar, eğitim ve klinik triyaj gibi alanlarda daha kesintisiz bir entegrasyon sağlayarak giderek artan bir şekilde ham konuşma girdileri üzerinde çalışmaktadır. Ancak bu geçiş, büyük ölçüde karakterize edilmemiş, belirgin bir zafiyet sınıfı ortaya çıkarmaktadır. Bu çalısma, anlatı tarzı bir ses akışı içine izin verilmeyen direktifleri yerleştiren bir text-to-audio jailbreak tasarlayarak bu modalite kaymasının güvenlik etkilerini incelemektedir. Saldırı, öncelikli olarak metin için kalibre edilmiş güvenlik mekanizmalarını aşmak amacıyla yapısal ve akustik özellikleri istismar etmek için gelişmiş bir instruction-following text-to-speech (TTS) modelinden yararlanmaktadır. Sentetik konuşma yoluyla iletildiğinde, anlatı formatı, Gemini 2.0 Flash dahil olmak üzere en gelişmiş modellerden, metin tabanlı temel yöntemleri önemli ölçüde aşan %98,26'lık bir başarı oranıyla kısıtlanmış çıktılar elde etmektedir. Bu sonuçlar, özellikle konuşma tabanlı arayüzler daha yaygın hale geldikçe, dilsel ve paralinguistik temsiller üzerinde ortaklaşa **reasoning** yapabilen güvenlik çerçevelerine duyulan ihtiyacı vurgulamaktadır."
    }
  }
]