[
  {
    "id": "2601.10440v1",
    "title": "AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior",
    "authors": [
      "Nadya Abaev",
      "Denis Klimov",
      "Gerard Levinov",
      "David Mimran",
      "Yuval Elovici"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.10440v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10440v1",
    "content": {
      "en": "Artificial intelligence (AI) agents are increasingly used in a variety of domains to automate tasks, interact with users, and make decisions based on data inputs. Ensuring that AI agents perform only authorized actions and handle inputs appropriately is essential for maintaining system integrity and preventing misuse. In this study, we introduce the AgentGuardian, a novel security framework that governs and protects AI agent operations by enforcing context-aware access-control policies. During a controlled staging phase, the framework monitors execution traces to learn legitimate agent behaviors and input patterns. From this phase, it derives adaptive policies that regulate tool calls made by the agent, guided by both real-time input context and the control flow dependencies of multi-step agent actions. Evaluation across two real-world AI agent applications demonstrates that AgentGuardian effectively detects malicious or misleading inputs while preserving normal agent functionality. Moreover, its control-flow-based governance mechanism mitigates hallucination-driven errors and other orchestration-level malfunctions.",
      "tr": "**Makale Başlığı:** AgentGuardian: Yapay Zeka Ajanlarının Davranışlarını Yönetmek İçin Erişim Kontrol Politikaları Öğrenme\n\n**Özet:**\n\nYapay zeka (AI) ajanları, görevleri otomatikleştirmek, kullanıcılarla etkileşim kurmak ve veri girdilerine dayalı kararlar almak için çeşitli alanlarda giderek daha fazla kullanılmaktadır. AI ajanlarının yalnızca yetkilendirilmiş eylemleri gerçekleştirmesini ve girdileri uygun şekilde işlemesini sağlamak, sistem bütünlüğünü korumak ve kötüye kullanımı önlemek için esastır. Bu çalışmada, bağlam-farkındalığı olan erişim-kontrol politikalarını uygulayarak AI ajan operasyonlarını yöneten ve koruyan yenilikçi bir güvenlik çerçevesi olan AgentGuardian'ı tanıtıyoruz. Kontrollü bir hazırlık aşaması sırasında çerçeve, meşru ajan davranışlarını ve girdi kalıplarını öğrenmek için yürütme izlerini izler. Bu aşamadan, hem gerçek zamanlı girdi bağlamı hem de çok adımlı ajan eylemlerinin kontrol akışı bağımlılıkları tarafından yönlendirilen, ajan tarafından yapılan tool call'ları düzenleyen uyarlanabilir politikalar türetir. İki gerçek dünya AI ajanı uygulaması üzerinden yapılan değerlendirme, AgentGuardian'ın normal ajan işlevselliğini korurken kötü amaçlı veya yanıltıcı girdileri etkili bir şekilde tespit ettiğini göstermektedir. Dahası, kontrol-akışı tabanlı yönetim mekanizması, halüsinasyon kaynaklı hataları ve diğer orkestrasyon düzeyindeki arızaları azaltır."
    }
  },
  {
    "id": "2601.10413v1",
    "title": "LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies",
    "authors": [
      "Haiyue Yuan",
      "Nikolay Matyunin",
      "Ali Raza",
      "Shujun Li"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.10413v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10413v1",
    "content": {
      "en": "Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.",
      "tr": "İşte akademik makale başlığı ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** LADFA: Gizlilik Politikalarındaki Kişisel Veri Akış Analizi İçin Large Language Models ve Retrieval-Augmented Generation Kullanımına Dayalı Bir Çerçeve\n\n**Özet:**\nGizlilik politikaları, kuruluşların kişisel verileri işleme uygulamaları hakkında insanları bilgilendirmeye yardımcı olur ve veri toplama, veri depolama ve kişisel verilerin üçüncü taraflarla paylaşılması gibi farklı yönleri kapsar. Kullanılan uzun ve karmaşık yasal dil ile farklı sektörler ve kuruluşlar arasındaki tutarsız uygulamalar nedeniyle gizlilik politikalarının tam olarak anlaşılması insanlar için genellikle zordur. Gizlilik politikalarının otomatik ve büyük ölçekli analizlerini yürütmeye yardımcı olmak için birçok araştırmacı, large language models (LLMs) dahil olmak üzere makine öğrenmesi ve doğal dil işleme tekniklerinin uygulamalarını incelemiştir. Sınırlı sayıda önceki çalışma, gizlilik politikalarından kişisel veri akışlarını çıkarmak için LLMs kullanmış olsa da, bizim yaklaşımımız, LLMs'yi retrieval-augmented generation (RAG) ve mevcut çalışmalardan türetilmiş özelleştirilmiş bir knowledge base ile birleştirerek bu çalışma alanını temel almaktadır. Bu makale, LADFA'nın geliştirilmesini sunmaktadır; bu, LADFA'nın, verilen bir gizlilik politikasındaki yapılandırılmamış metni işleyebilen, kişisel veri akışlarını çıkarabilen ve bir personal data flow graph oluşturabilen ve içgörü keşfini kolaylaştırmak için data flow graph analizi yapabilen uçtan uca bir computational framework'tür. Çerçeve, bir pre-processor, bir LLM-based processor ve bir data flow post-processor'dan oluşmaktadır. Otomotiv sektöründen seçilen on gizlilik politikasını inceleyen bir vaka çalışması yürüterek önerilen yaklaşımın etkinliğini ve doğruluğunu gösterdik ve doğruladık. Dahası, LADFA'nın esnek ve özelleştirilebilir olacak şekilde tasarlandığı ve gizlilik politikası analizi dışındaki bir dizi metin tabanlı analiz görevi için uygun olduğu belirtilmeye değerdir."
    }
  },
  {
    "id": "2601.10338v1",
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "authors": [
      "Yi Liu",
      "Weizhe Wang",
      "Ruitao Feng",
      "Yao Zhang",
      "Guangquan Xu"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2601.10338v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10338v1",
    "content": {
      "en": "The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.",
      "tr": "İşte akademik makalenin başlığı ve özetinin teknik terimler hariç tutularak ve resmi bir dille yapılmış Türkçe çevirisi:\n\n**Makale Başlığı:** Kendi Doğal Ortamındaki Ajan Yetenekleri: Büyük Ölçekli Siber Güvenlik Açıklarının Ampirik Bir Çalışması\n\n**Özet:**\nYapay zeka ajan çerçevelerinin yükselişi, ajan yeteneklerini dinamik olarak genişleten, talimatlar ve çalıştırılabilir kod içeren modüler paketler olan agent skills'i beraberinde getirmiştir. Bu mimari güçlü bir özelleştirme sağlarken, skills'ler örtük güven ve minimal inceleme ile yürütülmekte, bu da önemli ancak karakterize edilmemiş bir saldırı yüzeyi oluşturmaktadır. Bu gelişen ekosistemin ilk büyük ölçekli ampirik güvenlik analizini gerçekleştiriyoruz; iki büyük pazar yerinden 42.447 skill topladık ve statik analizi LLM-based semantic classification ile entegre eden çok aşamalı bir tespit çerçevesi olan SkillScan'ı kullanarak 31.132 tanesini sistematik olarak analiz ettik. Bulgularımız yaygın güvenlik risklerini ortaya koymaktadır: skills'lerin %26,1'i, dört kategoriye ayrılan 14 farklı deseni kapsayan en az bir güvenlik açığı içermektedir: prompt injection, veri sızdırma (data exfiltration), yetki yükseltme (privilege escalation) ve tedarik zinciri riskleri. Veri sızdırma (%13,3) ve yetki yükseltme (%11,8) en yaygın olanlardır, oysa skills'lerin %5,2'si kötü niyetli bir amacı güçlü bir şekilde düşündüren yüksek ciddiyette desenler sergilemektedir. Çalıştırılabilir betikler içeren skills'lerin yalnızca talimatlardan oluşan skills'lere göre (%OR=2.12, p<0.001) %2,12 daha fazla güvenlik açığı içerme olasılığı olduğunu bulduk. Katkılarımız şunları içermektedir: (1) 8.126 savunmasız skill'den türetilmiş temellendirilmiş bir güvenlik açığı taksonomisi, (2) %86,7 hassasiyet ve %82,5 recall elde eden doğrulanmış bir tespit metodolojisi ve (3) gelecekteki araştırmaları desteklemek için açık bir veri kümesi ve tespit araç takımı. Bu sonuçlar, bu saldırı vektörü daha fazla istismar edilmeden önce yetenek tabanlı izin sistemlerine ve zorunlu güvenlik incelemesine acil bir ihtiyaç olduğunu göstermektedir."
    }
  },
  {
    "id": "2601.10237v1",
    "title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
    "authors": [
      "Murat Bilgehan Ertan",
      "Marten van Dijk"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.10237v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10237v1",
    "content": {
      "en": "Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy   $σ\\ge \\frac{1}{\\sqrt{2\\ln M}}$ $\\quad\\text{or}\\quad$ $κ\\ge\\ \\frac{1}{\\sqrt{8}}\\!\\left(1-\\frac{1}{\\sqrt{4π\\ln M}}\\right)$,   and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \\to \\infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.",
      "tr": "**Makale Başlığı:** DP-SGD İçin Avantajlı Gizlilik-Fayda Garantilerinin Temel Sınırlılıkları\n\n**Özet:**\n\nDifferentially Private Stochastic Gradient Descent (DP-SGD), özel eğitim için baskın paradigmadır, ancak worst-case adversarial privacy tanımları altındaki temel sınırlılıkları hala tam olarak anlaşılmamıştır. Hipotez testi trade-off eğrileri aracılığıyla gizliliği karakterize eden $f$-differential privacy çerçevesinde DP-SGD'yi analiz ediyor ve $M$ gradyan güncellemesiyle tek bir epoch üzerinde shuffled sampling çalışıyoruz. Elde edilebilir trade-off eğrisi üzerinde açıkça suboptimal bir üst sınır türetiyoruz. Bu sonuç, mekanizmanın trade-off eğrisi ile ideal rastgele tahmin çizgisi arasındaki maksimum mesafe olan ayrım $κ$ üzerinde geometrik bir alt sınır yaratır. Büyük bir ayrım, önemli bir adversarial avantaj anlamına geldiğinden, anlamlı gizlilik küçük $κ$ gerektirir. Ancak, küçük bir ayrımı zorlamanın Gaussian noise multiplier $σ$ üzerinde kesin bir alt sınır getirdiğini kanıtlıyoruz; bu da elde edilebilir faydayı doğrudan sınırlar. Özellikle, standart worst-case adversarial model altında, shuffled DP-SGD şunları karşılamalıdır:   $σ\\ge \\frac{1}{\\sqrt{2\\ln M}}$ $\\quad\\text{veya}\\quad$ $κ\\ge\\ \\frac{1}{\\sqrt{8}}\\!\\left(1-\\frac{1}{\\sqrt{4π\\ln M}}\\right)$,   ve dolayısıyla hem güçlü gizliliği hem de yüksek faydayı aynı anda sağlayamaz. Bu sınır $M \\to \\infty$ olarak asimptotik olarak sıfırlansa da, yakınsama son derece yavaştır: pratik olarak ilgili sayıda güncelleme için bile gerekli gürültü büyüklüğü önemli ölçüde kalır. Ayrıca, aynı sınırlılığın Poisson subsampling için de sabit faktörlere kadar uzandığını gösteriyoruz. Deneysel çalışmalarımız, bu sınırın ima ettiği gürültü seviyelerinin gerçekçi eğitim ayarlarında önemli doğruluk düşüşlerine yol açtığını teyit ederek, standart worst-case adversarial varsayımları altında DP-SGD'de kritik bir darboğaz olduğunu göstermektedir."
    }
  },
  {
    "id": "2601.10212v1",
    "title": "PADER: Paillier-based Secure Decentralized Social Recommendation",
    "authors": [
      "Chaochao Chen",
      "Jiaming Qian",
      "Fei Zheng",
      "Yachuan Liu"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.10212v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10212v1",
    "content": {
      "en": "The prevalence of recommendation systems also brings privacy concerns to both the users and the sellers, as centralized platforms collect as much data as possible from them. To keep the data private, we propose PADER: a Paillier-based secure decentralized social recommendation system. In this system, the users and the sellers are nodes in a decentralized network. The training and inference of the recommendation model are carried out securely in a decentralized manner, without the involvement of a centralized platform. To this end, we apply the Paillier cryptosystem to the SoReg (Social Regularization) model, which exploits both user's ratings and social relations. We view the SoReg model as a two-party secure polynomial evaluation problem and observe that the simple bipartite computation may result in poor efficiency. To improve efficiency, we design secure addition and multiplication protocols to support secure computation on any arithmetic circuit, along with an optimal data packing scheme that is suitable for the polynomial computations of real values. Experiment results show that our method only takes about one second to iterate through one user with hundreds of ratings, and training with ~500K ratings for one epoch only takes <3 hours, which shows that the method is practical in real applications. The code is available at https://github.com/GarminQ/PADER.",
      "tr": "Makale Başlığı: PADER: Paillier Tabanlı Güvenli Dağıtık Sosyal Öneri\n\nÖzet:\nÖneri sistemlerinin yaygınlaşması, hem kullanıcılara hem de satıcılara yönelik gizlilik endişelerini de beraberinde getirmektedir, çünkü merkezi platformlar onlardan mümkün olduğunca çok veri toplar. Verileri gizli tutmak için PADER'ı öneriyoruz: Paillier tabanlı güvenli dağıtık bir sosyal öneri sistemi. Bu sistemde, kullanıcılar ve satıcılar, dağıtık bir ağdaki düğümlerdir. Öneri modelinin eğitimi ve çıkarımı, merkezi bir platformun müdahalesi olmadan güvenli bir şekilde dağıtık bir şekilde gerçekleştirilir. Bu amaçla, Paillier kriptosistemini, hem kullanıcının derecelendirmelerini hem de sosyal ilişkilerini kullanan SoReg (Social Regularization) modeline uygularız. SoReg modelini iki taraflı güvenli polinom değerlendirme problemi olarak ele alıyoruz ve basit iki taraflı hesaplamanın verimsizliğe yol açabileceğini gözlemliyoruz. Verimliliği artırmak için, herhangi bir aritmetik devrede güvenli hesaplamayı desteklemek üzere güvenli toplama ve çarpma protokolleri tasarlıyoruz, ayrıca gerçek değerlerin polinom hesaplamalarına uygun optimize bir veri paketleme şeması da geliştiriyoruz. Deney sonuçları, yöntemimizin yüzlerce derecelendirmeye sahip bir kullanıcı üzerinden iterasyon yapması için yaklaşık bir saniye sürdüğünü ve ~500K derecelendirme ile bir epoch eğitimin yalnızca <3 saat sürdüğünü göstermektedir; bu da yöntemin gerçek uygulamalarda pratik olduğunu ortaya koymaktadır. Kod, https://github.com/GarminQ/PADER adresinde mevcuttur."
    }
  },
  {
    "id": "2601.10173v1",
    "title": "ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack",
    "authors": [
      "Hao Li",
      "Yankai Yang",
      "G. Edward Suh",
      "Ning Zhang",
      "Chaowei Xiao"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2601.10173v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10173v1",
    "content": {
      "en": "Large Language Models (LLMs) have enabled the development of powerful agentic systems capable of automating complex workflows across various fields. However, these systems are highly vulnerable to indirect prompt injection attacks, where malicious instructions embedded in external data can hijack agent behavior. In this work, we present ReasAlign, a model-level solution to improve safety alignment against indirect prompt injection attacks. The core idea of ReasAlign is to incorporate structured reasoning steps to analyze user queries, detect conflicting instructions, and preserve the continuity of the user's intended tasks to defend against indirect injection attacks. To further ensure reasoning logic and accuracy, we introduce a test-time scaling mechanism with a preference-optimized judge model that scores reasoning steps and selects the best trajectory. Comprehensive evaluations across various benchmarks show that ReasAlign maintains utility comparable to an undefended model while consistently outperforming Meta SecAlign, the strongest prior guardrail. On the representative open-ended CyberSecEval2 benchmark, which includes multiple prompt-injected tasks, ReasAlign achieves 94.6% utility and only 3.6% ASR, far surpassing the state-of-the-art defensive model of Meta SecAlign (56.4% utility and 74.4% ASR). These results demonstrate that ReasAlign achieves the best trade-off between security and utility, establishing a robust and practical defense against prompt injection attacks in real-world agentic systems. Our code and experimental results could be found at https://github.com/leolee99/ReasAlign.",
      "tr": "**Makale Başlığı:** ReasAlign: Prompt Injection Saldırısına Karşı Reasoning ile Güçlendirilmiş Güvenlik Hizalaması\n\n**Özet:**\n\nBüyük Dil Modelleri (LLM), çeşitli alanlarda karmaşık iş akışlarını otomatikleştirebilen güçlü agentic sistemlerin geliştirilmesini sağlamıştır. Ancak, bu sistemler dolaylı prompt injection saldırılarına karşı oldukça savunmasızdır; bu saldırılarda harici verilere gömülü kötü niyetli talimatlar agent davranışını ele geçirebilir. Bu çalışmada, dolaylı prompt injection saldırılarına karşı güvenlik hizalamasını iyileştirmek için model düzeyinde bir çözüm olan ReasAlign'i sunuyoruz. ReasAlign'in temel fikri, kullanıcı sorgularını analiz etmek, çakışan talimatları tespit etmek ve dolaylı injection saldırılarına karşı savunmak için kullanıcının amaçladığı görevlerin sürekliliğini korumak üzere yapılandırılmış reasoning adımlarını entegre etmektir. Reasoning mantığını ve doğruluğunu daha da güvence altına almak için, reasoning adımlarını puanlayan ve en iyi trajectory'yi seçen, tercihe göre optimize edilmiş bir judge model ile test-zamanı bir scaling mekanizması sunuyoruz. Çeşitli benchmarklar üzerinde yapılan kapsamlı değerlendirmeler, ReasAlign'in savunmasız bir modelle karşılaştırılabilir düzeyde utility'yi koruduğunu ve en güçlü önceki guardrail olan Meta SecAlign'i sürekli olarak geride bıraktığını göstermektedir. Birden fazla prompt-injected görevi içeren temsili açık uçlu CyberSecEval2 benchmarkında, ReasAlign %94.6 utility ve yalnızca %3.6 ASR elde ederek, Meta SecAlign'in (56.4% utility ve 74.4% ASR) en gelişmiş savunma modelini uzakta geride bırakmıştır. Bu sonuçlar, ReasAlign'in güvenlik ve utility arasında en iyi dengeyi sağladığını ve gerçek dünya agentic sistemlerinde prompt injection saldırılarına karşı sağlam ve pratik bir savunma oluşturduğunu göstermektedir. Kodumuz ve deneysel sonuçlarımız https://github.com/leolee99/ReasAlign adresinde bulunabilir."
    }
  },
  {
    "id": "2601.10004v1",
    "title": "SoK: Privacy-aware LLM in Healthcare: Threat Model, Privacy Techniques, Challenges and Recommendations",
    "authors": [
      "Mohoshin Ara Tahera",
      "Karamveer Singh Sidhu",
      "Shuvalaxmi Dass",
      "Sajal Saha"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.10004v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10004v1",
    "content": {
      "en": "Large Language Models (LLMs) are increasingly adopted in healthcare to support clinical decision-making, summarize electronic health records (EHRs), and enhance patient care. However, this integration introduces significant privacy and security challenges, driven by the sensitivity of clinical data and the high-stakes nature of medical workflows. These risks become even more pronounced across heterogeneous deployment environments, ranging from small on-premise hospital systems to regional health networks, each with unique resource limitations and regulatory demands. This Systematization of Knowledge (SoK) examines the evolving threat landscape across the three core LLM phases: Data preprocessing, Fine-tuning, and Inference within realistic healthcare settings. We present a detailed threat model that characterizes adversaries, capabilities, and attack surfaces at each phase, and we systematize how existing privacy-preserving techniques (PPTs) attempt to mitigate these vulnerabilities. While existing defenses show promise, our analysis identifies persistent limitations in securing sensitive clinical data across diverse operational tiers. We conclude with phase-aware recommendations and future research directions aimed at strengthening privacy guarantees for LLMs in regulated environments. This work provides a foundation for understanding the intersection of LLMs, threats, and privacy in healthcare, offering a roadmap toward more robust and clinically trustworthy AI systems.",
      "tr": "**Makale Başlığı:** SoK: Sağlık Hizmetlerinde Gizlilik Farkındalığına Sahip LLM'ler: Tehdit Modeli, Gizlilik Teknikleri, Zorluklar ve Öneriler\n\n**Özet:**\n\nLarge Language Models (LLMs), klinik karar verme süreçlerini desteklemek, elektronik sağlık kayıtlarını (EHR) özetlemek ve hasta bakımını iyileştirmek amacıyla sağlık hizmetlerinde giderek daha fazla benimsenmektedir. Ancak, bu entegrasyon, klinik verilerin hassasiyeti ve tıbbi iş akışlarının yüksek riskli doğası nedeniyle önemli gizlilik ve güvenlik zorlukları ortaya çıkarmaktadır. Bu riskler, her biri kendine özgü kaynak kısıtlamaları ve düzenleyici talepleri olan küçük şirket içi hastane sistemlerinden bölgesel sağlık ağlarına kadar uzanan heterojen dağıtım ortamlarında daha da belirgin hale gelmektedir. Bu Systematization of Knowledge (SoK), gerçekçi sağlık ortamlarında veri ön işleme, Fine-tuning ve Inference olmak üzere üç temel LLM aşaması boyunca gelişen tehdit manzarasını incelemektedir. Her aşamada saldırganları, yetenekleri ve saldırı yüzeylerini karakterize eden ayrıntılı bir tehdit modeli sunuyoruz ve mevcut gizlilik koruyucu tekniklerin (PPTs) bu güvenlik açıklarını nasıl azaltmaya çalıştığını sistematik hale getiriyoruz. Mevcut savunmalar umut verici görünse de, analizimiz çeşitli operasyonel katmanlarda hassas klinik verilerin güvenliğini sağlamada kalıcı sınırlamaları ortaya koymaktadır. Düzenlenmiş ortamlarda LLM'ler için gizlilik güvencelerini güçlendirmeyi amaçlayan aşamaya duyarlı öneriler ve gelecek araştırma yönleriyle sonuçlandırıyoruz. Bu çalışma, LLM'lerin, tehditlerin ve sağlık hizmetlerinde gizliliğin kesişimini anlamak için bir temel oluşturarak, daha sağlam ve klinik olarak güvenilir Yapay Zeka sistemlerine doğru bir yol haritası sunmaktadır."
    }
  },
  {
    "id": "2601.09946v1",
    "title": "Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains",
    "authors": [
      "Chenxi Qiu"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.09946v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09946v1",
    "content": {
      "en": "Metric Differential Privacy (mDP) generalizes Local Differential Privacy (LDP) by adapting privacy guarantees based on pairwise distances, enabling context-aware protection and improved utility. While existing optimization-based methods reduce utility loss effectively in coarse-grained domains, optimizing mDP in fine-grained or continuous settings remains challenging due to the computational cost of constructing dense perterubation matrices and satisfying pointwise constraints.   In this paper, we propose an interpolation-based framework for optimizing lp-norm mDP in such domains. Our approach optimizes perturbation distributions at a sparse set of anchor points and interpolates distributions at non-anchor locations via log-convex combinations, which provably preserve mDP. To address privacy violations caused by naive interpolation in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms. in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms.",
      "tr": "Elbette, istenen çeviri aşağıdadır:\n\n**Makale Başlığı: Sürekli ve İnce Taneli Alanlarda lp-Norm Metrik Diferansiyel Gizliliği Sağlamak İçin İnterpolasyon Tabanlı Optimizasyon**\n\n**Özet:**\n\nMetric Differential Privacy (mDP), ikili mesafelere dayalı gizlilik garantilerini uyarlayarak Yerel Diferansiyel Gizliliği (LDP) genelleştirir, bağlam-duyarlı koruma ve gelişmiş fayda sağlar. Mevcut optimizasyon tabanlı yöntemler, kaba taneli alanlarda fayda kaybını etkili bir şekilde azaltırken, mDP'yi ince taneli veya sürekli ayarlarda optimize etmek, yoğun perturbation matrisleri oluşturmanın ve nokta bazlı kısıtlamaları karşılamanın hesaplama maliyeti nedeniyle zorluğunu sürdürmektedir. Bu makalede, bu tür alanlarda lp-norm mDP'yi optimize etmek için interpolasyon tabanlı bir çerçeve öneriyoruz. Yaklaşımımız, seyrek bir anchor point kümesindeki perturbation dağılımlarını optimize eder ve mDP'yi kanıtlanabilir şekilde koruyan log-convex kombinasyonlar aracılığıyla anchor olmayan konumlardaki dağılımları enterpole eder. Yüksek boyutlu uzaylarda kaba enterpolasyondan kaynaklanan gizlilik ihlallerini ele almak için enterpolasyon sürecini bir dizi tek boyutlu adıma ayırırız ve tasarımı gereği lp-norm mDP'yi uygulayan düzeltilmiş bir formülasyon türetiriz. Ayrıca, perturbation dağılımları ve boyutlar arasındaki gizlilik bütçe tahsisi üzerinde ortak optimizasyonu keşfederiz. Gerçek dünya konum veri kümeleri üzerindeki deneyler, yöntemimizin ince taneli alanlarda titiz gizlilik garantileri ve rekabetçi fayda sunduğunu ve temel mekanizmaların performansını aştığını göstermektedir."
    }
  },
  {
    "id": "2601.09933v1",
    "title": "Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method",
    "authors": [
      "Ashish Anand",
      "Bhupendra Singh",
      "Sunil Khemka",
      "Bireswar Banerjee",
      "Vishi Singh Bhatia"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.09933v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09933v1",
    "content": {
      "en": "Android malware has become an increasingly critical threat to organizations, society and individuals, posing significant risks to privacy, data security and infrastructure. As malware continues to evolve in terms of complexity and sophistication, the mitigation and detection of these malicious software instances have become more time consuming and challenging particularly due to the requirement of large number of features to identify potential malware. To address these challenges, this research proposes Fast Gradient Sign Method with Diluted Convolutional Neural Network (FGSM DICNN) method for malware classification. DICNN contains diluted convolutions which increases receptive field, enabling the model to capture dispersed malware patterns across long ranges using fewer features without adding parameters. Additionally, the FGSM strategy enhance the accuracy by using one-step perturbations during training that provides more defensive advantage of lower computational cost. This integration helps to manage high classification accuracy while reducing the dependence on extensive feature sets. The proposed FGSM DICNN model attains 99.44% accuracy while outperforming other existing approaches such as Custom Deep Neural Network (DCNN).",
      "tr": "İşte makalenin Türkçe çevirisi:\n\n**Makale Başlığı:** Fast Gradient Sign Method ile Seyreltilmiş Evrişimsel Sinir Ağı Kullanarak Kötü Amaçlı Yazılım Sınıflandırması\n\n**Özet:**\nAndroid kötü amaçlı yazılımları, bireyler, toplum ve kuruluşlar için giderek artan kritik bir tehdit haline gelmiş olup, gizlilik, veri güvenliği ve altyapı için önemli riskler taşımaktadır. Kötü amaçlı yazılımlar karmaşıklık ve sofistikasyon açısından evrimleşmeye devam ettikçe, bu kötü niyetli yazılım örneklerinin azaltılması ve tespiti, özellikle potansiyel kötü amaçlı yazılımları belirlemek için çok sayıda özelliğe duyulan ihtiyaç nedeniyle daha zaman alıcı ve zorlu hale gelmiştir. Bu zorlukların üstesinden gelmek için bu araştırma, kötü amaçlı yazılım sınıflandırması için Seyreltilmiş Evrişimsel Sinir Ağı ile Hızlı Gradyan İşaret Yöntemi (FGSM DICNN) yöntemini önermektedir. DICNN, alıcı alanı artıran seyreltilmiş konvolüsyonlar içerir, bu da modelin parametre eklemeden daha az özellik kullanarak uzun mesafelerde dağılmış kötü amaçlı yazılım desenlerini yakalamasını sağlar. Ek olarak, FGSM stratejisi, daha düşük hesaplama maliyetinin daha fazla savunma avantajını sağlayan, eğitim sırasında tek adımlı pertürbasyonlar kullanarak doğruluğu artırır. Bu entegrasyon, kapsamlı özellik kümelerine olan bağımlılığı azaltırken yüksek sınıflandırma doğruluğunu yönetmeye yardımcı olur. Önerilen FGSM DICNN modeli, Özel Derin Sinir Ağı (DCNN) gibi diğer mevcut yaklaşımlardan daha iyi performans göstererek %99,44 doğruluk elde etmektedir."
    }
  },
  {
    "id": "2601.09902v1",
    "title": "A Novel Contrastive Loss for Zero-Day Network Intrusion Detection",
    "authors": [
      "Jack Wilkie",
      "Hanan Hindy",
      "Craig Michie",
      "Christos Tachtatzis",
      "James Irvine"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2601.09902v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09902v1",
    "content": {
      "en": "Machine learning has achieved state-of-the-art results in network intrusion detection; however, its performance significantly degrades when confronted by a new attack class -- a zero-day attack. In simple terms, classical machine learning-based approaches are adept at identifying attack classes on which they have been previously trained, but struggle with those not included in their training data. One approach to addressing this shortcoming is to utilise anomaly detectors which train exclusively on benign data with the goal of generalising to all attack classes -- both known and zero-day. However, this comes at the expense of a prohibitively high false positive rate. This work proposes a novel contrastive loss function which is able to maintain the advantages of other contrastive learning-based approaches (robustness to imbalanced data) but can also generalise to zero-day attacks. Unlike anomaly detectors, this model learns the distributions of benign traffic using both benign and known malign samples, i.e. other well-known attack classes (not including the zero-day class), and consequently, achieves significant performance improvements. The proposed approach is experimentally verified on the Lycos2017 dataset where it achieves an AUROC improvement of .000065 and .060883 over previous models in known and zero-day attack detection, respectively. Finally, the proposed method is extended to open-set recognition achieving OpenAUC improvements of .170883 over existing approaches.",
      "tr": "**Makale Başlığı:** A Novel Contrastive Loss for Zero-Day Network Intrusion Detection\n\n**Özet:**\n\nMakine öğrenmesi, ağ saldırı tespitinde son teknoloji sonuçlar elde etmiş olsa da, yeni bir saldırı sınıfıyla, yani zero-day saldırısıyla karşılaştığında performansı önemli ölçüde düşmektedir. Basitçe ifade etmek gerekirse, klasik makine öğrenmesi tabanlı yaklaşımlar, önceden eğitildikleri saldırı sınıflarını belirlemede ustadır, ancak eğitim verilerinde yer almayanlarla mücadele etmektedir. Bu eksikliği gidermeye yönelik bir yaklaşım, hem bilinen hem de zero-day saldırı sınıflarına genelleme yapma hedefiyle yalnızca zararsız veriler üzerinde eğiten anomaly detectors'ları kullanmaktır. Ancak bu, kabul edilemez derecede yüksek bir false positive rate bedeline mal olmaktadır. Bu çalışma, diğer contrastive learning-based yaklaşımların avantajlarını (imbalanced data'ya karşı sağlamlık) koruyabilen ancak zero-day saldırılarına da genelleme yapabilen yeni bir contrastive loss fonksiyonu önermektedir. Anomaly detectors'ların aksine, bu model zararsız trafik dağılımlarını hem zararsız hem de bilinen kötü niyetli örnekler, yani zero-day sınıfını içermeyen diğer iyi bilinen saldırı sınıfları üzerinden öğrenir ve dolayısıyla önemli performans iyileştirmeleri elde eder. Önerilen yaklaşım, Lycos2017 veri kümesinde deneysel olarak doğrulanmış olup, bilinen ve zero-day saldırı tespitinde sırasıyla .000065 ve .060883 AUROC iyileştirmesi sağlamıştır. Son olarak, önerilen yöntem open-set recognition'a genişletilerek mevcut yaklaşımlar üzerinde .170883 OpenAUC iyileştirmesi elde etmiştir."
    }
  }
]