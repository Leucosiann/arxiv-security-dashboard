[
  {
    "id": "2512.16874v1",
    "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking",
    "authors": [
      "Tomáš Souček",
      "Pierre Fernandez",
      "Hady Elsahar",
      "Sylvestre-Alvise Rebuffi",
      "Valeriu Lacatusu"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16874v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16874v1",
    "content": {
      "en": "Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.",
      "tr": "Elbette, işte akademik makalenin başlık ve özetinin Türkçeye çevrilmiş hali:\n\n**Makale Başlığı:** Pixel Seal: Görünmez Görüntü ve Video Filigranı İçin Sadece Çekişmeli Eğitim\n\n**Özet:**\n\nGörünmez filigranlama, dijital içeriğin kökenini izlemek için esastır. Ancak, en son modellerin eğitimi oldukça zor olmaya devam etmektedir ve mevcut yaklaşımlar sıklıkla sağlamlığı gerçek algılanamazlıkla dengelemekte zorlanmaktadır. Bu çalışma, görüntü ve video filigranlamada yeni bir en son durumu belirleyen Pixel Seal'ı sunmaktadır. Öncelikle, mevcut yöntemlerin üç temel sorununu tespit ediyoruz: (i) insan algısını taklit edemeyen ve görünür filigran artıklarıyla sonuçlanan MSE ve LPIPS gibi vekil algısal kayıplara güvenilmesi; (ii) çelişkili hedeflerden kaynaklanan ve kapsamlı hiperparametre ayarı gerektiren optimizasyon istikrarsızlığı; ve (iii) modelleri yüksek çözünürlüklü görüntülere ve videolara ölçeklendirirken filigranların azaltılmış sağlamlığı ve algılanamazlığı. Bu sorunların üstesinden gelmek için, öncelikle güvenilmez piksel bazlı algılanamazlık kayıplarını ortadan kaldıran bir adversarial-only training paradigm öne sürüyoruz. İkinci olarak, sağlamlık ve algılanamazlığı ayırarak yakınlaşmayı stabilize eden üç aşamalı bir eğitim programı sunuyoruz. Üçüncü olarak, JND-based attenuation ve upscaling artifaktlarını ortadan kaldırmak için training-time inference simulation kullanarak yüksek çözünürlüklü adaptasyon yoluyla çözünürlük boşluğunu ele alıyoruz. Piksel Seal'ın sağlamlığını ve algılanamazlığını farklı görüntü türleri üzerinde ve geniş bir dönüşüm aralığında kapsamlı bir şekilde değerlendiriyor ve en son duruma göre net iyileştirmeler gösteriyoruz. Son olarak, modelin temporal watermark pooling aracılığıyla videoya verimli bir şekilde adapte olduğunu göstererek, Piksel Seal'ı gerçek dünya görüntü ve video ortamlarında güvenilir köken takibi için pratik ve ölçeklenebilir bir çözüm olarak konumlandırıyoruz."
    }
  },
  {
    "id": "2512.16851v1",
    "title": "PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy",
    "authors": [
      "Ripan Kumar Kundu",
      "Istiak Ahmed",
      "Khaza Anuarul Hoque"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "link": "http://arxiv.org/abs/2512.16851v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16851v1",
    "content": {
      "en": "The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.",
      "tr": "Makale Başlığı: PrivateXR: Açıklanabilir Yapay Zeka Rehberli Diferansiyel Gizlilik Yoluyla Genişletilmiş Gerçeklikte Gizlilik Saldırılarına Karşı Savunma\n\nÖzet:\nYapay zeka (AI) ve Genişletilmiş Gerçeklik (XR) teknolojilerinin (AI XR) yakınsaması birçok alanda yenilikçi uygulamalar vaat etmektedir. Bununla birlikte, bu sistemlerde kullanılan verilerin hassas doğası (örneğin, eye-tracking) önemli gizlilik endişelerini beraberinde getirmektedir; zira saldırganlar bu verileri ve modelleri kullanarak membership inference attacks (MIA) ve re-identification (RDA) yoluyla yüksek başarı oranıyla kişisel bilgileri çıkarıp sızdırabilirler. Araştırmacılar, diferansiyel gizlilik (DP) dahil olmak üzere bu tür gizlilik saldırılarını azaltmak için çeşitli teknikler önermişlerdir. Ancak, AI XR veri kümeleri genellikle çok sayıda özellik içermektedir ve DP'nin tekdüze bir şekilde uygulanması, daha az ilgili özelliklere gereksiz gürültü ekleyerek model doğruluğunu düşürebilir ve çıkarım süresini artırarak gerçek zamanlı XR dağıtımını sınırlayabilir. Bu durumdan hareketle, açıklanabilir yapay zeka (XAI) ve DP özellikli gizlilik koruma mekanizmalarını birleştiren, gizlilik saldırılarına karşı savunma sağlayan yeni bir çerçeve öneriyoruz. Spesifik olarak, AI XR modellerindeki en etkili özellikleri belirlemek ve çıkarım sırasında bu özelliklere seçici olarak DP uygulamak için post-hoc explanations'dan yararlanıyoruz. XAI rehberli DP yaklaşımımızı üç adet güncel AI XR modeli ve cybersickness, emotion ve activity classification olmak üzere üç veri kümesi üzerinde değerlendiriyoruz. Sonuçlarımız, önerilen yöntemin cybersickness görevleri için MIA ve RDA başarı oranlarını sırasıyla %43'e ve %39'a kadar azalttığını, aynı zamanda Transformer modelleri kullanarak %97'ye varan doğrulukla modelin kullanılabilirliğini koruduğunu göstermektedir. Ayrıca, geleneksel DP yaklaşımlarına kıyasla çıkarım süresini 2 katına kadar iyileştirmektedir. Pratikliği göstermek amacıyla, XAI rehberli DP AI XR modellerini bir HTC VIVE Pro headset'e dağıtıyor ve kullanıcıların gizlilik seviyelerini (örneğin, düşük, orta, yüksek) ayarlamalarına olanak tanıyan, aynı zamanda gerçek zamanlı görev tahminleri alarak XR oyun deneyimi sırasında kullanıcı gizliliğini koruyan PrivateXR adında bir kullanıcı arayüzü (UI) geliştiriyoruz."
    }
  },
  {
    "id": "2512.16778v1",
    "title": "Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences",
    "authors": [
      "Theshani Nuradha",
      "Ian George",
      "Christoph Hirche"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "quant-ph",
      "cs.CR",
      "cs.IT",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16778v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16778v1",
    "content": {
      "en": "Data-processing is a desired property of classical and quantum divergences and information measures. In information theory, the contraction coefficient measures how much the distinguishability of quantum states decreases when they are transmitted through a quantum channel, establishing linear strong data-processing inequalities (SDPI). However, these linear SDPI are not always tight and can be improved in most of the cases. In this work, we establish non-linear SDPI for quantum hockey-stick divergence for noisy channels that satisfy a certain noise criterion. We also note that our results improve upon existing linear SDPI for quantum hockey-stick divergences and also non-linear SDPI for classical hockey-stick divergence. We define $F_γ$ curves generalizing Dobrushin curves for the quantum setting while characterizing SDPI for the sequential composition of heterogeneous channels. In addition, we derive reverse-Pinsker type inequalities for $f$-divergences with additional constraints on hockey-stick divergences. We show that these non-linear SDPI can establish tighter finite mixing times that cannot be achieved through linear SDPI. Furthermore, we find applications of these in establishing stronger privacy guarantees for the composition of sequential private quantum channels when privacy is quantified by quantum local differential privacy.",
      "tr": "**Makale Başlığı:** Lineer Olmayan Güçlü Veri İşleme (Non-Linear Strong Data-Processing) ile Kuantum Hokey Sopası Iraksamaları (Quantum Hockey-Stick Divergences)\n\n**Özet:**\n\nVeri işleme (data-processing), klasik ve kuantum ıraksamalarının (divergences) ve bilgi ölçümlerinin (information measures) istenen bir özelliğidir. Bilgi teorisinde, karşılaştırılabilirlik katsayısı (contraction coefficient), kuantum durumlarının (quantum states) bir kuantum kanalından (quantum channel) iletildiğinde ayırt edilebilirliklerinin (distinguishability) ne kadar azaldığını ölçerek lineer güçlü veri işleme eşitsizliklerini (linear strong data-processing inequalities - SDPI) belirler. Ancak, bu lineer SDPI her zaman sıkı (tight) değildir ve çoğu durumda iyileştirilebilir. Bu çalışmada, belirli bir gürültü kriterini (noise criterion) sağlayan gürültülü kanallar (noisy channels) için kuantum hokey sopası ıraksaması (quantum hockey-stick divergence) için lineer olmayan SDPI'ler (non-linear SDPI) oluşturuyoruz. Ayrıca, sonuçlarımızın kuantum hokey sopası ıraksamaları için mevcut lineer SDPI'leri ve klasik hokey sopası ıraksaması için lineer olmayan SDPI'leri iyileştirdiğini belirtiyoruz. Dobrushin eğrilerini (Dobrushin curves) kuantum ortamına genelleştiren ve heterojen kanalların (heterogeneous channels) sıralı bileşimleri (sequential composition) için SDPI'leri karakterize eden $F_γ$ eğrilerini tanımlıyoruz. Ek olarak, hockey-stick ıraksamaları üzerindeki ek kısıtlamalarla $f$-ıraksamaları (f-divergences) için ters-Pinsker tipi eşitsizlikler (reverse-Pinsker type inequalities) türetiyoruz. Bu lineer olmayan SDPI'lerin, lineer SDPI ile elde edilemeyen daha sıkı sonlu karışım süreleri (finite mixing times) oluşturabildiğini gösteriyoruz. Dahası, bu sonuçların, mahremiyetin (privacy) kuantum yerel diferansiyel mahremiyet (quantum local differential privacy) ile ölçüldüğü sıralı özel kuantum kanallarının (sequential private quantum channels) bileşimleri için daha güçlü mahremiyet garantileri oluşturulmasında uygulamalarını buluyoruz."
    }
  },
  {
    "id": "2512.16717v1",
    "title": "Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering",
    "authors": [
      "Rudra Dubey",
      "Arpit Mani Tripathi",
      "Archit Srivastava",
      "Sarvpal Singh"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.16717v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16717v1",
    "content": {
      "en": "In actuality, phishing attacks remain one of the most prevalent cybersecurity risks in existence today, with malevolent actors constantly changing their strategies to successfully trick users. This paper presents an AI model for a phishing detection system that uses an ensemble approach to combine character-level Convolutional Neural Networks (CNN) and LightGBM with engineered features. Our system uses a character-level CNN to extract sequential features after extracting 36 lexical, structural, and domain-based features from the URLs. On a test dataset of 19,873 URLs, the ensemble model achieves an accuracy of 99.819 percent, precision of 100 percent, recall of 99.635 percent, and ROC-AUC of 99.947 percent. Through a FastAPI-based service with an intuitive user interface, the suggested system has been utilised to offer real-time detection. In contrast, the results demonstrate that the suggested solution performs better than individual models; LightGBM contributes 40 percent and character-CNN contributes 60 percent to the final prediction. The suggested method maintains extremely low false positive rates while doing a good job of identifying contemporary phishing techniques. Index Terms - Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis",
      "tr": "**Makale Başlığı:** Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering\n\n**Özet:**\n\nGünümüzde phising saldırıları, en yaygın siber güvenlik risklerinden biri olmaya devam etmekte olup, kötü niyetli aktörler kullanıcıları kandırmak için sürekli olarak stratejilerini değiştirmektedir. Bu çalışma, karakter-seviye Convolutional Neural Networks (CNN) ve engineered features ile LightGBM'yi birleştiren bir ensemble approach kullanan bir phishing detection system için bir AI modeli sunmaktadır. Sistemimiz, URL'lerden 36 leksikal, yapısal ve domain-tabanlı özellik çıkarıldıktan sonra ardışık özellikleri çıkarmak için karakter-seviye CNN kullanmaktadır. 19.873 URL'den oluşan bir test veri setinde, ensemble model %99,819 doğruluk, %100 kesinlik, %99,635 recall ve %99,947 ROC-AUC değeri elde etmiştir. Sezgisel bir kullanıcı arayüzüne sahip FastAPI tabanlı bir servis aracılığıyla önerilen sistem, gerçek zamanlı tespit sağlamak için kullanılmıştır. Karşılaştırıldığında, sonuçlar önerilen çözümün bireysel modellerden daha iyi performans gösterdiğini ortaya koymaktadır; nihai tahmine LightGBM %40, character-CNN ise %60 katkıda bulunmaktadır. Önerilen yöntem, güncel phishing tekniklerini başarılı bir şekilde tanımlarken son derece düşük yanlış pozitif oranlarını korumaktadır.\n\n**Index Terms -** Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis"
    }
  },
  {
    "id": "2512.16658v1",
    "title": "Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking",
    "authors": [
      "Sangeeth B",
      "Serena Nicolazzo",
      "Deepa K.",
      "Vinod P"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.16658v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16658v1",
    "content": {
      "en": "The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.",
      "tr": "Aşağıdaki akademik makale başlığının ve özetinin Türkçeye çevirisi yapılmıştır:\n\n**Makale Başlığı:** Chaos-Based White-Box Watermarking ile Derin Sinir Ağı Fikri Mülkiyetinin Korunması\n\n**Özet:**\nÇok sayıda alanda derin sinir ağlarının (DNNs) hızla yaygınlaşması, fikri mülkiyet (IP) korunması ve modelin kötüye kullanılmasıyla ilgili artan endişelere yol açmıştır. Eğitilmiş DNN'ler, genellikle önemli yatırımlar yoluyla geliştirilen değerli varlıkları temsil eder. Ancak, modellerin kopyalanması, yeniden dağıtılması veya yeniden amaçlandırılmasının kolaylığı, model sahipliğini iddia etmek ve doğrulamak için etkili mekanizmalara olan acil ihtiyacı vurgulamaktadır. Bu çalışmada, kaotik diziler kullanarak bir DNN'nin iç parametrelerine sahiplik bilgisini gömen verimli ve dayanıklı bir white-box watermarking çerçevesi öneriyoruz. Watermark, iyi bilinen bir kaotik fonksiyon olan ve başlangıç parametrelerine duyarlı bir dizi üreten bir lojistik haritası kullanılarak üretilir. Bu dizi, modelde yapısal değişiklikler veya öngörüsel performansta düşüş gerektirmeden, seçilen bir ara katmanın ağırlıklarına enjekte edilir. Sahipliği doğrulamak için, çıkarılan ve yeniden üretilen diziler arasındaki benzerliği optimize ederek orijinal kaotik parametreleri geri kazanan bir genetik algoritma tabanlı doğrulama süreci sunuyoruz. Önerilen yaklaşımın etkinliği, MNIST ve CIFAR-10 veri kümeleri kullanılarak görüntü sınıflandırma görevleri üzerinde yapılan kapsamlı deneylerle gösterilmiştir. Sonuçlar, gömülü watermark'ın ince ayardan sonra tespit edilebilir kaldığını ve model doğruluğunda ihmal edilebilir bir kayıp olduğunu göstermektedir. Watermark'ın sayısal olarak geri kazanılmasının yanı sıra, ağırlık yoğunluk grafikleri kullanarak görsel analizler yapıyoruz ve orijinal, watermark'lı ve kurcalanmış modelleri ayırt etmek için aktivasyon tabanlı sınıflandırıcılar oluşturuyoruz. Genel olarak, önerilen yöntem, IP korumasının kritik olduğu gerçek dünya senaryolarına çok uygun olan white-box ortamlarında model sahipliğini gömmek ve doğrulamak için esnek ve ölçeklenebilir bir çözüm sunmaktadır."
    }
  },
  {
    "id": "2512.16650v1",
    "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
    "authors": [
      "Jirui Yang",
      "Hengqi Guo",
      "Zhihui Lu",
      "Yi Zhao",
      "Yuansen Zhang"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.16650v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16650v1",
    "content": {
      "en": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
      "tr": "**Makale Başlığı:** Prefix Probing: Büyük Dil Modelleri için Hafif Zararlı İçerik Tespiti\n\n**Özet:**\n\nBüyük dil modelleri, gerçek dünya güvenlik açısından hassas uygulamalarda kullanıldığında sıklıkla tespit doğruluğu, çıkarım gecikmesi ve dağıtım maliyeti arasında üç yönlü bir ödünleşimle karşılaşır. Bu makale, \"anlaşma/yürütme\" ile \"ret/güvenlik\" açılış *prefix*'lerinin koşullu log-olasılıklarını karşılaştıran ve tespit yükünü ilk *token* gecikmesine yaklaştırmak için *prefix caching*'ten yararlanan siyah kutu zararlı içerik tespit yöntemi olan Prefix Probing'i tanıtmaktadır. Çıkarım sırasında, yöntem zararlılık puanı üretmek ve bir eşik uygulamak için herhangi bir ek model veya çok aşamalı çıkarım çağırmaksızın yalnızca *probe prefix*'ler üzerinde tek bir log-olasılık hesaplaması gerektirir. *Prefix*'lerin ayırt edici gücünü daha da artırmak için, oldukça bilgilendirici *prefix*'leri otomatik olarak keşfeden verimli bir *prefix* oluşturma algoritması tasarlıyoruz, bu da tespit performansını önemli ölçüde iyileştiriyor. Kapsamlı deneyler, Prefix Probing'in yalnızca minimal hesaplama maliyeti gerektirerek ve ek model dağıtımı gerektirmeyerek ana akım harici güvenlik modelleriyle karşılaştırılabilir tespit etkinliği elde ettiğini göstermektedir; bu da güçlü pratikliğini ve verimliliğini vurgulamaktadır."
    }
  },
  {
    "id": "2512.16538v1",
    "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
    "authors": [
      "Xiao Li",
      "Yue Li",
      "Hao Wu",
      "Yue Zhang",
      "Yechao Zhang"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16538v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16538v1",
    "content": {
      "en": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.",
      "tr": "**Makale Başlığı:** LLM Tabanlı Açık Bulma Tespiti Karşısında Kod Gizleme Üzerine Sistematik Bir Çalışma\n\n**Özet:**\n\nBüyük dil modellerinin (LLM) kod açığı tespiti için giderek daha fazla benimsenmesiyle birlikte, bunların çeşitli açık türleri karşısındaki güvenilirliği ve sağlamlığı acil bir endişe kaynağı haline gelmiştir. Geleneksel adversarial ortamlarda, kod gizleme (code obfuscation) araçları atlamak için uzun süredir genel bir strateji olarak kullanılmış, araçlara dokunmadan exploitability'yi korumuştur. Gizleme yöntemleri ve araçları üzerine sayısız çalışma yapılmış olmasına rağmen, desteklenen teknikler, granülerlik ve programlama dilleri açısından yetenekleri farklılık göstermekte olup, LLM tabanlı açık bulma tespiti üzerindeki etkilerini sistematik olarak değerlendirmeyi zorlaştırmaktadır. Bu boşluğu gidermek için, gizleme tekniklerinin yapılandırılmış bir sistematizasyonunu sunuyoruz ve bunları birleşik bir çerçeve altında değerlendiriyoruz. Özellikle, mevcut gizleme yöntemlerini üç ana sınıfa (layout, data flow ve control flow) ayırarak 11 alt kategori ve 19 somut tekniği kapsıyoruz. Bu teknikleri dört programlama dilinde (Solidity, C, C++ ve Python) tutarlı bir LLM-driven yaklaşımla uyguluyoruz ve dört model ailesini (DeepSeek, OpenAI, Qwen ve LLaMA) kapsayan 15 LLM üzerindeki etkilerini ve iki kodlama ajanı (GitHub Copilot ve Codex) üzerindeki etkilerini değerlendiriyoruz. Bulgularımız, LLM tabanlı açık bulma tespiti üzerinde kod gizlemenin hem olumlu hem de olumsuz etkilerini ortaya koymakta, gizlemenin performans iyileştirmelerine veya düşüşlerine yol açtığı koşulları vurgulamaktadır. Ayrıca, bu sonuçları açıkların karakteristikleri, kod özellikleri ve model nitelikleri açısından analiz ediyoruz. Son olarak, birkaç açık problem özetliyor ve gerçek dünya açık bulma tespiti için LLM'lerin sağlamlığını artırmaya yönelik gelecek yönelimler öneriyoruz."
    }
  },
  {
    "id": "2512.16310v1",
    "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
    "authors": [
      "Yuxuan Qiao",
      "Dongqin Liu",
      "Hongchang Yang",
      "Wei Zhou",
      "Songlin Hu"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2512.16310v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16310v1",
    "content": {
      "en": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
      "tr": "Makale Başlığı: Agent Tools Orchestration Leaks More: Dataset, Benchmark, ve Mitigation\n\nÖzet:\nBüyük Dil Modelleri tarafından yönlendirilen tek-agent, çok-araç mimarisi, basitliği ve etkinliği nedeniyle otonom ajanlar için popüler bir paradigma haline gelmiştir. Ancak bu mimari aynı zamanda yeni ve ciddi bir gizlilik riski yaratmaktadır; biz buna Tools Orchestration Privacy Risk (TOP-R) adını veriyoruz. Bu riskte, bir ajan, zararsız bir kullanıcı amacına ulaşmak için, birden fazla araç üzerinden bilgi parçacıklarını otonom olarak toplar ve beklenmedik hassas bilgileri sentezlemek için reasoning yeteneklerinden yararlanır. Bu riskin ilk sistematik çalışmasını sunuyoruz. Öncelikle, riskin kök nedenini, ajanın yanlış hizalanmış hedef fonksiyonuna atfederek formal bir çerçeve oluşturuyoruz: yardımseverlik için aşırı optimizasyon yaparken gizlilik farkındalığını ihmal etmek. İkinci olarak, bu riski kapsamlı bir şekilde değerlendirmek için eşleştirilmiş sızıntı ve zararsız senaryoları içeren TOP-Bench'i oluşturuyoruz. Güvenlik ve dayanıklılık arasındaki ödünleşmeyi ölçmek için H-Score'u bütüncül bir metrik olarak tanıtıyoruz. Değerlendirme sonuçları, TOP-R'nin ciddi bir risk olduğunu ortaya koymaktadır: sekiz temsili modelin ortalama Risk Leakage Rate (RLR)'i %90.24'e ulaşırken, ortalama H-Score yalnızca 0.167'dir ve hiçbir model 0.3'ü aşamamıştır. Son olarak, TOP-R'yi etkin bir şekilde hafifleten, Risk Leakage Rate'i %46.58'e düşüren ve H-Score'u önemli ölçüde 0.624'e yükselten Privacy Enhancement Principle (PEP) yöntemini öneriyoruz. Çalışmamız, hem yeni bir risk sınıfını hem de mevcut ajan mimarilerindeki doğuştan gelen yapısal sınırlamaları ortaya koyarken, aynı zamanda uygulanabilir hafifletme stratejileri de sunmaktadır."
    }
  },
  {
    "id": "2512.16307v1",
    "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks",
    "authors": [
      "Safwan Shaheer",
      "G. M. Refatul Islam",
      "Mohammad Rafid Hamid",
      "Tahsin Zaman Jilan"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.16307v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16307v1",
    "content": {
      "en": "In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.",
      "tr": "İşte akademik makale başlığı ve özetinin Türkçeye çevrilmiş hali:\n\n**Makale Başlığı:** Kıyas Noktasının Ötesinde: Prompt Injection Saldırılarına Karşı Yenilikçi Savunmalar\n\n**Özet:**\n\nBüyük Dil Modellerinin (LLMs) hızla gelişen bu alanında, makalemiz prompt injection saldırılarının sunduğu önemli güvenlik riskini tartışmaktadır. Özellikle LLaMA ailesi modelleri gibi küçük açık kaynaklı modellere odaklanmaktadır. Otomatik savunmalar üretebilen yeni savunma mekanizmaları tanıtmakta ve bu üretilen savunmaları kapsamlı bir dizi kıyaslanmış saldırıya karşı sistematik olarak değerlendirmekteyiz. Böylece, LLM'lerde hedef kaçırma (goal-hijacking) güvenlik açıklarını azaltmada yaklaşımımızın önerdiği iyileştirmeyi ampirik olarak göstermiş olduk. Çalışmamız, küçük açık kaynaklı LLM'lerin artan alaka düzeyini ve kenar cihazlarda (edge devices) geniş çapta konuşlandırma potansiyellerini, LLM uygulamalarındaki gelecek eğilimlerle uyumlu olarak kabul etmektedir. Açık kaynaklı LLM'lerin ve bunların güvenliğinin daha büyük ekosistemine aşağıdaki katkılarda bulunuyoruz: (1) mevcut prompt tabanlı savunmaları en son saldırılara karşı değerlendirme, (2) savunma prompt'larını iteratif olarak iyileştirmek için bir tohum savunma (Chain Of Thoughts) kullanan yeni bir çerçeve tanıtma ve (3) hedef kaçırma (goal-hijacking) saldırılarını tespit etmede önemli iyileştirmeler gösterme. Stratejilerimiz, saldırıların başarı oranlarını ve yanlış tespit oranlarını önemli ölçüde azaltırken, aynı zamanda hedef kaçırma (goal-hijacking) yeteneklerini etkili bir şekilde tespit ederek, kaynak kısıtlı ortamlarda küçük ve açık kaynaklı LLM'lerin daha güvenli ve verimli konuşlandırılmasının yolunu açmaktadır."
    }
  },
  {
    "id": "2512.16292v1",
    "title": "In-Context Probing for Membership Inference in Fine-Tuned Language Models",
    "authors": [
      "Zhexi Lu",
      "Hongliang Chi",
      "Nathalie Baracaldo",
      "Swanand Ravindra Kadhe",
      "Yuseok Jeon"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16292v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16292v1",
    "content": {
      "en": "Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.",
      "tr": "**Makale Başlığı:** In-Context Probing for Membership Inference in Fine-Tuned Language Models\n\n**Özet:**\n\nFine-tuned large language models (LLMs) için Membership Inference Attacks (MIAs), özellikle hassas verilerle alan-özgü görevlere adapte edildiğinde kritik bir gizlilik tehdidi oluşturmaktadır. Önceki black-box MIA teknikleri, confidence scores veya token likelihoods'a dayanırken, bu sinyaller genellikle bir örneğin intrinsic properties'i - örneğin content difficulty veya rarity - ile iç içe geçmiş durumdadır, bu da zayıf generalization ve düşük signal-to-noise ratios'a yol açmaktadır. Bu makalede, ICP-MIA'yı, özellikle optimization sırasındaki diminishing returns olgusunu içeren training dynamics teorisine dayanan yeni bir MIA framework'ü olarak öneriyoruz. Optimization Gap'i, membership'in temel bir sinyali olarak sunuyoruz: convergence'da, member samples minimal remaining loss-reduction potential sergilerken, non-members daha fazla optimization için önemli bir potential korur. Bu gap'i black-box bir ortamda tahmin etmek için, stratejik olarak oluşturulmuş input contexts aracılığıyla fine-tuning-like behavior'u simüle eden, training-free bir yöntem olan In-Context Probing (ICP)'yi öneriyoruz. İki probing stratejisi öneriyoruz: reference-data-based (semantically similar public samples kullanarak) ve self-perturbation (masking veya generation yoluyla). Üç görev ve birden fazla LLM üzerinde yapılan deneyler, ICP-MIA'nın, özellikle düşük false positive rates'de, önceki black-box MIAs'dan önemli ölçüde daha iyi performans gösterdiğini ortaya koymaktadır. Ayrıca reference data alignment, model type, PEFT configurations ve training schedules'ın attack effectiveness'ı nasıl etkilediğini analiz ediyoruz. Bulgularımız, ICP-MIA'yı, deployed LLMs'deki privacy risks'i denetlemek için pratik ve teorik olarak temellendirilmiş bir framework olarak konumlandırmaktadır."
    }
  }
]