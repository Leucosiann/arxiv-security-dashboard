[
  {
    "id": "2602.11015v1",
    "title": "CVPL: A Geometric Framework for Post-Hoc Linkage Risk Assessment in Protected Tabular Data",
    "authors": [
      "Valery Khvatov",
      "Alexey Neyman"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11015v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11015v1",
    "content": {
      "en": "Formal privacy metrics provide compliance-oriented guarantees but often fail to quantify actual linkability in released datasets. We introduce CVPL (Cluster-Vector-Projection Linkage), a geometric framework for post-hoc assessment of linkage risk between original and protected tabular data. CVPL represents linkage analysis as an operator pipeline comprising blocking, vectorization, latent projection, and similarity evaluation, yielding continuous, scenario-dependent risk estimates rather than binary compliance verdicts. We formally define CVPL under an explicit threat model and introduce threshold-aware risk surfaces, R(lambda, tau), that capture the joint effects of protection strength and attacker strictness. We establish a progressive blocking strategy with monotonicity guarantees, enabling anytime risk estimation with valid lower bounds. We demonstrate that the classical Fellegi-Sunter linkage emerges as a special case of CVPL under restrictive assumptions, and that violations of these assumptions can lead to systematic over-linking bias. Empirical validation on 10,000 records across 19 protection configurations demonstrates that formal k-anonymity compliance may coexist with substantial empirical linkability, with a significant portion arising from non-quasi-identifier behavioral patterns. CVPL provides interpretable diagnostics identifying which features drive linkage feasibility, supporting privacy impact assessment, protection mechanism comparison, and utility-risk trade-off analysis.",
      "tr": "**Makale Başlığı:** CVPL: Korunmuş Tablosal Verilerde Post-Hoc Bağlantı Riski Değerlendirmesi İçin Geometrik Bir Çerçeve\n\n**Özet:**\n\nFormal privacy metrics, uyumluluk odaklı güvenceler sağlasa da, yayınlanan veri kümelerindeki fiili linkability'yi nicelendirmede sıklıkla yetersiz kalmaktadır. Orijinal ve korunmuş tablosal veriler arasındaki linkage riskinin post-hoc değerlendirmesi için geometrik bir çerçeve olan CVPL'yi (Cluster-Vector-Projection Linkage) sunuyoruz. CVPL, linkage analysis'i blocking, vectorization, latent projection ve similarity evaluation aşamalarından oluşan bir operatör pipeline'ı olarak temsil eder, bu da ikili uyumluluk kararları yerine sürekli, senaryo-bağımlı risk tahminleri üretir. CVPL'yi açık bir threat model altında resmi olarak tanımlıyoruz ve koruma gücü ile attacker strictness'ın ortak etkilerini yakalayan threshold-aware risk surfaces, R(lambda, tau) tanıtıyoruz. Monotonicity guarantees içeren ilerleyici bir blocking stratejisi oluşturarak, geçerli alt sınırlara sahip anytime risk estimation'ı mümkün kılıyoruz. Klasik Fellegi-Sunter linkage'in kısıtlayıcı varsayımlar altında CVPL'nin özel bir durumu olarak ortaya çıktığını ve bu varsayımların ihlallerinin sistematik over-linking bias'ına yol açabileceğini gösteriyoruz. 19 koruma konfigürasyonu üzerinden 10.000 kayda uygulanan ampirik doğrulama, formal k-anonymity uyumluluğunun, önemli bir kısmının quasi-identifier olmayan davranışsal örüntülerden kaynaklanan önemli ampirik linkability ile bir arada bulunabileceğini göstermektedir. CVPL, linkage feasibility'yi yönlendiren özellikleri tanımlayan, privacy impact assessment, protection mechanism comparison ve utility-risk trade-off analysis'i destekleyen yorumlanabilir tanılamalar sağlar."
    }
  },
  {
    "id": "2602.10915v1",
    "title": "Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System",
    "authors": [
      "Zhenhua Zou",
      "Sheng Guo",
      "Qiuyang Zhan",
      "Lepeng Zhao",
      "Shuo Li"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.10915v1",
    "pdf_link": "https://arxiv.org/pdf/2602.10915v1",
    "content": {
      "en": "The evolution of Large Language Models (LLMs) has shifted mobile computing from App-centric interactions to system-level autonomous agents. Current implementations predominantly rely on a \"Screen-as-Interface\" paradigm, which inherits structural vulnerabilities and conflicts with the mobile ecosystem's economic foundations. In this paper, we conduct a systematic security analysis of state-of-the-art mobile agents using Doubao Mobile Assistant as a representative case. We decompose the threat landscape into four dimensions - Agent Identity, External Interface, Internal Reasoning, and Action Execution - revealing critical flaws such as fake App identity, visual spoofing, indirect prompt injection, and unauthorized privilege escalation stemming from a reliance on unstructured visual data.   To address these challenges, we propose Aura, an Agent Universal Runtime Architecture for a clean-slate secure agent OS. Aura replaces brittle GUI scraping with a structured, agent-native interaction model. It adopts a Hub-and-Spoke topology where a privileged System Agent orchestrates intent, sandboxed App Agents execute domain-specific tasks, and the Agent Kernel mediates all communication. The Agent Kernel enforces four defense pillars: (i) cryptographic identity binding via a Global Agent Registry; (ii) semantic input sanitization through a multilayer Semantic Firewall; (iii) cognitive integrity via taint-aware memory and plan-trajectory alignment; and (iv) granular access control with non-deniable auditing. Evaluation on MobileSafetyBench shows that, compared to Doubao, Aura improves low-risk Task Success Rate from roughly 75% to 94.3%, reduces high-risk Attack Success Rate from roughly 40% to 4.4%, and achieves near-order-of-magnitude latency gains. These results demonstrate Aura as a viable, secure alternative to the \"Screen-as-Interface\" paradigm.",
      "tr": "**Makale Başlığı:** Kör Tanrılar ve Bozuk Ekranlar: Güvenli, Amaç Odaklı Bir Mobil Ajan İşletim Sistemi Mimarisi\n\n**Özet:**\n\nLarge Language Models (LLMs) alanındaki evrim, mobil bilişimi App-centric etkileşimlerden sistem düzeyinde otonom ajanlara doğru kaydırmıştır. Mevcut uygulamalar, yapısal güvenlik açıklarını miras alan ve mobil ekosistemin ekonomik temelleriyle çelişen bir \"Screen-as-Interface\" paradigmasına büyük ölçüde dayanmaktadır. Bu makalede, Doubao Mobile Assistant'ı temsili bir vaka olarak kullanarak, son teknoloji mobil ajanların sistematik bir güvenlik analizini gerçekleştiriyoruz. Tehdit ortamını dört boyuta ayırıyoruz: Agent Identity, External Interface, Internal Reasoning ve Action Execution. Bu ayrıştırma, yapılandırılmamış görsel verilere dayanmaktan kaynaklanan sahte Uygulama kimliği, görsel sahtecilik, dolaylı prompt injection ve yetkisiz ayrıcalık yükseltme gibi kritik kusurları ortaya koymaktadır.\n\nBu zorlukları ele almak için, temiz bir başlangıçla güvenli bir ajan işletim sistemi için bir Agent Universal Runtime Architecture olan Aura'yı öneriyoruz. Aura, kırılgan GUI scraping'in yerini yapılandırılmış, ajan-native bir etkileşim modeliyle değiştirir. Aura, ayrıcalıklı bir System Agent'ın niyeti (intent) orkestre ettiği, sandboxed App Agent'ların alan-spesifik görevleri yürüttüğü ve Agent Kernel'ın tüm iletişimi aracılık ettiği bir Hub-and-Spoke topolojisi benimser. Agent Kernel, dört savunma filtresini zorlar: (i) Global Agent Registry aracılığıyla kriptografik kimlik bağlama (cryptographic identity binding); (ii) çok katmanlı bir Semantic Firewall aracılığıyla anlamsal girdi temizleme (semantic input sanitization); (iii) taint-aware memory ve plan-trajectory alignment yoluyla bilişsel bütünlük (cognitive integrity); ve (iv) reddedilemez denetim (non-deniable auditing) ile granüler erişim kontrolü (granular access control). MobileSafetyBench üzerindeki değerlendirmeler, Aura'nın Doubao'ya kıyasla düşük riskli Task Success Rate'i kabaca %75'ten %94,3'e, yüksek riskli Attack Success Rate'i kabaca %40'tan %4,4'e iyileştirdiğini ve neredeyse büyüklük sırası gecikme (latency) kazançları elde ettiğini göstermektedir. Bu sonuçlar, Aura'yı \"Screen-as-Interface\" paradigmasına uygulanabilir, güvenli bir alternatif olarak kanıtlamaktadır."
    }
  },
  {
    "id": "2602.10787v1",
    "title": "VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection",
    "authors": [
      "Samal Mukhtar",
      "Yinghua Yao",
      "Zhu Sun",
      "Mustafa Mustafa",
      "Yew Soon Ong"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.IR"
    ],
    "link": "http://arxiv.org/abs/2602.10787v1",
    "pdf_link": "https://arxiv.org/pdf/2602.10787v1",
    "content": {
      "en": "Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.",
      "tr": "Makale Başlığı: VulReaD: knowledge-graph-guided Yazılım Açığı reasoning ve Tespiti\n\nÖzet:\nYazılım açığı tespiti (Software vulnerability detection - SVD), modern sistemlerde kritik bir zorluktur. Large language models (LLMs), tahminlerin yanı sıra doğal dil açıklamaları sunar, ancak çoğu çalışma ikili değerlendirmeye odaklanır ve açıklamalar genellikle Common Weakness Enumeration (CWE) kategorileriyle anlamsal tutarlılıktan yoksundur. CWE seviyesi reasoning'e yönelik ikili sınıflandırmanın ötesine geçen, açığı reasoning ve tespiti için a knowledge-graph-guided yaklaşım olan VulReaD'ı öneriyoruz. VulReaD, bir güvenlik knowledge graph (KG) ile anlamsal bir omurga olarak yararlanır ve manuel annotasyonlar olmadan öğrenci model eğitimi sağlayan CWE-tutarlı kontrastif reasoning supervision üretmek için güçlü bir teacher LLM kullanır. Öğrenciler, taksonomiye uyumlu reasoning'i teşvik ederken desteklenmeyen açıklamaları bastırmak için Odds Ratio Preference Optimization (ORPO) ile fine-tune edilir. Üç gerçek dünya veri kümesi genelinde, VulReaD, ikili F1 skorunu %8-10 ve çok sınıflı sınıflandırmayı %30 Macro-F1 ve %18 Micro-F1 oranında state-of-the-art temellere kıyasla iyileştirir. Sonuçlar, LLM'lerin ikili tespitinde derin öğrenme temellerinden daha iyi performans gösterdiğini ve KG-guided reasoning'in CWE kapsamını ve yorumlanabilirliğini artırdığını göstermektedir."
    }
  },
  {
    "id": "2602.10780v1",
    "title": "Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks",
    "authors": [
      "Enrico Ahlers",
      "Daniel Passon",
      "Yannic Noller",
      "Lars Grunske"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2602.10780v1",
    "pdf_link": "https://arxiv.org/pdf/2602.10780v1",
    "content": {
      "en": "Machine learning models are increasingly present in our everyday lives; as a result, they become targets of adversarial attackers seeking to manipulate the systems we interact with. A well-known vulnerability is a backdoor introduced into a neural network by poisoned training data or a malicious training process. Backdoors can be used to induce unwanted behavior by including a certain trigger in the input. Existing mitigations filter training data, modify the model, or perform expensive input modifications on samples. If a vulnerable model has already been deployed, however, those strategies are either ineffective or inefficient. To address this gap, we propose our inference-time backdoor mitigation approach called FIRE (Feature-space Inference-time REpair). We hypothesize that a trigger induces structured and repeatable changes in the model's internal representation. We view the trigger as directions in the latent spaces between layers that can be applied in reverse to correct the inference mechanism. Therefore, we turn the backdoored model against itself by manipulating its latent representations and moving a poisoned sample's features along the backdoor directions to neutralize the trigger. Our evaluation shows that FIRE has low computational overhead and outperforms current runtime mitigations on image benchmarks across various attacks, datasets, and network architectures.",
      "tr": "**Makale Başlığı: Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks**\n\n**Özet:**\n\nMakine öğrenmesi modelleri günlük hayatımızda giderek daha fazla yer almakta ve bu durum, etkileşimde bulunduğumuz sistemleri manipüle etmeye çalışan düşmanca saldırganlar için hedef haline gelmelerine neden olmaktadır. İyi bilinen bir güvenlik açığı, zehirlenmiş eğitim verileri veya kötü niyetli bir eğitim süreciyle bir yapay sinir ağına yerleştirilen bir backdoor'dur. Backdoor'lar, girişe belirli bir trigger eklenerek istenmeyen davranışlara yol açmak için kullanılabilir. Mevcut mitigasyon yöntemleri eğitim verilerini filtrelemekte, modeli değiştirmekte veya örnekler üzerinde pahalı giriş modifikasyonları gerçekleştirmektedir. Ancak, savunmasız bir model zaten konuşlandırılmışsa, bu stratejiler ya etkisiz ya da verimsizdir. Bu boşluğu gidermek için, inference-time backdoor mitigation yaklaşımımız olan FIRE (Feature-space Inference-time REpair) öneriyoruz. Bir trigger'ın modelin içsel temsilinde yapısal ve tekrarlanabilir değişikliklere neden olduğu varsayımına dayanıyoruz. Trigger'ı, inference mekanizmasını düzeltmek için tersine uygulanabilen katmanlar arasındaki latent spaces'deki yönler olarak görüyoruz. Dolayısıyla, gizli temsillerini manipüle ederek ve zehirlenmiş bir örneğin özelliklerini backdoor yönleri boyunca hareket ettirerek trigger'ı etkisiz hale getirerek, backdoor'lu modeli kendisine karşı kullanıyoruz. Değerlendirmemiz, FIRE'ın düşük hesaplama yüküne sahip olduğunu ve çeşitli saldırılar, veri kümeleri ve ağ mimarileri genelinde görüntü benchmark'larında mevcut runtime mitigasyonlarından daha iyi performans gösterdiğini göstermektedir."
    }
  },
  {
    "id": "2602.10750v1",
    "title": "SecureScan: An AI-Driven Multi-Layer Framework for Malware and Phishing Detection Using Logistic Regression and Threat Intelligence Integration",
    "authors": [
      "Rumman Firdos",
      "Aman Dangi"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.10750v1",
    "pdf_link": "https://arxiv.org/pdf/2602.10750v1",
    "content": {
      "en": "The growing sophistication of modern malware and phishing campaigns has diminished the effectiveness of traditional signature-based intrusion detection systems. This work presents SecureScan, an AI-driven, triple-layer detection framework that integrates logistic regression-based classification, heuristic analysis, and external threat intelligence via the VirusTotal API for comprehensive triage of URLs, file hashes, and binaries. The proposed architecture prioritizes efficiency by filtering known threats through heuristics, classifying uncertain samples using machine learning, and validating borderline cases with third-party intelligence. On benchmark datasets, SecureScan achieves 93.1 percent accuracy with balanced precision (0.87) and recall (0.92), demonstrating strong generalization and reduced overfitting through threshold-based decision calibration. A calibrated threshold and gray-zone logic (0.45-0.55) were introduced to minimize false positives and enhance real-world stability. Experimental results indicate that a lightweight statistical model, when augmented with calibrated verification and external intelligence, can achieve reliability and performance comparable to more complex deep learning systems.",
      "tr": "Elbette, akademik makalenin başlığını ve özetini istenen şekilde Türkçeye çevirdim:\n\n**Makale Başlığı:** SecureScan: Logistic Regression ve Threat Intelligence Entegrasyonu Kullanarak Kötü Amaçlı Yazılım ve Kimlik Avı Tespiti İçin Yapay Zeka Güdümlü Çok Katmanlı Bir Çerçeve\n\n**Özet:**\nModern kötü amaçlı yazılım ve kimlik avı kampanyalarının artan sofistikeliği, geleneksel imza tabanlı saldırı tespit sistemlerinin etkinliğini azaltmıştır. Bu çalışma, URL'ler, dosya hash'leri ve çalıştırılabilir dosyaların kapsamlı bir şekilde sınıflandırılması için VirusTotal API aracılığıyla lojistik regresyon tabanlı sınıflandırmayı, sezgisel analizi ve harici threat intelligence'ı entegre eden yapay zeka güdümlü, üç katmanlı bir tespit çerçevesi olan SecureScan'ı sunmaktadır. Önerilen mimari, bilinen tehditleri sezgisel yöntemlerle filtreleyerek, belirsiz örnekleri machine learning kullanarak sınıflandırarak ve sınırda kalan durumları üçüncü taraf zeka ile doğrulayarak verimliliği önceliklendirir. Benchmark veri kümelerinde, SecureScan, dengeli precision (0.87) ve recall (0.92) ile yüzde 93,1 doğruluk elde ederek, eşik tabanlı karar kalibrasyonu yoluyla güçlü genelleme ve azaltılmış overfitting sergilemiştir. False positive'leri en aza indirmek ve gerçek dünya kararlılığını artırmak için kalibre edilmiş bir eşik ve gray-zone logic (0.45-0.55) tanıtılmıştır. Deneysel sonuçlar, kalibre edilmiş doğrulama ve harici zeka ile desteklenen hafif bir istatistiksel modelin, daha karmaşık deep learning sistemlerine kıyasla karşılaştırılabilir güvenilirlik ve performans elde edebileceğini göstermektedir."
    }
  },
  {
    "id": "2602.10510v1",
    "title": "Privacy-Utility Tradeoffs in Quantum Information Processing",
    "authors": [
      "Theshani Nuradha",
      "Sujeet Bhalerao",
      "Felix Leditzky"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "quant-ph",
      "cs.CR",
      "cs.IT",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.10510v1",
    "pdf_link": "https://arxiv.org/pdf/2602.10510v1",
    "content": {
      "en": "When sensitive information is encoded in data, it is important to ensure the privacy of information when attempting to learn useful information from the data. There is a natural tradeoff whereby increasing privacy requirements may decrease the utility of a learning protocol. In the quantum setting of differential privacy, such tradeoffs between privacy and utility have so far remained largely unexplored. In this work, we study optimal privacy-utility tradeoffs for both generic and application-specific utility metrics when privacy is quantified by $(\\varepsilon,δ)$-quantum local differential privacy. In the generic setting, we focus on optimizing fidelity and trace distance between the original state and the privatized state. We show that the depolarizing mechanism achieves the optimal utility for given privacy requirements. We then study the specific application of learning the expectation of an observable with respect to an input state when only given access to privatized states. We derive a lower bound on the number of samples of privatized data required to achieve a fixed accuracy guarantee with high probability. To prove this result, we employ existing lower bounds on private quantum hypothesis testing, thus showcasing the first operational use of them. We also devise private mechanisms that achieve optimal sample complexity with respect to the privacy parameters and accuracy parameters, demonstrating that utility can be significantly improved for specific tasks in contrast to the generic setting. In addition, we show that the number of samples required to privately learn observable expectation values scales as $Θ((\\varepsilon β)^{-2})$, where $\\varepsilon \\in (0,1)$ is the privacy parameter and $β$ is the accuracy tolerance. We conclude by initiating the study of private classical shadows, which promise useful applications for private learning tasks.",
      "tr": "**Makale Başlığı:** Kuantum Bilgi İşlemede Gizlilik-Fayda Dengeleri\n\n**Özet:**\n\nVeri içinde hassas bilgiler kodlandığında, veriden faydalı bilgiler öğrenmeye çalışırken bilginin gizliliğini sağlamak önemlidir. Gizlilik gereksinimlerindeki artışın bir öğrenme protokolünün faydasını azaltabileceği doğal bir denge mevcuttur. Diferansiyel gizliliğin kuantum ortamında, gizlilik ve fayda arasındaki bu tür dengeler bugüne kadar büyük ölçüde keşfedilmemiş kalmıştır. Bu çalışmada, gizliliğin $(\\varepsilon,δ)$-quantum local differential privacy ile ölçüldüğü durumlarda hem genel hem de uygulamaya özel fayda metrikleri için optimal gizlilik-fayda dengelerini inceliyoruz. Genel ayarda, orijinal durum ve özelleştirilmiş durum arasındaki fidelity ve trace distance optimizasyonuna odaklanıyoruz. Depolarizing mechanism'in verilen gizlilik gereksinimleri için optimal faydayı sağladığını gösteriyoruz. Daha sonra, yalnızca özelleştirilmiş durumlara erişimimiz olduğunda bir girdi durumuna göre bir observable'ın beklentisini öğrenme spesifik uygulamayı inceliyoruz. Yüksek olasılıkla sabit bir doğruluk garantisi elde etmek için gereken özelleştirilmiş veri örneklerinin sayısına ilişkin bir alt sınır türetiyoruz. Bu sonucu kanıtlamak için, private quantum hypothesis testing üzerindeki mevcut alt sınırları kullanıyoruz, böylece onların ilk operasyonel kullanımını gösteriyoruz. Ayrıca, gizlilik parametreleri ve doğruluk parametrelerine göre optimal sample complexity'yi elde eden private mechanisms tasarlıyoruz, böylece genel ayara kıyasla spesifik görevler için faydanın önemli ölçüde iyileştirilebileceğini gösteriyoruz. Ek olarak, observable beklenti değerlerini gizlice öğrenmek için gereken örnek sayısının $Θ((\\varepsilon β)^{-2})$ olarak ölçeklendiğini gösteriyoruz, burada $\\varepsilon \\in (0,1)$ gizlilik parametresi ve $β$ doğruluk toleransıdır. Private classical shadows'un incelenmesini başlatarak sonuçlandırıyoruz, bu da private learning görevleri için faydalı uygulamalar vaat etmektedir."
    }
  },
  {
    "id": "2602.10481v1",
    "title": "Protecting Context and Prompts: Deterministic Security for Non-Deterministic AI",
    "authors": [
      "Mohan Rajagopalan",
      "Vinay Rao"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2602.10481v1",
    "pdf_link": "https://arxiv.org/pdf/2602.10481v1",
    "content": {
      "en": "Large Language Model (LLM) applications are vulnerable to prompt injection and context manipulation attacks that traditional security models cannot prevent. We introduce two novel primitives--authenticated prompts and authenticated context--that provide cryptographically verifiable provenance across LLM workflows. Authenticated prompts enable self-contained lineage verification, while authenticated context uses tamper-evident hash chains to ensure integrity of dynamic inputs. Building on these primitives, we formalize a policy algebra with four proven theorems providing protocol-level Byzantine resistance--even adversarial agents cannot violate organizational policies. Five complementary defenses--from lightweight resource controls to LLM-based semantic validation--deliver layered, preventative security with formal guarantees. Evaluation against representative attacks spanning 6 exhaustive categories achieves 100% detection with zero false positives and nominal overhead. We demonstrate the first approach combining cryptographically enforced prompt lineage, tamper-evident context, and provable policy reasoning--shifting LLM security from reactive detection to preventative guarantees.",
      "tr": "İşte akademik makalenin başlığı ve özetinin Türkçeye çevirisi:\n\n**Makale Başlığı:** Bağlam ve İstemlerin Korunması: Belirlenimsiz Yapay Zeka İçin Deterministik Güvenlik\n\n**Özet:**\nBüyük Dil Modeli (LLM) uygulamaları, geleneksel güvenlik modellerinin engelleyemediği prompt injection ve bağlam manipülasyonu saldırılarına karşı savunmasızdır. LLM iş akışları boyunca kriptografik olarak doğrulanabilir bir köken sağlayan iki yeni ilkel - authenticated prompts ve authenticated context - sunuyoruz. Authenticated prompts, kendi içinde tutarlı bir soy (lineage) doğrulaması sağlarken, authenticated context dinamik girdilerin bütünlüğünü sağlamak için kurcalamaya dayanıklı hash zincirlerini kullanır. Bu ilkeler üzerine inşa ederek, protokol düzeyinde Byzantine resistance sağlayan ve dört kanıtlanmış teoremi içeren bir policy algebra formalize ediyoruz -- hatta düşmanca ajanlar dahi kurumsal politikaları ihlal edemez. Hafif kaynak kontrollerinden LLM tabanlı anlamsal doğrulamaya kadar beş tamamlayıcı savunma, biçimsel garantilerle katmanlı, önleyici güvenlik sunar. 6 kapsamlı kategoriye yayılan temsili saldırılara karşı yapılan değerlendirme, sıfır yanlış pozitif ve nominal ek yük ile %100 tespit sağlamıştır. Kriptografik olarak zorlanmış prompt lineage, kurcalamaya dayanıklı bağlam ve kanıtlanabilir policy reasoning'i birleştiren ilk yaklaşımı gösteriyoruz -- LLM güvenliğini reaktif tespitten önleyici garantilere taşıyoruz."
    }
  },
  {
    "id": "2602.10478v1",
    "title": "GPU-Fuzz: Finding Memory Errors in Deep Learning Frameworks",
    "authors": [
      "Zihao Li",
      "Hongyi Lu",
      "Yanan Guo",
      "Zhenkai Zhang",
      "Shuai Wang"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.10478v1",
    "pdf_link": "https://arxiv.org/pdf/2602.10478v1",
    "content": {
      "en": "GPU memory errors are a critical threat to deep learning (DL) frameworks, leading to crashes or even security issues. We introduce GPU-Fuzz, a fuzzer locating these issues efficiently by modeling operator parameters as formal constraints. GPU-Fuzz utilizes a constraint solver to generate test cases that systematically probe error-prone boundary conditions in GPU kernels. Applied to PyTorch, TensorFlow, and PaddlePaddle, we uncovered 13 unknown bugs, demonstrating the effectiveness of GPU-Fuzz in finding memory errors.",
      "tr": "**Makale Başlığı:** GPU-Fuzz: Derin Öğrenme Çerçevelerinde Bellek Hatalarını Bulma\n\n**Özet:**\n\nGPU bellek hataları, derin öğrenme (DL) çerçeveleri için kritik bir tehdit olup, çökemelere ve hatta güvenlik sorunlarına yol açmaktadır. Operatör parametrelerini formal kısıtlamalar olarak modelleyerek bu sorunları verimli bir şekilde tespit eden bir fuzzer olan GPU-Fuzz'ı sunuyoruz. GPU-Fuzz, GPU kernel'lerindeki hata eğilimli sınır koşulları sistematik olarak inceleyen test senaryoları üretmek için bir constraint solver kullanır. PyTorch, TensorFlow ve PaddlePaddle'a uygulanan GPU-Fuzz, 13 adet bilinmeyen hatayı ortaya çıkararak bellek hatalarını bulmadaki etkinliğini göstermiştir."
    }
  },
  {
    "id": "2602.10465v1",
    "title": "Authenticated Workflows: A Systems Approach to Protecting Agentic AI",
    "authors": [
      "Mohan Rajagopalan",
      "Vinay Rao"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2602.10465v1",
    "pdf_link": "https://arxiv.org/pdf/2602.10465v1",
    "content": {
      "en": "Agentic AI systems automate enterprise workflows but existing defenses--guardrails, semantic filters--are probabilistic and routinely bypassed. We introduce authenticated workflows, the first complete trust layer for enterprise agentic AI. Security reduces to protecting four fundamental boundaries: prompts, tools, data, and context. We enforce intent (operations satisfy organizational policies) and integrity (operations are cryptographically authentic) at every boundary crossing, combining cryptographic elimination of attack classes with runtime policy enforcement. This delivers deterministic security--operations either carry valid cryptographic proof or are rejected. We introduce MAPL, an AI-native policy language that expresses agentic constraints dynamically as agents evolve and invocation context changes, scaling as O(log M + N) policies versus O(M x N) rules through hierarchical composition with cryptographic attestations for workflow dependencies. We prove practicality through a universal security runtime integrating nine leading frameworks (MCP, A2A, OpenAI, Claude, LangChain, CrewAI, AutoGen, LlamaIndex, Haystack) through thin adapters requiring zero protocol modifications. Formal proofs establish completeness and soundness. Empirical validation shows 100% recall with zero false positives across 174 test cases, protection against 9 of 10 OWASP Top 10 risks, and complete mitigation of two high impact production CVEs.",
      "tr": "Kesinlikle, istenen şartlara uygun olarak makale başlığını ve özetini Türkçeye çevirdim:\n\n**Makale Başlığı:** Yetkilendirilmiş İş Akışları: Ajan Yapay Zekayı Koruma İçin Bir Sistem Yaklaşımı\n\n**Özet:**\n\nAjan yapay zeka sistemleri kurumsal iş akışlarını otomatikleştirir ancak mevcut savunmalar — guardrails, semantic filters — olasılıksaldır ve düzenli olarak atlatılmaktadır. Kurumsal ajan yapay zeka için ilk eksiksiz trust layer olan yetkilendirilmiş iş akışlarını sunuyoruz. Security, dört temel boundary'yi korumaya indirgenir: prompts, tools, data ve context. Her boundary crossing noktasında intent (operations, organizasyonel politikalara uygun olmalıdır) ve integrity (operations, cryptographically authentic olmalıdır) zorunlu kılarız. Bu, saldırı sınıflarının cryptographic elimination'ını runtime policy enforcement ile birleştirir. Bu, deterministic security sağlar — operations ya geçerli cryptographic proof taşır ya da reddedilir. Biz, MAPL'ı — bir AI-native policy language — tanıtıyoruz. Bu dil, ajanlar geliştikçe ve invocation context değiştikçe agentic constraints'leri dinamik olarak ifade eder. Hierarchical composition ve workflow dependencies için cryptographic attestations ile O(log M + N) policies ölçeklenir, O(M x N) rules'a kıyasla. Dokuz önde gelen framework (MCP, A2A, OpenAI, Claude, LangChain, CrewAI, AutoGen, LlamaIndex, Haystack) için ince adaptörler aracılığıyla ve sıfır protocol modification gerektirerek evrensel bir security runtime ile pratikliği kanıtlıyoruz. Formal proofs, completeness ve soundness'u kurar. Ampirik doğrulama, 174 test senaryosunda %100 recall ve sıfır false positives, OWASP Top 10 risklerinin 9'una karşı koruma ve iki yüksek etkiye sahip production CVE'nin tam olarak hafifletildiğini gösterir."
    }
  },
  {
    "id": "2602.10100v1",
    "title": "Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy",
    "authors": [
      "Júlio Oliveira",
      "Rodrigo Ferreira",
      "André Riker",
      "Glaucio H. S. Carvalho",
      "Eirini Eleni Tsilopoulou"
    ],
    "published_date": "2026-02-10",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.10100v1",
    "pdf_link": "https://arxiv.org/pdf/2602.10100v1",
    "content": {
      "en": "Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. However, there is a side effect adding DP: it harms the explainability of the system. So, this paper also presents the impact of DP protection on the explainability of the ML model. The carried out performance assessment shows improvements of FEXT-DP in terms of a faster training, i.e., numbers of rounds, Mean Squared Error and explainability.",
      "tr": "**Makale Başlığı:** Açıklanabilir Federe Öğrenmeye Doğru: Differential Privacy'nin Etkisini Anlamak\n\n**Özet:**\n\nVeri gizliliği ve eXplainable Artificial Intelligence (XAI), modern Makine Öğrenmesi sistemleri için iki önemli husustur. Veri gizliliğini artırmak amacıyla, son makine öğrenmesi modelleri bir Federated Learning (FL) sistemi olarak tasarlanmıştır. Bunun üzerine, Differential Privacy (DP) aracılığıyla ek gizlilik katmanları eklenebilir. Diğer yandan, açıklanabilirliği geliştirmek için ML, daha az sayıda özellik ve daha az karmaşık iç mimariye sahip, daha yorumlanabilir yaklaşımları göz önünde bulundurmalıdır. Bu bağlamda, bu makale, geliştirilmiş veri gizliliğini açıklanabilirlikle birleştiren bir makine öğrenmesi (ML) modeli elde etmeyi amaçlamaktadır. Bu nedenle, aşağıdaki özelliklere sahip, Federated EXplainable Trees with Differential Privacy (FEXT-DP) adında bir FL çözümü öneriyoruz: (i) hafif olmaları ve sinir ağları tabanlı FL sistemlerine kıyasla üstün açıklanabilirliğe sahip olmaları nedeniyle Decision Trees üzerine kurulmuştur; (ii) Tree-Based modele Differential Privacy (DP) uygulayarak ek bir veri gizliliği koruma katmanı sağlar. Ancak, DP eklemenin bir yan etkisi vardır: sistemin açıklanabilirliğine zarar verir. Bu nedenle, bu makale aynı zamanda DP korumasının ML modelinin açıklanabilirliği üzerindeki etkisini de sunmaktadır. Yapılan performans değerlendirmesi, FEXT-DP'nin daha hızlı eğitim (yani, tur sayısı), Mean Squared Error ve açıklanabilirlik açısından iyileşmelerini göstermektedir."
    }
  }
]