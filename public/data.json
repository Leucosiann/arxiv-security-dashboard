[
  {
    "id": "2601.02257v1",
    "title": "Improved Accuracy for Private Continual Cardinality Estimation in Fully Dynamic Streams via Matrix Factorization",
    "authors": [
      "Joel Daniel Andersson",
      "Palak Jain",
      "Satchit Sivakumar"
    ],
    "published_date": "2026-01-05",
    "tags": [
      "cs.CR",
      "cs.DS",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.02257v1",
    "pdf_link": "https://arxiv.org/pdf/2601.02257v1",
    "content": {
      "en": "We study differentially-private statistics in the fully dynamic continual observation model, where many updates can arrive at each time step and updates to a stream can involve both insertions and deletions of an item. Earlier work (e.g., Jain et al., NeurIPS 2023 for counting distinct elements; Raskhodnikova & Steiner, PODS 2025 for triangle counting with edge updates) reduced the respective cardinality estimation problem to continual counting on the difference stream associated with the true function values on the input stream. In such reductions, a change in the original stream can cause many changes in the difference stream, this poses a challenge for applying private continual counting algorithms to obtain optimal error bounds. We improve the accuracy of several such reductions by studying the associated $\\ell_p$-sensitivity vectors of the resulting difference streams and isolating their properties.   We demonstrate that our framework gives improved bounds for counting distinct elements, estimating degree histograms, and estimating triangle counts (under a slightly relaxed privacy model), thus offering a general approach to private continual cardinality estimation in streaming settings. Our improved accuracy stems from tight analysis of known factorization mechanisms for the counting matrix in this setting; the key technical challenge is arguing that one can use state-of-the-art factorizations for sensitivity vector sets with the properties we isolate. Empirically and analytically, we demonstrate that our improved error bounds offer a substantial improvement in accuracy for cardinality estimation problems over a large range of parameters.",
      "tr": "Makale Başlığı: Matris Ayrıştırma Yoluyla Tamamen Dinamik Akışlarda Özel Sürekli Kardinalite Tahmininde İyileştirilmiş Doğruluk\n\nÖzet:\nTamamen dinamik sürekli gözlem modelinde, her zaman adımında birden çok güncellemenin gelebildiği ve bir akışa yapılan güncellemelerin bir öğenin eklenmesini ve silinmesini içerebildiği durumlarda, differantiyel olarak özel istatistikleri inceliyoruz. Daha önceki çalışmalar (örneğin, Jain ve ark., NeurIPS 2023, benzersiz öğelerin sayılması için; Raskhodnikova & Steiner, PODS 2025, kenar güncellemeleriyle üçgen sayımı için), ilgili kardinalite tahmin problemini, giriş akışındaki gerçek fonksiyon değerleriyle ilişkili fark akışında sürekli saymaya indirgemiştir. Bu tür indirgemelerde, orijinal akıştaki bir değişiklik, fark akışında birçok değişikliğe neden olabilir; bu da optimum hata sınırları elde etmek için özel sürekli sayım algoritmalarının uygulanmasına bir zorluk teşkil eder. Sonuç olarak ortaya çıkan fark akışlarının $\\ell_p$-sensitivity vektörlerini inceleyerek ve özelliklerini izole ederek, bu tür indirgemelerin doğruluğunu iyileştiriyoruz. Çerçevemizin, benzersiz öğelerin sayılması, derece histogramlarının tahmin edilmesi ve üçgen sayılarının tahmin edilmesi (hafif gevşetilmiş bir gizlilik modeli altında) için iyileştirilmiş sınırlar sağladığını gösteriyoruz, böylece akış ortamlarında özel sürekli kardinalite tahmini için genel bir yaklaşım sunuyoruz. İyileştirilmiş doğruluğumuz, bu ortamdaki sayım matrisi için bilinen faktörizasyon mekanizmalarının sıkı analizinden kaynaklanmaktadır; temel teknik zorluk, izole ettiğimiz özelliklere sahip duyarlılık vektörü kümeleri için en yeni faktörizasyonların kullanılabileceğini savunmaktır. Ampirik ve analitik olarak, iyileştirilmiş hata sınırlarımızın, geniş bir parametre aralığında kardinalite tahmin problemleri için doğruluğu önemli ölçüde iyileştirdiğini gösteriyoruz."
    }
  },
  {
    "id": "2601.01786v1",
    "title": "UnPII: Unlearning Personally Identifiable Information with Quantifiable Exposure Risk",
    "authors": [
      "Intae Jeon",
      "Yujeong Kwon",
      "Hyungjoon Koo"
    ],
    "published_date": "2026-01-05",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.01786v1",
    "pdf_link": "https://arxiv.org/pdf/2601.01786v1",
    "content": {
      "en": "The ever-increasing adoption of Large Language Models in critical sectors like finance, healthcare, and government raises privacy concerns regarding the handling of sensitive Personally Identifiable Information (PII) during training. In response, regulations such as European Union's General Data Protection Regulation (GDPR) mandate the deletion of PII upon requests, underscoring the need for reliable and cost-effective data removal solutions. Machine unlearning has emerged as a promising direction for selectively forgetting data points. However, existing unlearning techniques typically apply a uniform forgetting strategy that neither accounts for the varying privacy risks posed by different PII attributes nor reflects associated business risks. In this work, we propose UnPII, the first PII-centric unlearning approach that prioritizes forgetting based on the risk of individual or combined PII attributes. To this end, we introduce the PII risk index (PRI), a composite metric that incorporates multiple dimensions of risk factors: identifiability, sensitivity, usability, linkability, permanency, exposability, and compliancy. The PRI enables a nuanced evaluation of privacy risks associated with PII exposures and can be tailored to align with organizational privacy policies. To support realistic assessment, we systematically construct a synthetic PII dataset (e.g., 1,700 PII instances) that simulates realistic exposure scenarios. UnPII seamlessly integrates with established unlearning algorithms, such as Gradient Ascent, Negative Preference Optimization, and Direct Preference Optimization, without modifying their underlying principles. Our experimental results demonstrate that UnPII achieves the improvements of accuracy up to 11.8%, utility up to 6.3%, and generalizability up to 12.4%, respectively, while incurring a modest fine-tuning overhead of 27.5% on average during unlearning.",
      "tr": "Makale Başlığı: UnPII: Nicelendirilebilir Maruz Kalma Riski ile Kişisel Olarak Tanımlanabilir Bilgilerin Unlearning'i\n\nÖzet:\nFinans, sağlık ve devlet gibi kritik sektörlerde Büyük Dil Modellerinin (Large Language Models) giderek artan benimsenmesi, eğitim sırasında hassas Kişisel Olarak Tanımlanabilir Bilgilerin (PII) işlenmesiyle ilgili gizlilik endişelerini artırmaktadır. Buna yanıt olarak, Avrupa Birliği'nin Genel Veri Koruma Tüzüğü (GDPR) gibi düzenlemeler, talepler üzerine PII'nin silinmesini zorunlu kılarak güvenilir ve uygun maliyetli veri kaldırma çözümlerine olan ihtiyacı vurgulamaktadır. Makine unlearning'i (Machine unlearning), veri noktalarının seçici olarak unutulması yönünde umut vadeden bir yön olarak ortaya çıkmıştır. Ancak mevcut unlearning teknikleri tipik olarak, farklı PII özniteliklerinin oluşturduğu değişen gizlilik risklerini dikkate almayan ve ilişkili iş risklerini yansıtmayan tekdüze bir unutma stratejisi uygular. Bu çalışmada, bireysel veya birleşik PII özniteliklerinin riskine dayanarak unutmayı önceliklendiren ilk PII odaklı unlearning yaklaşımı olan UnPII'yi öneriyoruz. Bu amaçla, PII risk index (PRI)'i tanıtmaktayız; bu, kimlik belirleyebilirlik, hassasiyet, kullanılabilirlik, bağlanabilirlik, kalıcılık, maruz kalabilirlik ve uyumluluk gibi çok boyutlu risk faktörlerini içeren bileşik bir metriktir. PRI, PII maruziyetleriyle ilişkili gizlilik risklerinin nüanslı bir değerlendirmesini sağlar ve kurumsal gizlilik politikalarıyla uyumlu olacak şekilde uyarlanabilir. Gerçekçi bir değerlendirmeyi desteklemek için, gerçekçi maruz kalma senaryolarını simüle eden sentetik bir PII veri seti (örneğin, 1.700 PII örneği) sistematik olarak oluşturduk. UnPII, Gradient Ascent, Negative Preference Optimization ve Direct Preference Optimization gibi yerleşik unlearning algoritmalarıyla, temel prensiplerini değiştirmeden sorunsuz bir şekilde entegre olur. Deneysel sonuçlarımız, UnPII'nin ortalama %27,5'lik mütevazı bir fine-tuning overhead'i (fine-tuning overhead) karşılığında, sırasıyla %11,8'e kadar doğruluk, %6,3'e kadar fayda ve %12,4'e kadar genelleştirilebilirlik iyileştirmeleri sağladığını göstermektedir."
    }
  },
  {
    "id": "2601.01747v1",
    "title": "Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization",
    "authors": [
      "Jiwei Guan",
      "Haibo Jin",
      "Haohan Wang"
    ],
    "published_date": "2026-01-05",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.01747v1",
    "pdf_link": "https://arxiv.org/pdf/2601.01747v1",
    "content": {
      "en": "Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs",
      "tr": "İşte akademik makale başlığı ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization\n\n**Özet:**\nBüyük Vision-Language Modellerindeki (LVLM'ler) son gelişmeler, çeşitli multimodal görevlerde çığır açan yetenekler sergilemiştir. Bununla birlikte, bu modeller, zararlı çıktılara neden olmak için güvenlik mekanizmalarını atlatmak üzere karmaşık pertürbasyonlar oluşturan rakip unsurların gerçekleştirdiği adversarial jailbreak saldırılarına karşı hala savunmasızdır. Mevcut white-box saldırı yöntemleri, tam model erişilebilirliği gerektirir, hesaplama maliyetlerinden muzdariptir ve yetersiz adversarial transfer edilebilirliği sergiler; bu da onları gerçek dünya, black-box ortamları için pratik olmaktan çıkarır. Bu sınırlamaları ele almak için, Simultaneous Perturbation Stochastic Approximation (ZO-SPSA) kullanarak Zeroth-Order optimizasyon aracılığıyla LVLM'lere karşı bir black-box jailbreak saldırısı öneriyoruz. ZO-SPSA üç temel avantaj sunar: (i) model bilgisi gerektirmeden girdi-çıktı etkileşimleri aracılığıyla gradyan-sız yaklaşım, (ii) yedek model olmadan model-agnostik optimizasyon ve (iii) azaltılmış GPU bellek tüketimi ile daha düşük kaynak gereksinimleri. ZO-SPSA'yı InstructBLIP, LLaVA ve MiniGPT-4 dahil olmak üzere üç LVLM üzerinde değerlendirdik. InstructBLIP üzerinde %83,0 ile en yüksek jailbreak başarı oranına ulaştık ve bu sırada white-box yöntemleriyle karşılaştırılabilir düzeyde algılanamayan pertürbasyonları koruduk. Dahası, MiniGPT-4'ten üretilen adversarial örnekler, %64,18'e varan ASR ile diğer LVLM'lere karşı güçlü bir transfer edilebilirliği sergilemektedir. Bu bulgular, black-box jailbreak'lerin gerçek dünya fizibilitesini vurgulamakta ve mevcut LVLM'lerin güvenlik mekanizmalarındaki kritik zayıflıkları ortaya koymaktadır."
    }
  },
  {
    "id": "2601.01673v1",
    "title": "Exposing Hidden Interfaces: LLM-Guided Type Inference for Reverse Engineering macOS Private Frameworks",
    "authors": [
      "Arina Kharlamova",
      "Youcheng Sun",
      "Ting Yu"
    ],
    "published_date": "2026-01-04",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.01673v1",
    "pdf_link": "https://arxiv.org/pdf/2601.01673v1",
    "content": {
      "en": "Private macOS frameworks underpin critical services and daemons but remain undocumented and distributed only as stripped binaries, complicating security analysis. We present MOTIF, an agentic framework that integrates tool-augmented analysis with a finetuned large language model specialized for Objective-C type inference. The agent manages runtime metadata extraction, binary inspection, and constraint checking, while the model generates candidate method signatures that are validated and refined into compilable headers. On MOTIF-Bench, a benchmark built from public frameworks with groundtruth headers, MOTIF improves signature recovery from 15% to 86% compared to baseline static analysis tooling, with consistent gains in tool-use correctness and inference stability. Case studies on private frameworks show that reconstructed headers compile, link, and facilitate downstream security research and vulnerability studies. By transforming opaque binaries into analyzable interfaces, MOTIF establishes a scalable foundation for systematic auditing of macOS internals.",
      "tr": "Makale Başlığı: Gizli Arayüzleri Ortaya Çıkarmak: macOS Özel Çatılarını Tersine Mühendislik İçin LLM Yönlendirmeli Tip Çıkarımı\n\nÖzet:\nÖzel macOS çatıları kritik hizmetlerin ve servislerin temelini oluşturur ancak belgelenmemiş ve sadece arındırılmış ikililer olarak dağıtılmaktadır, bu da security analysis'i karmaşıklaştırmaktadır. Biz, Objective-C tip çıkarımı için özelleştirilmiş bir finetuned large language model ile tool-augmented analysis'i entegre eden agentic bir framework olan MOTIF'i sunuyoruz. Agent, runtime metadata extraction, binary inspection ve constraint checking'i yönetirken, model derlenebilir başlıklar haline getirilen aday method signature'ları üretir. MOTIF-Bench üzerinde, groundtruth başlıklarla halka açık çatıları kullanarak oluşturulan bir benchmarkta, MOTIF, baseline static analysis tooling'e kıyasla imza kurtarma oranını %15'ten %86'ya çıkarmıştır; tool-use correctness ve inference stability'de de tutarlı iyileşmeler sağlamıştır. Özel çatıları içeren vaka çalışmaları, yeniden oluşturulan başlıkların derlendiğini, bağlandığını ve downstream security research ve vulnerability studies'i kolaylaştırdığını göstermektedir. Opak ikilileri analiz edilebilir arayüzlere dönüştürerek MOTIF, macOS iç yapılarının sistematik denetimi için ölçeklenebilir bir temel oluşturur."
    }
  },
  {
    "id": "2601.01436v1",
    "title": "Bithoven: Formal Safety for Expressive Bitcoin Smart Contracts",
    "authors": [
      "Hyunhum Cho",
      "Ik Rae Jeong"
    ],
    "published_date": "2026-01-04",
    "tags": [
      "cs.CR",
      "cs.PL"
    ],
    "link": "http://arxiv.org/abs/2601.01436v1",
    "pdf_link": "https://arxiv.org/pdf/2601.01436v1",
    "content": {
      "en": "The rigorous security model of Bitcoin's UTXO architecture often comes at the cost of developer usability, forcing a reliance on manual stack manipulation that leads to critical financial vulnerabilities like signature malleability, unspendable states and unconstrained execution paths. Industry standards such as Miniscript provide necessary abstractions for policy verification but do not model the full imperative logic required for complex contracts, leaving gaps in state management and resource liveness. This paper introduces Bithoven, a high-level language designed to bridge the gap between expressiveness and formal safety. By integrating a strict type checker and a resource liveness analyzer with a semantic control-flow analyzer, Bithoven eliminates major categories of consensus and logic defects defined in our fault model prior to deployment. Our results indicate that this safety comes at modest cost: Bithoven compiles to Bitcoin Script with efficiency comparable to hand-optimized code, demonstrating that type-safe, developer-friendly abstractions are viable even within the strict byte-size constraints of the Bitcoin blockchain.",
      "tr": "**Makale Başlığı:** Bithoven: İfade Gücü Yüksek Bitcoin Akıllı Sözleşmeleri İçin Formal Güvenlik\n\n**Özet:**\n\nBitcoin'in UTXO mimarisinin titiz güvenlik modeli, sıklıkla geliştirici kullanılabilirliği pahasına gelir ve manuel yığın manipülasyonuna güvenilmesini gerektirir. Bu durum, imza değiştirilebilirliği (signature malleability), harcanamaz durumlar (unspendable states) ve kısıtlanmamış yürütme yolları (unconstrained execution paths) gibi kritik finansal kırılganlıklara yol açar. Miniscript gibi endüstri standartları, politika doğrulama için gerekli soyutlamaları sağlar ancak karmaşık sözleşmeler için gereken tam zorunlu mantığı (imperative logic) modellemez, bu da durum yönetimi (state management) ve kaynak canlılığı (resource liveness) alanlarında boşluklar bırakır. Bu makale, ifade gücü (expressiveness) ve formal güvenlik arasındaki boşluğu doldurmak üzere tasarlanmış üst düzey bir dil olan Bithoven'ı tanıtmaktadır. Katı bir tip denetleyicisi (type checker) ve bir kaynak canlılığı analizcisi (resource liveness analyzer) ile anlamsal kontrol-akış analizcisi (semantic control-flow analyzer) entegre ederek, Bithoven dağıtımdan önce hata modelimizde tanımlanan büyük kategorilerdeki konsensüs (consensus) ve mantık kusurlarını ortadan kaldırır. Sonuçlarımız, bu güvenliğin makul bir bedeli olduğunu göstermektedir: Bithoven, Bitcoin Script'e el ile optimize edilmiş kodla karşılaştırılabilir verimlilikte derlenir. Bu durum, tip-güvenli (type-safe), geliştirici dostu soyutlamaların, Bitcoin blok zincirinin sıkı bayt-boyut kısıtlamaları dahilinde bile uygulanabilir olduğunu ortaya koymaktadır."
    }
  },
  {
    "id": "2601.01296v1",
    "title": "Aggressive Compression Enables LLM Weight Theft",
    "authors": [
      "Davis Brown",
      "Juan-Pablo Rivera",
      "Dan Hendrycks",
      "Mantas Mazeika"
    ],
    "published_date": "2026-01-03",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.01296v1",
    "pdf_link": "https://arxiv.org/pdf/2601.01296v1",
    "content": {
      "en": "As frontier AIs become more powerful and costly to develop, adversaries have increasing incentives to steal model weights by mounting exfiltration attacks. In this work, we consider exfiltration attacks where an adversary attempts to sneak model weights out of a datacenter over a network. While exfiltration attacks are multi-step cyber attacks, we demonstrate that a single factor, the compressibility of model weights, significantly heightens exfiltration risk for large language models (LLMs). We tailor compression specifically for exfiltration by relaxing decompression constraints and demonstrate that attackers could achieve 16x to 100x compression with minimal trade-offs, reducing the time it would take for an attacker to illicitly transmit model weights from the defender's server from months to days. Finally, we study defenses designed to reduce exfiltration risk in three distinct ways: making models harder to compress, making them harder to 'find,' and tracking provenance for post-attack analysis using forensic watermarks. While all defenses are promising, the forensic watermark defense is both effective and cheap, and therefore is a particularly attractive lever for mitigating weight-exfiltration risk.",
      "tr": "Makale Başlığı: Aggressive Compression Enables LLM Weight Theft\n\nÖzet:\nSınır ötesi yapay zekalar geliştikçe ve geliştirilmeleri daha maliyetli hale geldikçe, casusluk saldırıları düzenleyerek model ağırlıklarını çalmaya yönelik casusların ilgisi artmaktadır. Bu çalışmada, bir casusun ağ üzerinden bir veri merkezinden model ağırlıklarını sızdırmaya çalıştığı sızma saldırılarını ele alıyoruz. Sızma saldırıları çok adımlı siber saldırılar olsa da, büyük dil modelleri (LLM) için model ağırlıklarının sıkıştırılabilirliği faktörünün, sızdırma riskini önemli ölçüde artırdığını gösteriyoruz. Sıkıştırmayı, sıkıştırma çözme kısıtlamalarını gevşeterek özel olarak sızdırma için uyarlıyoruz ve saldırganların minimal ödünleşimlerle 16 ila 100 kat sıkıştırma elde edebileceğini, böylece saldırganın savunma sunucusundan model ağırlıklarını yasa dışı bir şekilde iletmesi için gereken süreyi aylardan günlere indirebileceğini gösteriyoruz. Son olarak, sızdırma riskini üç farklı şekilde azaltmak için tasarlanmış savunmaları inceliyoruz: modelleri sıkıştırmayı zorlaştırmak, onları \"bulmayı\" zorlaştırmak ve forensic watermarks kullanarak saldırı sonrası analiz için köken takibi yapmak. Tüm savunmalar umut verici olsa da, forensic watermark savunması hem etkili hem de ucuzdur ve bu nedenle ağırlık sızdırma riskini azaltmak için özellikle cazip bir kaldıraçtır."
    }
  },
  {
    "id": "2601.01134v1",
    "title": "AI-Powered Hybrid Intrusion Detection Framework for Cloud Security Using Novel Metaheuristic Optimization",
    "authors": [
      "Maryam Mahdi Alhusseini",
      "Alireza Rouhi",
      "Mohammad-Reza Feizi-Derakhshi"
    ],
    "published_date": "2026-01-03",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.01134v1",
    "pdf_link": "https://arxiv.org/pdf/2601.01134v1",
    "content": {
      "en": "Cybersecurity poses considerable problems to Cloud Computing (CC), especially regarding Intrusion Detection Systems (IDSs), facing difficulties with skewed datasets and suboptimal classification model performance. This study presents the Hybrid Intrusion Detection System (HyIDS), an innovative IDS that employs the Energy Valley Optimizer (EVO) for Feature Selection (FS). Additionally, it introduces a novel technique for enhancing the cybersecurity of cloud computing through the integration of machine learning methodologies with the EVO Algorithm. The Energy Valley Optimizer (EVO) effectively diminished features in the CIC-DDoS2019 dataset from 88 to 38 and in the CSE-CIC-IDS2018 data from 80 to 43, significantly enhancing computing efficiency. HyIDS incorporates four Machine Learning (ML) models: Support Vector Machine (SVM), Random Forest (RF), Decision Tree (D_Tree), and K-Nearest Neighbors (KNN). The proposed HyIDS was assessed utilizing two real-world intrusion datasets, CIC-DDoS2019 and CSE-CIC-IDS2018, both distinguished by considerable class imbalances. The CIC-DDoS2019 dataset has a significant imbalance between DDoS assault samples and legal traffic, while the CSE-CIC-IDS2018 dataset primarily comprises benign traffic with insufficient representation of attack types, complicating the detection of minority attacks. A downsampling technique was employed to balance the datasets, hence improving detection efficacy for both benign and malicious traffic. Twenty-four trials were done, revealing substantial enhancements in categorization accuracy, precision, and recall. Our suggested D_TreeEVO model attained an accuracy rate of 99.13% and an F1 score of 98.94% on the CIC-DDoS2019 dataset, and an accuracy rate of 99.78% and an F1 score of 99.70% on the CSE-CIC-IDS2018 data. These data demonstrate that EVO significantly improves cybersecurity in Cloud Computing (CC).",
      "tr": "**Makale Başlığı:** Yapay Zeka Destekli Hibrit Saldırı Tespit Çerçevesi ve Bulut Güvenliği İçin Yeni Meta-Sezgisel Optimizasyon Kullanımı\n\n**Özet:**\n\nSiber güvenlik, Bulut Bilişim (CC) için önemli zorluklar sunmakta, özellikle Saldırı Tespit Sistemleri (IDSs) eğik veri setleri ve suboptimal sınıflandırma modeli performansı ile mücadele etmektedir. Bu çalışma, Özellik Seçimi (FS) için Energy Valley Optimizer (EVO) kullanan yenilikçi bir IDS olan Hibrit Saldırı Tespit Sistemi (HyIDS)'ni sunmaktadır. Ek olarak, makine öğrenmesi metodolojilerinin EVO Algorithm ile entegrasyonu yoluyla bulut bilişimin siber güvenliğini artırmak için yeni bir teknik tanıtmaktadır. Energy Valley Optimizer (EVO), CIC-DDoS2019 veri setindeki özellikleri 88'den 38'e ve CSE-CIC-IDS2018 verilerindeki özellikleri 80'den 43'e düşürerek bilişim verimliliğini önemli ölçüde artırmıştır. HyIDS, dört Makine Öğrenmesi (ML) modelini içermektedir: Support Vector Machine (SVM), Random Forest (RF), Decision Tree (D_Tree) ve K-Nearest Neighbors (KNN). Önerilen HyIDS, önemli sınıf dengesizlikleriyle ayırt edilen iki gerçek dünya saldırı veri seti, CIC-DDoS2019 ve CSE-CIC-IDS2018 kullanılarak değerlendirilmiştir. CIC-DDoS2019 veri seti, DDoS saldırı örnekleri ile yasal trafik arasında önemli bir dengesizliğe sahipken, CSE-CIC-IDS2018 veri seti öncelikli olarak saldırı türlerinin yetersiz temsil edildiği iyi niyetli trafikten oluşmaktadır, bu da azınlık saldırılarının tespitini karmaşıklaştırmaktadır. Veri setlerini dengelemek ve dolayısıyla hem iyi niyetli hem de kötü niyetli trafik için tespit etkinliğini artırmak amacıyla bir downsampling tekniği kullanılmıştır. Yapılan yirmi dört deneme, kategorizasyon doğruluğu, precision ve recall'da önemli iyileşmeler ortaya koymuştur. Önerilen D_TreeEVO modelimiz, CIC-DDoS2019 veri setinde %99.13 doğruluk oranı ve %98.94 F1 skoru ile, CSE-CIC-IDS2018 verilerinde ise %99.78 doğruluk oranı ve %99.70 F1 skoru ile hedefe ulaşmıştır. Bu veriler, EVO'nun Bulut Bilişim (CC)'de siber güvenliği önemli ölçüde iyileştirdiğini göstermektedir."
    }
  },
  {
    "id": "2601.01053v1",
    "title": "Byzantine-Robust Federated Learning Framework with Post-Quantum Secure Aggregation for Real-Time Threat Intelligence Sharing in Critical IoT Infrastructure",
    "authors": [
      "Milad Rahmati",
      "Nima Rahmati"
    ],
    "published_date": "2026-01-03",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.01053v1",
    "pdf_link": "https://arxiv.org/pdf/2601.01053v1",
    "content": {
      "en": "The proliferation of Internet of Things devices in critical infrastructure has created unprecedented cybersecurity challenges, necessitating collaborative threat detection mechanisms that preserve data privacy while maintaining robustness against sophisticated attacks. Traditional federated learning approaches for IoT security suffer from two critical vulnerabilities: susceptibility to Byzantine attacks where malicious participants poison model updates, and inadequacy against future quantum computing threats that can compromise cryptographic aggregation protocols. This paper presents a novel Byzantine-robust federated learning framework integrated with post-quantum secure aggregation specifically designed for real-time threat intelligence sharing across critical IoT infrastructure. The proposed framework combines a adaptive weighted aggregation mechanism with lattice-based cryptographic protocols to simultaneously defend against model poisoning attacks and quantum adversaries. We introduce a reputation-based client selection algorithm that dynamically identifies and excludes Byzantine participants while maintaining differential privacy guarantees. The secure aggregation protocol employs CRYSTALS-Kyber for key encapsulation and homomorphic encryption to ensure confidentiality during parameter updates. Experimental evaluation on industrial IoT intrusion detection datasets demonstrates that our framework achieves 96.8% threat detection accuracy while successfully mitigating up to 40% Byzantine attackers, with only 18% computational overhead compared to non-secure federated approaches. The framework maintains sub-second aggregation latency suitable for real-time applications and provides 256-bit post-quantum security level.",
      "tr": "Elbette, istenen çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** Kritik IoT Altyapısında Gerçek Zamanlı Tehdit İstihbaratı Paylaşımı İçin Post-Quantum Güvenli Toplama ile Bizans-Dayanıklı Birleşik Öğrenme Çerçevesi\n\n**Özet:**\n\nKritik altyapılarda Nesnelerin İnterneti (IoT) cihazlarının yaygınlaşması, veri gizliliğini korurken gelişmiş saldırılara karşı dayanıklılığı sürdüren işbirliğine dayalı tehdit tespit mekanizmalarını zorunlu kılan emsalsiz siber güvenlik zorlukları yaratmıştır. IoT güvenliği için geleneksel birleşik öğrenme yaklaşımları iki kritik zafiyetten muzdariptir: model güncellemelerini bozan Bizans saldırılarına yatkınlık ve kriptografik toplama protokollerini tehlikeye atabilecek gelecekteki kuantum bilişim tehditlerine karşı yetersizlik. Bu çalışma, kritik IoT altyapısı boyunca gerçek zamanlı tehdit istihbaratı paylaşımı için özel olarak tasarlanmış, post-quantum secure aggregation ile entegre, yeni bir Byzantine-robust federated learning framework sunmaktadır. Önerilen framework, model zehirlenmesi saldırılarına ve kuantum düşmanlarına karşı eş zamanlı savunma sağlamak üzere kafes tabanlı kriptografik protokollerle bir adaptif weighted aggregation mechanism'ini birleştirmektedir. Bizans katılımcılarını dinamik olarak tanımlayan ve hariç tutan, aynı zamanda differential privacy garantilerini sürdüren itibar tabanlı bir client selection algorithm'i sunuyoruz. Secure aggregation protokolü, parametre güncellemeleri sırasında gizliliği sağlamak için key encapsulation ve homomorphic encryption için CRYSTALS-Kyber'i kullanmaktadır. Endüstriyel IoT saldırı tespit veri kümeleri üzerindeki deneysel değerlendirme, framework'ümüzün %96.8 tehdit tespit doğruluğu elde ettiğini, %40'a varan Bizans saldırganlarını başarıyla azalttığını ve güvenli olmayan birleşik yaklaşımlara kıyasla yalnızca %18 hesaplama yüküyle çalıştığını göstermektedir. Framework, gerçek zamanlı uygulamalara uygun sub-second aggregation latency'yi sürdürmekte ve 256-bit post-quantum security level sağlamaktadır."
    }
  },
  {
    "id": "2601.00936v1",
    "title": "Emoji-Based Jailbreaking of Large Language Models",
    "authors": [
      "M P V S Gopinadh",
      "S Mahaboob Hussain"
    ],
    "published_date": "2026-01-02",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.00936v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00936v1",
    "content": {
      "en": "Large Language Models (LLMs) are integral to modern AI applications, but their safety alignment mechanisms can be bypassed through adversarial prompt engineering. This study investigates emoji-based jailbreaking, where emoji sequences are embedded in textual prompts to trigger harmful and unethical outputs from LLMs. We evaluated 50 emoji-based prompts on four open-source LLMs: Mistral 7B, Qwen 2 7B, Gemma 2 9B, and Llama 3 8B. Metrics included jailbreak success rate, safety alignment adherence, and latency, with responses categorized as successful, partial and failed. Results revealed model-specific vulnerabilities: Gemma 2 9B and Mistral 7B exhibited 10 % success rates, while Qwen 2 7B achieved full alignment (0% success). A chi-square test (chi^2 = 32.94, p < 0.001) confirmed significant inter-model differences. While prior works focused on emoji attacks targeting safety judges or classifiers, our empirical analysis examines direct prompt-level vulnerabilities in LLMs. The results reveal limitations in safety mechanisms and highlight the necessity for systematic handling of emoji-based representations in prompt-level safety and alignment pipelines.",
      "tr": "**Makale Başlığı:** Büyük Dil Modellerinin (LLM) Emoji Tabanlı Jailbreak'i\n\n**Özet:**\n\nBüyük Dil Modelleri (LLM'ler), modern yapay zeka uygulamalarının ayrılmaz bir parçasıdır; ancak güvenlik uyum mekanizmaları, düşmanca prompt mühendisliği yoluyla aşılabilir. Bu çalışma, LLM'lerden zararlı ve etik olmayan çıktılara neden olmak amacıyla metinsel promptlara emoji dizilerinin gömüldüğü emoji tabanlı jailbreak'i araştırmaktadır. Mistral 7B, Qwen 2 7B, Gemma 2 9B ve Llama 3 8B olmak üzere dört açık kaynaklı LLM üzerinde 50 emoji tabanlı prompt değerlendirilmiştir. Metrikler arasında jailbreak başarı oranı, güvenlik uyumu uyumluluğu ve gecikme süresi yer almış, yanıtlar başarılı, kısmi ve başarısız olarak kategorize edilmiştir. Sonuçlar modele özgü zafiyetleri ortaya koymuştur: Gemma 2 9B ve Mistral 7B %10 başarı oranları sergilerken, Qwen 2 7B tam uyum (0% başarı) elde etmiştir. Bir ki-kare testi (chi^2 = 32.94, p < 0.001), modeller arası anlamlı farklılıkları doğrulamıştır. Önceki çalışmalar güvenlik yargıçlarını veya sınıflandırıcılarını hedef alan emoji saldırılarına odaklanırken, bizim ampirik analizimiz LLM'lerde doğrudan prompt düzeyindeki zafiyetleri incelemektedir. Elde edilen sonuçlar, güvenlik mekanizmalarındaki sınırlılıkları ortaya koymakta ve prompt düzeyinde güvenlik ile uyum hatlarında emoji tabanlı temsillerin sistematik olarak ele alınması gerekliliğini vurgulamaktadır."
    }
  },
  {
    "id": "2601.00559v1",
    "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?",
    "authors": [
      "Jason Quantrill",
      "Noura Khajehnouri",
      "Zihan Guo",
      "Manar H. Alalfi"
    ],
    "published_date": "2026-01-02",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.00559v1",
    "pdf_link": "https://arxiv.org/pdf/2601.00559v1",
    "content": {
      "en": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.",
      "tr": "**Makale Başlığı:** IoT Güvenliğini Kırmak: LLM'ler Statik Analiz Araçlarını Aşabilir mi?\n\n**Özet:**\n\nopenHAB gibi akıllı ev IoT platformları, cihaz davranışlarını otomatikleştirmek için Trigger Action Condition (TAC) kurallarına dayanır. Ancak, bu kurallar arasındaki etkileşim, örtük bağımlılıklardan, çakışan tetikleyicilerden veya üst üste binen koşullardan kaynaklanan etkileşim tehditlerine, istenmeyen veya güvensiz davranışlara yol açabilir. Bu tehditlerin tanımlanması, geleneksel olarak sembolik, kısıtlama güdümlü statik analize dayanan anlamsal anlayış ve yapısal reasoning gerektirir. Bu çalışma, büyük dil modellerinin (LLMs) çok kategorili bir etkileşim tehdidi taksonomisi boyunca ilk kapsamlı değerlendirmesini sunmakta olup, hem orijinal openHAB (oHC/IoTB) veri seti hem de kural dönüşümleri altındaki dayanıklılığı test etmek için tasarlanmış yapısal olarak zorlayıcı bir Mutation veri seti üzerindeki performanslarını değerlendirmektedir. Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro ve DeepSeek-R1 modellerini zero-, one- ve two-shot ayarlarda kıyaslayarak, sonuçlarını oHIT'in manuel olarak doğrulanmış ground truth'u ile karşılaştırıyoruz. Bulgularımız, LLM'lerin özellikle eylem ve koşulla ilgili tehditlerde umut verici anlamsal anlayış sergilemelerine rağmen, kural dışı yapısal reasoning gerektiren tehditlerde, özellikle de değiştirilmiş kural formları altında doğruluklarının önemli ölçüde düştüğünü göstermektedir. Model performansı, tehdit kategorileri ve prompt ayarları arasında geniş ölçüde değişkenlik göstermekte olup, hiçbir model tutarlı bir güvenilirlik sunmamaktadır. Buna karşılık, sembolik reasoning baseline veri setleri boyunca istikrarlı bir tespit sağlamakta, kural yeniden yazımlarından veya yapısal pertürbasyonlardan etkilenmemektedir. Bu sonuçlar, LLM'lerin tek başına IoT ortamlarında güvenlik açısından kritik etkileşim tehdidi tespiti için henüz güvenilir olmadığını vurgulamaktadır. Araç tasarımı üzerindeki etkileri tartışıyor ve yapısal titizliği korurken false positive'leri azaltmak için sembolik analiz ile LLM tabanlı anlamsal yorumlamayı birleştiren hibrit mimarilerin potansiyelini ortaya koyuyoruz."
    }
  }
]