[
  {
    "id": "2601.18754v1",
    "title": "$α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks",
    "authors": [
      "Mohamed Amine Ferrag",
      "Abderrahmane Lakas",
      "Merouane Debbah"
    ],
    "published_date": "2026-01-26",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.18754v1",
    "pdf_link": "https://arxiv.org/pdf/2601.18754v1",
    "content": {
      "en": "Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditions remains largely unexplored, particularly in emerging 6G-enabled settings.   We introduce $α^{3}$-SecBench, the first large-scale evaluation suite for assessing the security-aware autonomy of LLM-based UAV agents under realistic adversarial interference. Building on multi-turn conversational UAV missions from $α^{3}$-Bench, the framework augments benign episodes with 20,000 validated security overlay attack scenarios targeting seven autonomy layers, including sensing, perception, planning, control, communication, edge/cloud infrastructure, and LLM reasoning. $α^{3}$-SecBench evaluates agents across three orthogonal dimensions: security (attack detection and vulnerability attribution), resilience (safe degradation behavior), and trust (policy-compliant tool usage).   We evaluate 23 state-of-the-art LLMs from major industrial providers and leading AI labs using thousands of adversarially augmented UAV episodes sampled from a corpus of 113,475 missions spanning 175 threat types. While many models reliably detect anomalous behavior, effective mitigation, vulnerability attribution, and trustworthy control actions remain inconsistent. Normalized overall scores range from 12.9% to 57.1%, highlighting a significant gap between anomaly detection and security-aware autonomous decision-making. We release $α^{3}$-SecBench on GitHub: https://github.com/maferrag/AlphaSecBench",
      "tr": "**Makale Başlığı:** $α^3$-SecBench: 6G Ağları Üzerinde LLM Tabanlı İHA Temsilcileri İçin Güvenlik, Dayanıklılık ve Güvenliğin Büyük Ölçekli Değerlendirme Seti\n\n**Özet:**\nOtonom insansız hava aracı (İHA) sistemleri, giderek artan bir şekilde emniyet-kritik, ağa bağlı ortamlarda konuşlandırılmaktadır ve burada kötü niyetli saldırganların varlığında güvenilir bir şekilde çalışmaları gerekmektedir. Son zamanlardaki benchmarklar, LLM (Large Language Model) tabanlı İHA temsilcilerini reasoning, navigasyon ve verimlilik açısından değerlendirmiş olsa da, özellikle gelişmekte olan 6G destekli ortamlarda, saldırgan koşullar altında güvenlik, dayanıklılık ve güvenliğin sistematik değerlendirmesi büyük ölçüde keşfedilmemiş kalmıştır. Gerçekçi saldırgan müdahalesi altında LLM tabanlı İHA temsilcilerinin güvenlik farkındalığına sahip otonomisini değerlendirmek için ilk büyük ölçekli değerlendirme seti olan $α^{3}$-SecBench'i sunuyoruz. $α^{3}$-Bench'ten çok turlu konuşma tabanlı İHA görevlerine dayanarak, bu çerçeve, algılama, farkındalık, planlama, kontrol, iletişim, kenar/bulut altyapısı ve LLM reasoning dahil olmak üzere yedi otonomi katmanını hedefleyen 20.000 doğrulanmış güvenlik katmanı saldırı senaryosu ile iyi niyetli görevleri zenginleştirmektedir. $α^{3}$-SecBench, temsilcileri üç dik boyutlu alanda değerlendirir: güvenlik (saldırı tespiti ve zafiyet atıfı), dayanıklılık (güvenli bozulma davranışı) ve güven (politika uyumlu araç kullanımı). Başlıca endüstriyel sağlayıcılardan ve önde gelen AI laboratuvarlarından 23 adet en güncel LLM modelini, 175 tehdit türünü kapsayan 113.475 görevlik bir veri kümesinden örneklenen binlerce saldırganca zenginleştirilmiş İHA görevini kullanarak değerlendirdik. Birçok model anormal davranışı güvenilir bir şekilde tespit ederken, etkili azaltma, zafiyet atıfı ve güvenilir kontrol eylemleri tutarsız kalmaktadır. Normalize edilmiş genel puanlar %12,9 ile %57,1 arasında değişmekte olup, anomali tespiti ile güvenlik farkındalığına sahip otonom karar verme arasında önemli bir boşluk olduğunu vurgulamaktadır. $α^{3}$-SecBench'i GitHub'da yayınlıyoruz: https://github.com/maferrag/AlphaSecBench"
    }
  },
  {
    "id": "2601.18113v1",
    "title": "MalURLBench: A Benchmark Evaluating Agents' Vulnerabilities When Processing Web URLs",
    "authors": [
      "Dezhang Kong",
      "Zhuxi Wu",
      "Shiqi Liu",
      "Zhicheng Tan",
      "Kuichen Lu"
    ],
    "published_date": "2026-01-26",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.18113v1",
    "pdf_link": "https://arxiv.org/pdf/2601.18113v1",
    "content": {
      "en": "LLM-based web agents have become increasingly popular for their utility in daily life and work. However, they exhibit critical vulnerabilities when processing malicious URLs: accepting a disguised malicious URL enables subsequent access to unsafe webpages, which can cause severe damage to service providers and users. Despite this risk, no benchmark currently targets this emerging threat. To address this gap, we propose MalURLBench, the first benchmark for evaluating LLMs' vulnerabilities to malicious URLs. MalURLBench contains 61,845 attack instances spanning 10 real-world scenarios and 7 categories of real malicious websites. Experiments with 12 popular LLMs reveal that existing models struggle to detect elaborately disguised malicious URLs. We further identify and analyze key factors that impact attack success rates and propose URLGuard, a lightweight defense module. We believe this work will provide a foundational resource for advancing the security of web agents. Our code is available at https://github.com/JiangYingEr/MalURLBench.",
      "tr": "**Makale Başlığı:** MalURLBench: Web URL'lerini İşlerken Ajanların Kırılganlıklarını Değerlendiren Bir Benchmark\n\n**Özet:**\n\nLLM tabanlı web ajanları, günlük yaşamda ve işte sundukları fayda nedeniyle giderek daha popüler hale gelmiştir. Ancak, kötü amaçlı URL'leri işlerken kritik kırılganlıklar sergilemektedirler: gizlenmiş kötü amaçlı bir URL'yi kabul etmek, hizmet sağlayıcılar ve kullanıcılar için ciddi zararlara yol açabilecek güvensiz web sayfalarına sonraki erişime olanak tanır. Bu risklere rağmen, şu anda bu gelişmekte olan tehdidi hedef alan bir benchmark bulunmamaktadır. Bu boşluğu doldurmak için, kötü amaçlı URL'lere karşı LLM'lerin kırılganlıklarını değerlendirmeye yönelik ilk benchmark olan MalURLBench'i öneriyoruz. MalURLBench, 10 gerçek dünya senaryosunu ve 7 kategori gerçek kötü amaçlı web sitesini kapsayan 61.845 saldırı örneği içermektedir. 12 popüler LLM ile yapılan deneyler, mevcut modellerin ustaca gizlenmiş kötü amaçlı URL'leri tespit etmekte zorlandığını ortaya koymaktadır. Ayrıca, saldırı başarı oranlarını etkileyen temel faktörleri tanımlayıp analiz ediyor ve hafif bir savunma modülü olan URLGuard'ı öneriyoruz. Bu çalışmanın, web ajanlarının güvenliğini ilerletmek için temel bir kaynak sağlayacağına inanıyoruz. Kodumuz https://github.com/JiangYingEr/MalURLBench adresinde mevcuttur."
    }
  },
  {
    "id": "2601.18105v1",
    "title": "Mitigating the OWASP Top 10 For Large Language Models Applications using Intelligent Agents",
    "authors": [
      "Mohammad Fasha",
      "Faisal Abul Rub",
      "Nasim Matar",
      "Bilal Sowan",
      "Mohammad Al Khaldy"
    ],
    "published_date": "2026-01-26",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.18105v1",
    "pdf_link": "https://arxiv.org/pdf/2601.18105v1",
    "content": {
      "en": "Large Language Models (LLMs) have emerged as a transformative and disruptive technology, enabling a wide range of applications in natural language processing, machine translation, and beyond. However, this widespread integration of LLMs also raised several security concerns highlighted by the Open Web Application Security Project (OWASP), which has identified the top 10 security vulnerabilities inherent in LLM applications. Addressing these vulnerabilities is crucial, given the increasing reliance on LLMs and the potential threats to data integrity, confidentiality, and service availability. This paper presents a framework designed to mitigate the security risks outlined in the OWASP Top 10. Our proposed model leverages LLM-enabled intelligent agents, offering a new approach to proactively identify, assess, and counteract security threats in real-time. The proposed framework serves as an initial blueprint for future research and development, aiming to enhance the security measures of LLMs and protect against emerging threats in this rapidly evolving landscape.",
      "tr": "**Makale Başlığı:** LLM Uygulamaları İçin OWASP Top 10'un Zeki Ajanlar Kullanılarak Azaltılması\n\n**Özet:**\n\nBüyük Dil Modelleri (LLM'ler), doğal dil işleme, makine çevirisi ve ötesinde geniş bir uygulama yelpazesini mümkün kılan dönüştürücü ve yıkıcı bir teknoloji olarak ortaya çıkmıştır. Ancak, LLM'lerin bu yaygın entegrasyonu, Açık Web Uygulama Güvenliği Projesi (OWASP) tarafından vurgulanan çeşitli güvenlik endişelerini de beraberinde getirmiştir; OWASP, LLM uygulamalarına özgü en önemli 10 güvenlik açığını belirlemiştir. LLM'lere artan güven ve veri bütünlüğü, gizliliği ve hizmet kullanılabilirliğine yönelik potansiyel tehditler göz önüne alındığında, bu güvenlik açıklarını ele almak büyük önem taşımaktadır. Bu çalışma, OWASP Top 10'da belirtilen güvenlik risklerini azaltmak için tasarlanmış bir çerçeve sunmaktadır. Önerdiğimiz model, LLM destekli akıllı ajanlardan yararlanarak, güvenlik tehditlerini gerçek zamanlı olarak proaktif bir şekilde tanımlamak, değerlendirmek ve bunlara karşı koymak için yeni bir yaklaşım sunmaktadır. Önerilen çerçeve, LLM'lerin güvenlik önlemlerini iyileştirmeyi ve bu hızla gelişen ortamdaki ortaya çıkan tehditlere karşı korumayı amaçlayan gelecek araştırmaları ve geliştirme çalışmaları için ilk bir taslak görevi görmektedir."
    }
  },
  {
    "id": "2601.18068v1",
    "title": "XGuardian: Towards Explainable and Generalized AI Anti-Cheat on FPS Games",
    "authors": [
      "Jiayi Zhang",
      "Chenxin Sun",
      "Chenxiong Qian"
    ],
    "published_date": "2026-01-26",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.18068v1",
    "pdf_link": "https://arxiv.org/pdf/2601.18068v1",
    "content": {
      "en": "Aim-assist cheats are the most prevalent and infamous form of cheating in First-Person Shooter (FPS) games, which help cheaters illegally reveal the opponent's location and auto-aim and shoot, and thereby pose significant threats to the game industry. Although a considerable research effort has been made to automatically detect aim-assist cheats, existing works suffer from unreliable frameworks, limited generalizability, high overhead, low detection performance, and a lack of explainability of detection results. In this paper, we propose XGuardian, a server-side generalized and explainable system for detecting aim-assist cheats to overcome these limitations. It requires only two raw data inputs, pitch and yaw, which are all FPS games' must-haves, to construct novel temporal features and describe aim trajectories, which are essential for distinguishing cheaters and normal players. XGuardian is evaluated with the latest mainstream FPS game CS2, and validates its generalizability with another two different games. It achieves high detection performance and low overhead compared to prior works across different games with real-world and large-scale datasets, demonstrating wide generalizability and high effectiveness. It is able to justify its predictions and thereby shorten the ban cycle. We make XGuardian as well as our datasets publicly available.",
      "tr": "Makale Başlığı: XGuardian: FPS Oyunlarında Açıklanabilir ve Genelleştirilebilir Yapay Zeka Anti-Hileye Doğru\n\nÖzet:\nAim-assist hileleri, Birinci Şahıs Nişancı (FPS) oyunlarında en yaygın ve kötü şöhretli hile türüdür. Bu hileler, oyuncuların yasa dışı olarak rakibin konumunu ortaya çıkarmasına ve otomatik nişan alıp ateş etmesine yardımcı olarak oyun endüstrisi için önemli tehditler oluşturmaktadır. Aim-assist hilelerini otomatik olarak tespit etmek için önemli araştırma çabaları yapılmış olmasına rağmen, mevcut çalışmalar güvenilmez çerçevelerden, sınırlı genelleştirilebilirlikten, yüksek maliyetten, düşük tespit performansından ve tespit sonuçlarının açıklanabilirliğinin eksikliğinden muzdariptir. Bu çalışmada, bu sınırlamaların üstesinden gelmek için sunucu taraflı genelleştirilebilir ve açıklanabilir bir aim-assist hile tespiti sistemi olan XGuardian'ı sunuyoruz. XGuardian, yalnızca iki ham veri girdisi gerektirir: pitch ve yaw. Bu girdiler, tüm FPS oyunlarının olmazsa olmazlarıdır. XGuardian, hileciler ile normal oyuncuları ayırt etmek için kritik öneme sahip olan aim yörüngelerini tanımlamak ve yeni zamansal özellikler oluşturmak üzere tasarlanmıştır. XGuardian, en güncel ana akım FPS oyunu olan CS2 üzerinde değerlendirilmiş ve diğer iki farklı oyunda genelleştirilebilirliğini doğrulamıştır. Gerçek dünya ve büyük ölçekli veri kümeleri üzerinde, farklı oyunlarda önceki çalışmalara kıyasla yüksek tespit performansı ve düşük maliyet elde ederek geniş genelleştirilebilirlik ve yüksek etkililik göstermiştir. Tahminlerini gerekçelendirebilir ve böylece yasaklama döngüsünü kısaltabilir. XGuardian ve veri kümelerimizi kamuya açık hale getiriyoruz."
    }
  },
  {
    "id": "2601.17935v1",
    "title": "FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering",
    "authors": [
      "Daniel Commey",
      "Matilda Nkoom",
      "Yousef Alsenani",
      "Sena G. Hounsinou",
      "Garth V. Crosby"
    ],
    "published_date": "2026-01-25",
    "tags": [
      "cs.LG",
      "cs.CR",
      "cs.SI"
    ],
    "link": "http://arxiv.org/abs/2601.17935v1",
    "pdf_link": "https://arxiv.org/pdf/2601.17935v1",
    "content": {
      "en": "Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.",
      "tr": "Makale Başlığı: FedGraph-VASP: Kurumlararası Kara Para Aklamaya Karşı Siber Güvenlik, Yapay Zeka ve Sistem Analizi ile Gizliliği Koruyan Birleşik Grafik Öğrenme ve Post-Kuantum Güvenlik\n\nÖzet:\nVirtual Asset Service Providers (VASPs), kurumlararası kara para aklamayı tespit etme konusunda düzenleyici uyumluluk ile kullanıcı gizliliği arasında temel bir gerilimle karşı karşıyadır. Mevcut yaklaşımlar, ya hassas işlem verilerinin paylaşılmasını gerektirir ya da izole bir şekilde çalışarak kritik zincirler arası aklama kalıplarının tespit edilmeden kalmasına neden olur. Biz, ham kullanıcı verilerini ifşa etmeden işbirlikçi anti-money laundering (AML) sağlayan, gizliliği koruyan bir federated graph learning frameworki olan FedGraph-VASP'ı sunuyoruz. Ana katkımız, yalnızca sınır hesaplarının sıkıştırılmış, tersine çevrilemez graph neural network temsillerini paylaşan bir Boundary Embedding Exchange protokolüdür. Bu değişimler, post-quantum cryptography kullanılarak güvence altına alınmıştır; özel olarak NIST-standartlaştırılmış Kyber-512 key encapsulation mechanism, AES-256-GCM authenticated encryption ile birleştirilmiştir. Gerçekçi Louvain partitioning ile Elliptic Bitcoin veri kümesi üzerinde yapılan deneyler, FedGraph-VASP'ın binary fraud detection'da %12.1 oranında en iyi üretken temel FedSage+ (F1 = 0.453) modelini geride bırakarak 0.508 F1-score elde ettiğini göstermektedir. Düşük bağlantı durumlarında üretken imputation'ın performansı düşürdüğü durumlarda sağlamlığını da gösteriyoruz, yüksek bağlantı rejimlerinde ise merkezi performansla (F1 = 0.620) yaklaşıyoruz. Ayrıca, FedGraph-VASP'ın (F1 = 0.635) seyrek çapraz-silo bağlantısı altında daha az etkili olduğu, FedSage+'ın ise (F1 = 0.855) yerel eğitimi bile (F1 = 0.785) geride bırakarak üstün olduğu bir Ethereum fraud detection veri kümesi üzerinde genelleştirmeyi değerlendiriyoruz. Bu sonuçlar, topolojiye bağlı bir ödünleşmeye işaret etmektedir: embedding exchange bağlı işlem grafiklerinden faydalanırken, üretken imputation yüksek derecede modüler seyrek grafiklerde baskın olabilir. Bir gizlilik denetimi, embeddings'in yalnızca kısmen tersine çevrilebilir olduğunu (R^2 = 0.32) ve bu durumun exact feature recovery'yi sınırladığını göstermektedir."
    }
  },
  {
    "id": "2601.17907v1",
    "title": "FARM: Few-shot Adaptive Malware Family Classification under Concept Drift",
    "authors": [
      "Numan Halit Guldemir",
      "Oluwafemi Olukoya",
      "Jesús Martínez-del-Rincón"
    ],
    "published_date": "2026-01-25",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.17907v1",
    "pdf_link": "https://arxiv.org/pdf/2601.17907v1",
    "content": {
      "en": "Malware classification models often face performance degradation due to concept drift, arising from evolving threat landscapes and the emergence of novel malware families. This paper presents FARM (Few-shot Adaptive Recognition of Malware), a framework designed to detect and adapt to both covariate and label drift in Windows Portable Executable (PE) malware classification. FARM leverages a triplet autoencoder to project samples into a discriminative latent space, enabling unsupervised drift detection via DBSCAN clustering and dynamic thresholding. For rapid adaptation, it employs few-shot learning using prototype-based classification, requiring only a handful of labeled samples. FARM also supports full retraining when enough drifted samples accumulate, updating the latent space for long-term integration. Experiments on the BenchMFC dataset demonstrate that FARM improves classification performance under covariate drift by 5.6\\%, and achieves an average F1 score of 0.85 on unseen malware families using only few-shot adaptation, which further increases to 0.94 after retraining. These results highlight FARM's robustness and adaptability in dynamic malware detection environments under limited supervision.",
      "tr": "Makale Başlığı: FARM: Concept Drift Altında Az Örnekli Adaptif Zararlı Yazılım Aile Sınıflandırması\n\nÖzet:\nZararlı yazılım sınıflandırma modelleri, gelişen tehdit ortamları ve ortaya çıkan yeni zararlı yazılım aileleri nedeniyle oluşan concept drift'ten dolayı sıklıkla performans düşüşüyle karşı karşıya kalmaktadır. Bu makale, Windows Portable Executable (PE) zararlı yazılım sınıflandırmasında hem covariate hem de label drift'i tespit etmek ve bu durumlara adapte olmak üzere tasarlanmış bir framework olan FARM (Few-shot Adaptive Recognition of Malware)'ı sunmaktadır. FARM, örnekleri ayrıştırıcı bir latent space'e yansıtmak için bir triplet autoencoder'dan yararlanır, bu da DBSCAN clustering ve dinamik thresholding yoluyla gözetimsiz drift tespiti sağlar. Hızlı adaptasyon için, yalnızca birkaç etiketli örneğe ihtiyaç duyan prototype-based classification kullanarak few-shot learning'i kullanır. FARM ayrıca, yeterli miktarda drifted sample biriktiğinde tam yeniden eğitim desteği sunarak uzun vadeli entegrasyon için latent space'i günceller. BenchMFC veri seti üzerindeki deneyler, FARM'ın covariate drift altında sınıflandırma performansını %5.6 oranında iyileştirdiğini ve yalnızca few-shot adaptasyon kullanarak görülmemiş zararlı yazılım ailelerinde ortalama 0.85 F1 skoru elde ettiğini göstermektedir; bu skor yeniden eğitimden sonra 0.94'e yükselmektedir. Bu sonuçlar, FARM'ın sınırlı denetim altında dinamik zararlı yazılım tespit ortamlarındaki sağlamlığını ve adaptasyon yeteneğini vurgulamaktadır."
    }
  },
  {
    "id": "2601.17888v1",
    "title": "iResolveX: Multi-Layered Indirect Call Resolution via Static Reasoning and Learning-Augmented Refinement",
    "authors": [
      "Monika Santra",
      "Bokai Zhang",
      "Mark Lim",
      "Vishnu Asutosh Dasu",
      "Dongrui Zeng"
    ],
    "published_date": "2026-01-25",
    "tags": [
      "cs.SE",
      "cs.CR",
      "cs.PL"
    ],
    "link": "http://arxiv.org/abs/2601.17888v1",
    "pdf_link": "https://arxiv.org/pdf/2601.17888v1",
    "content": {
      "en": "Indirect call resolution remains a key challenge in reverse engineering and control-flow graph recovery, especially for stripped or optimized binaries. Static analysis is sound but often over-approximates, producing many false positives, whereas machine-learning approaches can improve precision but may sacrifice completeness and generalization. We present iResolveX, a hybrid multi-layered framework that combines conservative static analysis with learning-based refinement. The first layer applies a conservative value-set analysis (BPA) to ensure high recall. The second layer adds a learning-based soft-signature scorer (iScoreGen) and selective inter-procedural backward analysis with memory inspection (iScoreRefine) to reduce false positives. The final output, p-IndirectCFG, annotates indirect edges with confidence scores, enabling downstream analyses to choose appropriate precision--recall trade-offs. Across SPEC CPU2006 and real-world binaries, iScoreGen reduces predicted targets by 19.2% on average while maintaining BPA-level recall (98.2%). Combined with iScoreRefine, the total reduction reaches 44.3% over BPA with 97.8% recall (a 0.4% drop). iResolveX supports both conservative, recall-preserving and F1-optimized configurations and outperforms state-of-the-art systems.",
      "tr": "Elbette, isteğiniz doğrultusunda akademik makale başlığını ve özetini Türkçeye çevirdim:\n\n**Makale Başlığı:** iResolveX: Statik Reasoning ve Öğrenme Destekli İyileştirme ile Çok Katmanlı Dolaylı Çağrı Çözümlemesi\n\n**Özet:**\nDolaylı çağrı çözümlemesi, özellikle ayıklanmış (stripped) veya optimize edilmiş ikili dosyalar (binaries) için tersine mühendislik (reverse engineering) ve kontrol akış grafiği (control-flow graph) kurtarma alanlarında temel bir zorluk olmaya devam etmektedir. Statik analiz sağlamdır ancak sıklıkla aşırı genelleme yaparak çok sayıda yanlış pozitif (false positive) üretirken, makine öğrenmesi yaklaşımları hassasiyeti artırabilir ancak tamlığı (completeness) ve genelleştirilebilirliği (generalization) feda edebilir. Konservatif statik analizi öğrenme tabanlı iyileştirme ile birleştiren hibrit, çok katmanlı bir çerçeve olan iResolveX'i sunuyoruz. İlk katman, yüksek *recall*'ı sağlamak için konservatif bir değer kümesi analizi (value-set analysis - BPA) uygular. İkinci katman, yanlış pozitifleri azaltmak için öğrenme tabanlı bir yumuşak imza puanlayıcısı (soft-signature scorer - iScoreGen) ve bellek incelemesi ile seçici usuller arası geriye dönük analiz (selective inter-procedural backward analysis with memory inspection - iScoreRefine) ekler. Nihai çıktı olan p-IndirectCFG, dolaylı kenarları güven skorlarıyla (confidence scores) etiketleyerek, aşağı akış analizlerinin (downstream analyses) uygun hassasiyet-geriçağrı (precision--recall) ödünleşimlerini seçmelerini sağlar. SPEC CPU2006 ve gerçek dünya ikili dosyaları (real-world binaries) üzerinde, iScoreGen, BPA seviyesinde *recall*'ı (98.2%) korurken öngörülen hedef sayısını ortalama %19,2 oranında azaltır. iScoreRefine ile birleştiğinde, toplam azaltma BPA'ya göre %44,3'e ulaşır ve bu sırada %97,8'lik bir *recall* (0.4% düşüş) sağlanır. iResolveX, hem konservatif, *recall* koruyucu hem de F1 optimize edilmiş yapılandırmaları destekler ve en gelişmiş sistemlerden daha iyi performans gösterir."
    }
  },
  {
    "id": "2601.17744v1",
    "title": "Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems",
    "authors": [
      "Amjad Fatmi"
    ],
    "published_date": "2026-01-25",
    "tags": [
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "link": "http://arxiv.org/abs/2601.17744v1",
    "pdf_link": "https://arxiv.org/pdf/2601.17744v1",
    "content": {
      "en": "Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.",
      "tr": "**Makale Başlığı:** Faramesh: Otonom Ajan Sistemleri İçin Protokolden Bağımsız Bir Yürütme Kontrol Düzlemi\n\n**Özet:**\n\nOtonom ajan sistemleri giderek artan bir şekilde gerçek dünya yan etkilerine yol açmaktadır: altyapı dağıtma, veritabanlarını değiştirme, para hareket ettirme ve iş akışları yürütme. Ancak çoğu ajan yığını, kuruluşların gerçeği değiştirmeden önce bir eylemi kararlı bir şekilde onaylama, reddetme veya erteleme olanağı tanıyan zorunlu bir yürütme kontrol noktası sağlamamaktadır. Bu bildiri, protokolden bağımsız bir yürütme kontrol düzlemi olan Faramesh'i sunmaktadır. Bu düzlem, aşılmaz bir Action Authorization Boundary (AAB) aracılığıyla ajan tarafından yönlendirilen eylemler için yürütme zamanı yetkilendirmesini zorunlu kılar. Faramesh, ajan niyetini bir Canonical Action Representation (CAR) içine dönüştürür, eylemleri politika ve duruma karşı kararlı bir şekilde değerlendirir ve yürütücüler tarafından yürütme öncesinde doğrulanması gereken bir karar yapıtı (PERMIT/DEFER/DENY) yayınlar. Sistem, çerçeveden ve modelden bağımsız olacak şekilde tasarlanmıştır, çoklu ajan ve çoklu kiracı dağıtımlarını destekler ve taşıma protokollerinden (örneğin, MCP) bağımsız kalır. Faramesh ayrıca, denetlenebilirlik, doğrulama ve ajan reasoning'ini yeniden çalıştırmadan kararlı tekrarlama sağlayan, canonical action hashes ile anahtarlanmış karar odaklı, yalnızca eklemeli bir köken kaydı (provenance logging) sunar. Bu temel ilkelerin, düzenleme katmanlarına veya yalnızca gözlemlenebilirlik yaklaşımlarına gizli bir bağımlılıktan kaçınırken, otonom yürütme için uygulanabilir, öngörülebilir bir yönetişim sağladığını göstermekteyiz."
    }
  },
  {
    "id": "2601.17644v1",
    "title": "A Systemic Evaluation of Multimodal RAG Privacy",
    "authors": [
      "Ali Al-Lawati",
      "Suhang Wang"
    ],
    "published_date": "2026-01-25",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.17644v1",
    "pdf_link": "https://arxiv.org/pdf/2601.17644v1",
    "content": {
      "en": "The growing adoption of multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks (e.g. visual QA) introduces important privacy challenges. In particular, while mRAG provides a practical capability to connect private datasets to improve model performance, it risks the leakage of private information from these datasets during inference. In this paper, we perform an empirical study to analyze the privacy risks inherent in the mRAG pipeline observed through standard model prompting. Specifically, we implement a case study that attempts to infer the inclusion of a visual asset, e.g. image, in the mRAG, and if present leak the metadata, e.g. caption, related to it. Our findings highlight the need for privacy-preserving mechanisms and motivate future research on mRAG privacy.",
      "tr": "**Makale Başlığı:** Çok Modlu RAG Gizliliğinin Sistemik Bir Değerlendirmesi\n\n**Özet:**\n\nGörsel odaklı görevler (örn. görsel Soru-Cevap) için çok modlu Retrieval-Augmented Generation (mRAG) hatlarının artan benimsenmesi, önemli gizlilik zorluklarını beraberinde getirmektedir. Özellikle, mRAG model performansını iyileştirmek için özel veri kümelerini bağlama konusunda pratik bir yetenek sunarken, çıkarım sırasında bu veri kümelerinden özel bilgilerin sızması riskini taşımaktadır. Bu makalede, standart model istemi aracılığıyla gözlemlenen mRAG hattındaki gizlilik risklerini analiz etmek için ampirik bir çalışma gerçekleştiriyoruz. Spesifik olarak, bir mRAG'a bir görsel öğenin (örn. resim) dahil edilip edilmediğini ve mevcut ise ilgili meta verileri (örn. başlık) sızdırma girişiminde bulunan bir vaka çalışması uyguluyoruz. Bulgularımız, gizliliği koruyan mekanizmaların gerekliliğini vurgulamakta ve mRAG gizliliği üzerine gelecekteki araştırmalara ilham vermektedir."
    }
  },
  {
    "id": "2601.17569v1",
    "title": "Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations",
    "authors": [
      "Alireza Salemi",
      "Hamed Zamani"
    ],
    "published_date": "2026-01-24",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.IR"
    ],
    "link": "http://arxiv.org/abs/2601.17569v1",
    "pdf_link": "https://arxiv.org/pdf/2601.17569v1",
    "content": {
      "en": "Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.",
      "tr": "**Makale Başlığı:** Kişiselleştirilmiş Üretimde Kullanıcı Gizliliğini İyileştirme: Sunucu Taraflı Üretilen Spekülasyonların İstemci Taraflı Retrieval-Augmented Modifikasyonu\n\n**Özet:**\n\nKişiselleştirme, Büyük Dil Modeli (LLM) çıktılarının bireysel kullanıcı tercihlerine ve arka plan bilgilerine uyum sağlaması açısından kritik öneme sahiptir. Mevcut çözümler, LLM'nin tüketimi için bir kullanıcı profilinden ilgili bağlamın getirildiği retrieval augmentation temellidir. Bu yöntemler, getirilen özel verilerin bulut sağlayıcılara maruz bırakılması ile daha az yetenekli yerel modellere güvenme arasındaki bir ödünleşmeyle başa çıkar. Biz, özel profilleri sunucu taraflı LLM'lere ifşa etmeden yüksek kaliteli kişiselleştirme için etkileşimli bir framework olan $P^3$'ü sunuyoruz. $P^3$'te, büyük bir sunucu taraflı model yalnızca kullanıcı sorgusuna dayanarak $k$ adet taslak token dizisi üretir; bu sırada kullanıcının özel profiline retrieval erişimine sahip küçük bir istemci taraflı model, bu taslakları kullanıcı tercihlerini daha iyi yansıtacak şekilde değerlendirir ve değiştirir. Bu işlem, bir bitiş tokenı üretilene kadar tekrarlanır. Üç kişiselleştirilmiş soru cevaplama veri kümesinden oluşan yakın tarihli bir benchmark olan LaMP-QA üzerindeki deneyler, $P^3$'ün kişiselleştirilmemiş sunucu taraflı ve kişiselleştirilmiş istemci taraflı temellere göre tutarlı bir şekilde daha iyi performans gösterdiğini ve ortalama olarak istatistiksel olarak anlamlı %7,4 ila %9'luk iyileşmeler elde ettiğini göstermektedir. Önemlisi, $P^3$, tüm profilin büyük sunucu taraflı modele maruz bırakıldığı \"sızdıran\" bir üst sınır senaryosunun faydasının %90,3 ila %95,7'sini geri kazanır. Linkability ve attribute inference attacks gibi gizlilik analizleri, $P^3$'ün kişiselleştirilmemiş bir sunucu taraflı modelin gizliliğini koruduğunu ve herhangi bir kişisel bağlam olmaksızın bir sorgu göndermeye kıyasla yalnızca marjinal ek sızıntı (%1,5--%3,5) oluşturduğunu göstermektedir. Ek olarak, framework kenar (edge) dağıtımı için verimlidir; istemci taraflı model toplam tokenların yalnızca %9,2'sini üretir. Bu sonuçlar, $P^3$'ün iyileştirilmiş gizlilik ile kişiselleştirilmiş üretim için pratik ve etkili bir çözüm sunduğunu göstermektedir."
    }
  }
]