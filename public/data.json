[
  {
    "id": "2512.19297v1",
    "title": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
    "authors": [
      "Linzhi Chen",
      "Yang Sun",
      "Hongru Wei",
      "Yuqi Chen"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.19297v1",
    "pdf_link": "https://arxiv.org/pdf/2512.19297v1",
    "content": {
      "en": "Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.",
      "tr": "**Makale Başlığı:** Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models\n\n**Özet:**\n\nLow-Rank Adaptation (LoRA), büyük dil modellerini (LLM) ince ayar yapmak için verimli bir yöntem olarak ortaya çıkmış ve açık kaynak topluluğu içinde yaygın olarak benimsenmiştir. Ancak, Hugging Face gibi platformlar aracılığıyla LoRA adaptörlerinin merkezi olmayan yayılması yeni güvenlik açıkları yaratmaktadır: kötü amaçlı adaptörler kolayca dağıtılabilir ve geleneksel denetim mekanizmalarından kaçabilir. Bu risklere rağmen, LoRA tabanlı ince ayarı hedef alan backdoor saldırıları nispeten yeterince araştırılmamıştır. Mevcut backdoor saldırı stratejileri bu ortam için uygun değildir, çünkü genellikle erişilemeyen eğitim verilerine dayanırlar, LoRA'ya özgü yapısal özellikleri hesaba katamazlar veya yüksek false trigger rates (FTR) ile uğraşırlar, böylece gizliliklerini tehlikeye atarlar. Bu zorlukları ele almak için, açık ağırlıklı LoRA modelleri için özel olarak tasarlanmış yeni bir backdoor saldırı çerçevesi olan Causal-Guided Detoxify Backdoor Attack (CBA) öneriyoruz. CBA, orijinal eğitim verilerine erişim olmadan çalışır ve iki temel yenilik aracılığıyla yüksek gizlilik sağlar: (1) davranışsal keşif yoluyla göreve uyumlu girdileri sentezleyen bir coverage-guided data generation pipeline ve (2) göreve kritik nöronları koruyarak zehirlenmiş ve temiz adaptörleri birleştiren bir causal-guided detoxification strategy. Önceki yaklaşımlardan farklı olarak CBA, ağırlık tahsisini causal influence-based olarak kullanarak saldırı yoğunluğu üzerinde eğitim sonrası kontrolü mümkün kılar ve tekrarlanan yeniden eğitime olan ihtiyacı ortadan kaldırır. Altı LoRA modeli üzerinde değerlendirilen CBA, yüksek saldırı başarı oranları elde ederken, temel yöntemlere kıyasla FTR'yi %50-70 oranında azaltmaktadır. Ayrıca, en gelişmiş backdoor savunmalarına karşı artan direnç göstererek gizliliğini ve sağlamlığını vurgulamaktadır."
    }
  },
  {
    "id": "2512.19286v1",
    "title": "GShield: Mitigating Poisoning Attacks in Federated Learning",
    "authors": [
      "Sameera K. M.",
      "Serena Nicolazzo",
      "Antonino Nocera",
      "Vinod P.",
      "Rafidha Rehiman K. A"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.19286v1",
    "pdf_link": "https://arxiv.org/pdf/2512.19286v1",
    "content": {
      "en": "Federated Learning (FL) has recently emerged as a revolutionary approach to collaborative training Machine Learning models. In particular, it enables decentralized model training while preserving data privacy, but its distributed nature makes it highly vulnerable to a severe attack known as Data Poisoning. In such scenarios, malicious clients inject manipulated data into the training process, thereby degrading global model performance or causing targeted misclassification. In this paper, we present a novel defense mechanism called GShield, designed to detect and mitigate malicious and low-quality updates, especially under non-independent and identically distributed (non-IID) data scenarios. GShield operates by learning the distribution of benign gradients through clustering and Gaussian modeling during an initial round, enabling it to establish a reliable baseline of trusted client behavior. With this benign profile, GShield selectively aggregates only those updates that align with the expected gradient patterns, effectively isolating adversarial clients and preserving the integrity of the global model. An extensive experimental campaign demonstrates that our proposed defense significantly improves model robustness compared to the state-of-the-art methods while maintaining a high accuracy of performance across both tabular and image datasets. Furthermore, GShield improves the accuracy of the targeted class by 43\\% to 65\\% after detecting malicious and low-quality clients.",
      "tr": "**Makale Başlığı:** GShield: Federated Learning'de Zehirlenme Saldırılarının Azaltılması\n\n**Özet:**\n\nFederated Learning (FL), Makine Öğrenmesi modellerinin işbirliğiyle eğitilmesinde devrim niteliğinde bir yaklaşım olarak öne çıkmıştır. Özellikle, veri gizliliğini korurken merkezi olmayan model eğitimine olanak tanır, ancak dağıtık yapısı onu Data Poisoning olarak bilinen ciddi bir saldırıya karşı son derece savunmasız hale getirir. Bu tür senaryolarda, kötü niyetli istemciler eğitim sürecine manipüle edilmiş veriler enjekte ederek global model performansını düşürür veya hedeflenmiş yanlış sınıflandırmalara neden olur. Bu makalede, özellikle non-independent and identically distributed (non-IID) veri senaryolarında kötü niyetli ve düşük kaliteli güncellemeleri tespit etmek ve azaltmak için tasarlanmış GShield adında yeni bir savunma mekanizması sunmaktayız. GShield, ilk tur sırasında kümeleme ve Gaussian modeling aracılığıyla benign gradient'lerin dağılımını öğrenerek çalışır ve bu sayede güvenilir bir trusted client behavior temeli oluşturmasını sağlar. Bu benign profile sahip olan GShield, yalnızca beklenen gradient patterns ile uyumlu güncellemeleri seçici olarak toplar, bu da adversarial client'ları etkili bir şekilde izole eder ve global modelin bütünlüğünü korur. Kapsamlı bir deneysel kampanya, önerdiğimiz savunmanın hem tabular hem de görüntü veri kümelerinde yüksek bir performans doğruluğunu korurken, state-of-the-art yöntemlere kıyasla model direncini önemli ölçüde artırdığını göstermektedir. Dahası, GShield, kötü niyetli ve düşük kaliteli istemcileri tespit ettikten sonra hedeflenen sınıfın doğruluğunu %43 ila %65 oranında iyileştirmektedir."
    }
  },
  {
    "id": "2512.19037v1",
    "title": "Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms",
    "authors": [
      "Md Minhazul Islam Munna",
      "Md Mahbubur Rahman",
      "Jaroslav Frnda",
      "Muhammad Shahid Anwar",
      "Alpamis Kutlimuratov"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.19037v1",
    "pdf_link": "https://arxiv.org/pdf/2512.19037v1",
    "content": {
      "en": "The proliferation of IoT devices and their reliance on Wi-Fi networks have introduced significant security vulnerabilities, particularly the KRACK and Kr00k attacks, which exploit weaknesses in WPA2 encryption to intercept and manipulate sensitive data. Traditional IDS using classifiers face challenges such as model overfitting, incomplete feature extraction, and high false positive rates, limiting their effectiveness in real-world deployments. To address these challenges, this study proposes a robust multiclass machine learning based intrusion detection framework. The methodology integrates advanced feature selection techniques to identify critical attributes, mitigating redundancy and enhancing detection accuracy. Two distinct ML architectures are implemented: a baseline classifier pipeline and a stacked ensemble model combining noise injection, Principal Component Analysis (PCA), and meta learning to improve generalization and reduce false positives. Evaluated on the AWID3 data set, the proposed ensemble architecture achieves superior performance, with an accuracy of 98%, precision of 98%, recall of 98%, and a false positive rate of just 2%, outperforming existing state-of-the-art methods. This work demonstrates the efficacy of combining preprocessing strategies with ensemble learning to fortify network security against sophisticated Wi-Fi attacks, offering a scalable and reliable solution for IoT environments. Future directions include real-time deployment and adversarial resilience testing to further enhance the model's adaptability.",
      "tr": "Elbette, makale başlığını ve özetini istediğiniz gibi çevirelim:\n\n**Makale Başlığı:** Akıllı Ağlarda Siber Güvenlik ve Sistem Analizi İçin Keskin Makine Öğrenimi Paradigmaları Aracılığıyla Saldırı Tespiti ve Güvenlik Güçlendirmesinin Yükseltilmesi\n\n**Özet:**\nIoT cihazlarının yaygınlaşması ve Wi-Fi ağlarına olan bağımlılıkları, özellikle KRACK ve Kr00k saldırıları gibi ciddi güvenlik zafiyetleri oluşturmuştur. Bu saldırılar, hassas verilerin ele geçirilip manipüle edilmesine yol açan WPA2 şifrelemesindeki zayıflıklardan faydalanmaktadır. Sınıflandırıcılar kullanan geleneksel IDS'ler (Intrusion Detection Systems), model aşırı uyumu (model overfitting), eksik özellik çıkarımı (incomplete feature extraction) ve yüksek yanlış pozitif oranları gibi zorluklarla karşılaşmakta, bu da gerçek dünya uygulamalarındaki etkinliklerini sınırlamaktadır. Bu zorlukları ele almak amacıyla, bu çalışma sağlam bir multiclass machine learning tabanlı saldırı tespit çerçevesi önermektedir. Metodoloji, kritik öznitelikleri belirlemek, fazlalığı azaltmak ve tespit doğruluğunu artırmak için gelişmiş feature selection tekniklerini entegre eder. İki farklı ML mimarisi uygulanmıştır: bir baseline classifier pipeline ve gürültü enjeksiyonu (noise injection), Principal Component Analysis (PCA) ve meta learning'i birleştiren, genelleştirmeyi iyileştiren ve yanlış pozitifleri azaltan bir stacked ensemble model. AWID3 veri seti üzerinde değerlendirilen önerilen ensemble mimarisi, %98 doğruluk (accuracy), %98 kesinlik (precision), %98 geri çağırma (recall) ve yalnızca %2 yanlış pozitif oranı (false positive rate) ile üstün performans sergileyerek mevcut en ileri düzey yöntemlerden (state-of-the-art methods) daha iyi sonuç vermiştir. Bu çalışma, sofistike Wi-Fi saldırılarına karşı ağ güvenliğini güçlendirmek için ön işleme stratejileriyle (preprocessing strategies) ensemble learning'i birleştirmenin etkinliğini göstermekte ve IoT ortamları için ölçeklenebilir ve güvenilir bir çözüm sunmaktadır. Gelecekteki yönler arasında, modelin uyarlanabilirliğini daha da artırmak için gerçek zamanlı dağıtım (real-time deployment) ve düşmanca dayanıklılık testi (adversarial resilience testing) yer almaktadır."
    }
  },
  {
    "id": "2512.19025v1",
    "title": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
    "authors": [
      "Hengrui Jia",
      "Taoran Li",
      "Jonas Guan",
      "Varun Chandrasekaran"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.19025v1",
    "pdf_link": "https://arxiv.org/pdf/2512.19025v1",
    "content": {
      "en": "Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have ``forgotten'' the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose \\name, an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$β$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.",
      "tr": "Elbette, makale başlığını ve özetini istenen şekilde Türkçeye çevirdim:\n\n**Makale Başlığı:** The Erasure Illusion: LLM Forgetting Evaluation'ın Genellemesinin Stres Testi\n\n**Özet:**\nMachine unlearning, eğitilmiş modellerden belirli veri etkilerini kaldırmayı amaçlar; bu, telif hakkı yasalarına uymak ve AI safety'yi sağlamak için gerekli bir yetenektir. Mevcut unlearning metrikleri tipik olarak, unlearning dataset'i ($D_u$) üzerindeki model performans düşüşünü izleyerek başarıyı ölçer. Büyük Dil Modelleri (LLM'ler) için bu değerlendirme paradigmasının yetersiz ve potansiyel olarak yanıltıcı olduğunu savunuyoruz. Telif hakkı veya safety tarafından motive edilen unlearning'in birçok gerçek dünya kullanımı, yalnızca $D_u$'daki tam kelime kelime içeriği değil, aynı zamanda modelin ondan türettiği daha geniş genellemelerden etkilenen davranışları da örtük olarak hedefler. Standart unlearning değerlendirmesini geçen ve hedef bilgiyi \"unutmuş\" görünen LLM'lerin, aynı zamanda $D_u$'ya anlamsal olarak bitişik içeriğe ilişkin güçlü yetenekleri sürdürdüğünü gösteriyoruz. Bu olgu, tam cümleleri silmenin mutlaka temel bilgiyi kaldırmakla eşdeğer olmadığını göstermektedir. Bu boşluğu gidermek için, bir vekil veri kümesi, $\\tilde{D}_u$ oluşturan otomatik bir stres-test çerçevesi olan \\name'i öneriyoruz. Bu vekil set, $D_u$'dan anlamsal olarak türetilmiş ancak embedding space'de yeterince farklı olacak şekilde yapılandırılmıştır. $D_u$ ve $\\tilde{D}_u$ arasındaki unlearning metrik puanlarını karşılaştırarak, metriklerin kendi güvenilirliğini stres testinden geçirebiliriz. Üç LLM ailesi (Llama-3-8B, Qwen2.5-7B ve Zephyr-7B-$β$), üç farklı dataset ve yedi standart metrik üzerinde yapılan kapsamlı değerlendirmemiz, yaygın tutarsızlıkları ortaya koymaktadır. Mevcut metriklerin sıklıkla unlearning başarısını aşırı tahmin ettiğini ve stres testi veri kümelerimizin ortaya çıkardığı tutulan bilgiyi tespit edemediğini bulduk."
    }
  },
  {
    "id": "2512.19011v1",
    "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
    "authors": [
      "Akshaj Prashanth Rao",
      "Advait Singh",
      "Saumya Kumaar Saksena",
      "Dhruv Kumar"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.19011v1",
    "pdf_link": "https://arxiv.org/pdf/2512.19011v1",
    "content": {
      "en": "Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.   Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.   Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.",
      "tr": "**Makale Başlığı:** Çok Aşamalı Bir Boru Hattında Semantik Lineer Sınıflandırma Kullanarak Etkin Jailbreak Engellemesi\n\n**Özet:**\n\nPrompt injection ve jailbreaking saldırıları, büyük dil modeli (LLM) tabanlı sistemler için kalıcı güvenlik zorlukları ortaya koymaktadır. Hafif, çok aşamalı bir boru hattı aracılığıyla bu tehditleri azaltan, verimli ve sistematik olarak değerlendirilmiş bir savunma mimarisi sunmaktayız. Temel bileşeni, metin normalleştirme, TF-IDF temsilleri ve bir Linear SVM classifier'ına dayanan semantik bir filtredir. Basitliğine rağmen, bu modül elde tutulan veriler üzerinde %93,4 doğruluk ve %96,5 özgüllük elde ederek, ihmal edilebilir hesaplama yükü getirirken saldırı verimini önemli ölçüde azaltmaktadır. Bu verimli temel üzerine inşa edilen tam boru hattı, ardışık aşamalarda çalışan tamamlayıcı tespit ve azaltma mekanizmalarını entegre ederek minimum gecikme ile güçlü bir sağlamlık sağlamaktadır. Karşılaştırmalı deneylerde, SVM tabanlı konfigürasyonumuz genel doğruluğu %35,1'den %93,4'e çıkarırken, tamamlanma için ortalama süreyi yaklaşık 450 saniyeden 47 saniyeye düşürerek ShieldGemma'ya göre 10 kattan fazla daha düşük gecikme sağlamaktadır. Bu sonuçlar, önerilen tasarımın hem savunma hassasiyetini hem de verimliliğini eş zamanlı olarak ilerlettiğini ve mevcut model tabanlı moderatörlerin temel bir sınırlılığını giderdiğini göstermektedir. İyi huylu, jailbreak ve uygulama katmanı enjeksiyonları dahil olmak üzere 30.000'den fazla etiketli prompt içeren derlenmiş bir veri kümesi üzerinden yapılan değerlendirme, aşamalı, kaynak açısından verimli savunmaların modern LLM destekli uygulamaları sağlam bir şekilde güvence altına alabileceğini doğrulamaktadır."
    }
  },
  {
    "id": "2512.18932v1",
    "title": "DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems",
    "authors": [
      "Sarwan Ali"
    ],
    "published_date": "2025-12-22",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.18932v1",
    "pdf_link": "https://arxiv.org/pdf/2512.18932v1",
    "content": {
      "en": "Differential privacy (DP) has emerged as the gold standard for protecting user data in recommender systems, but existing privacy-preserving mechanisms face a fundamental challenge: the privacy-utility tradeoff inevitably degrades recommendation quality as privacy budgets tighten. We introduce DPSR (Differentially Private Sparse Reconstruction), a novel three-stage denoising framework that fundamentally addresses this limitation by exploiting the inherent structure of rating matrices -- sparsity, low-rank properties, and collaborative patterns.   DPSR consists of three synergistic stages: (1) \\textit{information-theoretic noise calibration} that adaptively reduces noise for high-information ratings, (2) \\textit{collaborative filtering-based denoising} that leverages item-item similarities to remove privacy noise, and (3) \\textit{low-rank matrix completion} that exploits latent structure for signal recovery. Critically, all denoising operations occur \\textit{after} noise injection, preserving differential privacy through the post-processing immunity theorem while removing both privacy-induced and inherent data noise.   Through extensive experiments on synthetic datasets with controlled ground truth, we demonstrate that DPSR achieves 5.57\\% to 9.23\\% RMSE improvement over state-of-the-art Laplace and Gaussian mechanisms across privacy budgets ranging from $\\varepsilon=0.1$ to $\\varepsilon=10.0$ (all improvements statistically significant with $p < 0.05$, most $p < 0.001$). Remarkably, at $\\varepsilon=1.0$, DPSR achieves RMSE of 0.9823, \\textit{outperforming even the non-private baseline} (1.0983), demonstrating that our denoising pipeline acts as an effective regularizer that removes data noise in addition to privacy noise.",
      "tr": "Elbette, akademik makale başlığı ve özetini istenen şekilde Türkçeye çevirelim. Teknik terimler İngilizce olarak bırakılacak, geri kalan metinde ise resmi ve akademik bir dil kullanılacaktır.\n\n**Makale Başlığı:** DPSR: Recommender Sistemleri İçin Çok Aşamalı Gürültü Giderme Yoluyla Differentially Private Sparse Reconstruction\n\n**Özet:**\n\nDifferential privacy (DP), recommender sistemlerinde kullanıcı verilerini korumak için altın standart olarak ortaya çıkmıştır; ancak mevcut gizlilik koruma mekanizmaları temel bir zorlukla karşı karşıyadır: gizlilik-fayda takası, gizlilik bütçesi daraldıkça kaçınılmaz olarak öneri kalitesini düşürmektedir. Rating matrislerinin doğasındaki seyrekliği (sparsity), düşük-rank özelliklerini ve işbirlikçi örüntüleri (collaborative patterns) kullanarak bu sınırlamayı temelden ele alan yeni bir üç aşamalı gürültü giderme çerçevesi olan DPSR (Differentially Private Sparse Reconstruction)'ı sunuyoruz. DPSR, üç sinerjik aşamadan oluşmaktadır: (1) yüksek bilgi içeren ratingler için gürültüyü adaptif olarak azaltan \\textit{information-theoretic noise calibration}, (2) gizlilik gürültüsünü gidermek için item-item benzerliklerinden yararlanan \\textit{collaborative filtering-based denoising} ve (3) sinyal kurtarma için latent yapıyı kullanan \\textit{low-rank matrix completion}. Kritik olarak, tüm gürültü giderme işlemleri gürültü enjeksiyonundan \\textit{sonra} gerçekleşerek, post-processing immunity theorem aracılığıyla differential privacy'yi korurken hem gizlilikten kaynaklanan hem de doğasında bulunan veri gürültüsünü gidermektedir. Kontrollü ground truth'a sahip sentetik veri setleri üzerinde yapılan kapsamlı deneyler aracılığıyla, DPSR'nin $\\varepsilon=0.1$ ile $\\varepsilon=10.0$ arasındaki gizlilik bütçelerinde, son teknoloji Laplace ve Gaussian mekanizmalarına göre %5.57 ila %9.23 arasında RMSE iyileştirmesi sağladığını (tüm iyileştirmeler istatistiksel olarak anlamlıdır ve $p < 0.05$, çoğunlukla $p < 0.001$), gösteriyoruz. Özellikle, $\\varepsilon=1.0$ seviyesinde DPSR, 0.9823 RMSE değeri elde ederek, \\textit{non-private baseline}'ı (1.0983) bile geride bırakmaktadır. Bu durum, gürültü giderme boru hattımızın gizlilik gürültüsüne ek olarak veri gürültüsünü de gideren etkili bir regularizör olarak işlev gördüğünü kanıtlamaktadır."
    }
  },
  {
    "id": "2512.18791v1",
    "title": "Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform",
    "authors": [
      "Yichuan Zhang",
      "Chengxin Li",
      "Yujie Gu"
    ],
    "published_date": "2025-12-21",
    "tags": [
      "cs.SD",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.18791v1",
    "pdf_link": "https://arxiv.org/pdf/2512.18791v1",
    "content": {
      "en": "Text-to-Speech (TTS) diffusion models generate high-quality speech, which raises challenges for the model intellectual property protection and speech tracing for legal use. Audio watermarking is a promising solution. However, due to the structural differences among various TTS diffusion models, existing watermarking methods are often designed for a specific model and degrade audio quality, which limits their practical applicability. To address this dilemma, this paper proposes a universal watermarking scheme for TTS diffusion models, termed Smark. This is achieved by designing a lightweight watermark embedding framework that operates in the common reverse diffusion paradigm shared by all TTS diffusion models. To mitigate the impact on audio quality, Smark utilizes the discrete wavelet transform (DWT) to embed watermarks into the relatively stable low-frequency regions of the audio, which ensures seamless watermark-audio integration and is resistant to removal during the reverse diffusion process. Extensive experiments are conducted to evaluate the audio quality and watermark performance in various simulated real-world attack scenarios. The experimental results show that Smark achieves superior performance in both audio quality and watermark extraction accuracy.",
      "tr": "Elbette, akademik makale başlığı ve özetinin çevirisi aşağıdadır:\n\n**Makale Başlığı:** Smark: Discrete Wavelet Transform Aracılığıyla Metin Okuma (Text-to-Speech) Difüzyon Modelleri İçin Bir Filigran\n\n**Özet:**\n\nMetin Okuma (Text-to-Speech - TTS) difüzyon modelleri yüksek kaliteli konuşma üretmektedir, bu durum modelin fikri mülkiyetinin korunması ve yasal kullanımlar için konuşma takibi konusunda zorluklar yaratmaktadır. Ses filigranlama, umut vadeden bir çözümdür. Bununla birlikte, çeşitli TTS difüzyon modelleri arasındaki yapısal farklılıklar nedeniyle, mevcut filigranlama yöntemleri genellikle belirli bir model için tasarlanmış olup ses kalitesini düşürmektedir, bu da pratik uygulanabilirliklerini sınırlamaktadır. Bu ikilemi ele almak için bu çalışma, Smark olarak adlandırılan TTS difüzyon modelleri için evrensel bir filigranlama şeması önermektedir. Bu, tüm TTS difüzyon modelleri tarafından paylaşılan yaygın ters difüzyon paradigmasında çalışan hafif bir filigran gömme çerçevesi tasarlayarak gerçekleştirilir. Ses kalitesi üzerindeki etkiyi azaltmak için Smark, filigranları sesin nispeten kararlı düşük frekanslı bölgelerine gömmek için discrete wavelet transform (DWT)'yi kullanır, bu da kusursuz filigran-ses entegrasyonunu sağlar ve ters difüzyon süreci sırasında kaldırılmaya karşı dirençlidir. Çeşitli simüle edilmiş gerçek dünya saldırı senaryolarında ses kalitesini ve filigran performansını değerlendirmek için kapsamlı deneyler yürütülmüştür. Deneysel sonuçlar, Smark'ın hem ses kalitesi hem de filigran çıkarma doğruluğu açısından üstün performans gösterdiğini ortaya koymaktadır."
    }
  },
  {
    "id": "2512.18733v1",
    "title": "Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection",
    "authors": [
      "Junjun Pan",
      "Yixin Liu",
      "Rui Miao",
      "Kaize Ding",
      "Yu Zheng"
    ],
    "published_date": "2025-12-21",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2512.18733v1",
    "pdf_link": "https://arxiv.org/pdf/2512.18733v1",
    "content": {
      "en": "Large language model (LLM)-based multi-agent systems (MAS) have shown strong capabilities in solving complex tasks. As MAS become increasingly autonomous in various safety-critical tasks, detecting malicious agents has become a critical security concern. Although existing graph anomaly detection (GAD)-based defenses can identify anomalous agents, they mainly rely on coarse sentence-level information and overlook fine-grained lexical cues, leading to suboptimal performance. Moreover, the lack of interpretability in these methods limits their reliability and real-world applicability. To address these limitations, we propose XG-Guard, an explainable and fine-grained safeguarding framework for detecting malicious agents in MAS. To incorporate both coarse and fine-grained textual information for anomalous agent identification, we utilize a bi-level agent encoder to jointly model the sentence- and token-level representations of each agent. A theme-based anomaly detector further captures the evolving discussion focus in MAS dialogues, while a bi-level score fusion mechanism quantifies token-level contributions for explanation. Extensive experiments across diverse MAS topologies and attack scenarios demonstrate robust detection performance and strong interpretability of XG-Guard.",
      "tr": "İşte akademik makale başlığının ve özetinin istenen şekilde çevrilmiş hali:\n\n**Makale Başlığı:** LLM Çoklu-Aj an Sistemlerinin Açıklanabilir ve İnce Taneli Korunması: İki Kademeli Grafik Anomali Tespiti Aracılığıyla\n\n**Özet:**\nBüyük dil modeli (LLM) tabanlı çoklu-ajan sistemleri (MAS), karmaşık görevleri çözmede güçlü yetenekler sergilemiştir. MAS'lar çeşitli güvenlik açısından kritik görevlerde giderek daha fazla otonom hale geldikçe, kötü niyetli ajanların tespiti kritik bir güvenlik endişesi haline gelmiştir. Mevcut grafik anomali tespitine (GAD) dayalı savunmalar, anormal ajanları tanımlayabilse de, bunlar çoğunlukla kaba cümle düzeyindeki bilgilere dayanır ve ince taneli kelime ipuçlarını göz ardı ederek optimum olmayan performansa yol açar. Dahası, bu yöntemlerde açıklanabilirlik eksikliği, güvenilirliklerini ve gerçek dünya uygulanabilirliklerini sınırlamaktadır. Bu sınırlamaları gidermek için, MAS'lardaki kötü niyetli ajanları tespit etmek üzere açıklanabilir ve ince taneli bir koruma çerçevesi olan XG-Guard'ı öneriyoruz. Anormal ajan tanımlaması için hem kaba hem de ince taneli metinsel bilgileri dahil etmek üzere, her ajanın cümle ve token düzeyindeki temsillerini ortaklaşa modellemek için bir bi-level agent encoder kullanıyoruz. Tema tabanlı bir anomaly detector, MAS diyaloglarındaki gelişen tartışma odağını daha da yakalar; aynı zamanda bir bi-level score fusion mekanizması, açıklama için token düzeyindeki katkıları ölçer. Çeşitli MAS topolojileri ve saldırı senaryoları üzerinden yapılan kapsamlı deneyler, XG-Guard'ın sağlam tespit performansını ve güçlü açıklanabilirliğini göstermektedir."
    }
  },
  {
    "id": "2512.18616v1",
    "title": "DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System",
    "authors": [
      "Zelin Wan",
      "Han Jun Yoon",
      "Nithin Alluru",
      "Terrence J. Moore",
      "Frederica F. Nelson"
    ],
    "published_date": "2025-12-21",
    "tags": [
      "cs.HC",
      "cs.AI",
      "cs.CR",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2512.18616v1",
    "pdf_link": "https://arxiv.org/pdf/2512.18616v1",
    "content": {
      "en": "We present DASH (Deception-Augmented Shared mental model for Human-machine teaming), a novel framework that enhances mission resilience by embedding proactive deception into Shared Mental Models (SMM). Designed for mission-critical applications such as surveillance and rescue, DASH introduces \"bait tasks\" to detect insider threats, e.g., compromised Unmanned Ground Vehicles (UGVs), AI agents, or human analysts, before they degrade team performance. Upon detection, tailored recovery mechanisms are activated, including UGV system reinstallation, AI model retraining, or human analyst replacement. In contrast to existing SMM approaches that neglect insider risks, DASH improves both coordination and security. Empirical evaluations across four schemes (DASH, SMM-only, no-SMM, and baseline) show that DASH sustains approximately 80% mission success under high attack rates, eight times higher than the baseline. This work contributes a practical human-AI teaming framework grounded in shared mental models, a deception-based strategy for insider threat detection, and empirical evidence of enhanced robustness under adversarial conditions. DASH establishes a foundation for secure, adaptive human-machine teaming in contested environments.",
      "tr": "**Makale Başlığı:** DASH: İnsan-Makine Takım Sistemi için Aldatma Destekli Paylaşılan Zihinsel Model\n\n**Özet:**\n\nBu çalışmada, Aktif aldatmayı Paylaşılan Zihinsel Modeller (SMM) içine gömerek görev dayanıklılığını artıran yeni bir framework olan DASH'ı (Deception-Augmented Shared mental model for Human-machine teaming) sunuyoruz. Gözetleme ve kurtarma gibi göreve kritik uygulamalar için tasarlanan DASH, takım performansını düşürmeden önce içeriden gelen tehditleri, örneğin hacklenmiş Unmanned Ground Vehicles (UGV), AI ajanları veya insan analistleri gibi unsurları tespit etmek için \"bait task\" mekanizmasını tanıtmaktadır. Tespit edildiğinde, UGV sisteminin yeniden yüklenmesi, AI modelinin yeniden eğitilmesi veya insan analistinin değiştirilmesi gibi özel kurtarma mekanizmaları devreye sokulur. İçeriden gelen riskleri göz ardı eden mevcut SMM yaklaşımlarının aksine DASH, hem koordinasyonu hem de güvenliği artırmaktadır. Dört şema üzerinde (DASH, SMM-yalnız, SMM-yok ve temel seviye) yapılan ampirik değerlendirmeler, DASH'ın yüksek saldırı oranları altında temel seviyeye göre sekiz kat daha yüksek, yaklaşık %80 görev başarısını sürdürdüğünü göstermektedir. Bu çalışma, paylaşılan zihinsel modellere dayanan pratik bir insan-yapay zeka takım framework'ü, içeriden gelen tehdit tespiti için aldatmaya dayalı bir strateji ve düşmanca koşullar altında artırılmış sağlamlık için ampirik kanıtlar sunmaktadır. DASH, karmaşık ortamlarda güvenli, uyarlanabilir insan-makine takım çalışması için bir temel oluşturmaktadır."
    }
  },
  {
    "id": "2512.18542v1",
    "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
    "authors": [
      "Scott Thornton"
    ],
    "published_date": "2025-12-20",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.18542v1",
    "pdf_link": "https://arxiv.org/pdf/2512.18542v1",
    "content": {
      "en": "AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).   Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.   Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.",
      "tr": "Makale Başlığı: SecureCode v2.0: Güvenlik Farkındalığına Sahip Kod Üretimi Modellerinin Eğitimi İçin Üretim Sınıfı Bir Veri Seti\n\nÖzet:\nYapay zeka asistanları, güvenlik açısından önem arz eden senaryoların %45'inde güvensiz kod üretmekte ve bu durum ölçeklenebilir bir şekilde üretim sistemlerine kusurlar bulaştırmaktadır. Mevcut güvenli kodlama veri setleri ise yetersiz kalmaktadır. Bu veri setleri, olay temelli analiz eksikliğine sahip olup, modern eğitimin gerektirdiği ölçeği sağlamamakta ve geliştiricilerin üretim ortamlarına dağıtım için ihtiyaç duyduğu operasyonel güvenlik bağlamını gözden kaçırmaktadır. Biz, yapısal doğrulama ve uzman güvenlik incelemesinden geçen 1.215 güvenlik odaklı kodlama örneğinden oluşan üretim sınıfı bir veri seti olan SecureCode v2.0'ı sunuyoruz. Her örnek, CVE referanslarıyla belgelenmiş gerçek güvenlik olaylarına bağlanmakta, güvensiz ve güvenli uygulamalar sunmakta, somut saldırıları göstermekte ve katmanlı savunma (defense-in-depth) operasyonel rehberlik içermektedir. Veri seti, 11 dilde (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin ve altyapı-kod olarak YAML) 11 zafiyet kategorisini (tam OWASP Top 10:2025 plus AI/ML Security Threats) kapsamaktadır. Kalite güvence çerçevemiz, tam olay temelli analizi (incident grounding) sağlamaktadır. Her örnek, SIEM integration strategies, altyapı sertleştirme önerileri (Docker, AppArmor, WAF configurations) ve dile uygun framework'ler kullanılarak test yaklaşımlarını içermektedir. Veri seti, gerçek geliştirici-yapay zeka etkileşimlerini yansıtan, temel uygulamalardan gelişmiş güvenlik hususlarına ve katmanlı savunma rehberliğine doğru tırmanan 4-turn conversational structure kullanmaktadır. Katkılarımız: (1) 989 eğitim, 122 doğrulama ve 104 test setine ayrılmış 1.215 titizlikle doğrulanmış örnek, (2) veri seti tutarlılığını sağlayan otomatik bir doğrulama framework'ü, (3) gerçekçi güvenlik iş akışlarını yakalayan bir 4-turn conversational structure, (4) SIEM integration strategies ile kapsamlı operasyonel güvenlik rehberliği, (5) dile özel uygulama sadakatinin tamlığı ve (6) veri, doğrulama araçları ve benchmarking protocols'ün açık kaynaklı sürümü."
    }
  }
]