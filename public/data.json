[
  {
    "id": "2602.15756v1",
    "title": "A Note on Non-Composability of Layerwise Approximate Verification for Neural Inference",
    "authors": [
      "Or Zamir"
    ],
    "published_date": "2026-02-17",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.15756v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15756v1",
    "content": {
      "en": "A natural and informal approach to verifiable (or zero-knowledge) ML inference over floating-point data is: ``prove that each layer was computed correctly up to tolerance $δ$; therefore the final output is a reasonable inference result''. This short note gives a simple counterexample showing that this inference is false in general: for any neural network, we can construct a functionally equivalent network for which adversarially chosen approximation-magnitude errors in individual layer computations suffice to steer the final output arbitrarily (within a prescribed bounded range).",
      "tr": "Aşağıda akademik makale başlığı ve özetinin Türkçe çevirisi bulunmaktadır:\n\n**Makale Başlığı:** Bir Sinirsel Çıkarım İçin Katman Bazlı Yaklaşık Doğrulamanın Bileşememezliği Üzerine Bir Not\n\n**Özet:**\nFloating-point veriler üzerinde doğrulanabilir (veya zero-knowledge) ML çıkarımı için doğal ve gayri resmi bir yaklaşım şudur: ``tolerans $\\delta$'ya kadar her katmanın doğru hesaplandığını kanıtla; dolayısıyla nihai çıktı makul bir çıkarım sonucudur.'' Bu kısa not, bu çıkarımın genel olarak yanlış olduğunu gösteren basit bir karşı örnek sunmaktadır: herhangi bir sinirsel ağ için, bireysel katman hesaplamalarındaki düşmanca seçilmiş yaklaştırma-büyüklük hatalarının, nihai çıktıyı keyfi olarak (belirlenmiş sınırlı bir aralık içinde) yönlendirmeye yetecek şekilde işlevsel olarak eşdeğer bir ağ oluşturabiliriz."
    }
  },
  {
    "id": "2602.15689v1",
    "title": "A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models",
    "authors": [
      "Meirav Segal",
      "Noa Linder",
      "Omer Antverg",
      "Gil Gekker",
      "Tomer Fichman"
    ],
    "published_date": "2026-02-17",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.15689v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15689v1",
    "content": {
      "en": "Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.",
      "tr": "İşte akademik makale başlığının ve özetinin Türkçeye çevrilmiş hali:\n\n**Makale Başlığı:** Large Language Modellerinde Siber Güvenlik Red Kararları İçin İçerik Tabanlı Bir Çerçeve\n\n**Özet:**\nLarge language modeller ve LLM-tabanlı ajanlar, giderek artan bir şekilde doğası gereği çift kullanımlı olan siber güvenlik görevlerinde kullanılmaktadır. Mevcut red yaklaşımları, akademik politika çerçevelerinden ticari olarak dağıtılmış sistemlere kadar geniş bir yelpazede yer almakta olup, genellikle geniş konu tabanlı yasaklamalara veya saldırı odaklı taksonomilere dayanmaktadır. Sonuç olarak, tutarsız kararlar üretebilir, meşru savunmacıları aşırı derecede kısıtlayabilir ve gizleme veya istek segmentasyonu altında kırılgan davranabilirler. Etkili bir red kararının, yalnızca niyete veya saldırı sınıflandırmasına güvenmek yerine, saldırı riski ile savunma faydası arasındaki dengeyi açıkça modellemeyi gerektirdiğini savunuyoruz. Bu makalede, saldırı-savunma dengelerini açıkça ortaya koyan siber red politikalarının tasarlanması ve denetlenmesi için içerik tabanlı bir çerçeve sunuyoruz. Çerçeve, istekleri beş boyut boyunca karakterize eder: Saldırı Eylemi Katkısı, Saldırı Riski, Teknik Karmaşıklık, Savunma Faydası ve Meşru Kullanıcılar İçin Beklenen Sıklık. Bu boyutlar, beyan edilen niyetten ziyade isteğin teknik özüne dayanmaktadır. Bu içerik temelli yaklaşımın, mevcut öncü model davranışındaki tutarsızlıkları giderdiğini ve kuruluşların ayarlanabilir, riske duyarlı red politikaları oluşturmalarına olanak tanıdığını gösteriyoruz."
    }
  },
  {
    "id": "2602.15654v1",
    "title": "Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections",
    "authors": [
      "Xianglin Yang",
      "Yufei He",
      "Shuo Ji",
      "Bryan Hooi",
      "Jin Song Dong"
    ],
    "published_date": "2026-02-17",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.15654v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15654v1",
    "content": {
      "en": "Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker.   We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.",
      "tr": "İşte akademik makalenin başlığı ve özetinin istenen şekilde Türkçeye çevirisi:\n\n**Makale Başlığı:** Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections\n\n**Özet:**\n\nSelf-evolving LLM agents, oturumlar boyunca iç durumlarını günceller, genellikle uzun süreli belleği yazarak ve yeniden kullanarak. Bu tasarım, uzun ufuklu görevlerde performansı artırır ancak bir güvenlik riski oluşturur: iyi niyetli bir oturum sırasında gözlemlenen güvenilmeyen harici içerik bellek olarak saklanabilir ve daha sonra talimat olarak değerlendirilebilir. Bu riski inceliyoruz ve Zombie Agent adını verdiğimiz ve bir saldırganın gizlice bir payload yerleştirdiği, bu payload'un oturumlar boyunca hayatta kaldığı ve etkili bir şekilde ajanı saldırganın kuklasına dönüştürdüğü kalıcı bir saldırıyı resmileştiriyoruz. Sadece saldırgan kontrollü web içeriği aracılığıyla dolaylı maruz kalmayı kullanan bir black-box saldırı framework'ü sunuyoruz. Saldırı iki aşamadan oluşur. Enfeksiyon sırasında, ajan iyi niyetli bir görevi tamamlarken zehirlenmiş bir kaynağı okur ve normal güncelleme süreci aracılığıyla payload'u uzun süreli belleğe yazar. Tetikleme sırasında, payload geri alınır veya ileri taşınır ve yetkisiz araç davranışına neden olur. Kesme ve relevance filtering'e karşı koymak için sliding-window ve retrieval-augmented memory dahil olmak üzere yaygın bellek uygulamaları için mekanizma-spesifik persistence strategies tasarlıyoruz. Saldırıyı temsilci ajan kurulumları ve görevleri üzerinde, hem zaman içindeki persistence'ı hem de iyi niyetli görev kalitesini korurken yetkisiz eylemleri tetikleme yeteneğini ölçerek değerlendiriyoruz. Sonuçlarımız, bellek evriminin tek seferlik dolaylı enjeksiyonu kalıcı bir tehlikeye dönüştürebileceğini göstermektedir, bu da sadece oturum başına prompt filtering'e odaklanan savunmaların self-evolving agents için yeterli olmadığını düşündürmektedir."
    }
  },
  {
    "id": "2602.15485v1",
    "title": "SecCodeBench-V2 Technical Report",
    "authors": [
      "Longfei Chen",
      "Ji Zhao",
      "Lanxiao Cui",
      "Tong Su",
      "Xingbo Pan"
    ],
    "published_date": "2026-02-17",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2602.15485v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15485v1",
    "content": {
      "en": "We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and Node.js. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.",
      "tr": "**Makale Başlığı:** SecCodeBench-V2 Teknik Raporu\n\n**Özet:**\n\nBu çalışmada, Büyük Dil Modeli (LLM) kod yardımcısı araçlarının güvenli kod üretme yeteneklerini değerlendirmek için kamuya açık olarak yayınlanan bir benchmark olan SecCodeBench-V2'yi tanıtıyoruz. SecCodeBench-V2, Alibaba Group'un endüstriyel üretimlerinden türetilmiş 98 adet üretme ve düzeltme senaryosunu içermektedir. Bu senaryolarda yer alan temel güvenlik sorunları, beş programlama dilinde (Java, C, Python, Go ve Node.js) 22 yaygın CWE (Common Weakness Enumeration) kategorisini kapsamaktadır. SecCodeBench-V2, fonksiyon düzeyinde bir görev formülasyonunu benimsemektedir: her senaryo eksiksiz bir proje iskeleti sunmakta ve modelden sabit arayüzler ve bağımlılıklar altında belirlenmiş hedef bir fonksiyonu uygulaması veya yamalaması beklenmektedir. Her senaryo için SecCodeBench-V2, hem fonksiyonel doğrulama hem de güvenlik doğrulama amacıyla çalıştırılabilir kavram ispatı (PoC) test senaryoları sağlamaktadır. Tüm test senaryoları, güvenlik uzmanları tarafından hazırlanmış ve çift gözden geçirilmiş olup, yüksek doğruluk, geniş kapsama alanı ve güvenilir zemin gerçeği sağlamaktadır. Benchmark'ın ötesinde, modelleri öncelikli olarak dinamik yürütme yoluyla değerlendiren bir birleşik değerlendirme hattı oluşturduk. Senaryoların çoğu için, model tarafından üretilen çıktıları izole ortamlarda derleyip çalıştırıyor ve hem fonksiyonel doğruluğu hem de güvenlik özelliklerini doğrulamak için PoC test senaryolarını yürütüyoruz. Güvenlik sorunlarının deterministik test senaryoları ile kararlaştırılamadığı senaryolarda, ek olarak LLM-as-a-judge oracle kullanıyoruz. Heterojen senaryolar ve zorluk seviyeleri boyunca performansı özetlemek amacıyla, senaryolar ve şiddet üzerinde prensipli toplama ile Pass@K tabanlı bir puanlama protokolü tasarlıyoruz. Bu protokol, modeller arasında bütünsel ve karşılaştırılabilir bir değerlendirme imkanı sunmaktadır. Genel olarak SecCodeBench-V2, yapay zeka kodlama yardımcılarının güvenlik duruşunu değerlendirmek için titiz ve tekrarlanabilir bir temel sağlamaktadır. Sonuçlar ve ilgili yapılar https://alibaba.github.io/sec-code-bench adresinde yayınlanmıştır. Benchmark, https://github.com/alibaba/sec-code-bench adresinde kamuya açıktır."
    }
  },
  {
    "id": "2602.15376v1",
    "title": "A Unified Evaluation of Learning-Based Similarity Techniques for Malware Detection",
    "authors": [
      "Udbhav Prasad",
      "Aniesh Chawla"
    ],
    "published_date": "2026-02-17",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.15376v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15376v1",
    "content": {
      "en": "Cryptographic digests (e.g., MD5, SHA-256) are designed to provide exact identity. Any single-bit change in the input produces a completely different hash, which is ideal for integrity verification but limits their usefulness in many real-world tasks like threat hunting, malware analysis and digital forensics, where adversaries routinely introduce minor transformations. Similarity-based techniques address this limitation by enabling approximate matching, allowing related byte sequences to produce measurably similar fingerprints. Modern enterprises manage tens of thousands of endpoints with billions of files, making the effectiveness and scalability of the proposed techniques more important than ever in security applications. Security researchers have proposed a range of approaches, including similarity digests and locality-sensitive hashes (e.g., ssdeep, sdhash, TLSH), as well as more recent machine-learning-based methods that generate embeddings from file features. However, these techniques have largely been evaluated in isolation, using disparate datasets and evaluation criteria. This paper presents a systematic comparison of learning-based classification and similarity methods using large, publicly available datasets. We evaluate each method under a unified experimental framework with industry-accepted metrics. To our knowledge, this is the first reproducible study to benchmark these diverse learning-based similarity techniques side by side for real-world security workloads. Our results show that no single approach performs well across all dimensions; instead, each exhibits distinct trade-offs, indicating that effective malware analysis and threat-hunting platforms must combine complementary classification and similarity techniques rather than rely on a single method.",
      "tr": "Makale Başlığı: **Zararlı Yazılım Tespiti için Öğrenme Tabanlı Benzerlik Tekniklerinin Birleşik Bir Değerlendirmesi**\n\nÖzet:\nKriptografik özetler (örneğin: MD5, SHA-256), tam kimlik sağlamak üzere tasarlanmıştır. Girişteki tek bir bitlik değişiklik bile tamamen farklı bir hash üretir; bu, bütünlük doğrulama için idealdir ancak tehdit avcılığı, malware analysis ve digital forensics gibi, saldırganların rutin olarak küçük dönüşümler uyguladığı birçok gerçek dünya görevinde kullanışlılığını sınırlar. Benzerlik tabanlı teknikler, yaklaşık eşleştirmeyi mümkün kılarak ve ilişkili bayt dizilerinin ölçülebilir şekilde benzer parmak izleri üretmesine izin vererek bu sınırlamayı ele alır. Modern işletmeler, milyarlarca dosyaya sahip on binlerce uç noktayı yönetmektedir; bu durum, önerilen tekniklerin etkinliği ve ölçeklenebilirliğinin security applications alanında her zamankinden daha önemli hale gelmesini sağlamaktadır. Güvenlik araştırmacıları, benzerlik özetleri ve locality-sensitive hashes (örneğin: ssdeep, sdhash, TLSH) gibi çeşitli yaklaşımların yanı sıra, dosya özelliklerinden embeddings üreten daha yeni machine-learning-based yöntemler önermişlerdir. Ancak, bu teknikler büyük ölçüde izole edilmiş şekilde, farklı veri kümeleri ve değerlendirme kriterleri kullanılarak değerlendirilmiştir. Bu makale, büyük, kamuya açık veri kümeleri kullanılarak öğrenme tabanlı sınıflandırma ve benzerlik yöntemlerinin sistematik bir karşılaştırmasını sunmaktadır. Her yöntemi, endüstri tarafından kabul edilen metriklerle birleşik bir deneysel çerçeve altında değerlendiriyoruz. Bildiğimiz kadarıyla bu, gerçek dünya security workloads için bu çeşitli öğrenme tabanlı benzerlik tekniklerini yan yana kıyaslayan ilk tekrarlanabilir çalışmadır. Sonuçlarımız, hiçbir tek yaklaşımın tüm boyutlarda iyi performans göstermediğini; bunun yerine, her birinin belirgin ödünleşimler sergilediğini göstermektedir; bu da etkili malware analysis ve threat-hunting platformlarının, tek bir yönteme güvenmek yerine tamamlayıcı sınıflandırma ve benzerlik tekniklerini birleştirmesi gerektiğini göstermektedir."
    }
  },
  {
    "id": "2602.15323v1",
    "title": "Unforgeable Watermarks for Language Models via Robust Signatures",
    "authors": [
      "Huijia Lin",
      "Kameron Shahabi",
      "Min Jae Song"
    ],
    "published_date": "2026-02-17",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.15323v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15323v1",
    "content": {
      "en": "Language models now routinely produce text that is difficult to distinguish from human writing, raising the need for robust tools to verify content provenance. Watermarking has emerged as a promising countermeasure, with existing work largely focused on model quality preservation and robust detection. However, current schemes provide limited protection against false attribution. We strengthen the notion of soundness by introducing two novel guarantees: unforgeability and recoverability. Unforgeability prevents adversaries from crafting false positives, texts that are far from any output from the watermarked model but are nonetheless flagged as watermarked. Recoverability provides an additional layer of protection: whenever a watermark is detected, the detector identifies the source text from which the flagged content was derived. Together, these properties strengthen content ownership by linking content exclusively to its generating model, enabling secure attribution and fine-grained traceability. We construct the first undetectable watermarking scheme that is robust, unforgeable, and recoverable with respect to substitutions (i.e., perturbations in Hamming metric). The key technical ingredient is a new cryptographic primitive called robust (or recoverable) digital signatures, which allow verification of messages that are close to signed ones, while preventing forgery of messages that are far from all previously signed messages. We show that any standard digital signature scheme can be boosted to a robust one using property-preserving hash functions (Boyle, LaVigne, and Vaikuntanathan, ITCS 2019).",
      "tr": "Elbette, makale başlığını ve özetini istediğiniz şekilde Türkçeye çevirdim:\n\n**Makale Başlığı:** Robust Signatures Aracılığıyla Dil Modelleri İçin Sahtesi Yapılamayan Filigranlar\n\n**Özet:**\nDil modelleri artık insanlar tarafından yazılmış metinlerden ayırt edilmesi zor metinler üretiyor ve bu durum içerik köken doğrulaması için sağlam araçlara olan ihtiyacı artırıyor. Filigranlama umut verici bir önlem olarak öne çıkıyor ve mevcut çalışmalar büyük ölçüde model kalitesi korunumu ve sağlam tespit üzerine odaklanmıştır. Ancak, mevcut şemalar yanlış atıflara karşı sınırlı koruma sağlıyor. Biz, iki yeni garanti sunarak soundness (sağlamlık) kavramını güçlendiriyoruz: unforgeability (sahtesi yapılamama) ve recoverability (geri kazanılabilirlik). Unforgeability, rakip adversarilerin, filigranlanmış modelin herhangi bir çıktısından uzak olmasına rağmen yine de filigranlı olarak işaretlenen false positive’ler (yanlış pozitifler) oluşturmasını engeller. Recoverability, ek bir koruma katmanı sağlar: bir filigran tespit edildiğinde, tespitçi işaretlenen içeriğin türetildiği kaynak metni belirler. Bu özellikler birlikte, içeriği yalnızca üreten modeline bağlayarak içerik sahipliğini güçlendirir, güvenli atıf ve fine-grained traceability (ince taneli izlenebilirlik) olanağı sunar. Robust (sağlam), unforgeable (sahtesi yapılamaz) ve substitutions (Hamming metriğinde bozulmalar) ile ilişkili olarak recoverable (geri kazanılabilir) olan ilk tespit edilemeyen filigran şemasını inşa ediyoruz. Temel teknik bileşen, yeni bir kriptografik primitive olan robust (veya recoverable) digital signatures'dır (dijital imzalar), bu primitive imzalanmış mesajlara yakın mesajların doğrulanmasına izin verirken, daha önce imzalanmış tüm mesajlardan uzak mesajların forgery'sini (sahtesini yapmayı) önler. Boyle, LaVigne ve Vaikuntanathan (ITCS 2019) tarafından sunulan property-preserving hash functions (özellik koruyucu hash fonksiyonları) kullanılarak herhangi bir standart digital signature scheme'in robust bir scheme'e yükseltilebileceğini gösteriyoruz."
    }
  },
  {
    "id": "2602.15238v1",
    "title": "Closing the Distribution Gap in Adversarial Training for LLMs",
    "authors": [
      "Chengzhi Hu",
      "Jonas Dornbusch",
      "David Lüdke",
      "Stephan Günnemann",
      "Leo Schwinn"
    ],
    "published_date": "2026-02-16",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.15238v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15238v1",
    "content": {
      "en": "Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.",
      "tr": "Makale Başlığı: LLM'ler için Adversarial Training'de Dağılım Boşluğunu Kapatmak\n\nÖzet:\nLLM'ler için adversarial training, adversayilere karşı dayanıklılığı güvenilir bir şekilde artırmak için en umut verici yöntemlerden biridir. Bununla birlikte, önemli ilerlemelere rağmen, modeller geçmiş zaman kipiyle prompt'ları yeniden yazmak veya başka dillere çevirmek gibi basit in-distribution exploit'lere karşı savunmasız kalmaktadır. Bu kalıcı kırılganlığın, mevcut adversarial training algoritmalarındaki temel bir sınırlamadan kaynaklandığını savunuyoruz: bu algoritmalar, eğitim veri setleri üzerindeki adversarial loss'u minimize eder ancak veri dağılımını yetersiz bir şekilde kapsar, bu da görünüşte basit saldırılara karşı savunmasızlığa yol açar. Bu boşluğu kapatmak için Distributional Adversarial Training, DAT'ı öneriyoruz. Prompt'lar ve yanıtların gerçek ortak dağılımını yaklaştırmak için Diffusion LLM'leri kullanıyoruz, böylece genelleme başarısızlıklarını ele alan çeşitli, yüksek olasılıklı örneklerin üretilmesini sağlıyoruz. Diffusion modelinin sağladığı veri dağılımı üzerinden optimizasyonu sürekli adversarial training ile birleştirerek DAT, önceki yöntemlere göre önemli ölçüde daha yüksek adversarial robustness elde etmektedir."
    }
  },
  {
    "id": "2602.15195v1",
    "title": "Weight space Detection of Backdoors in LoRA Adapters",
    "authors": [
      "David Puertolas Merenciano",
      "Ekaterina Vasyagina",
      "Raghav Dixit",
      "Kevin Zhu",
      "Ruizhe Li"
    ],
    "published_date": "2026-02-16",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.15195v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15195v1",
    "content": {
      "en": "LoRA adapters let users fine-tune large language models (LLMs) efficiently. However, LoRA adapters are shared through open repositories like Hugging Face Hub \\citep{huggingface_hub_docs}, making them vulnerable to backdoor attacks. Current detection methods require running the model with test input data -- making them impractical for screening thousands of adapters where the trigger for backdoor behavior is unknown. We detect poisoned adapters by analyzing their weight matrices directly, without running the model -- making our method data-agnostic. Our method extracts simple statistics -- how concentrated the singular values are, their entropy, and the distribution shape -- and flags adapters that deviate from normal patterns. We evaluate the method on 500 LoRA adapters -- 400 clean, and 100 poisoned for Llama-3.2-3B on instruction and reasoning datasets: Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval, and GLUE dataset. We achieve 97\\% detection accuracy with less than 2\\% false positives.",
      "tr": "**Makale Başlığı:** LoRA Adaptörlerindeki Backdoor'ların Ağırlık Alanı Tespiti\n\n**Özet:**\n\nLoRA adaptörleri, kullanıcılara büyük dil modellerini (LLM'ler) verimli bir şekilde ince ayar yapma imkanı sunmaktadır. Ancak, LoRA adaptörleri Hugging Face Hub \\citep{huggingface_hub_docs} gibi açık repozitörler aracılığıyla paylaşılmakta, bu da onları backdoor saldırılarına karşı savunmasız hale getirmektedir. Mevcut tespit yöntemleri, modeli test girdi verileriyle çalıştırmayı gerektirmekte, bu da tetikleyicisi bilinmeyen backdoor davranışları için binlerce adaptörü tarama konusunda pratik olmamaktadır. Biz, modeli çalıştırmadan, yalnızca ağırlık matrislerini analiz ederek zehirlenmiş adaptörleri tespit etmekteyiz; bu da yöntemimizi data-agnostic yapmaktadır. Yöntemimiz, singular değerlerin ne kadar yoğunlaştığı, entropileri ve dağılım şekli gibi basit istatistikleri çıkararak normal desenlerden sapan adaptörleri işaretlemektedir. Llama-3.2-3B modelinde, Alpaca, Dolly, GSM8K, ARC-Challenge, SQuADv2, NaturalQuestions, HumanEval ve GLUE veri setleri üzerinde talimat ve reasoning veri kümeleri için 400 temiz ve 100 zehirlenmiş olmak üzere 500 LoRA adaptörü üzerinde yöntemi değerlendirdik. Yüzde 2'den az yanlış pozitif oranıyla %97 tespit doğruluğu elde ettik."
    }
  },
  {
    "id": "2602.15161v1",
    "title": "Exploiting Layer-Specific Vulnerabilities to Backdoor Attack in Federated Learning",
    "authors": [
      "Mohammad Hadi Foroughi",
      "Seyed Hamed Rastegar",
      "Mohammad Sabokrou",
      "Ahmad Khonsari"
    ],
    "published_date": "2026-02-16",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.15161v1",
    "pdf_link": "https://arxiv.org/pdf/2602.15161v1",
    "content": {
      "en": "Federated learning (FL) enables distributed model training across edge devices while preserving data locality. This decentralized approach has emerged as a promising solution for collaborative learning on sensitive user data, effectively addressing the longstanding privacy concerns inherent in centralized systems. However, the decentralized nature of FL exposes new security vulnerabilities, especially backdoor attacks that threaten model integrity. To investigate this critical concern, this paper presents the Layer Smoothing Attack (LSA), a novel backdoor attack that exploits layer-specific vulnerabilities in neural networks. First, a Layer Substitution Analysis methodology systematically identifies backdoor-critical (BC) layers that contribute most significantly to backdoor success. Subsequently, LSA strategically manipulates these BC layers to inject persistent backdoors while remaining undetected by state-of-the-art defense mechanisms. Extensive experiments across diverse model architectures and datasets demonstrate that LSA achieves a remarkably backdoor success rate of up to 97% while maintaining high model accuracy on the primary task, consistently bypassing modern FL defenses. These findings uncover fundamental vulnerabilities in current FL security frameworks, demonstrating that future defenses must incorporate layer-aware detection and mitigation strategies.",
      "tr": "Makale Başlığı: Federated Learning'de Katmana Özgü Güvenlik Açıklarını Kötüye Kullanarak Backdoor Saldırısı\n\nÖzet:\nFederated learning (FL), veri yerelliğini koruyarak uç cihazlarda dağıtılmış model eğitimi sağlar. Bu merkezi olmayan yaklaşım, hassas kullanıcı verileri üzerinde işbirlikçi öğrenme için umut verici bir çözüm olarak ortaya çıkmış ve merkezi sistemlerde doğasında bulunan uzun süredir devam eden gizlilik endişelerini etkili bir şekilde ele almaktadır. Ancak, FL'nin merkezi olmayan doğası, özellikle model bütünlüğünü tehdit eden backdoor saldırıları gibi yeni güvenlik açıklarını ortaya çıkarmaktadır. Bu kritik endişeyi araştırmak için bu makale, sinir ağlarındaki katmana özgü güvenlik açıklarını istismar eden yeni bir backdoor saldırısı olan Layer Smoothing Attack'ı (LSA) sunmaktadır. İlk olarak, bir Layer Substitution Analysis metodolojisi, backdoor başarısına en önemli katkıda bulunan backdoor-critical (BC) katmanları sistematik olarak belirler. Ardından LSA, bu BC katmanlarını stratejik olarak manipüle ederek, en gelişmiş savunma mekanizmaları tarafından tespit edilmeden kalırken kalıcı backdoors enjekte eder. Çeşitli model mimarileri ve veri kümeleri üzerinde yapılan kapsamlı deneyler, LSA'nın birincil görevdeki yüksek model doğruluğunu korurken, %97'ye varan dikkate değer bir backdoor başarı oranına ulaştığını ve modern FL savunmalarını sürekli olarak aştığını göstermektedir. Bu bulgular, mevcut FL güvenlik çerçevelerindeki temel güvenlik açıklarını ortaya koymakta ve gelecekteki savunmaların katman farkındalığına sahip tespit ve azaltma stratejilerini içermesi gerektiğini göstermektedir."
    }
  },
  {
    "id": "2602.14689v1",
    "title": "Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks",
    "authors": [
      "Lukas Struppek",
      "Adam Gleave",
      "Kellin Pelrine"
    ],
    "published_date": "2026-02-16",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.14689v1",
    "pdf_link": "https://arxiv.org/pdf/2602.14689v1",
    "content": {
      "en": "As the capabilities of large language models continue to advance, so does their potential for misuse. While closed-source models typically rely on external defenses, open-weight models must primarily depend on internal safeguards to mitigate harmful behavior. Prior red-teaming research has largely focused on input-based jailbreaking and parameter-level manipulations. However, open-weight models also natively support prefilling, which allows an attacker to predefine initial response tokens before generation begins. Despite its potential, this attack vector has received little systematic attention. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models. Our results show that prefill attacks are consistently effective against all major contemporary open-weight models, revealing a critical and previously underexplored vulnerability with significant implications for deployment. While certain large reasoning models exhibit some robustness against generic prefilling, they remain vulnerable to tailored, model-specific strategies. Our findings underscore the urgent need for model developers to prioritize defenses against prefill attacks in open-weight LLMs.",
      "tr": "Makale Başlığı: Açık Ağırlıklı Modellerin Prefill Saldırılarına Karşı Sistematik Kırılganlığının Ortaya Çıkarılması\n\nÖzet:\nBüyük dil modellerinin yetenekleri ilerledikçe, kötüye kullanım potansiyelleri de artmaktadır. Kapalı kaynaklı modeller tipik olarak harici savunmalara dayanırken, açık ağırlıklı modeller zararlı davranışı azaltmak için öncelikli olarak dahili güvenlik önlemlerine güvenmek zorundadır. Önceki red-teaming araştırmaları büyük ölçüde girdi tabanlı jailbreaking ve parametre düzeyindeki manipülasyonlara odaklanmıştır. Ancak, açık ağırlıklı modeller aynı zamanda yerel olarak prefilling'i desteklemektedir, bu da bir saldırganın üretim başlamadan önce ilk yanıt jetonlarını önceden tanımlamasına olanak tanır. Potansiyeline rağmen, bu saldırı vektörü çok az sistematik ilgi görmüştür. Model aileleri ve en son teknoloji açık ağırlıklı modeller genelinde 20'den fazla mevcut ve yeni stratejiyi değerlendiren, prefill saldırılarının şimdiye kadarki en büyük ampirik çalışmasını sunuyoruz. Sonuçlarımız, prefill saldırılarının tüm büyük çağdaş açık ağırlıklı modellere karşı tutarlı bir şekilde etkili olduğunu göstermektedir; bu da dağıtım için önemli çıkarımlara sahip kritik ve daha önce yeterince araştırılmamış bir kırılganlığı ortaya koymaktadır. Belirli büyük reasoning modelleri genel prefilling'e karşı bir miktar dayanıklılık sergilese de, özel, modele özgü stratejilere karşı kırılgan kalmaktadırlar. Bulgularımız, model geliştiricilerinin açık ağırlıklı LLM'lerde prefill saldırılarına karşı savunmalara öncelik vermesi için acil bir ihtiyacın altını çizmektedir."
    }
  }
]