[
  {
    "id": "2602.12250v1",
    "title": "Community Concealment from Unsupervised Graph Learning-Based Clustering",
    "authors": [
      "Dalyapraz Manatova",
      "Pablo Moriano",
      "L. Jean Camp"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.LG",
      "cs.CR",
      "cs.SI"
    ],
    "link": "http://arxiv.org/abs/2602.12250v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12250v1",
    "content": {
      "en": "Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.",
      "tr": "**Makale Başlığı:** Gözetimsiz Grafik Öğrenme Tabanlı Kümelemeden Topluluk Gizliliği\n\n**Özet:**\n\nGraph neural networks (GNNs), temsiller öğrenmek için attributed graphs'ları kullanacak şekilde tasarlanmıştır. Bu tür temsiller, gözetimsiz kümelerin öğrenilmesinde ve topluluk tespitinde faydalıdır. Bununla birlikte, bu tür çıkarımlar hassas grupları, kümelenmiş sistemleri veya kolektif davranışları ortaya çıkarabilir ve grup düzeyinde gizlilik endişelerine yol açabilir. Örneğin, sosyal ve kritik altyapı ağlarındaki topluluk atıfları, profilleme veya istihbarat toplama için kullanılabilecek koordine varlık gruplarını, operasyonel hiyerarşileri ve sistem bağımlılıklarını açığa çıkarabilir. Bu çalışma, bir veri yayıncısının (savunmacı) ilgilenilen bir topluluğu gizlemeyi amaçladığı ve ağda sınırlı, fayda-farkındalığı olan değişiklikler yaptığı savunmacı bir ortamı incelemektedir. Analizimiz, topluluk gizliliğinin iki ölçülebilir faktörden güçlü bir şekilde etkilendiğini göstermektedir: topluluk sınırlarındaki bağlantısallık ve korunan topluluk ile komşu topluluklar arasındaki özellik benzerliği. Bu bulgulardan yola çıkarak, seçilmiş bir dizi kenarı yeniden bağlayan ve GNN message passing'in kullandığı farklılığı azaltmak için node feature'larını değiştiren bir perturbasyon stratejisi sunuyoruz. Önerilen yöntem, sentetik benchmark'lar ve gerçek ağ grafiklerinde aynı perturbasyon bütçeleri altında yapılan deneylerde DICE'dan daha iyi performans göstermektedir. Genel olarak, değerlendirilen ayarlarda ortalama %20-45 civarında medyan göreceli gizlilik iyileştirmeleri elde etmektedir. Bu bulgular, GNN tabanlı topluluk öğrenimine karşı bir azaltma stratejisi göstermekte ve graph learning'in doğasında bulunan grup düzeyinde gizlilik risklerini vurgulamaktadır."
    }
  },
  {
    "id": "2602.12092v1",
    "title": "DeepSight: An All-in-One LM Safety Toolkit",
    "authors": [
      "Bo Zhang",
      "Jiaxuan Guo",
      "Lijun Li",
      "Dongrui Liu",
      "Sujin Chen"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2602.12092v1",
    "pdf_link": "https://arxiv.org/pdf/2602.12092v1",
    "content": {
      "en": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.",
      "tr": "**Makale Başlığı:** DeepSight: Kapsamlı Bir LM Güvenliği Araç Seti\n\n**Özet:**\nBüyük Modellerin (LM) geliştirilmesi hızla ilerlerken, güvenlikleri de bir öncelik haline gelmektedir. Mevcut Büyük Dil Modelleri (LLM) ve Çok Modlu Büyük Dil Modelleri (MLLM) güvenlik iş akışlarında, değerlendirme, teşhis ve hizalama genellikle ayrı araçlarla ele alınmaktadır. Özellikle, güvenlik değerlendirmesi yalnızca harici davranışsal riskleri belirleyebilmekte ancak içsel temel nedenleri ortaya çıkaramamaktadır. Bu sırada, güvenlik teşhisi sıklıkla somut risk senaryolarından uzaklaşmakta ve açıklanabilir düzeyde kalmaktadır. Bu şekilde, güvenlik hizalaması iç mekanizmalardaki değişikliklerin özel açıklamalarından yoksundur, bu da genel yetenekleri potansiyel olarak düşürebilir. Bu sorunları sistematik olarak ele almak için, yeni bir güvenlik değerlendirme-teşhis entegre paradigmasını uygulamak üzere açık kaynaklı bir proje olan DeepSight'ı öneriyoruz. DeepSight, değerlendirme araç seti DeepSafe ve teşhis araç seti DeepScan'dan oluşan düşük maliyetli, tekrarlanabilir, verimli ve son derece ölçeklenebilir büyük ölçekli model güvenlik değerlendirme projesidir. Görev ve veri protokollerini birleştirerek, iki aşama arasında bir bağlantı kuruyor ve güvenlik değerlendirmesini kara kutudan beyaz kutu içgörüsüne dönüştürüyoruz. Ayrıca DeepSight, öncü yapay zeka riski değerlendirmesi ve ortak güvenlik değerlendirme ile teşhisi destekleyen ilk açık kaynaklı araç settir."
    }
  },
  {
    "id": "2602.11897v1",
    "title": "Agentic AI for Cybersecurity: A Meta-Cognitive Architecture for Governable Autonomy",
    "authors": [
      "Andrei Kojukhov",
      "Arkady Bovshover"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11897v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11897v1",
    "content": {
      "en": "Contemporary AI-driven cybersecurity systems are predominantly architected as model-centric detection and automation pipelines optimized for task-level performance metrics such as accuracy and response latency. While effective for bounded classification tasks, these architectures struggle to support accountable decision-making under adversarial uncertainty, where actions must be justified, governed, and aligned with organizational and regulatory constraints. This paper argues that cybersecurity orchestration should be reconceptualized as an agentic, multi-agent cognitive system, rather than a linear sequence of detection and response components. We introduce a conceptual architectural framework in which heterogeneous AI agents responsible for detection, hypothesis formation, contextual interpretation, explanation, and governance are coordinated through an explicit meta-cognitive judgement function. This function governs decision readiness and dynamically calibrates system autonomy when evidence is incomplete, conflicting, or operationally risky. By synthesizing distributed cognition theory, multi-agent systems research, and responsible AI governance frameworks, we demonstrate that modern security operations already function as distributed cognitive systems, albeit without an explicit organizing principle. Our contribution is to make this cognitive structure architecturally explicit and governable by embedding meta-cognitive judgement as a first-class system function. We discuss implications for security operations centers, accountable autonomy, and the design of next-generation AI-enabled cyber defence architectures. The proposed framework shifts the focus of AI in cybersecurity from optimizing isolated predictions to governing autonomy under uncertainty.",
      "tr": "**Makale Başlığı:** Siber Güvenlik İçin Agentic AI: Yönetilebilir Özerklik İçin Meta-Bilişsel Bir Mimari\n\n**Özet:**\n\nGünümüz yapay zeka güdümlü siber güvenlik sistemleri, çoğunlukla doğruluk ve yanıt gecikmesi gibi görev düzeyindeki performans metrikleri için optimize edilmiş, model merkezli tespit ve otomasyon hatları olarak tasarlanmaktadır. Sınırlı sınıflandırma görevlerinde etkili olmalarına rağmen, bu mimariler, eylemlerin gerekçelendirilmesi, yönetilmesi ve organizasyonel ve düzenleyici kısıtlamalarla uyumlu olması gereken düşmanca belirsizlik altında hesap verebilir karar vermeyi desteklemekte zorlanmaktadır. Bu makale, siber güvenlik orkestrasyonunun, doğrusal bir tespit ve yanıt bileşenleri dizisi yerine, bir agentic, çoklu-agent bilişsel sistem olarak yeniden kavramsallaştırılması gerektiğini savunmaktadır. Tespit, hipotez oluşturma, bağlamsal yorumlama, açıklama ve yönetişimden sorumlu heterojen AI agentlarının açık bir meta-cognitive judgement function aracılığıyla koordine edildiği kavramsal bir mimari çerçeve sunuyoruz. Bu fonksiyon, karar hazırlığını yönetir ve kanıtlar eksik, çelişkili veya operasyonel olarak riskli olduğunda sistem özerkliğini dinamik olarak kalibre eder. Dağıtılmış biliş teorisi, çoklu-agent sistemleri araştırması ve sorumlu AI yönetişim çerçevelerini sentezleyerek, modern güvenlik operasyonlarının, açık bir organizasyonel ilke olmasa da, zaten dağıtılmış bilişsel sistemler olarak işlev gördüğünü göstermekteyiz. Katkımız, meta-cognitive judgement'ı birinci sınıf bir sistem fonksiyonu olarak gömerek bu bilişsel yapıyı mimari olarak açık ve yönetilebilir hale getirmektir. Güvenlik operasyon merkezleri, hesap verebilir özerklik ve yeni nesil AI destekli siber savunma mimarilerinin tasarımı için çıkarımları tartışıyoruz. Önerilen çerçeve, siber güvenlikte AI'nin odağını izole edilmiş tahminleri optimize etmekten belirsizlik altında özerkliği yönetmeye kaydırmaktadır."
    }
  },
  {
    "id": "2602.11851v1",
    "title": "Resource-Aware Deployment Optimization for Collaborative Intrusion Detection in Layered Networks",
    "authors": [
      "André García Gómez",
      "Ines Rieger",
      "Wolfgang Hotwagner",
      "Max Landauer",
      "Markus Wurzenberger"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11851v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11851v1",
    "content": {
      "en": "Collaborative Intrusion Detection Systems (CIDS) are increasingly adopted to counter cyberattacks, as their collaborative nature enables them to adapt to diverse scenarios across heterogeneous environments. As distributed critical infrastructure operates in rapidly evolving environments, such as drones in both civil and military domains, there is a growing need for CIDS architectures that can flexibly accommodate these dynamic changes. In this study, we propose a novel CIDS framework designed for easy deployment across diverse distributed environments. The framework dynamically optimizes detector allocation per node based on available resources and data types, enabling rapid adaptation to new operational scenarios with minimal computational overhead. We first conducted a comprehensive literature review to identify key characteristics of existing CIDS architectures. Based on these insights and real-world use cases, we developed our CIDS framework, which we evaluated using several distributed datasets that feature different attack chains and network topologies. Notably, we introduce a public dataset based on a realistic cyberattack targeting a ground drone aimed at sabotaging critical infrastructure. Experimental results demonstrate that the proposed CIDS framework can achieve adaptive, efficient intrusion detection in distributed settings, automatically reconfiguring detectors to maintain an optimal configuration, without requiring heavy computation, since all experiments were conducted on edge devices.",
      "tr": "Makale Başlığı: Katmanlı Ağlarda İşbirlikçi Saldırı Tespitinde Kaynak-Farkındalıklı Konuşlandırma Optimizasyonu\n\nÖzet:\nİşbirlikçi Saldırı Tespit Sistemleri (CIDS), siber saldırılara karşı koymak için giderek daha fazla benimsenmektedir, çünkü işbirlikçi doğaları heterojen ortamlarda çeşitli senaryolara uyum sağlamalarına olanak tanır. Dağıtık kritik altyapı, hem sivil hem de askeri alanlardaki dronlar gibi hızla gelişen ortamlarda faaliyet gösterdiğinden, bu dinamik değişiklikleri esnek bir şekilde karşılayabilen CIDS mimarilerine olan ihtiyaç artmaktadır. Bu çalışmada, çeşitli dağıtık ortamlarda kolay konuşlandırma için tasarlanmış yeni bir CIDS çerçevesi önermekteyiz. Çerçeve, mevcut kaynaklara ve veri türlerine göre düğüm başına dedektör tahsisini dinamik olarak optimize eder ve minimum hesaplama yükü ile yeni operasyonel senaryolara hızlı adaptasyon sağlar. İlk olarak, mevcut CIDS mimarilerinin temel özelliklerini belirlemek için kapsamlı bir literatür taraması gerçekleştirdik. Bu içgörülere ve gerçek dünya kullanım senaryolarına dayanarak, CIDS çerçevemizi geliştirdik ve farklı saldırı zincirleri ve ağ topolojileri içeren birkaç dağıtık veri kümesi kullanarak değerlendirdik. Özellikle, kritik altyapıyı sabote etmeyi amaçlayan ve bir yer dronunu hedef alan gerçekçi bir siber saldırıya dayanan halka açık bir veri kümesi sunuyoruz. Deneysel sonuçlar, önerilen CIDS çerçevesinin dağıtık ortamlarda uyarlanabilir, verimli saldırı tespiti gerçekleştirebildiğini, dedektörleri otomatik olarak yeniden yapılandırarak optimal bir konfigürasyon sürdürdüğünü ve tüm deneyler edge devices üzerinde gerçekleştirildiği için ağır hesaplama gerektirmediğini göstermektedir."
    }
  },
  {
    "id": "2602.11655v1",
    "title": "LoRA-based Parameter-Efficient LLMs for Continuous Learning in Edge-based Malware Detection",
    "authors": [
      "Christian Rondanini",
      "Barbara Carminati",
      "Elena Ferrari",
      "Niccolò Lardo",
      "Ashish Kundu"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "link": "http://arxiv.org/abs/2602.11655v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11655v1",
    "content": {
      "en": "The proliferation of edge devices has created an urgent need for security solutions capable of detecting malware in real time while operating under strict computational and memory constraints. Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities in recognizing complex patterns, yet their deployment on edge devices remains impractical due to their resource demands. However, in edge malware detection, static or centrally retrained models degrade under evolving threats and heterogeneous traffic; locally trained models become siloed and fail to transfer across domains. To overcome these limitations, in this paper, we present a continuous learning architecture for edge-based malware detection that combines local adaptation on each device with global knowledge sharing through parameter-efficient LoRA adapters. Lightweight transformer models (DistilBERT, DistilGPT-2, TinyT5) run on edge nodes and are incrementally fine-tuned on device-specific traffic; only the resulting LoRA modules are aggregated by a lightweight coordinator and redistributed, enabling cross-device generalization without exchanging raw data. We evaluate on two public IoT security datasets, Edge-IIoTset and TON-IoT, under multi-round learning to simulate evolving threats. Compared to isolated fine-tuning, the LoRA-based exchange yields up to 20-25% accuracy gains when models encounter previously unseen attacks from another domain, while maintaining stable loss and F1 across rounds. LoRA adds less than 1% to model size (~0.6-1.8 MB), making updates practical for constrained edge hardware.",
      "tr": "İşte makalenin başlığı ve özetinin istenen kriterlere uygun olarak yapılmış çevirisi:\n\n**Makale Başlığı:** Edge Tabanlı Zararlı Yazılım Tespitinde Sürekli Öğrenme için LoRA Tabanlı Parametre Verimli LLM'ler\n\n**Özet:**\nEdge cihazlarının yaygınlaşması, hesaplama ve bellek kısıtlamaları altında gerçek zamanlı zararlı yazılım tespit edebilen güvenlik çözümlerine yönelik acil bir ihtiyaç doğurmuştur. Son zamanlarda, Large Language Models (LLM'ler), karmaşık desenleri tanıma konusunda dikkat çekici yetenekler sergilemiştir, ancak kaynak gereksinimleri nedeniyle edge cihazlarına konuşlandırılmaları pratik olmaktan uzaktır. Bununla birlikte, edge zararlı yazılım tespitinde, statik veya merkezi olarak yeniden eğitilen modeller gelişen tehditler ve heterojen trafik altında bozulur; yerel olarak eğitilen modeller izole kalır ve alanlar arası transferde başarısız olur. Bu sınırlamaların üstesinden gelmek için, bu makalede, her cihazda yerel adaptasyonu parametre verimli LoRA adapter'ları aracılığıyla küresel bilgi paylaşımıyla birleştiren edge tabanlı zararlı yazılım tespiti için bir sürekli öğrenme mimarisi sunuyoruz. Lightweight transformer modelleri (DistilBERT, DistilGPT-2, TinyT5) edge düğümlerinde çalışır ve cihazlara özgü trafik üzerinde aşamalı olarak fine-tune edilir; yalnızca ortaya çıkan LoRA modülleri hafif bir koordinatör tarafından toplanır ve yeniden dağıtılır, ham veri alışverişi olmadan cihazlar arası genelleştirme sağlar. Gelişen tehditleri simüle etmek için çok turlu öğrenme altında iki genel IoT güvenlik veri seti, Edge-IIoTset ve TON-IoT üzerinde değerlendirme yaptık. İzole fine-tuning ile karşılaştırıldığında, LoRA tabanlı değişim, modellerin başka bir alandan daha önce görülmemiş saldırılarla karşılaştığında, turlar boyunca istikrarlı loss ve F1'i korurken, %20-25'e kadar doğruluk artışı sağlamaktadır. LoRA, model boyutuna %1'den az eklenir (~0.6-1.8 MB), bu da güncellemeleri kısıtlı edge donanımları için pratik hale getirir."
    }
  },
  {
    "id": "2602.11651v1",
    "title": "DMind-3: A Sovereign Edge--Local--Cloud AI System with Controlled Deliberation and Correction-Based Tuning for Safe, Low-Latency Transaction Execution",
    "authors": [
      "Enhao Huang",
      "Frank Li",
      "Tony Lin",
      "Lowes Yang"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11651v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11651v1",
    "content": {
      "en": "This paper introduces DMind-3, a sovereign Edge-Local-Cloud intelligence stack designed to secure irreversible financial execution in Web3 environments against adversarial risks and strict latency constraints. While existing cloud-centric assistants compromise privacy and fail under network congestion, and purely local solutions lack global ecosystem context, DMind-3 resolves these tensions by decomposing capability into three cooperating layers: a deterministic signing-time intent firewall at the edge, a private high-fidelity reasoning engine on user hardware, and a policy-governed global context synthesizer in the cloud. We propose policy-driven selective offloading to route computation based on privacy sensitivity and uncertainty, supported by two novel training objectives: Hierarchical Predictive Synthesis (HPS) for fusing time-varying macro signals, and Contrastive Chain-of-Correction Supervised Fine-Tuning (C$^3$-SFT) to enhance local verification reliability. Extensive evaluations demonstrate that DMind-3 achieves a 93.7% multi-turn success rate in protocol-constrained tasks and superior domain reasoning compared to general-purpose baselines, providing a scalable framework where safety is bound to the edge execution primitive while maintaining sovereignty over sensitive user intent.",
      "tr": "**Makale Başlığı:** DMind-3: Güvenli, Düşük Gecikmeli İşlem Yürütme için Kontrollü Müzakere ve Düzeltme-Temelli Ayarlama ile Egemen Uç-Yerel-Bulut Yapay Zeka Sistemi\n\n**Özet:**\n\nBu makale, DMind-3'ü tanıtmaktadır; bu, Web3 ortamlarında geri döndürülemez finansal işlemlerin düşmanca risklere ve katı gecikme kısıtlamalarına karşı güvenliğini sağlamak üzere tasarlanmış egemen bir Uç-Yerel-Bulut zeka yığınıdır. Mevcut bulut merkezli asistanlar gizliliği tehlikeye atarken ve ağ tıkanıklığında başarısız olurken, tamamen yerel çözümler küresel ekosistem bağlamından yoksundur. DMind-3, yeteneği üç işbirliği yapan katmana ayırarak bu gerilimleri çözmektedir: uçta deterministik bir imza-zamanı niyet güvenlik duvarı, kullanıcı donanımında özel yüksek-doğruluklu reasoning engine ve bulutta politika güdümlü küresel bağlam sentezleyicisi. Gizlilik hassasiyeti ve belirsizliğe dayalı olarak hesaplamayı yönlendirmek için politika güdümlü seçici offloading öneriyoruz; bu, iki yeni eğitim hedefiyle desteklenmektedir: zaman-değişken makro sinyalleri birleştirmek için Hierarchical Predictive Synthesis (HPS) ve yerel doğrulama güvenilirliğini artırmak için Contrastive Chain-of-Correction Supervised Fine-Tuning (C$^3$-SFT). Kapsamlı değerlendirmeler, DMind-3'ün protokol-kısıtlı görevlerde %93,7'lik çok turlu başarı oranı elde ettiğini ve genel amaçlı taban hatlarına kıyasla üstün alan reasoning sağladığını göstermektedir; böylece güvenliğin uç yürütme primitive'ine bağlı olduğu, hassas kullanıcı niyeti üzerindeki egemenliği korurken ölçeklenebilir bir çerçeve sunmaktadır."
    }
  },
  {
    "id": "2602.11528v1",
    "title": "Stop Tracking Me! Proactive Defense Against Attribute Inference Attack in LLMs",
    "authors": [
      "Dong Yan",
      "Jian Liang",
      "Ran He",
      "Tieniu Tan"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2602.11528v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11528v1",
    "content": {
      "en": "Recent studies have shown that large language models (LLMs) can infer private user attributes (e.g., age, location, gender) from user-generated text shared online, enabling rapid and large-scale privacy breaches. Existing anonymization-based defenses are coarse-grained, lacking word-level precision in anonymizing privacy-leaking elements. Moreover, they are inherently limited as altering user text to hide sensitive cues still allows attribute inference to occur through models' reasoning capabilities. To address these limitations, we propose a unified defense framework that combines fine-grained anonymization (TRACE) with inference-preventing optimization (RPS). TRACE leverages attention mechanisms and inference chain generation to identify and anonymize privacy-leaking textual elements, while RPS employs a lightweight two-stage optimization strategy to induce model rejection behaviors, thereby preventing attribute inference. Evaluations across diverse LLMs show that TRACE-RPS reduces attribute inference accuracy from around 50\\% to below 5\\% on open-source models. In addition, our approach offers strong cross-model generalization, prompt-variation robustness, and utility-privacy tradeoffs. Our code is available at https://github.com/Jasper-Yan/TRACE-RPS.",
      "tr": "**Makale Başlığı:** Beni Takip Etmeyi Bırakın! LLM'lerde Öznitelik Çıkarım Saldırılarına Karşı Proaktif Savunma\n\n**Özet:**\nSon çalışmalar, büyük dil modellerinin (LLM'ler), çevrimiçi paylaşılan kullanıcı tarafından üretilen metinlerden özel kullanıcı özniteliklerini (örneğin, yaş, konum, cinsiyet) çıkarabildiğini ve bu durumun hızla yayılan büyük ölçekli gizlilik ihlallerine yol açtığını göstermiştir. Mevcut anonimleştirme tabanlı savunma yöntemleri kaba taneli olup, gizlilik sızdıran unsurları anonimleştirirken kelime düzeyinde bir hassasiyetten yoksundur. Dahası, hassas ipuçlarını gizlemek için kullanıcı metnini değiştirmek, modellerin _reasoning_ yetenekleri aracılığıyla öznitelik çıkarımının gerçekleşmesine hala izin verdiği için bu yöntemler doğası gereği sınırlıdır. Bu sınırlamaların üstesinden gelmek için, ince taneli anonimleştirmeyi (TRACE) çıkarım önleyici optimizasyon (RPS) ile birleştiren birleşik bir savunma çerçevesi öneriyoruz. TRACE, gizlilik sızdıran metinsel unsurları tanımlamak ve anonimleştirmek için dikkat mekanizmalarından ve _inference chain generation_dan yararlanırken, RPS öznitelik çıkarımını önlemek amacıyla model reddetme davranışlarını teşvik etmek için hafif bir iki aşamalı optimizasyon stratejisi kullanır. Çeşitli LLM'ler üzerinde yapılan değerlendirmeler, TRACE-RPS'nin açık kaynaklı modellerde öznitelik çıkarım doğruluğunu yaklaşık %50'den %5'in altına düşürdüğünü göstermektedir. Ek olarak, yaklaşımımız güçlü model genellemesi, _prompt-variation robustness_ ve fayda-gizlilik dengeleri sunmaktadır. Kodumuz https://github.com/Jasper-Yan/TRACE-RPS adresinde mevcuttur."
    }
  },
  {
    "id": "2602.11513v1",
    "title": "Differentially Private and Communication Efficient Large Language Model Split Inference via Stochastic Quantization and Soft Prompt",
    "authors": [
      "Yujie Gu",
      "Richeng Jin",
      "Xiaoyu Ji",
      "Yier Jin",
      "Wenyuan Xu"
    ],
    "published_date": "2026-02-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11513v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11513v1",
    "content": {
      "en": "Large Language Models (LLMs) have achieved remarkable performance and received significant research interest. The enormous computational demands, however, hinder the local deployment on devices with limited resources. The current prevalent LLM inference paradigms require users to send queries to the service providers for processing, which raises critical privacy concerns. Existing approaches propose to allow the users to obfuscate the token embeddings before transmission and utilize local models for denoising. Nonetheless, transmitting the token embeddings and deploying local models may result in excessive communication and computation overhead, preventing practical implementation. In this work, we propose \\textbf{DEL}, a framework for \\textbf{D}ifferentially private and communication \\textbf{E}fficient \\textbf{L}LM split inference. More specifically, an embedding projection module and a differentially private stochastic quantization mechanism are proposed to reduce the communication overhead in a privacy-preserving manner. To eliminate the need for local models, we adapt soft prompt at the server side to compensate for the utility degradation caused by privacy. To the best of our knowledge, this is the first work that utilizes soft prompt to improve the trade-off between privacy and utility in LLM inference, and extensive experiments on text generation and natural language understanding benchmarks demonstrate the effectiveness of the proposed method.",
      "tr": "**Makale Başlığı:** Differentially Private ve Communication Efficient Large Language Model Split Inference, Stochastic Quantization ve Soft Prompt Üzerinden\n\n**Özet:**\n\nLarge Language Models (LLMs), dikkate değer performanslar sergilemiş ve önemli araştırma ilgisi görmüştür. Ancak, devasa hesaplama gereksinimleri, sınırlı kaynaklara sahip cihazlarda yerel dağıtımı engellemektedir. Mevcut yaygın LLM inference paradigmaları, kullanıcıların işleme için sorguları servis sağlayıcılara göndermesini gerektirmekte, bu da ciddi privacy endişelerini doğurmaktadır. Mevcut yaklaşımlar, kullanıcıların token embeddinglerini iletim öncesinde karartmasına ve denoise için yerel modelleri kullanmasına olanak tanımayı önermektedir. Buna rağmen, token embeddinglerinin iletimi ve yerel modellerin konuşlandırılması, aşırı communication ve computation overhead'e yol açarak pratik uygulamayı engelleyebilir. Bu çalışmada, \\textbf{D}ifferentially private ve communication \\textbf{E}fficient \\textbf{L}LM split inference için bir framework olan \\textbf{DEL}'i öneriyoruz. Daha spesifik olarak, communication overhead'i privacy-preserving bir şekilde azaltmak için bir embedding projection module ve differentially private stochastic quantization mechanism önerilmektedir. Yerel modellere duyulan ihtiyacı ortadan kaldırmak için, privacy'nin neden olduğu utility degradation'ı telafi etmek üzere server tarafında soft prompt'u adapte ediyoruz. Bildiğimiz kadarıyla, bu çalışma LLM inference'ta privacy ve utility arasındaki trade-off'u iyileştirmek için soft prompt'u kullanan ilk çalışmadır ve text generation ile natural language understanding benchmark'ları üzerindeki kapsamlı deneyler, önerilen yöntemin etkinliğini göstermektedir."
    }
  },
  {
    "id": "2602.11416v1",
    "title": "Optimizing Agent Planning for Security and Autonomy",
    "authors": [
      "Aashish Kolluri",
      "Rishi Sharma",
      "Manuel Costa",
      "Boris Köpf",
      "Tobias Nießen"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.11416v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11416v1",
    "content": {
      "en": "Indirect prompt injection attacks threaten AI agents that execute consequential actions, motivating deterministic system-level defenses. Such defenses can provably block unsafe actions by enforcing confidentiality and integrity policies, but currently appear costly: they reduce task completion rates and increase token usage compared to probabilistic defenses. We argue that existing evaluations miss a key benefit of system-level defenses: reduced reliance on human oversight. We introduce autonomy metrics to quantify this benefit: the fraction of consequential actions an agent can execute without human-in-the-loop (HITL) approval while preserving security. To increase autonomy, we design a security-aware agent that (i) introduces richer HITL interactions, and (ii) explicitly plans for both task progress and policy compliance. We implement this agent design atop an existing information-flow control defense against prompt injection and evaluate it on the AgentDojo and WASP benchmarks. Experiments show that this approach yields higher autonomy without sacrificing utility.",
      "tr": "**Makale Başlığı:** Güvenlik ve Özerklik İçin Ajan Planlamasının Optimize Edilmesi\n\n**Özet:**\n\nDolaylı komut enjeksiyonu saldırıları, sonuçları olan eylemleri yürüten yapay zeka ajanlarını tehdit ederek deterministik sistem düzeyinde savunmaları gerektirmektedir. Bu tür savunmalar, gizlilik ve bütünlük politikalarını zorunlu kılarak güvenli olmayan eylemleri kanıtlanabilir şekilde engelleyebilir, ancak mevcut durumda maliyetli görünmektedir: olasılıksal savunmalara kıyasla görev tamamlama oranlarını düşürmekte ve token kullanımını artırmaktadır. Sistem düzeyindeki savunmaların önemli bir faydasını mevcut değerlendirmelerin gözden kaçırdığını savunuyoruz: insan denetimine olan bağımlılığın azalması. Bu faydayı ölçmek için özerklik metrikleri sunuyoruz: bir ajanın güvenliği sağlarken insan-döngü-içinde (HITL) onayı olmadan yürütebileceği sonuçları olan eylemlerin oranı. Özerkliği artırmak için, (i) daha zengin HITL etkileşimleri sunan ve (ii) hem görev ilerlemesini hem de politika uyumluluğunu açıkça planlayan güvenlik-farkındalığına sahip bir ajan tasarlıyoruz. Bu ajan tasarımını, komut enjeksiyonuna karşı mevcut bir bilgi akışı kontrol savunmasının üzerine uyguluyoruz ve AgentDojo ve WASP kıyaslamalarında değerlendiriyoruz. Deneyler, bu yaklaşımın faydadan ödün vermeden daha yüksek özerklik sağladığını göstermektedir."
    }
  },
  {
    "id": "2602.11327v1",
    "title": "Security Threat Modeling for Emerging AI-Agent Protocols: A Comparative Analysis of MCP, A2A, Agora, and ANP",
    "authors": [
      "Zeynab Anbiaee",
      "Mahdi Rabbani",
      "Mansur Mirani",
      "Gunjan Piya",
      "Igor Opushnyev"
    ],
    "published_date": "2026-02-11",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.11327v1",
    "pdf_link": "https://arxiv.org/pdf/2602.11327v1",
    "content": {
      "en": "The rapid development of the AI agent communication protocols, including the Model Context Protocol (MCP), Agent2Agent (A2A), Agora, and Agent Network Protocol (ANP), is reshaping how AI agents communicate with tools, services, and each other. While these protocols support scalable multi-agent interaction and cross-organizational interoperability, their security principles remain understudied, and standardized threat modeling is limited; no protocol-centric risk assessment framework has been established yet. This paper presents a systematic security analysis of four emerging AI agent communication protocols. First, we develop a structured threat modeling analysis that examines protocol architectures, trust assumptions, interaction patterns, and lifecycle behaviors to identify protocol-specific and cross-protocol risk surfaces. Second, we introduce a qualitative risk assessment framework that identifies twelve protocol-level risks and evaluates security posture across the creation, operation, and update phases through systematic assessment of likelihood, impact, and overall protocol risk, with implications for secure deployment and future standardization. Third, we provide a measurement-driven case study on MCP that formalizes the risk of missing mandatory validation/attestation for executable components as a falsifiable security claim by quantifying wrong-provider tool execution under multi-server composition across representative resolver policies. Collectively, our results highlight key design-induced risk surfaces and provide actionable guidance for secure deployment and future standardization of agent communication ecosystems.",
      "tr": "Elbette, istenen çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** Yeni Nesil Yapay Zeka Ajanı Protokolleri İçin Siber Güvenlik Tehdit Modellemesi: MCP, A2A, Agora ve ANP'nin Karşılaştırmalı Analizi\n\n**Özet:**\n\nModel Context Protocol (MCP), Agent2Agent (A2A), Agora ve Agent Network Protocol (ANP) gibi yapay zeka ajanları iletişim protokollerinin hızlı gelişimi, yapay zeka ajanlarının araçlar, servisler ve birbirleriyle nasıl iletişim kurduğunu yeniden şekillendirmektedir. Bu protokoller ölçeklenebilir çoklu ajan etkileşimini ve organizasyonlar arası birlikte çalışabilirliği desteklese de, güvenlik prensipleri yeterince incelenmemiş olup, standartlaştırılmış tehdit modellemesi sınırlıdır; henüz protokol odaklı bir risk değerlendirme çerçevesi oluşturulmamıştır. Bu makale, dört yeni nesil yapay zeka ajan iletişim protokolünün sistematik bir siber güvenlik analizini sunmaktadır. İlk olarak, protokol mimarilerini, güven varsayımlarını, etkileşim modellerini ve yaşam döngüsü davranışlarını inceleyen yapılandırılmış bir tehdit modelleme analizi geliştirerek protokol özgü ve protokoller arası risk yüzeylerini tanımlıyoruz. İkinci olarak, on iki protokol düzeyinde risk tanımlayan ve yaratma, işletme ve güncelleme aşamaları boyunca güvenlik duruşunu olasılık, etki ve genel protokol riski üzerinden sistematik bir değerlendirme yoluyla analiz eden nitel bir risk değerlendirme çerçevesi sunuyoruz; bu, güvenli dağıtım ve gelecekteki standartlaştırma için çıkarımlar içermektedir. Üçüncü olarak, yürütülebilir bileşenler için zorunlu doğrulama/onay eksikliğinin riskini, temsilci çözümleyici politikaları boyunca çok sunuculu bileşim altında yanlış sağlayıcı araç yürütmesini niceliklendirerek yanlışlanabilir bir güvenlik iddiası olarak biçimlendiren, ölçüm odaklı bir MCP örnek olay çalışması sunuyoruz. Topluca, sonuçlarımız temel tasarım kaynaklı risk yüzeylerini vurgulamakta ve ajan iletişim ekosistemlerinin güvenli dağıtımı ve gelecekteki standartlaştırması için eyleme geçirilebilir rehberlik sağlamaktadır."
    }
  }
]