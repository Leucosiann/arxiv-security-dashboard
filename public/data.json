[
  {
    "id": "2601.11447v1",
    "title": "IMS: Intelligent Hardware Monitoring System for Secure SoCs",
    "authors": [
      "Wadid Foudhaili",
      "Aykut Rencber",
      "Anouar Nechi",
      "Rainer Buchty",
      "Mladen Berekovic"
    ],
    "published_date": "2026-01-16",
    "tags": [
      "cs.CR",
      "cs.AR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.11447v1",
    "pdf_link": "https://arxiv.org/pdf/2601.11447v1",
    "content": {
      "en": "In the modern Systems-on-Chip (SoC), the Advanced eXtensible Interface (AXI) protocol exhibits security vulnerabilities, enabling partial or complete denial-of-service (DoS) through protocol-violation attacks. The recent countermeasures lack a dedicated real-time protocol semantic analysis and evade protocol compliance checks. This paper tackles this AXI vulnerability issue and presents an intelligent hardware monitoring system (IMS) for real-time detection of AXI protocol violations. IMS is a hardware module leveraging neural networks to achieve high detection accuracy. For model training, we perform DoS attacks through header-field manipulation and systematic malicious operations, while recording AXI transactions to build a training dataset. We then deploy a quantization-optimized neural network, achieving 98.7% detection accuracy with <=3% latency overhead, and throughput of >2.5 million inferences/s. We subsequently integrate this IMS into a RISC-V SoC as a memory-mapped IP core to monitor its AXI bus. For demonstration and initial assessment for later ASIC integration, we implemented this IMS on an AMD Zynq UltraScale+ MPSoC ZCU104 board, showing an overall small hardware footprint (9.04% look-up-tables (LUTs), 0.23% DSP slices, and 0.70% flip-flops) and negligible impact on the overall design's achievable frequency. This demonstrates the feasibility of lightweight, security monitoring for resource-constrained edge environments.",
      "tr": "Makale Başlığı: IMS: Güvenli SoC'ler için Akıllı Donanım İzleme Sistemi\n\nÖzet:\nModern Sistem-on-Chip (SoC) tasarımlarında, Advanced eXtensible Interface (AXI) protokolü, protokol ihlali saldırıları yoluyla kısmi veya tam hizmet reddi (DoS) sağlamasına olanak tanıyan güvenlik açıkları sergilemektedir. Son alınan önlemler, özel bir gerçek zamanlı protokol anlambilimsel analizi eksikliğinden muzdariip olmakta ve protokol uyumluluk denetimlerinden kaçınmaktadır. Bu çalışma, bu AXI güvenlik açığı sorununu ele almakta ve AXI protokol ihlallerinin gerçek zamanlı tespiti için akıllı bir donanım izleme sistemi (IMS) sunmaktadır. IMS, yüksek tespit doğruluğu elde etmek için neural networks'ten yararlanan bir donanım modülüdür. Model eğitimi için, eğitim veri kümesi oluşturmak üzere AXI işlemlerini kaydederken, header-field manipulation ve systematic malicious operations aracılığıyla DoS saldırıları gerçekleştiriyoruz. Ardından, <=3% latency overhead ile %98.7 tespit doğruluğu ve >2.5 milyon inferences/s verimlilik sağlayan, quantization-optimized bir neural network konuşlandırıyoruz. Müteakiben, bu IMS'yi AXI veriyolunu izlemek üzere bir memory-mapped IP core olarak bir RISC-V SoC'ye entegre ediyoruz. Gösterim ve daha sonraki ASIC entegrasyonu için ilk değerlendirme amacıyla, bu IMS'yi bir AMD Zynq UltraScale+ MPSoC ZCU104 kartı üzerinde uyguladık. Bu uygulama, genel olarak küçük bir donanım ayak izi (%9.04 look-up-tables (LUTs), %0.23 DSP slices ve %0.70 flip-flops) ve genel tasarımın ulaşılabilir frekansı üzerinde ihmal edilebilir bir etki göstermiştir. Bu, kaynak kısıtlı edge ortamları için hafif, güvenlik izlemenin fizibilitesini göstermektedir."
    }
  },
  {
    "id": "2601.11207v1",
    "title": "LoRA as Oracle",
    "authors": [
      "Marco Arazzi",
      "Antonino Nocera"
    ],
    "published_date": "2026-01-16",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.11207v1",
    "pdf_link": "https://arxiv.org/pdf/2601.11207v1",
    "content": {
      "en": "Backdoored and privacy-leaking deep neural networks pose a serious threat to the deployment of machine learning systems in security-critical settings. Existing defenses for backdoor detection and membership inference typically require access to clean reference models, extensive retraining, or strong assumptions about the attack mechanism. In this work, we introduce a novel LoRA-based oracle framework that leverages low-rank adaptation modules as a lightweight, model-agnostic probe for both backdoor detection and membership inference.   Our approach attaches task-specific LoRA adapters to a frozen backbone and analyzes their optimization dynamics and representation shifts when exposed to suspicious samples. We show that poisoned and member samples induce distinctive low-rank updates that differ significantly from those generated by clean or non-member data. These signals can be measured using simple ranking and energy-based statistics, enabling reliable inference without access to the original training data or modification of the deployed model.",
      "tr": "Makale Başlığı: LoRA as Oracle\n\nÖzet:\n\nSiber güvenlik açısından kritik ortamlarda makine öğrenmesi sistemlerinin dağıtımını olumsuz etkileyen gizli saldırılar ve gizlilik sızıntılarına neden olan derin sinir ağları ciddi bir tehdit oluşturmaktadır. Geri kapı tespiti ve üyelik çıkarımı için mevcut savunma mekanizmaları genellikle temiz referans modellerine erişim, kapsamlı yeniden eğitim veya saldırı mekanizması hakkında güçlü varsayımlar gerektirir. Bu çalışmada, geri kapı tespiti ve üyelik çıkarımı için hafif, modelden bağımsız bir prob olarak low-rank adaptation (LoRA) modüllerini kullanan yenilikçi bir LoRA tabanlı oracle çerçevesi sunmaktayız. Yaklaşımımız, dondurulmuş bir omurgaya göreve özgü LoRA adapterları ekler ve şüpheli örneklere maruz kaldıklarında optimizasyon dinamiklerini ve temsil kaymalarını analiz eder. Zehirlenmiş ve üye örneklerin, temiz veya üye olmayan veriler tarafından üretilenlerden önemli ölçüde farklı, belirgin low-rank güncellemeleri tetiklediğini göstermekteyiz. Bu sinyaller, basit sıralama ve enerji tabanlı istatistikler kullanılarak ölçülebilir, böylece orijinal eğitim verilerine erişim olmaksızın veya dağıtılan modelde değişiklik yapmadan güvenilir çıkarım yapılmasını sağlar."
    }
  },
  {
    "id": "2601.11199v1",
    "title": "SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation",
    "authors": [
      "Aiman Al Masoud",
      "Marco Arazzi",
      "Antonino Nocera"
    ],
    "published_date": "2026-01-16",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.11199v1",
    "pdf_link": "https://arxiv.org/pdf/2601.11199v1",
    "content": {
      "en": "Retrieval-Augmented Generation (RAG) has attracted significant attention due to its ability to combine the generative capabilities of Large Language Models (LLMs) with knowledge obtained through efficient retrieval mechanisms over large-scale data collections. Currently, the majority of existing approaches overlook the risks associated with exposing sensitive or access-controlled information directly to the generation model. Only a few approaches propose techniques to instruct the generative model to refrain from disclosing sensitive information; however, recent studies have also demonstrated that LLMs remain vulnerable to prompt injection attacks that can override intended behavioral constraints. For these reasons, we propose a novel approach to Selective Disclosure in Retrieval-Augmented Generation, called SD-RAG, which decouples the enforcement of security and privacy constraints from the generation process itself. Rather than relying on prompt-level safeguards, SD-RAG applies sanitization and disclosure controls during the retrieval phase, prior to augmenting the language model's input. Moreover, we introduce a semantic mechanism to allow the ingestion of human-readable dynamic security and privacy constraints together with an optimized graph-based data model that supports fine-grained, policy-aware retrieval. Our experimental evaluation demonstrates the superiority of SD-RAG over baseline existing approaches, achieving up to a $58\\%$ improvement in the privacy score, while also showing a strong resilience to prompt injection attacks targeting the generative model.",
      "tr": "**Makale Başlığı:** SD-RAG: Retrieval-Augmented Generation'da Seçici Açıklama İçin Prompt-Injection'a Dayanıklı Bir Çerçeve\n\n**Özet:**\n\nRetrieval-Augmented Generation (RAG), büyük ölçekli veri koleksiyonları üzerinde verimli retrieval mekanizmaları yoluyla elde edilen bilgiyi Büyük Dil Modellerinin (LLMs) üretme yetenekleriyle birleştirme kabiliyeti sayesinde büyük ilgi görmüştür. Mevcut yaklaşımların çoğunluğu, hassas veya erişim kontrollü bilgilerin doğrudan üretim modeline maruz bırakılmasıyla ilişkili riskleri göz ardı etmektedir. Yalnızca birkaç yaklaşım, üretken modeli hassas bilgileri açıklamaktan kaçınması için yönlendirmeye yönelik teknikler önermektedir; ancak son çalışmalar, LLMs'nin amaçlanan davranışsal kısıtlamaları geçersiz kılabilecek prompt injection saldırılarına karşı hala savunmasız olduğunu göstermiştir. Bu nedenlerle, Retrieval-Augmented Generation'da Seçici Açıklama için SD-RAG adını verdiğimiz yeni bir yaklaşım öneriyoruz. Bu yaklaşım, güvenlik ve gizlilik kısıtlamalarının uygulanmasını üretim sürecinden ayırır. Prompt düzeyindeki korumalara güvenmek yerine, SD-RAG dil modelinin girdisini artırmadan önce retrieval aşamasında sanitizasyon ve açıklama kontrolleri uygular. Ayrıca, insan tarafından okunabilir dinamik güvenlik ve gizlilik kısıtlamalarının, ince taneli, politika-farkındalığı olan retrieval'ı destekleyen optimize edilmiş bir graph-based data model ile birlikte alınmasına olanak tanıyan semantik bir mekanizma sunuyoruz. Deneysel değerlendirmemiz, SD-RAG'ın temel mevcut yaklaşımlara göre üstünlüğünü göstermektedir. Gizlilik puanında %58'e varan bir iyileşme sağlarken, üretken modeli hedef alan prompt injection saldırılarına karşı da güçlü bir dayanıklılık sergilemektedir."
    }
  },
  {
    "id": "2601.11113v1",
    "title": "Differentially Private Subspace Fine-Tuning for Large Language Models",
    "authors": [
      "Lele Zheng",
      "Xiang Wang",
      "Tao Zhang",
      "Yang Cao",
      "Ke Cheng"
    ],
    "published_date": "2026-01-16",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.11113v1",
    "pdf_link": "https://arxiv.org/pdf/2601.11113v1",
    "content": {
      "en": "Fine-tuning large language models on downstream tasks is crucial for realizing their cross-domain potential but often relies on sensitive data, raising privacy concerns. Differential privacy (DP) offers rigorous privacy guarantees and has been widely adopted in fine-tuning; however, naively injecting noise across the high-dimensional parameter space creates perturbations with large norms, degrading performance and destabilizing training. To address this issue, we propose DP-SFT, a two-stage subspace fine-tuning method that substantially reduces noise magnitude while preserving formal DP guarantees. Our intuition is that, during fine-tuning, significant parameter updates lie within a low-dimensional, task-specific subspace, while other directions change minimally. Hence, we only inject DP noise into this subspace to protect privacy without perturbing irrelevant parameters. In phase one, we identify the subspace by analyzing principal gradient directions to capture task-specific update signals. In phase two, we project full gradients onto this subspace, add DP noise, and map the perturbed gradients back to the original parameter space for model updates, markedly lowering noise impact. Experiments on multiple datasets demonstrate that DP-SFT enhances accuracy and stability under rigorous DP constraints, accelerates convergence, and achieves substantial gains over DP fine-tuning baselines.",
      "tr": "**Makale Başlığı:** Büyük Dil Modelleri İçin Farklı Ayrışmalı Altuzay İnce Ayarı (Differentially Private Subspace Fine-Tuning for Large Language Models)\n\n**Özet:**\n\nBüyük dil modellerini aşağı akış görevlerinde ince ayardan geçirmek, alanlar arası potansiyellerini gerçekleştirmek için kritik öneme sahiptir ancak genellikle hassas verilere dayanır, bu da gizlilik endişelerini artırır. Differential privacy (DP), katı gizlilik garantileri sunar ve ince ayarda yaygın olarak benimsenmiştir; ancak, yüksek boyutlu parametre uzayında gürültünün naif bir şekilde enjekte edilmesi, büyük normlara sahip pertürbasyonlar oluşturarak performansı düşürmekte ve eğitimi istikrarsızlaştırmaktadır. Bu sorunu çözmek için, formal DP garantilerini korurken gürültü büyüklüğünü önemli ölçüde azaltan iki aşamalı bir subspace fine-tuning yöntemi olan DP-SFT'yi öneriyoruz. Sezgisel yaklaşımımız, ince ayar sırasında önemli parametre güncellemelerinin düşük boyutlu, göreve özgü bir subspace içinde yer aldığı, diğer yönlerin ise minimal düzeyde değiştiğidir. Dolayısıyla, ilgili olmayan parametreleri pertürbe etmeden gizliliği korumak için DP gürültüsünü yalnızca bu subspace'e enjekte ederiz. Birinci aşamada, göreve özgü güncelleme sinyallerini yakalamak için principal gradient yönlerini analiz ederek subspace'i tanımlarız. İkinci aşamada, tam gradyanları bu subspace'e yansıtır, DP gürültüsü ekler ve model güncellemeleri için pertürbe edilmiş gradyanları orijinal parametre uzayına geri haritalayarak gürültü etkisini belirgin şekilde düşürürüz. Birden fazla veri kümesi üzerinde yapılan deneyler, DP-SFT'nin katı DP kısıtlamaları altında doğruluğu ve kararlılığı artırdığını, yakınsamayı hızlandırdığını ve DP fine-tuning baselines'lerine göre önemli kazanımlar elde ettiğini göstermektedir."
    }
  },
  {
    "id": "2601.10955v1",
    "title": "Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents",
    "authors": [
      "Kaiyu Zhou",
      "Yongsen Zheng",
      "Yicheng He",
      "Meng Xue",
      "Xueluan Gong"
    ],
    "published_date": "2026-01-16",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.10955v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10955v1",
    "content": {
      "en": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions. We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Our method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation. Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.",
      "tr": "İşte akademik makalenin başlığının ve özetinin çevirisi:\n\n**Makale Başlığı:** Beyond Max Tokens: LLM Ajanlarında Araç Çağırma Zincirleri Yoluyla Gizli Kaynak Yükseltmesi\n\n**Özet:**\nAjan-araç iletişim döngüsü, modern Large Language Model (LLM) ajanlarında kritik bir saldırı yüzeyidir. Temel olarak kullanıcı komutları veya enjekte edilmiş retrieval-augmented generation (RAG) bağlamı aracılığıyla tetiklenen mevcut Hizmet Reddi (DoS) saldırıları, bu yeni paradigma için etkisizdir. Bunlar temelde tek turluktur ve genellikle görev odaklı bir yaklaşımdan yoksundurlar, bu da onları hedef odaklı iş akışlarında göze çarpan hale getirir ve çok turlu ajan-araç etkileşimlerinin birleşen maliyetlerinden yararlanamaz. Görevin doğru tamamlanmış bir görevi süsü altında, araç katmanında işleyen gizli, çok turlu bir ekonomik DoS saldırısı sunuyoruz. Yöntemimiz, Monte Carlo Tree Search (MCTS) optimizer ile bu düzenlemeleri optimize ederek, zararsız, Model Context Protocol (MCP)-uyumlu bir araç sunucusunda metin görünür alanlarını ve şablon güdümlü bir iade politikasını ayarlar. Bu düzenlemeler, işlev imzalarını değiştirmeden bırakır ve nihai yükü koruyarak, yalnızca metin tabanlı bildirimler aracılığıyla ajanı uzun, ayrıntılı araç çağırma dizilerine yönlendirir. Bu, tek turluk sınırları aşarken maliyetleri turlar boyunca birleştirir ve nihai cevabın doğru kalmasını sağlayarak doğrulamadan kaçınır. ToolBench ve BFCL kıyaslamalarında altı LLM üzerinde yapılan saldırımız, görevleri 60.000 token'ı aşan yörüngelere genişletir, maliyetleri 658 kata kadar şişirir ve enerjiyi 100-560 kat artırır. GPU KV cache doluluğunu %1'in altından %35-74'e çıkarır ve eş zamanlı işlem throughput'unu yaklaşık %50 oranında azaltır. Sunucu protokol uyumlu kaldığı ve görev sonuçları doğru olduğu için, geleneksel kontroller başarısız olur. Bu sonuçlar, ajan-araç arayüzünü birinci sınıf bir güvenlik sınırı olarak yükselterek, doğrulama işlemlerini nihai cevapları doğrulamaktan tüm ajanlık sürecin ekonomik ve hesaplama maliyetini izlemeye doğru bir paradigma değişikliği talep etmektedir."
    }
  },
  {
    "id": "2601.10440v1",
    "title": "AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior",
    "authors": [
      "Nadya Abaev",
      "Denis Klimov",
      "Gerard Levinov",
      "David Mimran",
      "Yuval Elovici"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.10440v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10440v1",
    "content": {
      "en": "Artificial intelligence (AI) agents are increasingly used in a variety of domains to automate tasks, interact with users, and make decisions based on data inputs. Ensuring that AI agents perform only authorized actions and handle inputs appropriately is essential for maintaining system integrity and preventing misuse. In this study, we introduce the AgentGuardian, a novel security framework that governs and protects AI agent operations by enforcing context-aware access-control policies. During a controlled staging phase, the framework monitors execution traces to learn legitimate agent behaviors and input patterns. From this phase, it derives adaptive policies that regulate tool calls made by the agent, guided by both real-time input context and the control flow dependencies of multi-step agent actions. Evaluation across two real-world AI agent applications demonstrates that AgentGuardian effectively detects malicious or misleading inputs while preserving normal agent functionality. Moreover, its control-flow-based governance mechanism mitigates hallucination-driven errors and other orchestration-level malfunctions.",
      "tr": "İşte akademik makale başlığı ve özetinin istenen çevirisi:\n\n**Makale Başlığı:** AgentGuardian: AI Agent Davranışını Yönetmek İçin Erişim Kontrol Politikaları Öğrenme\n\n**Özet:**\nYapay zeka (AI) ajanları, görevleri otomatikleştirmek, kullanıcılarla etkileşim kurmak ve veri girdilerine dayalı kararlar almak için çeşitli alanlarda giderek daha fazla kullanılmaktadır. AI ajanlarının yalnızca yetkilendirilmiş eylemleri gerçekleştirmesini ve girdileri uygun şekilde işlemesini sağlamak, sistem bütünlüğünü korumak ve kötüye kullanımı önlemek için esastır. Bu çalışmada, bağlam-farkındalığına sahip erişim kontrolü politikalarını uygulayarak AI ajan işlemlerini yöneten ve koruyan yeni bir güvenlik çerçevesi olan AgentGuardian'ı sunuyoruz. Kontrollü bir hazırlık aşaması boyunca çerçeve, meşru ajan davranışlarını ve girdi modellerini öğrenmek için yürütme izlerini izler. Bu aşamadan, hem gerçek zamanlı girdi bağlamı hem de çok adımlı ajan eylemlerinin kontrol akışı bağımlılıkları tarafından yönlendirilen, ajan tarafından yapılan araç çağrılarını düzenleyen uyarlanabilir politikalar türetir. İki gerçek dünya AI ajanı uygulaması üzerinde yapılan değerlendirme, AgentGuardian'ın normal ajan işlevselliğini korurken kötü niyetli veya yanıltıcı girdileri etkili bir şekilde tespit ettiğini göstermektedir. Dahası, kontrol akışı tabanlı yönetim mekanizması, halüsinasyon kaynaklı hataları ve diğer orkestrasyon düzeyindeki arızaları azaltır."
    }
  },
  {
    "id": "2601.10413v1",
    "title": "LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies",
    "authors": [
      "Haiyue Yuan",
      "Nikolay Matyunin",
      "Ali Raza",
      "Shujun Li"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.10413v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10413v1",
    "content": {
      "en": "Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.",
      "tr": "**Makale Başlığı:** LADFA: Gizlilik Politikalarındaki Kişisel Veri Akışı Analizi İçin Large Language Models ve Retrieval-Augmented Generation Kullanımına Yönelik Bir Çerçeve\n\n**Özet:**\n\nGizlilik politikaları, kuruluşların kişisel veri işleme uygulamaları hakkında insanları bilgilendirmeye yardımcı olur ve veri toplama, veri depolama ve üçüncü taraflarla kişisel veri paylaşımı gibi farklı yönleri kapsar. Gizlilik politikaları, kullanılan uzun ve karmaşık yasal dil ile farklı sektörler ve kuruluşlar arasındaki tutarsız uygulamalar nedeniyle insanlar tarafından tam olarak anlaşılması zor olabilir. Gizlilik politikalarının otomatik ve büyük ölçekli analizlerini gerçekleştirmeye yardımcı olmak için birçok araştırmacı, large language models (LLMs) dahil olmak üzere makine öğrenmesi ve doğal dil işleme tekniklerinin uygulamalarını incelemiştir. LLM'lerin gizlilik politikalarından kişisel veri akışlarını çıkarmak için sınırlı sayıda önceki çalışma kullanılmış olsa da, yaklaşımımız, LLM'leri retrieval-augmented generation (RAG) ve mevcut çalışmalardan türetilen özelleştirilmiş bir knowledge base ile birleştirerek bu alandaki çalışmalara dayanmaktadır. Bu makale, verilen bir gizlilik politikasındaki yapılandırılmamış metni işleyebilen, kişisel veri akışlarını çıkarıp bir personal data flow graph inşa edebilen ve insight discovery'yi kolaylaştırmak için data flow graph analizini gerçekleştirebilen uçtan uca bir hesaplamalı çerçeve olan LADFA'nın gelişimini sunmaktadır. Çerçeve, bir pre-processor, bir LLM-based processor ve bir data flow post-processor'dan oluşmaktadır. Otomotiv endüstrisinden seçilen on gizlilik politikasını incelemeyi içeren bir vaka çalışması yürüterek önerilen yaklaşımın etkinliğini ve doğruluğunu gösterdik ve doğruladık. Ayrıca, LADFA'nın esnek ve özelleştirilebilir olacak şekilde tasarlandığı ve gizlilik politikası analizinin ötesinde çeşitli metin tabanlı analiz görevleri için uygun hale geldiği belirtilmeye değerdir."
    }
  },
  {
    "id": "2601.10338v1",
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "authors": [
      "Yi Liu",
      "Weizhe Wang",
      "Ruitao Feng",
      "Yao Zhang",
      "Guangquan Xu"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2601.10338v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10338v1",
    "content": {
      "en": "The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.",
      "tr": "**Makale Başlığı:** Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale\n\n**Özet:**\n\nYapay zeka agent framework'lerinin yükselişi, agent yeteneklerini dinamik olarak genişleten, talimatlar ve çalıştırılabilir kod içeren modüler paketler olan agent skills'i beraberinde getirmiştir. Bu mimari güçlü bir özelleştirme imkanı sunarken, skills'ler örtük güven ve minimal incelemeyle çalıştırılmakta, bu da önemli ancak henüz karakterize edilmemiş bir saldırı yüzeyi oluşturmaktadır. Bu gelişmekte olan ekosistemin ilk büyük ölçekli ampirik güvenlik analizini yürütüyoruz. İki büyük pazar yerinden 42.447 skill topladık ve SkillScan kullanarak 31.132'sini sistematik olarak analiz ettik. SkillScan, statik analiz ile LLM tabanlı semantik sınıflandırmayı entegre eden çok aşamalı bir tespit framework'üdür. Bulgularımız yaygın güvenlik risklerini ortaya koymaktadır: skills'lerin %26,1'i, dört kategoride 14 farklı örüntü kapsayan en az bir güvenlik açığı içermektedir: prompt injection, data exfiltration, privilege escalation ve supply chain risks. Data exfiltration (%13,3) ve privilege escalation (%11,8) en yaygın olanlardır, bu sırada skills'lerin %5,2'si kötü niyetli amacı kuvvetle düşündüren yüksek şiddetli örüntüler sergilemektedir. Çalıştırılabilir script'leri birleştiren skills'lerin, sadece talimat içeren skills'lere göre güvenlik açığı barındırma olasılığının 2,12 kat daha fazla olduğunu bulduk (OR=2,12, p<0,001). Katkılarımız şunları içermektedir: (1) 8.126 açık barındıran skill'den türetilmiş, temellendirilmiş bir güvenlik açığı taksonomisi, (2) %86,7 kesinlik ve %82,5 recall sağlayan doğrulanmış bir tespit metodolojisi ve (3) gelecekteki araştırmaları desteklemek için açık bir veri seti ve tespit toolkit'i. Bu sonuçlar, bu saldırı vektörü daha fazla istismar edilmeden önce, yetenek tabanlı izin sistemlerine ve zorunlu güvenlik incelemelerine acil bir ihtiyaç olduğunu göstermektedir."
    }
  },
  {
    "id": "2601.10237v1",
    "title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
    "authors": [
      "Murat Bilgehan Ertan",
      "Marten van Dijk"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.10237v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10237v1",
    "content": {
      "en": "Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy   $σ\\ge \\frac{1}{\\sqrt{2\\ln M}}$ $\\quad\\text{or}\\quad$ $κ\\ge\\ \\frac{1}{\\sqrt{8}}\\!\\left(1-\\frac{1}{\\sqrt{4π\\ln M}}\\right)$,   and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \\to \\infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.",
      "tr": "Elbette, akademik makale başlığını ve özetini istenen şekilde Türkçeye çevirdim:\n\n**Makale Başlığı:** DP-SGD için Elverişli Gizlilik-Fayda Güvencelerinin Temel Sınırlılıkları\n\n**Özet:**\n\nDifferentially Private Stochastic Gradient Descent (DP-SGD), gizli eğitim için baskın paradigma olup, ancak en kötü durum düşmanca gizlilik tanımları altındaki temel sınırlılıkları tam olarak anlaşılamamıştır. DP-SGD'yi, gizliliği hipotez testi ödünleşme eğrileri aracılığıyla karakterize eden $f$-differential privacy framework içinde analiz ediyoruz ve $M$ gradyan güncellemesi ile tek bir epoch üzerinden shuffled sampling'i inceliyoruz. Elde edilebilir ödünleşme eğrisi üzerinde açıkça suboptimal bir üst sınır türetiriz. Bu sonuç, mekanizmanın ödünleşme eğrisi ile ideal rastgele tahmin çizgisi arasındaki maksimum mesafe olan ayrım $κ$ üzerinde geometrik bir alt sınır oluşturur. Büyük bir ayrım, önemli düşmanca avantaj anlamına geldiği için, anlamlı gizlilik küçük $κ$ gerektirir. Ancak, küçük bir ayrımın zorlanmasının Gaussian noise multiplier $σ$ üzerinde kesin bir alt sınır getirdiğini kanıtlıyoruz, bu da elde edilebilir faydayı doğrudan sınırlar. Özellikle, standart en kötü durum düşmanca modeli altında, shuffled DP-SGD şu şartları sağlamalıdır: $σ\\ge \\frac{1}{\\sqrt{2\\ln M}}$ veya $κ\\ge\\ \\frac{1}{\\sqrt{8}}\\!\\left(1-\\frac{1}{\\sqrt{4π\\ln M}}\\right)$, ve bu nedenle aynı anda hem güçlü gizliliği hem de yüksek faydayı elde edemez. Bu sınır, $M \\to \\infty$ olarak asimptotik olarak sıfırlansa da, yakınsama son derece yavaştır: pratik olarak ilgili sayıda güncelleme için bile gerekli gürültü büyüklüğü önemli kalmaktadır. Ayrıca, aynı sınırlılığın Poisson subsampling'e sabit faktörlere kadar uzandığını gösteriyoruz. Deneylerimiz, bu sınırlamanın ima ettiği gürültü seviyelerinin gerçekçi eğitim ortamlarında önemli doğruluk düşüşlerine yol açtığını doğrulamaktadır; dolayısıyla, standart en kötü durum düşmanca varsayımlar altında DP-SGD'de kritik bir darboğaz göstermektedir."
    }
  },
  {
    "id": "2601.10212v1",
    "title": "PADER: Paillier-based Secure Decentralized Social Recommendation",
    "authors": [
      "Chaochao Chen",
      "Jiaming Qian",
      "Fei Zheng",
      "Yachuan Liu"
    ],
    "published_date": "2026-01-15",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.10212v1",
    "pdf_link": "https://arxiv.org/pdf/2601.10212v1",
    "content": {
      "en": "The prevalence of recommendation systems also brings privacy concerns to both the users and the sellers, as centralized platforms collect as much data as possible from them. To keep the data private, we propose PADER: a Paillier-based secure decentralized social recommendation system. In this system, the users and the sellers are nodes in a decentralized network. The training and inference of the recommendation model are carried out securely in a decentralized manner, without the involvement of a centralized platform. To this end, we apply the Paillier cryptosystem to the SoReg (Social Regularization) model, which exploits both user's ratings and social relations. We view the SoReg model as a two-party secure polynomial evaluation problem and observe that the simple bipartite computation may result in poor efficiency. To improve efficiency, we design secure addition and multiplication protocols to support secure computation on any arithmetic circuit, along with an optimal data packing scheme that is suitable for the polynomial computations of real values. Experiment results show that our method only takes about one second to iterate through one user with hundreds of ratings, and training with ~500K ratings for one epoch only takes <3 hours, which shows that the method is practical in real applications. The code is available at https://github.com/GarminQ/PADER.",
      "tr": "**Makale Başlığı:** PADER: Paillier Tabanlı Güvenli Dağıtık Sosyal Tavsiye\n\n**Özet:**\n\nTavsiye sistemlerinin yaygınlığı, merkezi platformların kullanıcılar ve satıcılardan mümkün olduğunca çok veri topladığı için hem kullanıcılara hem de satıcılara yönelik gizlilik endişelerini de beraberinde getirmektedir. Verileri özel tutmak amacıyla, Paillier tabanlı güvenli dağıtık bir sosyal tavsiye sistemi olan PADER'i öneriyoruz. Bu sistemde, kullanıcılar ve satıcılar dağıtık bir ağdaki düğümlerdir. Tavsiye modelinin eğitimi ve çıkarımı, merkezi bir platformun dahil edilmeksizin güvenli bir şekilde dağıtık bir biçimde gerçekleştirilir. Bu amaçla, hem kullanıcının derecelendirmelerini hem de sosyal ilişkilerini kullanan SoReg (Social Regularization) modeline Paillier kriptosistemini uyguluyoruz. SoReg modelini iki taraflı güvenli polinom değerlendirme problemi olarak ele alıyoruz ve basit iki-taraflı hesaplamanın düşük verimliliğe yol açabileceğini gözlemliyoruz. Verimliliği artırmak için, herhangi bir aritmetik devre üzerinde güvenli hesaplamayı destekleyen güvenli toplama ve çarpma protokolleri tasarlıyoruz, bunun yanı sıra reel değerlerin polinom hesaplamalarına uygun optimal bir veri paketleme şeması da geliştiriyoruz. Deney sonuçları, yöntemimizin yüzlerce derecelendirmeye sahip bir kullanıcıyı yinelemesinin yalnızca yaklaşık bir saniye sürdüğünü ve ~500K derecelendirme ile bir epoch'luk eğitimin yalnızca 3 saatten az sürdüğünü göstermektedir, bu da yöntemin gerçek uygulamalarda pratik olduğunu ortaya koymaktadır. Kod, https://github.com/GarminQ/PADER adresinde mevcuttur."
    }
  }
]