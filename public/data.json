[
  {
    "id": "2601.14054v1",
    "title": "SecureSplit: Mitigating Backdoor Attacks in Split Learning",
    "authors": [
      "Zhihao Dou",
      "Dongfei Cui",
      "Weida Wang",
      "Anjun Gao",
      "Yueyang Quan"
    ],
    "published_date": "2026-01-20",
    "tags": [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.14054v1",
    "pdf_link": "https://arxiv.org/pdf/2601.14054v1",
    "content": {
      "en": "Split Learning (SL) offers a framework for collaborative model training that respects data privacy by allowing participants to share the same dataset while maintaining distinct feature sets. However, SL is susceptible to backdoor attacks, in which malicious clients subtly alter their embeddings to insert hidden triggers that compromise the final trained model. To address this vulnerability, we introduce SecureSplit, a defense mechanism tailored to SL. SecureSplit applies a dimensionality transformation strategy to accentuate subtle differences between benign and poisoned embeddings, facilitating their separation. With this enhanced distinction, we develop an adaptive filtering approach that uses a majority-based voting scheme to remove contaminated embeddings while preserving clean ones. Rigorous experiments across four datasets (CIFAR-10, MNIST, CINIC-10, and ImageNette), five backdoor attack scenarios, and seven alternative defenses confirm the effectiveness of SecureSplit under various challenging conditions.",
      "tr": "**Makale Başlığı:** SecureSplit: Split Learning'de Arka Kapı Saldırılarını Azaltma\n\n**Özet:**\n\nSplit Learning (SL), katılımcıların aynı veri kümesini paylaşmalarına izin vererek ancak farklı öznitelik kümelerini koruyarak veri gizliliğini gözeten işbirlikçi model eğitimi için bir çerçeve sunar. Bununla birlikte, SL, kötü niyetli istemcilerin gizli tetikleyiciler yerleştirmek için gömülerini (embeddings) ustaca değiştirdiği ve nihai olarak eğitilmiş modeli tehlikeye attığı arka kapı saldırılarına karşı savunmasızdır. Bu savunmasızlığı gidermek için SL'ye özel bir savunma mekanizması olan SecureSplit'i sunuyoruz. SecureSplit, iyi niyetli ve zehirlenmiş gömüler arasındaki ince farklılıkları vurgulamak ve bunların ayrılmasını kolaylaştırmak için bir boyutluluk dönüşüm stratejisi uygular. Bu geliştirilmiş ayrım ile, temiz olanları korurken kontamine olmuş gömüleri kaldırmak için çoğunluk tabanlı bir oylama şeması kullanan adaptif bir filtreleme yaklaşımı geliştiriyoruz. Dört veri kümesi (CIFAR-10, MNIST, CINIC-10 ve ImageNette), beş arka kapı saldırı senaryosu ve yedi alternatif savunma üzerindeki titiz deneyler, çeşitli zorlu koşullar altında SecureSplit'in etkinliğini doğrulamaktadır."
    }
  },
  {
    "id": "2601.14033v1",
    "title": "PAC-Private Responses with Adversarial Composition",
    "authors": [
      "Xiaochen Zhu",
      "Mayuri Sridhar",
      "Srinivas Devadas"
    ],
    "published_date": "2026-01-20",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.14033v1",
    "pdf_link": "https://arxiv.org/pdf/2601.14033v1",
    "content": {
      "en": "Modern machine learning models are increasingly deployed behind APIs. This renders standard weight-privatization methods (e.g. DP-SGD) unnecessarily noisy at the cost of utility. While model weights may vary significantly across training datasets, model responses to specific inputs are much lower dimensional and more stable. This motivates enforcing privacy guarantees directly on model outputs.   We approach this under PAC privacy, which provides instance-based privacy guarantees for arbitrary black-box functions by controlling mutual information (MI). Importantly, PAC privacy explicitly rewards output stability with reduced noise levels. However, a central challenge remains: response privacy requires composing a large number of adaptively chosen, potentially adversarial queries issued by untrusted users, where existing composition results on PAC privacy are inadequate. We introduce a new algorithm that achieves adversarial composition via adaptive noise calibration and prove that mutual information guarantees accumulate linearly under adaptive and adversarial querying.   Experiments across tabular, vision, and NLP tasks show that our method achieves high utility at extremely small per-query privacy budgets. On CIFAR-10, we achieve 87.79% accuracy with a per-step MI budget of $2^{-32}$. This enables serving one million queries while provably bounding membership inference attack (MIA) success rates to 51.08% -- the same guarantee of $(0.04, 10^{-5})$-DP. Furthermore, we show that private responses can be used to label public data to distill a publishable privacy-preserving model; using an ImageNet subset as a public dataset, our model distilled from 210,000 responses achieves 91.86% accuracy on CIFAR-10 with MIA success upper-bounded by 50.49%, which is comparable to $(0.02,10^{-5})$-DP.",
      "tr": "**Makale Başlığı:** PAC-Private Responses with Adversarial Composition\n\n**Özet:**\n\nModern makine öğrenmesi modelleri giderek daha fazla API'lerin arkasına konuşlandırılmaktadır. Bu durum, standart ağırlık-özelleştirme (weight-privatization) yöntemlerini (örn. DP-SGD) fayda (utility) maliyetine gereksiz yere gürültülü hale getirmektedir. Model ağırlıkları eğitim veri kümeleri arasında önemli ölçüde farklılık gösterebilse de, modellerin belirli girdilere verdiği yanıtlar çok daha düşük boyutlu ve daha stabildir. Bu, gizlilik garantilerinin doğrudan model çıktılarına uygulanmasını teşvik eder. Bu konuya, mutual information (MI) kontrolü yoluyla keyfi black-box fonksiyonlar için örneğe dayalı (instance-based) gizlilik garantileri sağlayan PAC privacy kapsamında yaklaşmaktayız. Önemle belirtmek gerekir ki PAC privacy, daha düşük gürültü seviyeleri ile çıktı stabilitesini açıkça ödüllendirir. Ancak merkezi bir zorluk devam etmektedir: yanıt gizliliği, güvenilmeyen kullanıcılar tarafından verilen, uyarlanabilir şekilde seçilmiş ve potansiyel olarak adversarial (adversarial) bir sorgu kümesinin büyük bir sayısını gerektirir; bu tür durumlarda PAC privacy üzerindeki mevcut bileşim (composition) sonuçları yetersizdir. Biz, uyarlanabilir gürültü kalibrasyonu (adaptive noise calibration) yoluyla adversarial bileşim (adversarial composition) elde eden yeni bir algoritma sunuyoruz ve mutual information garantilerinin uyarlanabilir ve adversarial sorgulama altında doğrusal olarak biriktiğini kanıtlıyoruz. Tabular, vision ve NLP görevlerindeki deneyler, yöntemimizin aşırı derecede küçük sorgu başına gizlilik bütçeleri (per-query privacy budgets) ile yüksek fayda sağladığını göstermektedir. CIFAR-10 üzerinde, $2^{-32}$ sorgu başına MI bütçesi ile %87.79 doğruluk elde etmekteyiz. Bu, üyelik çıkarım saldırısı (membership inference attack - MIA) başarı oranlarını %51.08 ile sınırlayarak bir milyon sorgu sunmamızı sağlamaktadır; bu da $(0.04, 10^{-5})$-DP ile aynı garantiyi sunar. Dahası, gizli yanıtların, yayınlanabilir bir gizlilik-koruyucu model damıtmak (distill) için halka açık verileri etiketlemek amacıyla kullanılabileceğini göstermekteyiz; halka açık bir veri kümesi olarak bir ImageNet alt kümesi kullanarak, 210.000 yanıttan damıtılmış modelimiz, CIFAR-10 üzerinde %91.86 doğruluk elde etmekte ve MIA başarı oranı %50.49 ile sınırlanmaktadır, bu da $(0.02, 10^{-5})$-DP ile karşılaştırılabilirdir."
    }
  },
  {
    "id": "2601.13864v1",
    "title": "HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation",
    "authors": [
      "Qirui Chen",
      "Jingxian Shuai",
      "Shuangwu Chen",
      "Shenghao Ye",
      "Zijian Wen"
    ],
    "published_date": "2026-01-20",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.13864v1",
    "pdf_link": "https://arxiv.org/pdf/2601.13864v1",
    "content": {
      "en": "Large language models (LLMs) are being increasingly integrated into practical hardware and firmware development pipelines for code generation. Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues. However, LLM-generated code that appears functionally sound may embed security flaws which could induce catastrophic damages after deployment. This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications. In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. Each task includes a structured specification, a secure reference implementation, and executable tests. To automate artifact synthesis, we propose a multi-agent pipeline that decouples synthesis from verification and grounds evaluation in execution evidence, enabling reliable evaluation. Using HardSecBench, we evaluate a range of LLMs on hardware and firmware code generation and find that models often satisfy functional requirements while still leaving security risks. We also find that security results vary with prompting. These findings highlight pressing challenges and offer actionable insights for future advancements in LLM-assisted hardware design. Our data and code will be released soon.",
      "tr": "Makale Başlığı: HardSecBench: Donanım Kodu Üretimi İçin LLM'lerin Güvenlik Farkındalığının Kıyaslanması\n\nÖzet:\nBüyük dil modelleri (LLM'ler), kod üretimi için donanım ve firmware geliştirme süreçlerine giderek daha fazla entegre edilmektedir. Mevcut çalışmalar öncelikli olarak LLM tarafından üretilen kodun işlevsel doğruluğunu değerlendirmeye odaklanmış olsa da, güvenlik sorunlarına yeterince dikkat göstermemiştir. Ancak, işlevsel olarak sağlam görünen LLM tarafından üretilen kod, dağıtıldıktan sonra yıkıcı hasarlara yol açabilecek güvenlik kusurları içerebilir. Bu kritik araştırma boşluğu, gerçekçi spesifikasyonlar altında güvenlik farkındalığını değerlendirmek için bir benchmark tasarlamamızı teşvik etmektedir. Bu çalışmada, Verilog Register Transfer Level (RTL) ve firmware seviyesi C'yi kapsayan, 76 donanımla ilgili Common Weakness Enumeration (CWE) girişini içeren 924 görevden oluşan bir benchmark olan HardSecBench'i tanıtıyoruz. Her görev, yapılandırılmış bir spesifikasyon, güvenli bir referans uygulama ve yürütülebilir testler içermektedir. Artifact sentezini otomatikleştirmek için, sentezi doğrulama işleminden ayıran ve değerlendirmeyi yürütme kanıtlarına dayandıran, güvenilir değerlendirmeye olanak tanıyan çoklu ajanlı bir pipeline öneriyoruz. HardSecBench'i kullanarak, bir dizi LLM'yi donanım ve firmware kodu üretiminde değerlendirdik ve modellerin genellikle işlevsel gereksinimleri karşılarken hala güvenlik riskleri bıraktığını bulduk. Ayrıca, güvenlik sonuçlarının prompting ile değiştiğini de tespit ettik. Bu bulgular, LLM destekli donanım tasarımında acil zorlukları vurgulamakta ve gelecekteki gelişmeler için uygulanabilir içgörüler sunmaktadır. Verilerimiz ve kodumuz yakında yayınlanacaktır."
    }
  },
  {
    "id": "2601.13569v1",
    "title": "DRGW: Learning Disentangled Representations for Robust Graph Watermarking",
    "authors": [
      "Jiasen Li",
      "Yanwei Liu",
      "Zhuoyi Shang",
      "Xiaoyan Gu",
      "Weiping Wang"
    ],
    "published_date": "2026-01-20",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.13569v1",
    "pdf_link": "https://arxiv.org/pdf/2601.13569v1",
    "content": {
      "en": "Graph-structured data is foundational to numerous web applications, and watermarking is crucial for protecting their intellectual property and ensuring data provenance. Existing watermarking methods primarily operate on graph structures or entangled graph representations, which compromise the transparency and robustness of watermarks due to the information coupling in representing graphs and uncontrollable discretization in transforming continuous numerical representations into graph structures. This motivates us to propose DRGW, the first graph watermarking framework that addresses these issues through disentangled representation learning. Specifically, we design an adversarially trained encoder that learns an invariant structural representation against diverse perturbations and derives a statistically independent watermark carrier, ensuring both robustness and transparency of watermarks. Meanwhile, we devise a graph-aware invertible neural network to provide a lossless channel for watermark embedding and extraction, guaranteeing high detectability and transparency of watermarks. Additionally, we develop a structure-aware editor that resolves the issue of latent modifications into discrete graph edits, ensuring robustness against structural perturbations. Experiments on diverse benchmark datasets demonstrate the superior effectiveness of DRGW.",
      "tr": "**Makale Başlığı:** DRGW: Sağlam Grafik Filigranlama İçin Ayrıştırılmış Gösterimler Öğrenme\n\n**Özet:**\n\nGrafik yapıdaki veriler, çok sayıda web uygulaması için temel oluşturmakta ve filigranlama (watermarking), bunların fikri mülkiyetini korumak ve veri kökenini güvence altına almak için kritik öneme sahiptir. Mevcut filigranlama yöntemleri esas olarak grafik yapıları veya entangled graph representations üzerinde çalışmaktadır. Bu durum, grafiklerin temsilindeki bilgi eşleşmesi ve continuous numerical representations'ın grafik yapılarına dönüştürülmesindeki kontrolsüz ayrıştırma (discretization) nedeniyle filigranların şeffaflığını ve sağlamlığını (robustness) tehlikeye atmaktadır. Bu durum, bizi disentangled representation learning aracılığıyla bu sorunları ele alan ilk grafik filigranlama çerçevesi olan DRGW'yi önermeye motive etmektedir. Spesifik olarak, çeşitli pertürbasyonlara (perturbations) karşı değişmez bir structural representation öğrenen ve istatistiksel olarak bağımsız bir watermark carrier türeten, hem filigranların sağlamlığını hem de şeffaflığını sağlayan adversarially trained bir encoder tasarlıyoruz. Eş zamanlı olarak, watermark embedding ve extraction için lossless bir kanal sağlayan, filigranların yüksek tespit edilebilirliğini ve şeffaflığını garanti eden graph-aware bir invertible neural network geliştiriyoruz. Ek olarak, gizli değişikliklerin discrete graph edits sorununu çözen ve structural perturbations'a karşı sağlamlığı sağlayan structure-aware bir editor geliştiriyoruz. Çeşitli benchmark veri setleri üzerindeki deneyler, DRGW'nin üstün etkinliğini göstermektedir."
    }
  },
  {
    "id": "2601.13528v1",
    "title": "Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs",
    "authors": [
      "Jackson Kaunismaa",
      "Avery Griffin",
      "John Hughes",
      "Christina Q. Knight",
      "Mrinank Sharma"
    ],
    "published_date": "2026-01-20",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2601.13528v1",
    "pdf_link": "https://arxiv.org/pdf/2601.13528v1",
    "content": {
      "en": "Model developers implement safeguards in frontier models to prevent misuse, for example, by employing classifiers to filter dangerous outputs. In this work, we demonstrate that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. Our elicitation attacks consist of three stages: (i) constructing prompts in adjacent domains to a target harmful task that do not request dangerous information; (ii) obtaining responses to these prompts from safeguarded frontier models; (iii) fine-tuning open-source models on these prompt-output pairs. Since the requested prompts cannot be used to directly cause harm, they are not refused by frontier model safeguards. We evaluate these elicitation attacks within the domain of hazardous chemical synthesis and processing, and demonstrate that our attacks recover approximately 40% of the capability gap between the base open-source model and an unrestricted frontier model. We then show that the efficacy of elicitation attacks scales with the capability of the frontier model and the amount of generated fine-tuning data. Our work demonstrates the challenge of mitigating ecosystem level risks with output-level safeguards.",
      "tr": "**Makale Başlığı:** Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs\n\n**Özet:**\n\nModel geliştiricileri, kötüye kullanımı önlemek amacıyla en yeni modellerde, örneğin tehlikeli çıktıları filtrelemek için sınıflandırıcılar kullanarak, koruma mekanizmaları uygulamaktadır. Bu çalışmada, sağlam bir şekilde korunmuş modellerin bile, elicitation attacks yoluyla açık kaynaklı modellerde zararlı yetenekleri ortaya çıkarabileceğini göstermekteyiz. Elicitation attacks’lerimiz üç aşamadan oluşmaktadır: (i) hedef zararlı bir görevle ilişkili ancak tehlikeli bilgi talep etmeyen komşu alanlarda prompt’lar oluşturulması; (ii) bu prompt’lara korunmuş en yeni modellerden yanıtlar elde edilmesi; (iii) bu prompt-çıktı çiftleri üzerinde açık kaynaklı modellerin fine-tuning edilmesi. Talep edilen prompt’lar doğrudan zarar vermek için kullanılamayacağından, en yeni model koruma mekanizmaları tarafından reddedilmezler. Bu elicitation attacks’leri, tehlikeli kimyasal sentez ve işleme alanında değerlendirerek, saldırılarımızın temel açık kaynaklı model ile kısıtlanmamış en yeni model arasındaki yetenek boşluğunun yaklaşık %40'ını geri kazandığını göstermekteyiz. Ardından, elicitation attacks’lerinin etkinliğinin, en yeni modelin yeteneği ve üretilen fine-tuning verisi miktarıyla ölçeklendiğini göstermekteyiz. Çalışmamız, çıktı düzeyindeki korumalarla ekosistem düzeyindeki riskleri azaltmanın zorluğunu ortaya koymaktadır."
    }
  },
  {
    "id": "2601.13515v1",
    "title": "Automatic Adjustment of HPA Parameters and Attack Prevention in Kubernetes Using Random Forests",
    "authors": [
      "Hanlin Zhou",
      "Huah Yong Chan",
      "Jingfei Ni",
      "Mengchun Wu",
      "Qing Deng"
    ],
    "published_date": "2026-01-20",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "link": "http://arxiv.org/abs/2601.13515v1",
    "pdf_link": "https://arxiv.org/pdf/2601.13515v1",
    "content": {
      "en": "In this paper, HTTP status codes are used as custom metrics within the HPA as the experimental scenario. By integrating the Random Forest classification algorithm from machine learning, attacks are assessed and predicted, dynamically adjusting the maximum pod parameter in the HPA to manage attack traffic. This approach enables the adjustment of HPA parameters using machine learning scripts in targeted attack scenarios while effectively managing attack traffic. All access from attacking IPs is redirected to honeypot pods, achieving a lower incidence of 5XX status codes through HPA pod adjustments under high load conditions. This method also ensures effective isolation of attack traffic, preventing excessive HPA expansion due to attacks. Additionally, experiments conducted under various conditions demonstrate the importance of setting appropriate thresholds for HPA adjustments.",
      "tr": "**Makale Başlığı:** Automatic Adjustment of HPA Parameters and Attack Prevention in Kubernetes Using Random Forests\n\n**Özet:**\n\nBu çalışmada, HPA içinde deneysel senaryo olarak HTTP durum kodları özel metrikler olarak kullanılmaktadır. Makine öğrenmesinden Random Forest sınıflandırma algoritmasının entegrasyonu ile saldırılar değerlendirilip tahmin edilmekte, böylece saldırı trafiğini yönetmek amacıyla HPA'daki maksimum pod parametresi dinamik olarak ayarlanmaktadır. Bu yaklaşım, hedefli saldırı senaryolarında makine öğrenmesi scriptleri kullanılarak HPA parametrelerinin ayarlanmasına olanak tanıyarak saldırı trafiğinin etkili bir şekilde yönetilmesini sağlamaktadır. Saldıran IP'lerden gelen tüm erişimler honeypot podlarına yönlendirilmekte, yüksek yük koşulları altında HPA pod ayarlamaları sayesinde daha düşük oranda 5XX durum kodları elde edilmektedir. Bu yöntem ayrıca saldırı trafiğinin etkili bir şekilde izole edilmesini sağlayarak saldırılar nedeniyle HPA'nın aşırı genişlemesini önlemektedir. Ek olarak, çeşitli koşullar altında yürütülen deneyler, HPA ayarlamaları için uygun eşik değerlerinin belirlenmesinin önemini ortaya koymaktadır."
    }
  },
  {
    "id": "2601.13359v1",
    "title": "Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection",
    "authors": [
      "Asen Dotsinski",
      "Panagiotis Eustratiadis"
    ],
    "published_date": "2026-01-19",
    "tags": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.13359v1",
    "pdf_link": "https://arxiv.org/pdf/2601.13359v1",
    "content": {
      "en": "As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce \"sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., \"Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.",
      "tr": "Makale Başlığı: Sockpuppetting: Çıktı Öneki Enjeksiyonu ile Optimizasyon Yapmadan LLM'leri Kırmak\n\nÖzet:\nAçık ağırlıklı büyük dil modellerinin (LLM'ler) yetenekleri arttıkça, onları kötü amaçlı komutlardan korumak ve olası saldırı vektörlerini anlamak giderek daha önemli hale gelmektedir. GCG [Zou et al., 2023] gibi otomatik kırma yöntemleri etkili olmaya devam etse de, genellikle önemli hesaplama kaynakları ve özel uzmanlık gerektirirler. Biz, bir modelin çıktısının başına bir kabul dizisi (örneğin, \"Elbette, işte nasıl yapılacağı...\") ekleyerek ve yanıtı tamamlamasına izin vererek açık ağırlıklı LLM'leri kırmak için basit bir yöntem olan \"sockpuppetting\"i tanıtıyoruz. Sadece tek satır kod ve optimizasyon gerektirmeyen sockpuppetting, Qwen3-8B üzerinde komut bazlı karşılaştırmalarda GCG'den %80'e varan oranda daha yüksek saldırı başarı oranı (ASR) elde etmektedir. Ayrıca, komut bağımsız bir ortamda Llama-3.1-8B üzerinde GCG'ye kıyasla ASR'yi %64 artıran, kullanıcı komutundan ziyade asistan mesaj bloğundaki düşmanca soneki optimize eden hibrit bir yaklaşımı da inceliyoruz. Elde edilen sonuçlar, sockpuppetting'i sofistike olmayan rakiplerin erişebileceği etkili ve düşük maliyetli bir saldırı olarak ortaya koymakta, açık ağırlıklı modellerde çıktı öneki enjeksiyonuna karşı savunma ihtiyacını vurgulamaktadır."
    }
  },
  {
    "id": "2601.13197v1",
    "title": "Diffusion-Driven Synthetic Tabular Data Generation for Enhanced DoS/DDoS Attack Classification",
    "authors": [
      "Aravind B",
      "Anirud R. S.",
      "Sai Surya Teja N",
      "Bala Subrahmanya Sriranga Navaneeth A",
      "Karthika R"
    ],
    "published_date": "2026-01-19",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.13197v1",
    "pdf_link": "https://arxiv.org/pdf/2601.13197v1",
    "content": {
      "en": "Class imbalance refers to a situation where certain classes in a dataset have significantly fewer samples than oth- ers, leading to biased model performance. Class imbalance in network intrusion detection using Tabular Denoising Diffusion Probability Models (TabDDPM) for data augmentation is ad- dressed in this paper. Our approach synthesizes high-fidelity minority-class samples from the CIC-IDS2017 dataset through iterative denoising processes. For the minority classes that have smaller samples, synthetic samples were generated and merged with the original dataset. The augmented training data enables an ANN classifier to achieve near-perfect recall on previously underrepresented attack classes. These results establish diffusion models as an effective solution for tabular data imbalance in security domains, with potential applications in fraud detection and medical diagnostics.",
      "tr": "**Makale Başlığı:** Diffusion-Driven Sentetik Tablo Veri Üretimi ile Geliştirilmiş DoS/DDoS Saldırı Sınıflandırması\n\n**Özet:**\n\nClass imbalance, bir veri kümesindeki belirli sınıfların diğerlerine göre anlamlı derecede daha az örneğe sahip olması durumunu ifade eder ve bu da yanlı model performansına yol açar. Bu makalede, ağ saldırı tespiti bağlamında class imbalance, veri artırma amacıyla Tabular Denoising Diffusion Probability Models (TabDDPM) kullanılarak ele alınmaktadır. Yaklaşımımız, CIC-IDS2017 veri kümesinden iteratif denoise etme süreçleri aracılığıyla yüksek doğrulukta azınlık sınıfı örnekleri sentezler. Daha az örneğe sahip azınlık sınıfları için, sentetik örnekler üretilip orijinal veri kümesiyle birleştirilmiştir. Artırılmış eğitim verisi, bir ANN classifier'ın daha önce yeterince temsil edilmeyen saldırı sınıflarında neredeyse mükemmel bir recall elde etmesini sağlamaktadır. Elde edilen bu sonuçlar, diffusion models'ın güvenlik alanlarındaki tablo verisi dengesizliği için etkili bir çözüm olduğunu ve dolandırıcılık tespiti ile tıbbi teşhis gibi alanlarda potansiyel uygulamalara sahip olduğunu göstermektedir."
    }
  },
  {
    "id": "2601.13082v1",
    "title": "Adversarial News and Lost Profits: Manipulating Headlines in LLM-Driven Algorithmic Trading",
    "authors": [
      "Advije Rizvani",
      "Giovanni Apruzzese",
      "Pavel Laskov"
    ],
    "published_date": "2026-01-19",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.13082v1",
    "pdf_link": "https://arxiv.org/pdf/2601.13082v1",
    "content": {
      "en": "Large Language Models (LLMs) are increasingly adopted in the financial domain. Their exceptional capabilities to analyse textual data make them well-suited for inferring the sentiment of finance-related news. Such feedback can be leveraged by algorithmic trading systems (ATS) to guide buy/sell decisions. However, this practice bears the risk that a threat actor may craft \"adversarial news\" intended to mislead an LLM. In particular, the news headline may include \"malicious\" content that remains invisible to human readers but which is still ingested by the LLM. Although prior work has studied textual adversarial examples, their system-wide impact on LLM-supported ATS has not yet been quantified in terms of monetary risk. To address this threat, we consider an adversary with no direct access to an ATS but able to alter stock-related news headlines on a single day. We evaluate two human-imperceptible manipulations in a financial context: Unicode homoglyph substitutions that misroute models during stock-name recognition, and hidden-text clauses that alter the sentiment of the news headline. We implement a realistic ATS in Backtrader that fuses an LSTM-based price forecast with LLM-derived sentiment (FinBERT, FinGPT, FinLLaMA, and six general-purpose LLMs), and quantify monetary impact using portfolio metrics. Experiments on real-world data show that manipulating a one-day attack over 14 months can reliably mislead LLMs and reduce annual returns by up to 17.7 percentage points. To assess real-world feasibility, we analyze popular scraping libraries and trading platforms and survey 27 FinTech practitioners, confirming our hypotheses. We notified trading platform owners of this security issue.",
      "tr": "Elbette, akademik makale başlığını ve özetini istediğiniz şekilde çevirdim:\n\n**Makale Başlığı:** Adversarial News ve Kayıp Kârlar: LLM Güdümlü Algoritmik Ticarette Manşetlerin Manipülasyonu\n\n**Özet:**\n\nLarge Language Models (LLMs), finans sektöründe giderek daha fazla benimsenmektedir. Metin verilerini analiz etme konusundaki olağanüstü yetenekleri, finansla ilgili haberlerin sentimentini (duygu durumunu) çıkarmak için onları oldukça uygun hale getirmektedir. Bu geri bildirimler, algoritmik trading sistemleri (ATS) tarafından al/sat kararlarını yönlendirmek amacıyla kullanılabilir. Ancak bu uygulama, bir tehdit aktörünün bir LLM'yi yanıltmak amacıyla tasarlanmış \"adversarial news\" (düşmanca haberler) üretebilmesi riskini taşır. Özellikle, haber manşeti insan okuyucular tarafından görünmez kalan ancak LLM tarafından yine de işlenen \"kötü niyetli\" içerikler barındırabilir. Önceki çalışmalar metinsel adversarial examples'ları incelemiş olsa da, LLM destekli ATS üzerindeki sistem çapındaki etkileri, parasal risk açısından henüz nicelleştirilmemiştir. Bu tehdide karşı koymak amacıyla, bir ATS'ye doğrudan erişimi olmayan ancak tek bir günde borsa ile ilgili haber manşetlerini değiştirebilen bir saldırganı ele alıyoruz. Finansal bir bağlamda insan tarafından algılanamayan iki manipülasyonu değerlendiriyoruz: borsa adı tanıma sırasında modelleri yanlış yönlendiren Unicode homoglyph ikameleri ve haber manşetinin sentimentini değiştiren gizli metin maddeleri. Backtrader'da, bir LSTM tabanlı fiyat tahminini LLM'den türetilen sentiment ile birleştiren gerçekçi bir ATS uyguluyoruz (FinBERT, FinGPT, FinLLMA ve altı genel amaçlı LLM) ve portföy metriklerini kullanarak parasal etkiyi nicelleştiriyoruz. Gerçek dünya verileri üzerinde yapılan deneyler, 14 aylık bir süre zarfında tek günlük bir saldırının manipülasyonunun LLM'leri güvenilir bir şekilde yanıltabileceğini ve yıllık getirileri %17,7'ye kadar azaltabileceğini göstermektedir. Gerçek dünya fizibilitesini değerlendirmek için popüler scraping kütüphanelerini ve trading platformlarını analiz ediyoruz ve 27 FinTech uygulayıcısını anketleyerek hipotezlerimizi doğruluyoruz. Bu güvenlik sorununu trading platformu sahiplerine bildirdik."
    }
  },
  {
    "id": "2601.12937v1",
    "title": "On the Evidentiary Limits of Membership Inference for Copyright Auditing",
    "authors": [
      "Murat Bilgehan Ertan",
      "Emirhan Böge",
      "Min Chen",
      "Kaleel Mahmood",
      "Marten van Dijk"
    ],
    "published_date": "2026-01-19",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.12937v1",
    "pdf_link": "https://arxiv.org/pdf/2601.12937v1",
    "content": {
      "en": "As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training, despite growing concerns about their reliability under realistic conditions. We ask whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content, and formalize this setting through a judge-prosecutor-accused communication protocol. To test robustness under this protocol, we introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Our experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.",
      "tr": "**Makale Başlığı:** Telif Hakkı Denetimi İçin Üyelik Çıkarımının Kanıtlayıcı Sınırları Üzerine\n\n**Özet:**\n\nBüyük dil modelleri (LLMs) giderek daha opak veri kümeleri üzerinde eğitildikçe, üyelik çıkarım saldırıları (MIAs), artan güvenilirlik endişelerine rağmen, telif hakkıyla korunan metinlerin eğitim sırasında kullanılıp kullanılmadığını denetlemek için önerilmiştir. Savunulabilir bir şekilde üretilmiş metinler aracılığıyla adli telif hakkı anlaşmazlıklarında MIAs'ın kabul edilebilir kanıt olarak hizmet edip edemeyeceğini sorguluyoruz; burada suçlanan model geliştiricisi, anlamsal içeriği korurken eğitim verilerini gizleyebilir. Bu durumu, bir yargıç-savcı-sanık iletişim protokolü aracılığıyla resmileştiriyoruz. Bu protokol altında dayanıklılığı test etmek için, SAGE'ı (Structure-Aware SAE-Guided Extraction) tanıtıyoruz; bu, anlamsal içeriği ve aşağı akış kullanımını korurken sözlüksel yapıyı değiştirmek için eğitim verilerini yeniden yazan, Sparse Autoencoders (SAEs) tarafından yönlendirilen bir paralelizasyon çerçevesidir. Deneylerimiz, SAGE tarafından oluşturulan paralelizasyonlar üzerinde ince ayar yapılmış modellerde en gelişmiş MIAs'ın bozulduğunu göstermektedir; bu da sinyallerinin anlamsal olarak koruyucu dönüşümlere karşı dayanıklı olmadığını göstermektedir. Belirli ince ayar rejimlerinde bir miktar sızıntı devam etse de, bu sonuçlar MIAs'ın düşmanca ortamlarda kırılgan olduğunu ve LLM'lerin telif hakkı denetimi için bağımsız bir mekanizma olarak tek başına yetersiz kaldığını göstermektedir."
    }
  }
]