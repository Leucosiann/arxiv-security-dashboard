[
  {
    "id": "2602.17452v1",
    "title": "Jolt Atlas: Verifiable Inference via Lookup Arguments in Zero Knowledge",
    "authors": [
      "Wyatt Benno",
      "Alberto Centelles",
      "Antoine Douchet",
      "Khalil Gibran"
    ],
    "published_date": "2026-02-19",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.17452v1",
    "pdf_link": "https://arxiv.org/pdf/2602.17452v1",
    "content": {
      "en": "We present Jolt Atlas, a zero-knowledge machine learning (zkML) framework that extends the Jolt proving system to model inference. Unlike zkVMs (zero-knowledge virtual machines), which emulate CPU instruction execution, Jolt Atlas adapts Jolt's lookup-centric approach and applies it directly to ONNX tensor operations. The ONNX computational model eliminates the need for CPU registers and simplifies memory consistency verification. In addition, ONNX is an open-source, portable format, which makes it easy to share and deploy models across different frameworks, hardware platforms, and runtime environments without requiring framework-specific conversions.   Our lookup arguments, which use sumcheck protocol, are well-suited for non-linear functions -- key building blocks in modern ML. We apply optimisations such as neural teleportation to reduce the size of lookup tables while preserving model accuracy, as well as several tensor-level verification optimisations detailed in this paper. We demonstrate that Jolt Atlas can prove model inference in memory-constrained environments -- a prover property commonly referred to as \\textit{streaming}. Furthermore, we discuss how Jolt Atlas achieves zero-knowledge through the BlindFold technique, as introduced in Vega. In contrast to existing zkML frameworks, we show practical proving times for classification, embedding, automated reasoning, and small language models.   Jolt Atlas enables cryptographic verification that can be run on-device, without specialised hardware. The resulting proofs are succinctly verifiable. This makes Jolt Atlas well-suited for privacy-centric and adversarial environments. In a companion work, we outline various use cases of Jolt Atlas, including how it serves as guardrails in agentic commerce and for trustless AI context (often referred to as \\textit{AI memory}).",
      "tr": "İşte akademik makale başlığının ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** Jolt Atlas: Zero Knowledge'ta Lookup Arguments Aracılığıyla Doğrulanabilir Inference\n\n**Özet:**\nJolt Atlas'ı, inference'ı modellemek üzere Jolt ispat sistemini genişleten bir zero-knowledge machine learning (zkML) framework'ü olarak sunuyoruz. CPU komut yürütülmesini taklit eden zkVM'lerin (zero-knowledge virtual machines) aksine, Jolt Atlas, Jolt'un lookup-merkezli yaklaşımını uyarlar ve doğrudan ONNX tensör operasyonlarına uygular. ONNX hesaplama modeli, CPU register'larına olan ihtiyacı ortadan kaldırır ve memory consistency verification'ı basitleştirir. Ek olarak, ONNX açık kaynaklı, taşınabilir bir formattır, bu da framework-spesifik dönüşümler gerektirmeksizin modellerin farklı framework'ler, donanım platformları ve runtime ortamları arasında paylaşılmasını ve dağıtılmasını kolaylaştırır. Sumcheck protocol'ünü kullanan lookup arguments'larımız, modern ML'nin temel yapı taşları olan non-linear fonksiyonlar için oldukça uygundur. Model doğruluğunu korurken lookup table boyutunu azaltmak için neural teleportation gibi optimizasyonlar uyguluyoruz, ayrıca bu makalede detaylandırılan çeşitli tensör seviyesi verification optimizasyonlarını da kullanıyoruz. Jolt Atlas'ın, hafıza kısıtlı ortamlarda model inference'ını ispat edebildiğini gösteriyoruz – bu, prover property'si genellikle *streaming* olarak anılır. Dahası, Jolt Atlas'ın, Vega'da tanıtıldığı şekliyle BlindFold tekniği aracılığıyla zero-knowledge'ı nasıl elde ettiğini tartışıyoruz. Mevcut zkML framework'leriyle karşılaştırıldığında, classification, embedding, automated reasoning ve küçük dil modelleri için pratik proving süreleri sunuyoruz. Jolt Atlas, özel donanım gerektirmeden, on-device çalıştırılabilen kriptografik verification'ı mümkün kılar. Elde edilen ispatlar, succinctly verifiable'dır. Bu, Jolt Atlas'ı privacy-centric ve adversarial ortamlar için oldukça uygun hale getirir. Tamamlayıcı bir çalışmada, Jolt Atlas'ın çeşitli kullanım durumlarını, agentic commerce'da guardrails olarak nasıl hizmet ettiğini ve trustless AI context'i (genellikle *AI memory* olarak anılır) için nasıl kullanıldığını ana hatlarıyla belirtiyoruz."
    }
  },
  {
    "id": "2602.17345v1",
    "title": "What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?",
    "authors": [
      "Boyang Ma",
      "Hechuan Guo",
      "Peizhuo Lv",
      "Minghui Xu",
      "Xuelong Dai"
    ],
    "published_date": "2026-02-19",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.17345v1",
    "pdf_link": "https://arxiv.org/pdf/2602.17345v1",
    "content": {
      "en": "Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation.",
      "tr": "Makale Başlığı: Embodied AI Güvenliğini Ne Bozuyor: LLM Güvenlik Açıkları, CPS Hataları mı, Yoksa Başka Bir Şey mi?\n\nÖzet:\nEmbodied AI sistemleri (örneğin, otonom araçlar, servis robotları ve LLM güdümlü etkileşimli ajanlar), kontrollü ortamlardan hızla güvenlik açısından kritik gerçek dünya kullanımlarına geçiş yapmaktadır. Disembodied AI'dan farklı olarak, embodied intelligence'taki başarısızlıklar geri döndürülemez fiziksel sonuçlara yol açarak güvenlik, emniyet ve güvenilirlik hakkında temel soruları gündeme getirmektedir. Mevcut araştırmalar ağırlıklı olarak embodied AI'yı Large Language Model (LLM) güvenlik açıklarının veya klasik Cyber-Physical System (CPS) arızalarının merceğinden analiz etse de, bu anket, bu perspektiflerin modern embodied sistemlerde gözlemlenen birçok arızayı açıklamak için bireysel olarak yetersiz olduğunu savunmaktadır. İzole model kusurlarından veya geleneksel CPS saldırılarından ziyade, bir grup önemli arızanın embodiment kaynaklı sistem düzeyindeki uyumsuzluklardan kaynaklandığını ileri sürüyoruz. Spesifik olarak, embodied AI'nın neden temel olarak güvence altına alınmasının daha zor olduğunu açıklayan dört temel içgörü tespit ediyoruz: (i) anlamsal doğruluk, dil düzeyindeki reasoning'in geometri, dinamik ve temas kısıtlamalarını soyutlaması nedeniyle fiziksel güvenliği ima etmez; (ii) doğrusal olmayan dinamikler ve durum belirsizliği nedeniyle aynı eylemler farklı fiziksel durumlarda dramatik olarak farklı sonuçlara yol açabilir; (iii) küçük hatalar, sıkıca bağlı algı-karar-eylem döngüleri boyunca yayılır ve büyür; ve (iv) güvenlik, zaman veya sistem katmanları boyunca bileşimsellik göstermez, yerel olarak güvenli kararların küresel olarak güvensiz davranışlara birikmesine olanak tanır. Bu içgörüler, embodied AI'nın güvence altına alınmasının, bileşen düzeyindeki savunmaların ötesine geçerek fiziksel risk, belirsizlik ve arıza yayılımı hakkında sistem düzeyinde reasoning yapmayı gerektirdiğini önermektedir."
    }
  },
  {
    "id": "2602.17223v1",
    "title": "Privacy-Preserving Mechanisms Enable Cheap Verifiable Inference of LLMs",
    "authors": [
      "Arka Pal",
      "Louai Zahran",
      "William Gvozdjak",
      "Akilesh Potti",
      "Micah Goldblum"
    ],
    "published_date": "2026-02-19",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.17223v1",
    "pdf_link": "https://arxiv.org/pdf/2602.17223v1",
    "content": {
      "en": "As large language models (LLMs) continue to grow in size, fewer users are able to host and run models locally. This has led to increased use of third-party hosting services. However, in this setting, there is a lack of guarantees on the computation performed by the inference provider. For example, a dishonest provider may replace an expensive large model with a cheaper-to-run weaker model and return the results from the weaker model to the user. Existing tools to verify inference typically rely on methods from cryptography such as zero-knowledge proofs (ZKPs), but these add significant computational overhead, and remain infeasible for use for large models. In this work, we develop a new insight -- that given a method for performing private LLM inference, one can obtain forms of verified inference at marginal extra cost. Specifically, we propose two new protocols which leverage privacy-preserving LLM inference in order to provide guarantees over the inference that was carried out. Our approaches are cheap, requiring the addition of a few extra tokens of computation, and have little to no downstream impact. As the fastest privacy-preserving inference methods are typically faster than ZK methods, the proposed protocols also improve verification runtime. Our work provides novel insights into the connections between privacy and verifiability in LLM inference.",
      "tr": "Makale Başlığı: Gizlilik Korumalı Mekanizmalar, LLM'lerin Ucuz Doğrulanabilir Çıkarımını Sağlar\n\nÖzet:\nBüyük dil modelleri (LLM'ler) boyut olarak büyümeye devam ettikçe, daha az kullanıcı modelleri yerel olarak barındırabilir ve çalıştırabilir hale gelmektedir. Bu durum, üçüncü taraf barındırma hizmetlerinin kullanımının artmasına yol açmıştır. Ancak, bu ortamda çıkarım (inference) işlemini gerçekleştiren sağlayıcının gerçekleştirdiği hesaplamalara ilişkin garanti eksikliği bulunmaktadır. Örneğin, dürüst olmayan bir sağlayıcı, pahalı bir büyük modeli daha ucuza çalıştırılabilen daha zayıf bir modelle değiştirebilir ve sonuçları kullanıcıya daha zayıf modelden döndürebilir. Çıkarımın doğrulanmasına yönelik mevcut araçlar genellikle sıfır bilgi ispatları (zero-knowledge proofs - ZKPs) gibi kriptografi yöntemlerine dayanır, ancak bunlar önemli hesaplama ek yükü getirir ve büyük modeller için kullanışlı olmaktan uzaktır. Bu çalışmada, yeni bir anlayış geliştiriyoruz: gizlilik korumalı LLM çıkarımı için bir yöntem mevcut olduğunda, marjinal ek maliyetle doğrulanmış çıkarım biçimleri elde edilebilir. Özellikle, gerçekleştirilen çıkarım üzerinde garantiler sağlamak amacıyla gizlilik korumalı LLM çıkarımından yararlanan iki yeni protokol öneriyoruz. Yaklaşımlarımız ucuzdur, az miktarda ek token hesaplaması eklenmesini gerektirir ve aşağı akış üzerinde çok az veya hiç etkiye sahip değildir. En hızlı gizlilik korumalı çıkarım yöntemleri tipik olarak ZK yöntemlerinden daha hızlı olduğundan, önerilen protokoller doğrulama çalışma süresini de iyileştirmektedir. Çalışmamız, LLM çıkarımında gizlilik ve doğrulanabilirlik arasındaki bağlantılara ilişkin yeni içgörüler sunmaktadır."
    }
  },
  {
    "id": "2602.16980v1",
    "title": "Discovering Universal Activation Directions for PII Leakage in Language Models",
    "authors": [
      "Leo Marchyok",
      "Zachary Coalson",
      "Sungho Keum",
      "Sooel Son",
      "Sanghyun Hong"
    ],
    "published_date": "2026-02-19",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.16980v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16980v1",
    "content": {
      "en": "Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.",
      "tr": "Makale Başlığı: Dil Modellerinde Kişisel Tanımlayıcı Bilgi (PII) Sızıntısı için Evrensel Aktivasyon Yönlerinin Keşfi\n\nÖzet:\nModern dil modelleri zengin bir iç yapı sergilemektedir, ancak kişisel tanımlayıcı bilgi (PII) sızıntısı gibi gizlilik açısından hassas davranışların gizli durumları içinde nasıl temsil edildiği ve modüle edildiği hakkında çok az şey bilinmektedir. UniLeak'i sunuyoruz; bu, bir modelin residual stream'inde bulunan ve çıkarım zamanında doğrusal eklenmesinin, çeşitli prompt'lar boyunca PII üretme olasılığını tutarlı bir şekilde artıran evrensel aktivasyon yönlerini belirleyen mekanistik-interpretability bir çerçevedir. Bu modele özgü yönler, bağlamlar arasında genelleşir ve PII üretim olasılığını, üretim kalitesi üzerinde minimal bir etkiyle artırır. UniLeak, eğitim verilerine veya groundtruth PII'ye erişim olmaksızın, yalnızca kendi tarafından üretilen metne dayanarak bu tür yönleri kurtarır. Birden çok model ve veri kümesi üzerinde, bu evrensel yönler boyunca yönlendirme yapmak, mevcut prompt tabanlı extraction yöntemlerine kıyasla PII sızıntısını önemli ölçüde artırır. Sonuçlarımız, PII sızıntısına yeni bir bakış açısı sunmaktadır: modelin temsillerinde latent bir sinyalin süperpozisyonu, hem risk amplification hem de mitigation'a olanak tanır."
    }
  },
  {
    "id": "2602.16977v1",
    "title": "Fail-Closed Alignment for Large Language Models",
    "authors": [
      "Zachary Coalson",
      "Beth Sohler",
      "Aiden Gabriel",
      "Sanghyun Hong"
    ],
    "published_date": "2026-02-19",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.16977v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16977v1",
    "content": {
      "en": "We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.",
      "tr": "Makale Başlığı: Büyük Dil Modelleri İçin Fail-Closed Alignment\n\nÖzet:\nMevcut büyük dil modeli (LLM) alignment'ında yapısal bir zayıflık tespit ediyoruz: modern red mekanizmaları fail-open'dır. Mevcut yaklaşımlar, red davranışlarını birden fazla latent feature üzerinden kodlasa da, tek bir baskın feature'ın prompt-based jailbreak'ler yoluyla bastırılması, alignment'ın çökmesine neden olarak güvensiz üretime yol açabilir. Bu durumdan esinlenerek, sağlam LLM güvenliği için bir tasarım prensibi olarak fail-closed alignment'ı öneriyoruz: red mekanizmaları, redundant, independent causal pathways aracılığıyla kısmi arızalar altında bile etkili kalmalıdır. Bu prensibin somut bir örneğini sunuyoruz: iteratif olarak daha önce öğrenilmiş red yönlerini tanımlayan ve ablate eden, modeli yeni, independent subspaces boyunca güvenliği yeniden yapılandırmaya zorlayan aşamalı bir alignment framework'ü. Dört jailbreak saldırısı boyunca, aşırı red'i azaltırken ve üretim kalitesini korurken, küçük hesaplama yüküyle en güçlü genel dayanıklılığı elde ettik. Mekanistik analizlerimiz, yöntemimizle eğitilen modellerin, prompt-based jailbreak'lerin aynı anda bastıramadığı birden fazla, causally independent red yönünü kodladığını doğrulamaktadır, bu da sağlam LLM güvenliği için prensipli bir temel olarak fail-closed alignment'a ampirik destek sağlamaktadır."
    }
  },
  {
    "id": "2602.16891v1",
    "title": "OpenSage: Self-programming Agent Generation Engine",
    "authors": [
      "Hongwei Li",
      "Zhun Wang",
      "Qinrun Dai",
      "Yuzhou Nie",
      "Jinjun Peng"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.AI",
      "cs.CR",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2602.16891v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16891v1",
    "content": {
      "en": "Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.",
      "tr": "**Makale Başlığı:** OpenSage: Kendi Kendine Programlanan Ajan Üretim Motoru\n\n**Özet:**\n\nAgent development kits (ADKs), ajanların oluşturulması için etkili platformlar ve araçlar sunar ve tasarımları, özellikle ajan topolojisi, araçları ve belleği için sağlanan işlevsellik, oluşturulan ajanların performansında kritik rol oynar. Bununla birlikte, mevcut ADK'ler ya yeterli fonksiyonel desteğe sahip değildir ya da bu bileşenleri manuel olarak tasarlamak için insanlara dayanır, bu da ajanların genelleştirilebilirliğini ve genel performansını sınırlar. Biz OpenSage'i öneriyoruz; bu, LLM'lerin, kapsamlı ve yapılandırılmış bellek desteği sağlarken, kendi kendine üretilen topoloji ve araç setlerine sahip ajanları otomatik olarak oluşturmasına olanak tanıyan ilk ADK'dir. OpenSage, ajanların kendi alt-ajanlarını ve araç setlerini oluşturup yönetmeleri için etkili işlevsellik sunar. Ayrıca, verimli yönetim için hiyerarşik, graph-based memory system'e ve yazılım mühendisliği görevlerine özel olarak hazırlanmış uzmanlaşmış bir toolkit'e sahiptir. Çeşitli backbone modellerle üç adet state-of-the-art benchmark üzerinde yapılan kapsamlı deneyler, OpenSage'in mevcut ADK'lere kıyasla avantajlarını göstermektedir. Ayrıca, her bir bileşenin etkinliğini göstermek için titiz ablation studies gerçekleştirdik. OpenSage'in, insan merkezli yaklaşımlardan AI-centered paradigm'lara geçiş yaparak ajan geliştirmenin bir sonraki neslinin yolunu açabileceğine inanıyoruz."
    }
  },
  {
    "id": "2602.16835v1",
    "title": "NeST: Neuron Selective Tuning for LLM Safety",
    "authors": [
      "Sasha Behrouzi",
      "Lichao Wu",
      "Mohamadreza Rostami",
      "Ahmad-Reza Sadeghi"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.16835v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16835v1",
    "content": {
      "en": "Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains.   We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.",
      "tr": "Elbette, makale başlığı ve özetinin Türkçeye çevirisi aşağıdadır:\n\n**Makale Başlığı:** NeST: LLM Güvenliği için Nöron Seçici Ayarlama\n\n**Özet:**\nSafety alignment, büyük dil modellerinin (LLM) sorumlu bir şekilde kullanıma sunulması için elzemdir. Ancak mevcut yaklaşımlar, model aileleri genelinde güncellenmesi, denetlenmesi ve sürdürülmesi maliyetli olan ağır fine-tuning yöntemlerine sıklıkla dayanmaktadır. Full fine-tuning, önemli hesaplama ve depolama ek yükü getirirken, LoRA gibi parametre-etkin yöntemler, tutarsız safety kazanımları ve tasarım tercihlerine karşı hassasiyet karşılığında verimlilikten ödün verir. Circuit breakers gibi safety müdahale mekanizmaları, model ağırlıklarını değiştirmeden güvensiz çıktıları azaltır, ancak safety behavior'ı yöneten içsel temsilleri doğrudan şekillendirmez veya korumaz. Bu sınırlılıklar, özellikle modellerin sık sık geliştiği veya yeni politikalara ve alanlara uyum sağlaması gereken ortamlarda, hızlı ve güvenilir safety güncellemelerini engellemektedir. Biz, modelin geri kalanını dondurarak küçük bir safety-relevant nöron alt kümesini seçici olarak uyarlayarak refusal behavior'ı güçlendiren, lightweight, structure-aware bir safety alignment framework olan NeST'i sunuyoruz. NeST, fonksiyonel olarak tutarlı safety nöronlarını kümeleyerek ve her küme içinde paylaşılan güncellemeleri zorunlu kılarak parametre güncellemelerini safety behavior'ın içsel organizasyonuyla hizalar, bu da geniş model modifikasyonu veya inference-time overhead olmadan hedeflenmiş ve kararlı safety adaptasyonunu mümkün kılar. NeST'i üç baskın temel modele karşı karşılaştırıyoruz: full fine-tuning, LoRA-based fine-tuning ve circuit breakers, 10 adet açık kaynaklı LLM üzerinde, farklı model ailelerini ve boyutlarını kapsayacak şekilde. Değerlendirilen tüm modellerde, NeST saldırı başarı oranını ortalama %44,5'ten %4,36'ya düşürerek, güvensiz üretimin %90,2'lik bir azalmasına karşılık gelmekte ve ortalama yalnızca 0,44 milyon eğitilebilir parametre gerektirmektedir. Bu, full fine-tuning'e kıyasla güncellenen parametrelerde 17.310 kat, LoRA'ya göre ise 9,25 kat bir azalma anlamına gelirken, alignment için tutarlı bir şekilde daha güçlü safety performansı elde etmektedir."
    }
  },
  {
    "id": "2602.16800v1",
    "title": "Large-scale online deanonymization with LLMs",
    "authors": [
      "Simon Lermen",
      "Daniel Paleka",
      "Joshua Swanson",
      "Michael Aerni",
      "Nicholas Carlini"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.16800v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16800v1",
    "content": {
      "en": "We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would take hours for a dedicated human investigator. We then design attacks for the closed-world setting. Given two databases of pseudonymous individuals, each containing unstructured text written by or about that individual, we implement a scalable attack pipeline that uses LLMs to: (1) extract identity-relevant features, (2) search for candidate matches via semantic embeddings, and (3) reason over top candidates to verify matches and reduce false positives. Compared to prior deanonymization work (e.g., on the Netflix prize) that required structured data or manual feature engineering, our approach works directly on raw user content across arbitrary platforms. We construct three datasets with known ground-truth data to evaluate our attacks. The first links Hacker News to LinkedIn profiles, using cross-platform references that appear in the profiles. Our second dataset matches users across Reddit movie discussion communities; and the third splits a single user's Reddit history in time to create two pseudonymous profiles to be matched. In each setting, LLM-based methods substantially outperform classical baselines, achieving up to 68% recall at 90% precision compared to near 0% for the best non-LLM method. Our results show that the practical obscurity protecting pseudonymous users online no longer holds and that threat models for online privacy need to be reconsidered.",
      "tr": "İşte akademik makale başlığı ve özetinin istediğiniz çevirisi:\n\n**Makale Başlığı:** Large-scale online deanonymization with LLMs\n\n**Özet:**\n\nBüyük dil modellerinin (LLMs) büyük ölçekte kimliksizleştirme (deanonymization) işlemleri için kullanılabileceğini gösteriyoruz. Tam İnternet erişimiyle, ajanımız, takma adlı çevrimiçi profiller ve konuşmalar aracılığıyla, özel bir insan araştırmacısının saatlerini alacak eşleştirmeleri yüksek hassasiyetle gerçekleştirebilmektedir. Hacker News kullanıcılarını ve Anthropic Interviewer katılımcılarını yeniden tanımlayabilmektedir. Ardından, kapalı dünya (closed-world) ortamı için saldırılar tasarlıyoruz. Her biri bireye ait veya birey hakkında yazılmış yapılandırılmamış metinler içeren iki takma adlı birey veritabanı verildiğinde, LLM'leri kullanarak ölçeklenebilir bir saldırı hattı uyguluyoruz: (1) kimlik açısından ilgili özelliklerin çıkarılması, (2) anlamsal embeddings aracılığıyla aday eşleşmelerin aranması ve (3) eşleşmeleri doğrulamak ve yanlış pozitifleri azaltmak için en iyi adaylar üzerinde reasoning yapılması. Yapılandırılmış veri veya manuel özellik mühendisliği gerektiren önceki kimliksizleştirme çalışmalarıyla (örneğin, Netflix prize üzerinde) karşılaştırıldığında, yaklaşımımız rastgele platformlardaki ham kullanıcı içeriği üzerinde doğrudan çalışmaktadır. Saldırılarımızı değerlendirmek için bilinen yer gerçeği (ground-truth) verilerine sahip üç veri kümesi oluşturduk. Birincisi, profillerde görünen platformlar arası referansları kullanarak Hacker News'i LinkedIn profilleriyle ilişkilendirir. İkinci veri kümemiz Reddit film tartışma topluluklarındaki kullanıcıları eşleştirir; üçüncü veri kümesi ise tek bir kullanıcının Reddit geçmişini zaman içinde bölerek eşleştirilecek iki takma adlı profil oluşturur. Her bir senaryoda, LLM tabanlı yöntemler, en iyi LLM olmayan yöntemin neredeyse %0'ına kıyasla %90 hassasiyette %68'e varan recall elde ederek klasik taban çizgilerini (baselines) önemli ölçüde aşmaktadır. Sonuçlarımız, takma adlı kullanıcıları çevrimiçi ortamda koruyan pratik gizliliğin (practical obscurity) artık geçerli olmadığını ve çevrimiçi gizlilik için tehdit modellerinin yeniden gözden geçirilmesi gerektiğini göstermektedir."
    }
  },
  {
    "id": "2602.16708v2",
    "title": "Policy Compiler for Secure Agentic Systems",
    "authors": [
      "Nils Palumbo",
      "Sarthak Choudhary",
      "Jihye Choi",
      "Prasad Chalasani",
      "Somesh Jha"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2602.16708v2",
    "pdf_link": "https://arxiv.org/pdf/2602.16708v2",
    "content": {
      "en": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.   Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.   PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "tr": "**Makale Başlığı:** Secure Agentic Systems için Policy Compiler\n\n**Özet:**\n\nLLM tabanlı ajanlar, giderek artan bir şekilde karmaşık yetkilendirme politikaları gerektiren bağlamlarda konuşlandırılmaktadır: müşteri hizmetleri protokolleri, onay iş akışları, veri erişim kısıtlamaları ve düzenleyici uyumluluk. Bu politikaların prompt'lara gömülmesi, herhangi bir yaptırım garantisi sunmamaktadır. Biz, deterministik politika yaptırımı sağlayan Agentic Systems için bir Policy Compiler olan PCAS'ı sunuyoruz. Bu tür politikaları yaptırmak, ajanlar arasındaki bilgi akışını takip etmeyi gerektirir ki bu, doğrusal mesaj geçmişleri ile yakalanamaz. Bunun yerine, PCAS ajan sistem durumunu, tool çağrıları, tool sonuçları ve mesajlar gibi olaylar arasındaki nedensel ilişkileri yakalayan bir dependency graph olarak modeller. Politikalar, transitive information flow ve cross-agent provenance'ı hesaba katan deklaratif kurallar olarak, Datalog'dan türetilmiş bir dilde ifade edilir. Bir reference monitor tüm eylemleri engeller ve yürütmeden önce ihlalleri durdurarak, model reasoning'den bağımsız deterministik yaptırım sağlar. PCAS, mevcut bir ajan uygulaması ve bir politika spesifikasyonu alır ve bunları, herhangi bir güvenlik özelinde yeniden yapılandırma gerektirmeyen, construction by policy-compliant olarak enstrümente edilmiş bir sisteme derler. PCAS'ı üç vaka çalışmasında değerlendiriyoruz: prompt injection savunması için information flow politikaları, multi-agent pharmacovigilance sisteminde approval workflows ve müşteri hizmetleri için organizational policies. Müşteri hizmetleri görevlerinde, PCAS, frontier modellerde politika uyumluluğunu %48'den %93'e yükseltir ve enstrümante edilmiş çalışmalarda sıfır politika ihlali ile sonuçlanır."
    }
  },
  {
    "id": "2602.16596v1",
    "title": "Sequential Membership Inference Attacks",
    "authors": [
      "Thomas Michel",
      "Debabrota Basu",
      "Emilie Kaufmann"
    ],
    "published_date": "2026-02-18",
    "tags": [
      "cs.LG",
      "cs.CR",
      "math.ST",
      "stat.ML"
    ],
    "link": "http://arxiv.org/abs/2602.16596v1",
    "pdf_link": "https://arxiv.org/pdf/2602.16596v1",
    "content": {
      "en": "Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.",
      "tr": "**Makale Başlığı:** Ardışık Üyelik Çıkarım Saldırıları\n\n**Özet:**\n\nModern AI modelleri durağan değildir; yaşam döngüleri boyunca birçok güncelleme geçirirler. Bu nedenle, daha güçlü Membership Inference (MI) saldırıları ve daha sıkı gizlilik denetimleri oluşturmak için model dinamiklerinden yararlanmak güncel sorulardır. Literatür, model güncellemelerinden oluşan bir dizinin MI saldırılarının gücünü artırabildiğini ampirik olarak gösterse de, \"optimal\" MI saldırılarının titiz analizi, sonsuz örneklem içeren durağan modellerle sınırlıdır. Bu nedenle, hedefli bir veri parçasının belirli bir güncelleme adımında varlığını belirlemek için model güncelleştirmeleri dizisini kullanan \"optimal\" bir MI saldırısı olan SeMI*'yi geliştiriyoruz. Ampirik ortalama hesaplaması için, gizlilikle veya gizlilik olmaksızın sonlu sayıda örneğe erişim sağlayarak SeMI*'nin optimal gücünü türetiyoruz. Elde ettiğimiz sonuçlar mevcut asimptotik analizleri tekrar ortaya koymaktadır. Model dizisine erişimin, MI sinyallerinin seyreltilmesini engellediğini gözlemliyoruz; bu durum, MI sinyalinin eğitim verileri biriktikçe kaybolduğu son model üzerindeki mevcut saldırılardan farklıdır. Dahası, bir saldırgan, hem ekleme zamanını hem de \"canary\"yi ayarlamak için SeMI*'yi kullanarak daha sıkı gizlilik denetimleri sağlayabilir. Son olarak, DP-SGD ile eğitilmiş veya ince ayarlanmış veri dağılımları ve modelleri üzerinden deneyler yaparak, SeMI*'nin pratik varyantlarının temel modellere göre daha sıkı gizlilik denetimleri sağladığını gösteriyoruz."
    }
  }
]