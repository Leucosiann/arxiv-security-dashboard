[
  {
    "id": "2601.20548v1",
    "title": "IoT Device Identification with Machine Learning: Common Pitfalls and Best Practices",
    "authors": [
      "Kahraman Kostas",
      "Rabia Yasa Kostas"
    ],
    "published_date": "2026-01-28",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2601.20548v1",
    "pdf_link": "https://arxiv.org/pdf/2601.20548v1",
    "content": {
      "en": "This paper critically examines the device identification process using machine learning, addressing common pitfalls in existing literature. We analyze the trade-offs between identification methods (unique vs. class based), data heterogeneity, feature extraction challenges, and evaluation metrics. By highlighting specific errors, such as improper data augmentation and misleading session identifiers, we provide a robust guideline for researchers to enhance the reproducibility and generalizability of IoT security models.",
      "tr": "Makale Başlığı: IoT Cihaz Kimlik Tespiti ve Makine Öğrenmesi: Yaygın Tuzaklar ve En İyi Uygulamalar\n\nÖzet:\nBu makale, mevcut literatürdeki yaygın tuzakları ele alarak, makine öğrenmesi kullanarak cihaz kimlik tespiti sürecini eleştirel bir şekilde incelemektedir. Kimlik tespiti yöntemleri (unique vs. class based) arasındaki ödünleşmeleri, veri heterojenliğini, feature extraction zorluklarını ve evaluation metrics'i analiz etmekteyiz. Improper data augmentation ve misleading session identifiers gibi spesifik hataları vurgulayarak, araştırmacıların IoT security models'in reproducibility ve generalizability'sini geliştirmeleri için sağlam bir rehber sunmaktayız."
    }
  },
  {
    "id": "2601.20346v1",
    "title": "Multimodal Multi-Agent Ransomware Analysis Using AutoGen",
    "authors": [
      "Asifullah Khan",
      "Aimen Wadood",
      "Mubashar Iqbal",
      "Umme Zahoora"
    ],
    "published_date": "2026-01-28",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.20346v1",
    "pdf_link": "https://arxiv.org/pdf/2601.20346v1",
    "content": {
      "en": "Ransomware has become one of the most serious cybersecurity threats causing major financial losses and operational disruptions worldwide.Traditional detection methods such as static analysis, heuristic scanning and behavioral analysis often fall short when used alone. To address these limitations, this paper presents multimodal multi agent ransomware analysis framework designed for ransomware classification. Proposed multimodal multiagent architecture combines information from static, dynamic and network sources. Each data type is handled by specialized agent that uses auto encoder based feature extraction. These representations are then integrated through a fusion agent. After that fused representation are used by transformer based classifier. It identifies the specific ransomware family. The agents interact through an interagent feedback mechanism that iteratively refines feature representations by suppressing low confidence information. The framework was evaluated on large scale datasets containing thousands of ransomware and benign samples. Multiple experiments were conducted on ransomware dataset. It outperforms single modality and nonadaptive fusion baseline achieving improvement of up to 0.936 in Macro-F1 for family classification and reducing calibration error. Over 100 epochs, the agentic feedback loop displays a stable monotonic convergence leading to over +0.75 absolute improvement in terms of agent quality and a final composite score of around 0.88 without fine tuning of the language models. Zeroday ransomware detection remains family dependent on polymorphism and modality disruptions. Confidence aware abstention enables reliable real world deployment by favoring conservativeand trustworthy decisions over forced classification. The findings indicate that proposed approach provides a practical andeffective path toward improving real world ransomware defense systems.",
      "tr": "**Makale Başlığı: Multimodal Multi-Agent Ransomware Analysis Using AutoGen**\n\n**Özet:**\n\nRansomware, dünya çapında büyük finansal kayıplara ve operasyonel kesintilere neden olan en ciddi siber güvenlik tehditlerinden biri haline gelmiştir. Statik analiz, sezgisel tarama ve davranış analizi gibi geleneksel tespit yöntemleri, tek başlarına kullanıldığında genellikle yetersiz kalmaktadır. Bu sınırlılıkların üstesinden gelmek amacıyla, bu makale ransomware sınıflandırması için tasarlanmış multimodal multi-agent ransomware analysis frameworkünü sunmaktadır. Önerilen multimodal multi-agent mimarisi, statik, dinamik ve ağ kaynaklarından gelen bilgileri birleştirmektedir. Her veri türü, auto encoder tabanlı feature extraction kullanan uzman bir agent tarafından işlenir. Bu temsiller daha sonra bir fusion agent aracılığıyla entegre edilir. Ardından, bu fused representation transformer tabanlı bir classifier tarafından kullanılır. Bu classifier, belirli ransomware ailesini tanımlar. Agentlar, düşük güvenilirliğe sahip bilgileri baskılayarak feature representations'ı tekrarlayan bir şekilde iyileştiren bir inter-agent feedback mechanism aracılığıyla etkileşim kurar. Framework, binlerce ransomware ve zararsız örneği içeren büyük ölçekli veri kümelerinde değerlendirilmiştir. Ransomware veri kümesi üzerinde çok sayıda deney gerçekleştirilmiştir. Bu deneyler, tekil modalite ve adaptif olmayan fusion baseline'larına kıyasla daha iyi performans göstererek, family classification için Macro-F1'de 0.936'ya varan bir iyileşme sağlamış ve kalibrasyon hatasını azaltmıştır. 100'den fazla epoch boyunca, agentic feedback loop, agent kalitesi açısından +0.75'in üzerinde mutlak bir iyileşme ve language models'in fine-tuning'i olmaksızın yaklaşık 0.88'lik bir final composite score ile istikrarlı monoton bir yakınsama sergilemiştir. Zeroday ransomware tespiti, polimorfizm ve modality disruptions'a bağlı aile bağımlılığını sürdürmektedir. Confidence aware abstention, zorunlu sınıflandırma yerine muhafazakar ve güvenilir kararları tercih ederek güvenilir gerçek dünya uygulamalarına olanak tanır. Bulgular, önerilen yaklaşımın, gerçek dünya ransomware savunma sistemlerini iyileştirmeye yönelik pratik ve etkili bir yol sunduğunu göstermektedir."
    }
  },
  {
    "id": "2601.20310v1",
    "title": "SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks",
    "authors": [
      "Xin Zhang",
      "Zijin Yang",
      "Kejiang Chen",
      "Linfeng Ma",
      "Weiming Zhang"
    ],
    "published_date": "2026-01-28",
    "tags": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.20310v1",
    "pdf_link": "https://arxiv.org/pdf/2601.20310v1",
    "content": {
      "en": "Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.",
      "tr": "İşte istenen çeviri:\n\n**Makale Başlığı:** SemBind: Siyah Kutu Sahtecilik Saldırılarına Karşı Semantiklere Bağlanmış Difüzyon Filigranları\n\n**Özet:**\nLatent diffusion modellerinin (LDMs) üretim sürecine entegre edilmiş latent tabanlı filigranlar, üretilen görüntülerin tespitini ve kaynak gösterilmesini kolaylaştırır. Bununla birlikte, bir saldırganın en az bir filigranlı görüntüye ve sağlayıcının modeline black-box erişimine ihtiyaç duyduğu son zamanlardaki black-box forgery attacks, sağlayıcı tarafından üretilmeyen görüntülere sağlayıcının filigranını gömebilir ve bu da köken ve güven için orantısız bir risk oluşturur. Biz, learned semantic masker aracılığıyla latent sinyallerini görüntü semantiğine bağlayarak black-box forgery'ye direnen ilk latent tabanlı filigranlar için savunma çerçevesi olan SemBind'i öneriyoruz. Contrastive learning ile eğitilen masker, aynı prompt için neredeyse değişmeyen kodlar ve prompt'lar arasında neredeyse ortogonal kodlar üretir; bu kodlar, herhangi bir standart latent tabanlı filigrandan önce hedef latent'i modüle etmek için yeniden şekillendirilir ve permüte edilir. SemBind, mevcut latent tabanlı watermark scheme'leriyle genel olarak uyumludur ve görüntü kalitesini esasen değişmeden tutar, aynı zamanda basit bir mask-ratio parametresi anti-forgery gücü ve robustness arasında ayarlanabilir bir denge sunar. Dört ana akım latent tabanlı watermark yöntemi boyunca, SemBind etkinleştirilmiş anti-forgery varyantlarımız, black-box forgery altında yanlış kabulü belirgin şekilde azaltırken kontrol edilebilir bir robustness-security dengesi sağlar."
    }
  },
  {
    "id": "2601.20270v1",
    "title": "Eliciting Least-to-Most Reasoning for Phishing URL Detection",
    "authors": [
      "Holly Trikilis",
      "Pasindu Marasinghe",
      "Fariza Rashid",
      "Suranga Seneviratne"
    ],
    "published_date": "2026-01-28",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.20270v1",
    "pdf_link": "https://arxiv.org/pdf/2601.20270v1",
    "content": {
      "en": "Phishing continues to be one of the most prevalent attack vectors, making accurate classification of phishing URLs essential. Recently, large language models (LLMs) have demonstrated promising results in phishing URL detection. However, their reasoning capabilities that enabled such performance remain underexplored. To this end, in this paper, we propose a Least-to-Most prompting framework for phishing URL detection. In particular, we introduce an \"answer sensitivity\" mechanism that guides Least-to-Most's iterative approach to enhance reasoning and yield higher prediction accuracy. We evaluate our framework using three URL datasets and four state-of-the-art LLMs, comparing against a one-shot approach and a supervised model. We demonstrate that our framework outperforms the one-shot baseline while achieving performance comparable to that of the supervised model, despite requiring significantly less training data. Furthermore, our in-depth analysis highlights how the iterative reasoning enabled by Least-to-Most, and reinforced by our answer sensitivity mechanism, drives these performance gains. Overall, we show that this simple yet powerful prompting strategy consistently outperforms both one-shot and supervised approaches, despite requiring minimal training or few-shot guidance. Our experimental setup can be found in our Github repository github.sydney.edu.au/htri0928/least-to-most-phishing-detection.",
      "tr": "İşte akademik makale başlığının ve özetinin istenen çevirisi:\n\n**Makale Başlığı:** Eliciting Least-to-Most Reasoning for Phishing URL Detection\n\n**Özet:**\nPhishing, en yaygın saldırı vektörlerinden biri olmaya devam etmekte, bu da phishing URL'lerinin doğru sınıflandırılmasını zorunlu kılmaktadır. Son zamanlarda, large language models (LLMs), phishing URL tespiti alanında umut verici sonuçlar sergilemiştir. Ancak, bu performansa olanak tanıyan reasoning yetenekleri yeterince araştırılmamıştır. Bu amaçla, bu makalede, phishing URL tespiti için bir Least-to-Most prompting framework'ü sunmaktayız. Özellikle, reasoning'i geliştirmek ve daha yüksek tahmin doğruluğu elde etmek için Least-to-Most'un iteratif yaklaşımını yönlendiren bir \"answer sensitivity\" mekanizması tanıtıyoruz. Framework'ümüzü, üç URL veri kümesi ve dört state-of-the-art LLM kullanarak değerlendirmekteyiz; bunları bir one-shot yaklaşımı ve bir supervised model ile karşılaştırıyoruz. Framework'ümüzün, one-shot baseline'ından daha iyi performans gösterdiğini, buna rağmen önemli ölçüde daha az training data gerektirerek supervised model ile karşılaştırılabilir bir performans elde ettiğini göstermekteyiz. Dahası, derinlemesine analizimiz, Least-to-Most'un sağladığı ve answer sensitivity mekanizmamız tarafından güçlendirilen iteratif reasoning'in bu performans artışlarını nasıl sağladığını vurgulamaktadır. Genel olarak, bu basit ancak güçlü prompting stratejisinin, minimal training veya few-shot guidance gerektirmesine rağmen, hem one-shot hem de supervised yaklaşımlarından tutarlı bir şekilde daha iyi performans gösterdiğini göstermekteyiz. Deneysel kurulumumuza github.sydney.edu.au/htri0928/least-to-most-phishing-detection adresindeki Github reposunda ulaşılabilir."
    }
  },
  {
    "id": "2601.19970v1",
    "title": "Benchmarking LLAMA Model Security Against OWASP Top 10 For LLM Applications",
    "authors": [
      "Nourin Shahin",
      "Izzat Alsmadi"
    ],
    "published_date": "2026-01-27",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.19970v1",
    "pdf_link": "https://arxiv.org/pdf/2601.19970v1",
    "content": {
      "en": "As large language models (LLMs) move from research prototypes to enterprise systems, their security vulnerabilities pose serious risks to data privacy and system integrity. This study benchmarks various Llama model variants against the OWASP Top 10 for LLM Applications framework, evaluating threat detection accuracy, response safety, and computational overhead. Using the FABRIC testbed with NVIDIA A30 GPUs, we tested five standard Llama models and five Llama Guard variants on 100 adversarial prompts covering ten vulnerability categories. Our results reveal significant differences in security performance: the compact Llama-Guard-3-1B model achieved the highest detection rate of 76% with minimal latency (0.165s per test), whereas base models such as Llama-3.1-8B failed to detect threats (0% accuracy) despite longer inference times (0.754s). We observe an inverse relationship between model size and security effectiveness, suggesting that smaller, specialized models often outperform larger general-purpose ones in security tasks. Additionally, we provide an open-source benchmark dataset including adversarial prompts, threat labels, and attack metadata to support reproducible research in AI security, [1].",
      "tr": "**Makale Başlığı:** LLAMA Modelinin OWASP Top 10 LLM Uygulamaları Kapsamında Güvenliğinin Karşılaştırılması\n\n**Özet:**\nBüyük dil modelleri (LLM'ler) araştırma prototiplerinden kurumsal sistemlere doğru ilerlerken, güvenlik açıkları veri gizliliği ve sistem bütünlüğü için ciddi riskler oluşturmaktadır. Bu çalışma, çeşitli Llama model varyantlarını OWASP Top 10 for LLM Applications çerçevesi ile karşılaştırmalı olarak inceleyerek tehdit algılama doğruluğunu, yanıt güvenliğini ve hesaplama yükünü değerlendirmektedir. NVIDIA A30 GPU'lar ile FABRIC testbed'ini kullanarak, on bir güvenlik açığı kategorisini kapsayan 100 düşmanca (adversarial) komut dizisi üzerinde beş standart Llama modeli ve beş Llama Guard varyantını test ettik. Sonuçlarımız, güvenlik performansında önemli farklılıklar ortaya koymaktadır: Kompakt Llama-Guard-3-1B modeli, minimal gecikme (test başına 0.165s) ile %76 en yüksek algılama oranını elde ederken, Llama-3.1-8B gibi temel modeller, daha uzun çıkarım sürelerine (0.754s) rağmen tehditleri algılamada başarısız olmuştur (%0 doğruluk). Model boyutu ile güvenlik etkinliği arasında ters bir ilişki gözlemlemekteyiz; bu da daha küçük, özelleşmiş modellerin güvenlik görevlerinde genellikle daha büyük genel amaçlı modellere göre daha iyi performans gösterdiğini düşündürmektedir. Ek olarak, yapay zeka güvenliği alanında tekrarlanabilir araştırmaları desteklemek amacıyla düşmanca (adversarial) komut dizileri, tehdit etiketleri ve saldırı meta verilerini içeren açık kaynaklı bir kıyaslama veri kümesi sunuyoruz [1]."
    }
  },
  {
    "id": "2601.19768v1",
    "title": "GAVEL: Towards rule-based safety through activation monitoring",
    "authors": [
      "Shir Rozenfeld",
      "Rahul Pankajakshan",
      "Itay Zloczower",
      "Eyal Lenga",
      "Gilad Gressel"
    ],
    "published_date": "2026-01-27",
    "tags": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.19768v1",
    "pdf_link": "https://arxiv.org/pdf/2601.19768v1",
    "content": {
      "en": "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.",
      "tr": "**Makale Başlığı:** GAVEL: Aktivasyon İzleme Yoluyla Kural Tabanlı Güvenliğe Doğru\n\n**Özet:**\n\nBüyük dil modelleri (LLM'ler), yüzey metni seviyesinde belirgin olmayabilecek zararlı davranışları tespit etmek ve önlemek için giderek artan bir şekilde aktivasyon tabanlı izleme ile eşleştirilmektedir. Ancak, geniş kötüye kullanım veri kümeleri üzerinde eğitilmiş mevcut aktivasyon güvenliği yaklaşımları, düşük hassasiyet, sınırlı esneklik ve yorumlanabilirlik eksikliği ile mücadele etmektedir. Bu makale, siber güvenlik alanındaki kural paylaşım uygulamalarından ilham alan yeni bir paradigma sunmaktadır: kural tabanlı aktivasyon güvenliği. Aktivasyonları, \"tehdit oluşturma\" ve \"ödeme işleme\" gibi ince taneli, yorumlanabilir faktörler olan bilişsel öğeler (CE'ler) olarak modellemeyi öneriyoruz; bunlar, daha yüksek hassasiyetle nüanslı, alana özgü davranışları yakalamak için birleştirilebilir. Bu temsile dayanarak, CE'ler üzerinde yüklem kuralları tanımlayan ve ihlalleri gerçek zamanlı olarak tespit eden pratik bir çerçeve sunuyoruz. Bu, uygulayıcıların modelleri veya dedektörleri yeniden eğitmeden güvenlik önlemlerini yapılandırmasına ve güncellemesine olanak tanırken, şeffaflığı ve denetlenebilirliği de desteklemektedir. Sonuçlarımız, bileşimsel kural tabanlı aktivasyon güvenliğinin hassasiyeti iyileştirdiğini, alan özelleştirmesini desteklediğini ve ölçeklenebilir, yorumlanabilir ve denetlenebilir Yapay Zeka yönetişimi için zemin hazırladığını göstermektedir. GAVEL'i açık kaynaklı bir çerçeve olarak yayınlayacak ve eşlik eden otomatik kural oluşturma aracı sağlayacağız."
    }
  },
  {
    "id": "2601.19726v1",
    "title": "RvB: Automating AI System Hardening via Iterative Red-Blue Games",
    "authors": [
      "Lige Huang",
      "Zicheng Liu",
      "Jie Zhang",
      "Lewen Yan",
      "Dongrui Liu"
    ],
    "published_date": "2026-01-27",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2601.19726v1",
    "pdf_link": "https://arxiv.org/pdf/2601.19726v1",
    "content": {
      "en": "The dual offensive and defensive utility of Large Language Models (LLMs) highlights a critical gap in AI security: the lack of unified frameworks for dynamic, iterative adversarial adaptation hardening. To bridge this gap, we propose the Red Team vs. Blue Team (RvB) framework, formulated as a training-free, sequential, imperfect-information game. In this process, the Red Team exposes vulnerabilities, driving the Blue Team to learning effective solutions without parameter updates. We validate our framework across two challenging domains: dynamic code hardening against CVEs and guardrail optimization against jailbreaks. Our empirical results show that this interaction compels the Blue Team to learn fundamental defensive principles, leading to robust remediations that are not merely overfitted to specific exploits. RvB achieves Defense Success Rates of 90\\% and 45\\% across the respective tasks while maintaining near 0\\% False Positive Rates, significantly surpassing baselines. This work establishes the iterative adversarial interaction framework as a practical paradigm that automates the continuous hardening of AI systems.",
      "tr": "Makale Başlığı: RvB: Yinelemeli Kırmızı-Mavi Oyunlar Aracılığıyla Yapay Zeka Sistemi Sertleştirmeyi Otomatikleştirme\n\nÖzet:\nBüyük Dil Modellerinin (LLMs) hem saldırı hem de savunma amaçlı çift yönlü kullanımları, yapay zeka güvenliğinde kritik bir boşluğa işaret etmektedir: dinamik, yinelemeli düşmanca adaptasyon sertleştirmesi için birleşik çerçevelerin yokluğu. Bu boşluğu doldurmak amacıyla, eğitim gerektirmeyen, sıralı, kusurlu bilgi oyunu olarak formüle edilen Kırmızı Takım - Mavi Takım (RvB) çerçevesini sunuyoruz. Bu süreçte Kırmızı Takım, parametre güncellemeleri olmadan Mavi Takım'ı etkili çözümler öğrenmeye zorlayarak güvenlik açıklarını ortaya çıkarır. Çerçevemizi iki zorlu alanda doğruluyoruz: CVE'lere karşı dinamik kod sertleştirmesi ve jailbreak'lere karşı guardrail optimizasyonu. Ampirik sonuçlarımız, bu etkileşimin Mavi Takım'ı temel savunma prensiplerini öğrenmeye zorladığını ve yalnızca belirli istismarlara aşırı uyum sağlanmış olmayan sağlam iyileştirmelere yol açtığını göstermektedir. RvB, ilgili görevlerde sırasıyla %90 ve %45 Savunma Başarı Oranları elde ederken, %0'a yakın Yanlış Pozitif Oranlarını koruyarak karşılaştırmalı analizleri önemli ölçüde geride bırakmaktadır. Bu çalışma, yinelemeli düşmanca etkileşim çerçevesini, yapay zeka sistemlerinin sürekli sertleştirmesini otomatikleştiren pratik bir paradigma olarak tesis etmektedir."
    }
  },
  {
    "id": "2601.19448v1",
    "title": "From Internal Diagnosis to External Auditing: A VLM-Driven Paradigm for Online Test-Time Backdoor Defense",
    "authors": [
      "Binyan Xu",
      "Fan Yang",
      "Xilin Dai",
      "Di Tang",
      "Kehuan Zhang"
    ],
    "published_date": "2026-01-27",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.19448v1",
    "pdf_link": "https://arxiv.org/pdf/2601.19448v1",
    "content": {
      "en": "Deep Neural Networks remain inherently vulnerable to backdoor attacks. Traditional test-time defenses largely operate under the paradigm of internal diagnosis methods like model repairing or input robustness, yet these approaches are often fragile under advanced attacks as they remain entangled with the victim model's corrupted parameters. We propose a paradigm shift from Internal Diagnosis to External Semantic Auditing, arguing that effective defense requires decoupling safety from the victim model via an independent, semantically grounded auditor. To this end, we present a framework harnessing Universal Vision-Language Models (VLMs) as evolving semantic gatekeepers. We introduce PRISM (Prototype Refinement & Inspection via Statistical Monitoring), which overcomes the domain gap of general VLMs through two key mechanisms: a Hybrid VLM Teacher that dynamically refines visual prototypes online, and an Adaptive Router powered by statistical margin monitoring to calibrate gating thresholds in real-time. Extensive evaluation across 17 datasets and 11 attack types demonstrates that PRISM achieves state-of-the-art performance, suppressing Attack Success Rate to <1% on CIFAR-10 while improving clean accuracy, establishing a new standard for model-agnostic, externalized security.",
      "tr": "Makale Başlığı: İç Teşhisten Harici Denetime: Çevrimiçi Test Zamanlı Arka Kapı Savunması İçin VLM Güdümlü Bir Paradigma\n\nÖzet:\nDerin Sinir Ağları, arka kapı saldırılarına karşı doğası gereği savunmasız kalmaktadır. Geleneksel test zamanlı savunmalar büyük ölçüde model onarımı veya girdi sağlamlığı gibi internal diagnosis yöntemlerinin paradigması altında işlemektedir, ancak bu yaklaşımlar genellikle gelişmiş saldırılar altında kırılgan olup, kurban modelin bozulmuş parametreleriyle iç içe geçmişlerdir. Biz, etkili savunmanın güvenliği, bağımsız, semantik olarak temellendirilmiş bir denetleyici aracılığıyla kurban modelden ayrıştırmayı gerektirdiği argümanıyla, Internal Diagnosis'tan External Semantic Auditing'e bir paradigma kayması öneriyoruz. Bu amaçla, evrimleşen semantik kapı bekçileri olarak Universal Vision-Language Models (VLMs)'ı kullanan bir çerçeve sunuyoruz. Genel VLMs'ın domain gap'ini iki anahtar mekanizma aracılığıyla aşan PRISM'i (Prototype Refinement & Inspection via Statistical Monitoring) tanıtıyoruz: görsel prototipleri dinamik olarak çevrimiçi iyileştiren bir Hybrid VLM Teacher ve gating eşiklerini gerçek zamanlı olarak kalibre etmek için statistical margin monitoring ile güçlendirilmiş bir Adaptive Router. 17 veri seti ve 11 saldırı türü boyunca yapılan kapsamlı değerlendirmeler, PRISM'in state-of-the-art performans gösterdiğini, CIFAR-10'da Attack Success Rate'i <%1'e düşürdüğünü ve clean accuracy'yi iyileştirdiğini göstermekte, böylece model-agnostic, externalized security için yeni bir standart belirlemektedir."
    }
  },
  {
    "id": "2601.19367v1",
    "title": "CHEHAB RL: Learning to Optimize Fully Homomorphic Encryption Computations",
    "authors": [
      "Bilel Sefsaf",
      "Abderraouf Dandani",
      "Abdessamed Seddiki",
      "Arab Mohammed",
      "Eduardo Chielle"
    ],
    "published_date": "2026-01-27",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.19367v1",
    "pdf_link": "https://arxiv.org/pdf/2601.19367v1",
    "content": {
      "en": "Fully Homomorphic Encryption (FHE) enables computations directly on encrypted data, but its high computational cost remains a significant barrier. Writing efficient FHE code is a complex task requiring cryptographic expertise, and finding the optimal sequence of program transformations is often intractable. In this paper, we propose CHEHAB RL, a novel framework that leverages deep reinforcement learning (RL) to automate FHE code optimization. Instead of relying on predefined heuristics or combinatorial search, our method trains an RL agent to learn an effective policy for applying a sequence of rewriting rules to automatically vectorize scalar FHE code while reducing instruction latency and noise growth. The proposed approach supports the optimization of both structured and unstructured code. To train the agent, we synthesize a diverse dataset of computations using a large language model (LLM). We integrate our proposed approach into the CHEHAB FHE compiler and evaluate it on a suite of benchmarks, comparing its performance against Coyote, a state-of-the-art vectorizing FHE compiler. The results show that our approach generates code that is $5.3\\times$ faster in execution, accumulates $2.54\\times$ less noise, while the compilation process itself is $27.9\\times$ faster than Coyote (geometric means).",
      "tr": "Makale Başlığı: CHEHAB RL: Tam Homomorfik Şifreleme Hesaplamalarını Optimize Etmeyi Öğrenme\n\nÖzet:\nFully Homomorphic Encryption (FHE), şifrelenmiş veriler üzerinde doğrudan hesaplamalar yapılmasına olanak tanır, ancak yüksek hesaplama maliyeti önemli bir engel olmaya devam etmektedir. Verimli FHE kodu yazmak, kriptografik uzmanlık gerektiren karmaşık bir görevdir ve program dönüşümlerinin optimal dizisini bulmak genellikle çözülemezdir. Bu makalede, FHE kod optimizasyonunu otomatikleştirmek için derin pekiştirmeli öğrenme (RL) kullanan yeni bir çerçeve olan CHEHAB RL'yi öneriyoruz. Önceden tanımlanmış sezgisel yöntemlere veya kombinatoryal aramaya güvenmek yerine, yöntemimiz bir RL ajanını, skaler FHE kodunu otomatik olarak vektörleştirmek için bir dizi yeniden yazma kuralını uygulama ve böylece komut gecikmesini ve gürültü artışını azaltma konusunda etkili bir politika öğrenmesi için eğitir. Önerilen yaklaşım, hem yapılandırılmış hem de yapılandırılmamış kodun optimizasyonunu desteklemektedir. Ajanı eğitmek için, büyük bir dil modeli (LLM) kullanarak çeşitli hesaplamalar veri kümesi sentezliyoruz. Önerilen yaklaşımımızı CHEHAB FHE derleyicimize entegre ediyoruz ve bir dizi benchmark üzerinde değerlendirerek performansını, son teknoloji bir vektörleştiren FHE derleyicisi olan Coyote ile karşılaştırıyoruz. Sonuçlar, yaklaşımımızın yürütmede $5.3\\times$ daha hızlı, $2.54\\times$ daha az gürültü biriktiren kod ürettiğini, derleme sürecinin kendisinin ise Coyote'ye kıyasla $27.9\\times$ daha hızlı olduğunu göstermektedir (geometrik ortalamalar)."
    }
  },
  {
    "id": "2601.19174v1",
    "title": "SHIELD: An Auto-Healing Agentic Defense Framework for LLM Resource Exhaustion Attacks",
    "authors": [
      "Nirhoshan Sivaroopan",
      "Kanchana Thilakarathna",
      "Albert Zomaya",
      "Manu",
      "Yi Guo"
    ],
    "published_date": "2026-01-27",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.19174v1",
    "pdf_link": "https://arxiv.org/pdf/2601.19174v1",
    "content": {
      "en": "Sponge attacks increasingly threaten LLM systems by inducing excessive computation and DoS. Existing defenses either rely on statistical filters that fail on semantically meaningful attacks or use static LLM-based detectors that struggle to adapt as attack strategies evolve. We introduce SHIELD, a multi-agent, auto-healing defense framework centered on a three-stage Defense Agent that integrates semantic similarity retrieval, pattern matching, and LLM-based reasoning. Two auxiliary agents, a Knowledge Updating Agent and a Prompt Optimization Agent, form a closed self-healing loop, when an attack bypasses detection, the system updates an evolving knowledgebase, and refines defense instructions. Extensive experiments show that SHIELD consistently outperforms perplexity-based and standalone LLM defenses, achieving high F1 scores across both non-semantic and semantic sponge attacks, demonstrating the effectiveness of agentic self-healing against evolving resource-exhaustion threats.",
      "tr": "**Makale Başlığı:** SHIELD: LLM Kaynak Tükenmesi Saldırılarına Karşı Otomatik İyileşen Etkin Savunma Çerçevesi\n\n**Özet:**\nSponge attacks, LLM sistemlerini aşırı hesaplama ve DoS (Denial of Service) tetikleyerek giderek daha fazla tehdit etmektedir. Mevcut savunmalar, anlamsal olarak anlamlı saldırılarda başarısız olan istatistiksel filtrelemeye dayanmakta veya saldırı stratejileri geliştikçe uyum sağlamakta zorlanan statik LLM tabanlı dedektörler kullanmaktadır. Biz, anlamsal benzerlik retrieval, pattern matching ve LLM tabanlı reasoning'i entegre eden üç aşamalı bir Savunma Ajanı etrafında odaklanan çoklu-ajanlı, otomatik iyileşen bir savunma çerçevesi olan SHIELD'ı sunuyoruz. Bir Bilgi Güncelleme Ajanı ve bir Prompt Optimizasyon Ajanı olmak üzere iki yardımcı ajan, kapalı bir kendi kendini iyileştirme döngüsü oluşturmaktadır; bir saldırı tespitten kaçtığında, sistem gelişen bir knowledgebase'i günceller ve savunma talimatlarını iyileştirir. Kapsamlı deneyler, SHIELD'ın perplexity-tabanlı ve bağımsız LLM savunmalarını tutarlı bir şekilde geride bıraktığını, hem anlamsal olmayan hem de anlamsal sponge saldırılarında yüksek F1 skorları elde ettiğini ve gelişen kaynak-tükenmesi tehditlerine karşı agentic self-healing'in etkinliğini gösterdiğini ortaya koymaktadır."
    }
  }
]