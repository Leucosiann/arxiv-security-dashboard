[
  {
    "id": "2512.24571v1",
    "title": "SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System",
    "authors": [
      "Md Hasan Saju",
      "Austin Page",
      "Akramul Azim",
      "Jeff Gardiner",
      "Farzaneh Abazari"
    ],
    "published_date": "2025-12-31",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.24571v1",
    "pdf_link": "https://arxiv.org/pdf/2512.24571v1",
    "content": {
      "en": "Security Information and Event Management (SIEM) systems are essential for large enterprises to monitor their IT infrastructure by ingesting and analyzing millions of logs and events daily. Security Operations Center (SOC) analysts are tasked with monitoring and analyzing this vast data to identify potential threats and take preventive actions to protect enterprise assets. However, the diversity among SIEM platforms, such as Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel and the Elastic Stack, poses significant challenges. As these systems differ in attributes, architecture, and query languages, making it difficult for analysts to effectively monitor multiple platforms without undergoing extensive training or forcing enterprises to expand their workforce. To address this issue, we introduce SynRAG, a unified framework that automatically generates threat detection or incident investigation queries for multiple SIEM platforms from a platform-agnostic specification. SynRAG can generate platformspecific queries from a single high-level specification written by analysts. Without SynRAG, analysts would need to manually write separate queries for each SIEM platform, since query languages vary significantly across systems. This framework enables seamless threat detection and incident investigation across heterogeneous SIEM environments, reducing the need for specialized training and manual query translation. We evaluate SynRAG against state-of-the-art language models, including GPT, Llama, DeepSeek, Gemma, and Claude, using Qradar and SecOps as representative SIEM systems. Our results demonstrate that SynRAG generates significantly better queries for crossSIEM threat detection and incident investigation compared to the state-of-the-art base models.",
      "tr": "Makale Başlığı: SynRAG: Heterojen SIEM Sistemleri İçin Yürütülebilir Sorgu Üretimi Amaçlı Bir Büyük Dil Modeli Çerçevesi\n\nÖzet:\nSecurity Information and Event Management (SIEM) sistemleri, büyük kurumsal işletmelerin BT altyapılarını günlük milyonlarca log ve olayı alıp analiz ederek izlemeleri için elzemdir. Security Operations Center (SOC) analistleri, potansiyel tehditleri tespit etmek ve kurumsal varlıkları korumak için önleyici eylemler almak üzere bu devasa veriyi izleme ve analiz etme görevini üstlenirler. Ancak, Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel ve Elastic Stack gibi SIEM platformları arasındaki çeşitlilik, önemli zorluklar sunmaktadır. Bu sistemler özellikler, mimari ve sorgu dilleri açısından farklılık gösterdiğinden, analistlerin kapsamlı eğitim almadan veya işletmelerin iş gücünü genişletmek zorunda kalmadan birden fazla platformu etkili bir şekilde izlemeleri zorlaşmaktadır. Bu sorunu ele almak için, platformdan bağımsız bir spesifikasyondan çoklu SIEM platformları için otomatik olarak tehdit tespiti veya olay araştırma sorguları üreten birleşik bir çerçeve olan SynRAG'ı sunuyoruz. SynRAG, analistler tarafından yazılan tek bir üst düzey spesifikasyondan platforma özel sorgular üretebilir. SynRAG olmadan, sorgu dilleri sistemler arasında önemli ölçüde farklılaştığı için analistlerin her SIEM platformu için ayrı sorgular yazmaları gerekirdi. Bu çerçeve, özel eğitim ve manuel sorgu çevirisi ihtiyacını azaltarak heterojen SIEM ortamlarında sorunsuz tehdit tespiti ve olay araştırmayı mümkün kılar. SynRAG'ı, temsili SIEM sistemleri olarak Qradar ve SecOps'u kullanarak, GPT, Llama, DeepSeek, Gemma ve Claude dahil olmak üzere en gelişmiş dil modellerine karşı değerlendiriyoruz. Sonuçlarımız, SynRAG'ın çapraz-SIEM tehdit tespiti ve olay araştırması için en gelişmiş temel modellere kıyasla önemli ölçüde daha iyi sorgular ürettiğini göstermektedir."
    }
  },
  {
    "id": "2512.24452v1",
    "title": "Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations",
    "authors": [
      "Yalin E. Sagduyu",
      "Tugba Erpek",
      "Aylin Yener",
      "Sennur Ulukus"
    ],
    "published_date": "2025-12-30",
    "tags": [
      "cs.NI",
      "cs.AI",
      "cs.CR",
      "cs.IT",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.24452v1",
    "pdf_link": "https://arxiv.org/pdf/2512.24452v1",
    "content": {
      "en": "Semantic communications conveys task-relevant meaning rather than focusing solely on message reconstruction, improving bandwidth efficiency and robustness for next-generation wireless systems. However, learned semantic representations can still leak sensitive information to unintended receivers (eavesdroppers). This paper presents a deep learning-based semantic communication framework that jointly supports multiple receiver tasks while explicitly limiting semantic leakage to an eavesdropper. The legitimate link employs a learned encoder at the transmitter, while the receiver trains decoders for semantic inference and data reconstruction. The security problem is formulated via an iterative min-max optimization in which an eavesdropper is trained to improve its semantic inference, while the legitimate transmitter-receiver pair is trained to preserve task performance while reducing the eavesdropper's success. We also introduce an auxiliary layer that superimposes a cooperative, adversarially crafted perturbation on the transmitted waveform to degrade semantic leakage to an eavesdropper. Performance is evaluated over Rayleigh fading channels with additive white Gaussian noise using MNIST and CIFAR-10 datasets. Semantic accuracy and reconstruction quality improve with increasing latent dimension, while the min-max mechanism reduces the eavesdropper's inference performance significantly without degrading the legitimate receiver. The perturbation layer is successful in reducing semantic leakage even when the legitimate link is trained only for its own task. This comprehensive framework motivates semantic communication designs with tunable, end-to-end privacy against adaptive adversaries in realistic wireless settings.",
      "tr": "**Makale Başlığı:** Çoklu Görev Öğrenmesi ve Adversarial Perturbations Yoluyla Gizliliği Koruyan Semantik İletişim\n\n**Özet:**\n\nSemantik iletişim, bir sonraki nesil kablosuz sistemler için bant genişliği verimliliğini ve dayanıklılığını artırarak, yalnızca mesaj yeniden yapılandırmasına odaklanmak yerine görevle ilgili anlamı iletir. Bununla birlikte, öğrenilen anlamsal temsiller hala istenmeyen alıcılara (dinleyicilere) hassas bilgiler sızdırabilir. Bu çalışma, birden fazla alıcı görevini müştereken destekleyen ve açıkça bir dinleyiciye yönelik anlamsal sızıntıyı sınırlayan derin öğrenme tabanlı bir semantik iletişim çerçevesi sunmaktadır. Meşru bağlantı, verici tarafında öğrenilmiş bir encoder kullanırken, alıcı tarafında anlamsal çıkarım ve veri yeniden yapılandırması için decoders eğitilir. Güvenlik problemi, bir dinleyicinin anlamsal çıkarımını iyileştirmek için eğitildiği ve meşru verici-alıcı çiftinin görev performansını korurken dinleyicinin başarısını azaltmak için eğitildiği iteratif bir min-max optimizasyonu aracılığıyla formüle edilmiştir. Ayrıca, iletilen dalga formuna işbirlikçi, adversarially crafted bir perturbation ekleyerek bir dinleyiciye yönelik anlamsal sızıntıyı bozan yardımcı bir katman eklenmiştir. Performans, Rayleigh fading kanallarında ve eklemeli beyaz Gaussian gürültüsü ile MNIST ve CIFAR-10 veri kümeleri üzerinde değerlendirilmiştir. Anlamsal doğruluk ve yeniden yapılandırma kalitesi, latent dimension arttıkça iyileşirken, min-max mekanizması meşru alıcıyı bozmadan dinleyicinin çıkarım performansını önemli ölçüde azaltır. Perturbation katmanı, meşru bağlantı yalnızca kendi görevi için eğitilmiş olsa bile anlamsal sızıntıyı azaltmada başarılıdır. Bu kapsamlı çerçeve, gerçekçi kablosuz ortamlarda adaptif adversarilere karşı ayarlanabilir, uçtan uca gizliliğe sahip semantik iletişim tasarımlarını teşvik etmektedir."
    }
  },
  {
    "id": "2512.24391v1",
    "title": "FAST-IDS: A Fast Two-Stage Intrusion Detection System with Hybrid Compression for Real-Time Threat Detection in Connected and Autonomous Vehicles",
    "authors": [
      "Devika S",
      "Vishnu Hari",
      "Pratik Narang",
      "Tejasvi Alladi",
      "Vinay Chamola"
    ],
    "published_date": "2025-12-30",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.24391v1",
    "pdf_link": "https://arxiv.org/pdf/2512.24391v1",
    "content": {
      "en": "We have implemented a multi-stage IDS for CAVs that can be deployed to resourec-constrained environments after hybrid model compression.",
      "tr": "İşte makale başlığı ve özetinin istenen şekilde çevrilmiş hali:\n\n**Makale Başlığı:** FAST-IDS: Bağlantılı ve Otonom Araçlarda Gerçek Zamanlı Tehdit Tespiti İçin Hibrit Sıkıştırmalı Hızlı İki Aşamalı Bir Saldırı Tespit Sistemi\n\n**Özet:**\nKaynak kısıtlı ortamlara dağıtılabilecek, hibrit model sıkıştırması sonrası çok aşamalı bir IDS'yi CAV'ler için uyguladık."
    }
  },
  {
    "id": "2512.24345v1",
    "title": "FedSecureFormer: A Fast, Federated and Secure Transformer Framework for Lightweight Intrusion Detection in Connected and Autonomous Vehicles",
    "authors": [
      "Devika S",
      "Vishnu Hari",
      "Pratik Narang",
      "Tejasvi Alladi",
      "F. Richard Yu"
    ],
    "published_date": "2025-12-30",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.24345v1",
    "pdf_link": "https://arxiv.org/pdf/2512.24345v1",
    "content": {
      "en": "This works presents an encoder-only transformer built with minimum layers for intrusion detection in the domain of Connected and Autonomous Vehicles using Federated Learning.",
      "tr": "Elbette, istenen şekilde çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** FedSecureFormer: Bağlantılı ve Otonom Araçlarda Hafif Yetenekli Saldırı Tespitine Yönelik Hızlı, Federatif ve Güvenli Bir Transformer Çerçevesi\n\n**Özet:**\nBu çalışma, Bağlantılı ve Otonom Araçlar alanında saldırı tespiti için minimum katmanlarla inşa edilmiş, encoder-only bir transformer'ı Federated Learning kullanarak sunmaktadır."
    }
  },
  {
    "id": "2512.24088v1",
    "title": "FedLiTeCAN : A Federated Lightweight Transformer for Fast and Robust CAN Bus Intrusion Detection",
    "authors": [
      "Devika S",
      "Pratik Narang",
      "Tejasvi Alladi"
    ],
    "published_date": "2025-12-30",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.24088v1",
    "pdf_link": "https://arxiv.org/pdf/2512.24088v1",
    "content": {
      "en": "This work implements a lightweight Transformer model for IDS in the domain of Connected and Autonomous Vehicles",
      "tr": "**Makale Başlığı:** FedLiTeCAN : Hızlı ve Güvenilir CAN Bus Saldırı Tespiti İçin Bir Federated Lightweight Transformer\n\n**Özet:**\n\nBu çalışma, Bağlantılı ve Otonom Araçlar alanında IDS (Intrusion Detection System) için lightweight Transformer modelini uygulamaktadır. Özellikle, CAN bus protokolü üzerinden gelen verilerin incelenmesi yoluyla araç içi siber güvenlik tehditlerinin tespitini hedeflemektedir. FedLiTeCAN, dağıtık öğrenme prensiplerini benimseyerek, her aracın yerel verileriyle bir Transformer modeli eğitilirken, hassas veri paylaşımını en aza indirmektedir. Bu yaklaşım, merkezi bir sunucuya büyük miktarda veri aktarımına duyulan ihtiyacı ortadan kaldırarak veri gizliliğini ve güvenliğini artırmaktadır. Modelin 'lightweight' tasarımı, sınırlı işlem gücüne sahip araç içi sistemlerde verimli çalışmasını sağlamaktadır. Ayrıca, FedLiTeCAN'ın 'fast' ve 'robust' özellikleri, gerçek zamanlı saldırı tespitinde yüksek doğruluk ve dayanıklılık sunmaktadır. Bu makale, federe öğrenme, Transformer mimarisi ve CAN bus saldırı tespitindeki zorluklar gibi konuları ele alarak, FedLiTeCAN'ın bu alandaki potansiyelini ortaya koymaktadır."
    }
  },
  {
    "id": "2512.24044v1",
    "title": "Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?",
    "authors": [
      "Yuan Xin",
      "Dingfan Chen",
      "Linyi Yang",
      "Michael Backes",
      "Xiao Zhang"
    ],
    "published_date": "2025-12-30",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2512.24044v1",
    "pdf_link": "https://arxiv.org/pdf/2512.24044v1",
    "content": {
      "en": "As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, previous evaluations have focused solely on the models, neglecting the full deployment pipeline, which typically incorporates additional safety mechanisms like content moderation filters. To address this gap, we present the first systematic evaluation of jailbreak attacks targeting LLM safety alignment, assessing their success across the full inference pipeline, including both input and output filtering stages. Our findings yield two key insights: first, nearly all evaluated jailbreak techniques can be detected by at least one safety filter, suggesting that prior assessments may have overestimated the practical success of these attacks; second, while safety filters are effective in detection, there remains room to better balance recall and precision to further optimize protection and user experience. We highlight critical gaps and call for further refinement of detection accuracy and usability in LLM safety systems.",
      "tr": "Makale Başlığı: Jailbreaking Saldırıları ve İçerik Güvenliği Filtreleri: LLM Güvenlik Yarışında Ne Kadar İlerideyiz?\n\nÖzet:\nBüyük dil modellerinin (LLM) yaygınlaşmasıyla birlikte, güvenli kullanımlarını sağlamak büyük önem taşımaktadır. Modellerin hizalanmasını aşarak zararlı çıktılara neden olan rakip komutlar olan jailbreaking, önemli riskler taşımakta olup, mevcut çalışmaların yaygın LLM'leri atlatmada yüksek başarı oranları bildirdiği gözlemlenmektedir. Ancak, önceki değerlendirmeler yalnızca modellere odaklanmış, genellikle içerik moderasyon filtreleri gibi ek güvenlik mekanizmalarını içeren tam dağıtım hattını ihmal etmiştir. Bu boşluğu gidermek için, LLM güvenlik hizalanmasını hedef alan jailbreak saldırılarının ilk sistematik değerlendirmesini sunuyoruz ve hem girdi hem de çıktı filtreleme aşamalarını içeren tam çıkarım hattı boyunca başarılarını değerlendiriyoruz. Bulgularımız iki temel içgörü sağlamaktadır: birinci olarak, değerlendirilen jailbreak tekniklerinin neredeyse tamamı en az bir güvenlik filtresi tarafından tespit edilebilir, bu da önceki değerlendirmelerin bu saldırıların pratik başarısını abartmış olabileceğini düşündürmektedir; ikinci olarak, güvenlik filtreleri tespit etmede etkili olsa da, korumayı ve kullanıcı deneyimini daha da optimize etmek için recall ve precision dengesini daha iyi kurmak için alan bulunmaktadır. LLM güvenlik sistemlerinde tespit doğruluğu ve kullanılabilirlik alanındaki kritik boşlukları vurguluyor ve daha fazla iyileştirme çağrısında bulunuyoruz."
    }
  },
  {
    "id": "2512.23995v1",
    "title": "RepetitionCurse: Measuring and Understanding Router Imbalance in Mixture-of-Experts LLMs under DoS Stress",
    "authors": [
      "Ruixuan Huang",
      "Qingyue Wang",
      "Hantao Huang",
      "Yudong Gao",
      "Dong Chen"
    ],
    "published_date": "2025-12-30",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.23995v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23995v1",
    "content": {
      "en": "Mixture-of-Experts architectures have become the standard for scaling large language models due to their superior parameter efficiency. To accommodate the growing number of experts in practice, modern inference systems commonly adopt expert parallelism to distribute experts across devices. However, the absence of explicit load balancing constraints during inference allows adversarial inputs to trigger severe routing concentration. We demonstrate that out-of-distribution prompts can manipulate the routing strategy such that all tokens are consistently routed to the same set of top-$k$ experts, which creates computational bottlenecks on certain devices while forcing others to idle. This converts an efficiency mechanism into a denial-of-service attack vector, leading to violations of service-level agreements for time to first token. We propose RepetitionCurse, a low-cost black-box strategy to exploit this vulnerability. By identifying a universal flaw in MoE router behavior, RepetitionCurse constructs adversarial prompts using simple repetitive token patterns in a model-agnostic manner. On widely deployed MoE models like Mixtral-8x7B, our method increases end-to-end inference latency by 3.063x, degrading service availability significantly.",
      "tr": "Elbette, istenen çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** RepetitionCurse: Mixture-of-Experts LLM'lerde DoS Baskısı Altında Router Dengesizliğinin Ölçülmesi ve Anlaşılması\n\n**Özet:**\nMixture-of-Experts mimarileri, üstün parametre verimlilikleri sayesinde büyük dil modellerini ölçeklendirmek için standart haline gelmiştir. Pratikte artan expert sayısını karşılamak için, modern inference sistemleri genellikle expert paralelliğini benimseyerek expert'leri cihazlara dağıtır. Ancak, inference sırasında açık yük dengeleme kısıtlamalarının olmaması, kötü niyetli girdilerin ciddi routing konsantrasyonunu tetiklemesine izin verir. Dağıtım dışı (out-of-distribution) prompt'ların, tüm token'ların tutarlı bir şekilde aynı top-$k$ expert setine yönlendirildiği bir routing stratejisini manipüle edebildiğini gösteriyoruz, bu da belirli cihazlarda hesaplama darboğazları yaratırken diğerlerini boşta kalmaya zorlar. Bu, bir verimlilik mekanizmasını denial-of-service saldırı vektörüne dönüştürerek, ilk token'a kadar geçen süre için hizmet seviyesi anlaşmalarının ihlal edilmesine yol açar. Bu zafiyetten yararlanmak için düşük maliyetli bir black-box strateji olan RepetitionCurse'ü öneriyoruz. MoE router davranışındaki evrensel bir kusuru tespit ederek, RepetitionCurse model-agnostic bir şekilde basit tekrarlayan token desenlerini kullanarak adversarial prompt'lar oluşturur. Mixtral-8x7B gibi yaygın olarak kullanılan MoE modellerinde, yöntemimiz uçtan uca inference gecikmesini 3.063 kat artırarak hizmet kullanılabilirliğini önemli ölçüde bozar."
    }
  },
  {
    "id": "2512.23987v1",
    "title": "MeLeMaD: Adaptive Malware Detection via Chunk-wise Feature Selection and Meta-Learning",
    "authors": [
      "Ajvad Haneef K",
      "Karan Kuwar Singh",
      "Madhu Kumar S D"
    ],
    "published_date": "2025-12-30",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.23987v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23987v1",
    "content": {
      "en": "Confronting the substantial challenges of malware detection in cybersecurity necessitates solutions that are both robust and adaptable to the ever-evolving threat environment. The paper introduces Meta Learning Malware Detection (MeLeMaD), a novel framework leveraging the adaptability and generalization capabilities of Model-Agnostic Meta-Learning (MAML) for malware detection. MeLeMaD incorporates a novel feature selection technique, Chunk-wise Feature Selection based on Gradient Boosting (CFSGB), tailored for handling large-scale, high-dimensional malware datasets, significantly enhancing the detection efficiency. Two benchmark malware datasets (CIC-AndMal2020 and BODMAS) and a custom dataset (EMBOD) were used for rigorously validating the MeLeMaD, achieving a remarkable performance in terms of key evaluation measures, including accuracy, precision, recall, F1-score, MCC, and AUC. With accuracies of 98.04\\% on CIC-AndMal2020 and 99.97\\% on BODMAS, MeLeMaD outperforms the state-of-the-art approaches. The custom dataset, EMBOD, also achieves a commendable accuracy of 97.85\\%. The results underscore the MeLeMaD's potential to address the challenges of robustness, adaptability, and large-scale, high-dimensional datasets in malware detection, paving the way for more effective and efficient cybersecurity solutions.",
      "tr": "Makale Başlığı: MeLeMaD: Parça Tabanlı Özellik Seçimi ve Meta-Learning Yoluyla Uyarlanabilir Kötü Amaçlı Yazılım Tespiti\n\nÖzet:\nSiber güvenlik alanında kötü amaçlı yazılım tespitindeki önemli zorluklarla yüzleşmek, sürekli gelişen tehdit ortamına hem sağlam hem de uyarlanabilir çözümler gerektirmektedir. Bu makale, kötü amaçlı yazılım tespiti için Model-Agnostic Meta-Learning (MAML)'in uyarlanabilirlik ve genelleme yeteneklerinden faydalanan yeni bir çerçeve olan Meta Learning Malware Detection (MeLeMaD)'ı tanıtmaktadır. MeLeMaD, büyük ölçekli, yüksek boyutlu kötü amaçlı yazılım veri kümelerini işlemeye yönelik uyarlanmış, Gradient Boosting tabanlı yeni bir özellik seçme tekniği olan Chunk-wise Feature Selection based on Gradient Boosting (CFSGB)'yi içermektedir ve bu da tespit verimliliğini önemli ölçüde artırmaktadır. CIC-AndMal2020 ve BODMAS olmak üzere iki kıyaslama kötü amaçlı yazılım veri kümesi ve özel bir veri kümesi (EMBOD), MeLeMaD'ın titizlikle doğrulanması için kullanılmıştır. Bu doğrulama sonucunda doğruluk (accuracy), kesinlik (precision), recall, F1-score, MCC ve AUC gibi temel değerlendirme ölçütleri açısından dikkate değer bir performans elde edilmiştir. CIC-AndMal2020 üzerinde %98,04 ve BODMAS üzerinde %99,97 doğruluk oranları ile MeLeMaD, mevcut en gelişmiş yaklaşımlardan daha iyi performans göstermiştir. Özel veri kümesi EMBOD de %97,85'lik takdire şayan bir doğruluk oranına ulaşmıştır. Elde edilen sonuçlar, MeLeMaD'ın kötü amaçlı yazılım tespitinde sağlamlık, uyarlanabilirlik ve büyük ölçekli, yüksek boyutlu veri kümeleriyle ilgili zorlukların üstesindeki potansiyelini vurgulamakta ve daha etkili ve verimli siber güvenlik çözümlerinin yolunu açmaktadır."
    }
  },
  {
    "id": "2512.23948v1",
    "title": "DivQAT: Enhancing Robustness of Quantized Convolutional Neural Networks against Model Extraction Attacks",
    "authors": [
      "Kacem Khaled",
      "Felipe Gohring de Magalhães",
      "Gabriela Nicolescu"
    ],
    "published_date": "2025-12-30",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.23948v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23948v1",
    "content": {
      "en": "Convolutional Neural Networks (CNNs) and their quantized counterparts are vulnerable to extraction attacks, posing a significant threat of IP theft. Yet, the robustness of quantized models against these attacks is little studied compared to large models. Previous defenses propose to inject calculated noise into the prediction probabilities. However, these defenses are limited since they are not incorporated during the model design and are only added as an afterthought after training. Additionally, most defense techniques are computationally expensive and often have unrealistic assumptions about the victim model that are not feasible in edge device implementations and do not apply to quantized models. In this paper, we propose DivQAT, a novel algorithm to train quantized CNNs based on Quantization Aware Training (QAT) aiming to enhance their robustness against extraction attacks. To the best of our knowledge, our technique is the first to modify the quantization process to integrate a model extraction defense into the training process. Through empirical validation on benchmark vision datasets, we demonstrate the efficacy of our technique in defending against model extraction attacks without compromising model accuracy. Furthermore, combining our quantization technique with other defense mechanisms improves their effectiveness compared to traditional QAT.",
      "tr": "Makale Başlığı: DivQAT: Model Çıkarma Saldırılarına Karşı Nicelendirilmiş Evrişimli Sinir Ağlarının Dayanıklılığını Artırmak\n\nÖzet:\nEvrişimli Sinir Ağları (CNNs) ve onların nicelendirilmiş (quantized) versiyonları, model çıkarma saldırılarına (extraction attacks) karşı savunmasızdır ve Fikri Mülkiyet (IP) hırsızlığı gibi önemli tehditler oluşturmaktadır. Ancak, nicelendirilmiş modellerin bu saldırılara karşı dayanıklılığı, büyük modellere kıyasla yeterince incelenmemiştir. Önceki savunma yöntemleri, tahmin olasılıklarına hesaplanmış gürültü (calculated noise) enjekte etmeyi önermektedir. Bununla birlikte, bu savunmalar model tasarımına dahil edilmedikleri ve yalnızca eğitim sonrasında bir sonradan düşünme olarak eklendikleri için sınırlıdır. Ek olarak, çoğu savunma tekniği hesaplama açısından maliyetlidir ve genellikle kenar cihaz (edge device) uygulamalarında pratik olmayan ve nicelendirilmiş modellere uygulanmayan kurban model (victim model) hakkında gerçekçi olmayan varsayımlar içermektedir. Bu çalışmada, nicelendirilmiş CNN'lerin model çıkarma saldırılarına karşı dayanıklılığını artırmayı hedefleyen, Nicelendirmeye Duyarlı Eğitim (Quantization Aware Training - QAT) tabanlı yeni bir algoritma olan DivQAT'ı sunuyoruz. Bildiğimiz kadarıyla, bizim tekniğimiz, model çıkarma savunmasını eğitim sürecine entegre etmek için nicelendirme sürecini (quantization process) değiştiren ilk çalışmadır. Karşılaştırmalı vizyon veri setleri (benchmark vision datasets) üzerinde ampirik doğrulama yoluyla, model doğruluğundan ödün vermeden model çıkarma saldırılarına karşı tekniğimizin etkinliğini gösteriyoruz. Dahası, nicelendirme tekniğimizi diğer savunma mekanizmalarıyla birleştirmenin, geleneksel QAT'a kıyasla etkinliklerini artırdığını göstermekteyiz."
    }
  },
  {
    "id": "2512.23881v1",
    "title": "Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack",
    "authors": [
      "Roee Ziv",
      "Raz Lapid",
      "Moshe Sipper"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.SD",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.23881v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23881v1",
    "content": {
      "en": "Audio-language models combine audio encoders with large language models to enable multimodal reasoning, but they also introduce new security vulnerabilities. We propose a universal targeted latent space attack, an encoder-level adversarial attack that manipulates audio latent representations to induce attacker-specified outputs in downstream language generation. Unlike prior waveform-level or input-specific attacks, our approach learns a universal perturbation that generalizes across inputs and speakers and does not require access to the language model. Experiments on Qwen2-Audio-7B-Instruct demonstrate consistently high attack success rates with minimal perceptual distortion, revealing a critical and previously underexplored attack surface at the encoder level of multimodal systems.",
      "tr": "**Makale Başlığı:** Sadece Encoder'a Saldırarak Ses Büyük Dil Modellerini Kırmak: Evrensel Hedefli Gizli Alan Ses Saldırısı\n\n**Özet:**\nSes-dil modelleri, multimodal reasoning'i mümkün kılmak için ses encoder'larını büyük dil modelleriyle birleştirir; ancak bu durum aynı zamanda yeni güvenlik açıkları da ortaya çıkarmaktadır. Saldırgan tarafından belirtilen çıktılara yol açmak amacıyla ses gizli temsillerini manipüle eden, encoder seviyesinde bir adversarial attack olan evrensel hedefli gizli alan saldırısı (universal targeted latent space attack) öneriyoruz. Önceki waveform-level veya input-specific saldırıların aksine, yaklaşımımız girdiler ve konuşmacılar arasında genelleme yapan ve dil modeline erişim gerektirmeyen evrensel bir perturbation öğrenir. Qwen2-Audio-7B-Instruct üzerindeki deneyler, minimal algısal bozulma ile tutarlı bir şekilde yüksek attack success rates göstermekte ve multimodal sistemlerin encoder seviyesinde kritik ve daha önce yeterince araştırılmamış bir attack surface ortaya koymaktadır."
    }
  }
]