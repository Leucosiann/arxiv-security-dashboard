[
  {
    "id": "2602.07878v1",
    "title": "Rethinking Latency Denial-of-Service: Attacking the LLM Serving Framework, Not the Model",
    "authors": [
      "Tianyi Wang",
      "Huawei Fan",
      "Yuanchao Shu",
      "Peng Cheng",
      "Cong Wang"
    ],
    "published_date": "2026-02-08",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.07878v1",
    "pdf_link": "https://arxiv.org/pdf/2602.07878v1",
    "content": {
      "en": "Large Language Models face an emerging and critical threat known as latency attacks. Because LLM inference is inherently expensive, even modest slowdowns can translate into substantial operating costs and severe availability risks. Recently, a growing body of research has focused on algorithmic complexity attacks by crafting inputs to trigger worst-case output lengths. However, we report a counter-intuitive finding that these algorithmic latency attacks are largely ineffective against modern LLM serving systems. We reveal that system-level optimization such as continuous batching provides a logical isolation to mitigate contagious latency impact on co-located users. To this end, in this paper, we shift the focus from the algorithm to the system layer, and introduce a new Fill and Squeeze attack strategy targeting the state transition of the scheduler. \"Fill\" first exhausts the global KV cache to induce Head-of-Line blocking, while \"Squeeze\" forces the system into repetitive preemption. By manipulating output lengths using methods from simple plain-text prompts to more complex prompt engineering, and leveraging side-channel probing of memory status, we demonstrate that the attack can be orchestrated in a black-box setting with much less cost. Extensive evaluations indicate by up to 20-280x average slowdown on Time to First Token and 1.5-4x average slowdown on Time Per Output Token compared to existing attacks with 30-40% lower attack cost.",
      "tr": "Makale Başlığı: Gecikme Reddi Hizmeti (Denial-of-Service) Kavramını Yeniden Düşünmek: Modele Değil, LLM Serving Framework'üne Saldırı\n\nÖzet:\nBüyük Dil Modelleri (LLM), \"latency attacks\" olarak bilinen ortaya çıkan ve kritik bir tehdit ile karşı karşıyadır. LLM çıkarımı doğası gereği maliyetli olduğundan, mütevazı yavaşlamalar bile önemli işletme maliyetlerine ve ciddi kullanılabilirlik risklerine dönüşebilir. Son zamanlarda, araştırmaların önemli bir kısmı, en kötü durum çıktı uzunluklarını tetiklemek için girdi hazırlayarak algoritmik karmaşıklık saldırılarına odaklanmıştır. Ancak, bu algoritmik latency saldırılarının modern LLM serving sistemlerine karşı büyük ölçüde etkisiz olduğu yönünde sezgiye aykırı bir bulguyu rapor ediyoruz. Sürekli batching gibi sistem düzeyindeki optimizasyonun, birlikte konumlandırılmış kullanıcılara bulaşıcı gecikme etkisini azaltmak için mantıksal bir izolasyon sağladığını ortaya koyuyoruz. Bu amaçla, bu makalede odağımızı algoritmadan sistem katmanına kaydırıyor ve scheduler'ın state transition'ını hedefleyen yeni bir Fill and Squeeze saldırı stratejisi sunuyoruz. \"Fill\" önce global KV cache'i tüketerek Head-of-Line blocking'e neden olurken, \"Squeeze\" sistemi tekrarlayan preemption'a zorlar. Basit düz metin istemlerinden daha karmaşık prompt engineering yöntemlerine kadar çıktı uzunluklarını manipüle ederek ve bellek durumunun side-channel probing'inden yararlanarak, saldırının black-box bir ortamda çok daha düşük maliyetle organize edilebileceğini gösteriyoruz. Kapsamlı değerlendirmeler, mevcut saldırılara kıyasla Time to First Token'da 20-280 kat ortalama yavaşlama ve Time Per Output Token'da 1.5-4 kat ortalama yavaşlama ile %30-40 daha düşük saldırı maliyeti göstermektedir."
    }
  },
  {
    "id": "2602.07666v1",
    "title": "SoK: DARPA's AI Cyber Challenge (AIxCC): Competition Design, Architectures, and Lessons Learned",
    "authors": [
      "Cen Zhang",
      "Younggi Park",
      "Fabian Fleischer",
      "Yu-Fu Fu",
      "Jiho Kim"
    ],
    "published_date": "2026-02-07",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.07666v1",
    "pdf_link": "https://arxiv.org/pdf/2602.07666v1",
    "content": {
      "en": "DARPA's AI Cyber Challenge (AIxCC, 2023--2025) is the largest competition to date for building fully autonomous cyber reasoning systems (CRSs) that leverage recent advances in AI -- particularly large language models (LLMs) -- to discover and remediate vulnerabilities in real-world open-source software. This paper presents the first systematic analysis of AIxCC. Drawing on design documents, source code, execution traces, and discussions with organizers and competing teams, we examine the competition's structure and key design decisions, characterize the architectural approaches of finalist CRSs, and analyze competition results beyond the final scoreboard. Our analysis reveals the factors that truly drove CRS performance, identifies genuine technical advances achieved by teams, and exposes limitations that remain open for future research. We conclude with lessons for organizing future competitions and broader insights toward deploying autonomous CRSs in practice.",
      "tr": "İşte makale başlığı ve özetinin akademik ve resmi bir dille, teknik terimler İngilizce bırakılarak yapılmış çevirisi:\n\n**Makale Başlığı:** SoK: DARPA'nın Yapay Zeka Siber Yarışması (AIxCC): Yarışma Tasarımı, Mimari Yaklaşımları ve Edinilen Dersler\n\n**Özet:**\n\nDARPA'nın Yapay Zeka Siber Yarışması (AIxCC, 2023-2025), yapay zekadaki (özellikle large language models - LLMs) son gelişmeleri kullanarak, gerçek dünya açık kaynaklı yazılımlardaki zafiyetleri keşfeden ve düzelten tamamen otonom siber reasoning systems (CRSs) inşa etmeye yönelik bugüne kadarki en büyük yarışmadır. Bu makale, AIxCC'nin ilk sistematik analizini sunmaktadır. Tasarım belgeleri, kaynak kodu, yürütme izleri ve organizatörler ile yarışmacı ekiplerle yapılan görüşmelerden yararlanarak, yarışmanın yapısını ve temel tasarım kararlarını incelemekte, finalist CRSs'lerin mimari yaklaşımlarını karakterize etmekte ve nihai skor tablosunun ötesindeki yarışma sonuçlarını analiz etmekteyiz. Analizimiz, CRS performansını gerçek anlamda yönlendiren faktörleri ortaya çıkarmakta, ekiplerin elde ettiği gerçek teknik ilerlemeleri belirlemekte ve gelecekteki araştırmalar için açık kalan sınırlılıkları gözler önüne sermektedir. Sonuç olarak, gelecekteki yarışmaların düzenlenmesi için dersler ve otonom CRSs'lerin pratikte konuşlandırılmasına yönelik daha geniş içgörüler sunuyoruz."
    }
  },
  {
    "id": "2602.07652v1",
    "title": "Agent-Fence: Mapping Security Vulnerabilities Across Deep Research Agents",
    "authors": [
      "Sai Puppala",
      "Ismail Hossain",
      "Md Jahangir Alam",
      "Yoonpyo Lee",
      "Jay Yoo"
    ],
    "published_date": "2026-02-07",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.07652v1",
    "pdf_link": "https://arxiv.org/pdf/2602.07652v1",
    "content": {
      "en": "Large language models are increasingly deployed as *deep agents* that plan, maintain persistent state, and invoke external tools, shifting safety failures from unsafe text to unsafe *trajectories*. We introduce **AgentFence**, an architecture-centric security evaluation that defines 14 trust-boundary attack classes spanning planning, memory, retrieval, tool use, and delegation, and detects failures via *trace-auditable conversation breaks* (unauthorized or unsafe tool use, wrong-principal actions, state/objective integrity violations, and attack-linked deviations). Holding the base model fixed, we evaluate eight agent archetypes under persistent multi-turn interaction and observe substantial architectural variation in mean security break rate (MSBR), ranging from $0.29 \\pm 0.04$ (LangGraph) to $0.51 \\pm 0.07$ (AutoGPT). The highest-risk classes are operational: Denial-of-Wallet ($0.62 \\pm 0.08$), Authorization Confusion ($0.54 \\pm 0.10$), Retrieval Poisoning ($0.47 \\pm 0.09$), and Planning Manipulation ($0.44 \\pm 0.11$), while prompt-centric classes remain below $0.20$ under standard settings. Breaks are dominated by boundary violations (SIV 31%, WPA 27%, UTI+UTA 24%, ATD 18%), and authorization confusion correlates with objective and tool hijacking ($ρ\\approx 0.63$ and $ρ\\approx 0.58$). AgentFence reframes agent security around what matters operationally: whether an agent stays within its goal and authority envelope over time.",
      "tr": "İşte akademik makalenin başlığı ve özetinin istenen şekilde Türkçeye çevrilmiş hali:\n\n**Makale Başlığı:** Agent-Fence: Derin Araştırma Ajanları Boyunca Güvenlik Açıklarının Haritalandırılması\n\n**Özet:**\nBüyük dil modelleri, planlama yapan, kalıcı durum sürdüren ve harici araçları çağıran *deep agents* olarak giderek daha fazla konuşlandırılmakta, bu da güvenlik başarısızlıklarını güvensiz metinden güvensiz *trajectories*'e kaydırmaktadır. Biz, planlama, memory, retrieval, tool use ve delegation'ı kapsayan 14 adet trust-boundary attack class'ı tanımlayan ve *trace-auditable conversation breaks* (yetkisiz veya güvensiz araç kullanımı, wrong-principal actions, state/objective integrity violations ve attack-linked deviations) yoluyla başarısızlıkları tespit eden bir mimari merkezli güvenlik değerlendirmesi olan **AgentFence**'i sunuyoruz. Temel modeli sabit tutarak, sekiz ajan arketipini persistent multi-turn interaction altında değerlendiriyoruz ve ortalama güvenlik kesinti oranında (mean security break rate - MSBR) kayda değer mimari varyasyonlar gözlemliyoruz; bu oran $0.29 \\pm 0.04$ (LangGraph) ile $0.51 \\pm 0.07$ (AutoGPT) arasında değişmektedir. En yüksek riskli sınıflar operasyonel olanlardır: Denial-of-Wallet ($0.62 \\pm 0.08$), Authorization Confusion ($0.54 \\pm 0.10$), Retrieval Poisoning ($0.47 \\pm 0.09$) ve Planning Manipulation ($0.44 \\pm 0.11$), standart ayarlarda prompt-centric sınıflar ise $0.20$'nin altında kalmaktadır. Kesintiler büyük ölçüde boundary violations (SIV %31, WPA %27, UTI+UTA %24, ATD %18) tarafından domine edilmektedir ve authorization confusion, objective ve tool hijacking ile ilişkilidir ($ρ\\approx 0.63$ ve $ρ\\approx 0.58$). AgentFence, ajan güvenliğini operasyonel olarak önemli olan bir şeye yeniden çerçevelemektedir: bir ajanın zaman içinde hedefi ve yetki zarfı içinde kalıp kalmadığı."
    }
  },
  {
    "id": "2602.07517v1",
    "title": "MemPot: Defending Against Memory Extraction Attack with Optimized Honeypots",
    "authors": [
      "Yuhao Wang",
      "Shengfang Zhai",
      "Guanghao Jin",
      "Yinpeng Dong",
      "Linyi Yang"
    ],
    "published_date": "2026-02-07",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.DB"
    ],
    "link": "http://arxiv.org/abs/2602.07517v1",
    "pdf_link": "https://arxiv.org/pdf/2602.07517v1",
    "content": {
      "en": "Large Language Model (LLM)-based agents employ external and internal memory systems to handle complex, goal-oriented tasks, yet this exposes them to severe extraction attacks, and effective defenses remain lacking. In this paper, we propose MemPot, the first theoretically verified defense framework against memory extraction attacks by injecting optimized honeypots into the memory. Through a two-stage optimization process, MemPot generates trap documents that maximize the retrieval probability for attackers while remaining inconspicuous to benign users. We model the detection process as Wald's Sequential Probability Ratio Test (SPRT) and theoretically prove that MemPot achieves a lower average number of sampling rounds compared to optimal static detectors. Empirically, MemPot significantly outperforms state-of-the-art baselines, achieving a 50% improvement in detection AUROC and an 80% increase in True Positive Rate under low False Positive Rate constraints. Furthermore, our experiments confirm that MemPot incurs zero additional online inference latency and preserves the agent's utility on standard tasks, verifying its superiority in safety, harmlessness, and efficiency.",
      "tr": "**Makale Başlığı:** MemPot: Optimized Honeypots ile Bellek Çıkarma Saldırılarına Karşı Savunma\n\n**Özet:**\n\nBüyük Dil Modeli (LLM) tabanlı ajanlar, karmaşık ve hedef odaklı görevleri yerine getirmek için harici ve dahili bellek sistemlerini kullanırlar. Ancak bu durum, onları ciddi bellek çıkarma saldırılarına maruz bırakmakta ve etkili savunma mekanizmaları henüz yetersiz kalmaktadır. Bu çalışmada, belleğe optimize edilmiş honeypot'lar enjekte ederek bellek çıkarma saldırılarına karşı ilk teorik olarak doğrulanmış savunma çerçevesi olan MemPot'u sunuyoruz. İki aşamalı bir optimizasyon süreci aracılığıyla MemPot, saldırganlar için bellek erişim (retrieval) olasılığını en üst düzeye çıkarırken zararsız kullanıcılar için fark edilmeyecek şekilde tuzak belgeler üretir. Algılama sürecini Wald'un Sequential Probability Ratio Test (SPRT) olarak modeller ve MemPot'un optimal statik dedektörlere kıyasla daha düşük ortalama örnekleme turu (sampling rounds) elde ettiğini teorik olarak kanıtlarız. Ampirik olarak, MemPot en son teknolojiye sahip temel sistemlerden önemli ölçüde daha iyi performans göstererek, düşük Yanlış Pozitif Oranı (False Positive Rate) kısıtlamaları altında algılamada %50 iyileşme ile AUROC ve %80 artışla Doğru Pozitif Oranı (True Positive Rate) sağlar. Ayrıca, deneylerimiz MemPot'un sıfır ek çevrimiçi çıkarım (online inference) gecikmesi getirdiğini ve standart görevlerde ajanların kullanılabilirliğini koruduğunu doğrulamakta, böylece güvenlik, zararsızlık ve verimlilik açısından üstünlüğünü teyit etmektedir."
    }
  },
  {
    "id": "2602.07422v1",
    "title": "Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model",
    "authors": [
      "Tianyi Wu",
      "Mingzhe Du",
      "Yue Liu",
      "Chengran Yang",
      "Terry Yue Zhuo"
    ],
    "published_date": "2026-02-07",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2602.07422v1",
    "pdf_link": "https://arxiv.org/pdf/2602.07422v1",
    "content": {
      "en": "Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.",
      "tr": "İşte makale başlığı ve özetinin istenen şekilde çevrilmiş hali:\n\n**Makale Başlığı:** Güvenlik Açığı Ödül Modeli ile Çevrimiçi Pekiştirmeli Öğrenme Yoluyla Güvenli Kod Üretimi\n\n**Özet:**\nBüyük dil modelleri (LLM'ler) yazılım geliştirmede giderek daha fazla kullanılmaktadır, ancak güvensiz kod üretme eğilimleri gerçek dünya dağıtımında önemli bir engel olmaya devam etmektedir. Mevcut güvenli kod hizalama yöntemleri sıklıkla işlevsellik-güvenlik paradoksu ile karşı karşıya kalır; yani güvenlik iyileştirilirken önemli ölçüde fayda kaybı yaşanır. Biz, işlevselliği koruyarak güvenli kod üretimi için çevrimiçi pekiştirmeli öğrenme çerçevesi olan SecCoderX'i öneriyoruz. SecCoderX, öncelikle olgun tespit kaynaklarını iki şekilde yeniden tasarlayarak güvenlik açığı tespiti ile güvenli kod üretimini birleştirir: (i) çevrimiçi RL rollouts için çeşitli, gerçekliğe dayalı güvenlik açığı oluşturan kodlama görevlerini sentezlemek ve (ii) ölçeklenebilir ve güvenilir güvenlik denetimi sağlayan, reasoning-based bir güvenlik açığı ödül modeli eğitmek. Bu bileşenler birlikte, kod LLM'lerini güvenli ve işlevsel kod üretmeye hizalamak için çevrimiçi bir RL döngüsünde birleştirilir. Kapsamlı deneyler, SecCoderX'in, hizalanmamış modellerden yaklaşık %10 oranında Etkin Güvenlik Oranını (ESR) iyileştirerek en gelişmiş performansı gösterdiğini ve önceki yöntemlerin ESR'yi %14-54 oranında düşürmesine karşılık geldiğini göstermektedir. Kodumuzu, veri setimizi ve model kontrol noktalarımızı https://github.com/AndrewWTY/SecCoderX adresinde yayımlıyoruz."
    }
  },
  {
    "id": "2602.07398v1",
    "title": "AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management",
    "authors": [
      "Ruoyao Wen",
      "Hao Li",
      "Chaowei Xiao",
      "Ning Zhang"
    ],
    "published_date": "2026-02-07",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.07398v1",
    "pdf_link": "https://arxiv.org/pdf/2602.07398v1",
    "content": {
      "en": "Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack.   We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length.   On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.",
      "tr": "Elbette, istediğiniz şekilde akademik makale başlığını ve özetini Türkçeye çevirdim:\n\n**Makale Başlığı:** AgentSys: Açık Hiyerarşik Bellek Yönetimiyle Güvenli ve Dinamik LLM Ajanları\n\n**Özet:**\nDolaylı prompt injection, harici içeriklere kötü niyetli talimatlar gömerek LLM ajanlarını tehdit eder, yetkisiz eylemlere ve veri hırsızlığına olanak tanır. LLM ajanları, karar verme süreçleri için etkileşim geçmişini depolayan context window aracılığıyla working memory'lerini sürdürür. Geleneksel ajanlar, tüm tool çıktılarını ve reasoning traces'lerini bu bellekte ayrım gözetmeksizin biriktirir, bu da iki kritik güvenlik açığı yaratır: (1) enjekte edilen talimatlar iş akışı boyunca kalır ve saldırganlara davranışı manipüle etmek için birden çok fırsat tanır ve (2) ayrıntılı, gereksiz içerik karar verme yeteneklerini bozar. Mevcut savunmalar, şişmiş belleği olduğu gibi kabul eder ve saldırıyı önlemek için gereksiz birikimi azaltmak yerine direncini korumaya odaklanır. AgentSys'i, açık bellek yönetimi aracılığıyla dolaylı prompt injection'a karşı savunan bir framework olarak sunuyoruz. İşletim sistemlerindeki process memory isolation'dan esinlenen AgentSys, ajanları hiyerarşik olarak düzenler: ana bir ajan, her biri izole bir context'te çalışan ve alt görevler için iç içe worker'lar başlatabilen tool çağrıları için worker ajanları başlatır. Harici veriler ve alt görev traces'leri asla ana ajanın belleğine girmez; yalnızca schema-validated return values, deterministik JSON parsing aracılığıyla sınırlar boyunca geçebilir. Ablasyonlar, izolasyonun tek başına saldırı başarısını %2,19'a kestiğini ve bir validator/sanitizer eklemenin, maliyeti context uzunluğuna değil işlemlere göre ölçeklenen olay tetiklemeli kontrollerle savunmayı daha da iyileştirdiğini göstermektedir. AgentDojo ve ASB üzerinde AgentSys, savunmasız temel modellerin benign utility'sini hafifçe iyileştirirken %0,78 ve %4,25 saldırı başarısı elde eder. Adaptif saldırganlara ve birden çok foundation modeline karşı sağlamlığını korur, bu da açık bellek yönetiminin güvenli, dinamik LLM ajan mimarilerine olanak tanıdığını göstermektedir. Kodumuz şu adreste mevcuttur: https://github.com/ruoyaow/agentsys-memory."
    }
  },
  {
    "id": "2602.07370v1",
    "title": "Privately Learning Decision Lists and a Differentially Private Winnow",
    "authors": [
      "Mark Bun",
      "William Fang"
    ],
    "published_date": "2026-02-07",
    "tags": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "link": "http://arxiv.org/abs/2602.07370v1",
    "pdf_link": "https://arxiv.org/pdf/2602.07370v1",
    "content": {
      "en": "We give new differentially private algorithms for the classic problems of learning decision lists and large-margin halfspaces in the PAC and online models. In the PAC model, we give a computationally efficient algorithm for learning decision lists with minimal sample overhead over the best non-private algorithms. In the online model, we give a private analog of the influential Winnow algorithm for learning halfspaces with mistake bound polylogarithmic in the dimension and inverse polynomial in the margin. As an application, we describe how to privately learn decision lists in the online model, qualitatively matching state-of-the art non-private guarantees.",
      "tr": "**Makale Başlığı:** Karar Listelerinin Gizlice Öğrenilmesi ve Farklılıksal Olarak Gizli bir Winnow Algoritması\n\n**Özet:**\n\nKlasik karar listeleri ve PAC (Probably Approximately Correct) ile çevrimiçi modellerdeki büyük-marjlı yarım uzaylar öğrenme problemlerine yönelik yeni farklılıksal olarak gizli algoritmalar sunuyoruz. PAC modelinde, en iyi özel olmayan algoritmalara kıyasla minimum örnek yükü ile karar listelerini öğrenmek için hesaplama açısından verimli bir algoritma sağlıyoruz. Çevrimiçi modelde, boyutun logaritmik katı ve marjın ters polinomu şeklinde hata sınırına sahip yarım uzayları öğrenmek için etkili Winnow algoritmasının özel bir analoğunu sunuyoruz. Bir uygulama olarak, çevrimiçi modelde karar listelerini gizlice nasıl öğrenebileceğimizi ve durum-sanatı özel olmayan garantilerle niteliksel olarak eşleştiğini açıklıyoruz."
    }
  },
  {
    "id": "2602.07249v1",
    "title": "Beyond Crash: Hijacking Your Autonomous Vehicle for Fun and Profit",
    "authors": [
      "Qi Sun",
      "Ahmed Abdo",
      "Luis Burbano",
      "Ziyang Li",
      "Yaxing Yao"
    ],
    "published_date": "2026-02-06",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.07249v1",
    "pdf_link": "https://arxiv.org/pdf/2602.07249v1",
    "content": {
      "en": "Autonomous Vehicles (AVs), especially vision-based AVs, are rapidly being deployed without human operators. As AVs operate in safety-critical environments, understanding their robustness in an adversarial environment is an important research problem. Prior physical adversarial attacks on vision-based autonomous vehicles predominantly target immediate safety failures (e.g., a crash, a traffic-rule violation, or a transient lane departure) by inducing a short-lived perception or control error. This paper shows a qualitatively different risk: a long-horizon route integrity compromise, where an attacker gradually steers a victim AV away from its intended route and into an attacker-chosen destination while the victim continues to drive \"normally.\" This will not pose a danger to the victim vehicle itself, but also to potential passengers sitting inside the vehicle.   In this paper, we design and implement the first adversarial framework, called JackZebra, that performs route-level hijacking of a vision-based end-to-end driving stack using a physically plausible attacker vehicle with a reconfigurable display mounted on the rear. The central challenge is temporal persistence: adversarial influence must remain effective in changing viewpoints, lighting, weather, traffic, and the victim's continual replanning -- without triggering conspicuous failures. Our key insight is to treat route hijacking as a closed-loop control problem and to convert adversarial patches into steering primitives that can be selected online via an interactive adjustment loop. Our adversarial patches are also carefully designed against worst-case background and sensor variations so that the adversarial impacts on the victim. Our evaluation shows that JackZebra can successfully hijack victim vehicles to deviate from original routes and stop at adversarial destinations with a high success rate.",
      "tr": "Elbette, akademik makale başlığını ve özetini istediğiniz gibi Türkçeye çevirdim:\n\n**Makale Başlığı:** Çöküşün Ötesinde: Eğlence ve Kazanç İçin Otonom Aracınızı Kaçırma\n\n**Özet:**\nOtonom Araçlar (AV'ler), özellikle görüş tabanlı AV'ler, insan operatörler olmadan hızla konuşlandırılmaktadır. AV'ler güvenlik açısından kritik ortamlarda çalıştığından, düşmanca bir ortamdaki dayanıklılıklarını anlamak önemli bir araştırma problemidir. Görüş tabanlı otonom araçlara yönelik önceki fiziksel düşmanca saldırılar, kısa ömürlü bir algılama veya kontrol hatası oluşturarak baskın olarak acil güvenlik arızalarını (örneğin, bir çarpışma, trafik kuralı ihlali veya geçici bir şeritten ayrılma) hedef almıştır. Bu makale niteliksel olarak farklı bir risk ortaya koymaktadır: saldırganın kurban AV'yi kasıtlı olarak belirlenmiş bir rotadan uzaklaştırarak saldırgan tarafından seçilen bir varış noktasına yönlendirdiği, kurbanın ise \"normal\" bir şekilde ilerlemeye devam ettiği uzun vadeli bir rota bütünlüğü tehlikesi. Bu durum, yalnızca kurban araca değil, aynı zamanda araç içinde oturan potansiyel yolculara da tehlike arz edecektir. Bu makalede, arkasına monte edilmiş yeniden yapılandırılabilir bir ekranı olan fiziksel olarak makul bir saldırgan araç kullanarak, görüş tabanlı uçtan uca bir sürüş yığınının rota düzeyinde kaçırılmasını gerçekleştiren JackZebra adlı ilk düşmanca çerçeveyi tasarlayıp uyguluyoruz. Temel zorluk zamansal devamlılıktır: düşmanca etki, belirgin başarısızlıkları tetiklemeksizin, bakış açılarını, aydınlatmayı, hava durumunu, trafiği ve kurbanın sürekli yeniden planlamasını değiştirmede etkili olmalıdır. Anahtar içgörümüz, rota kaçırmayı kapalı döngü bir kontrol problemi olarak ele almak ve düşmanca yamaları, etkileşimli bir ayarlama döngüsü aracılığıyla çevrimiçi olarak seçilebilen direksiyon ilkelere dönüştürmektir. Düşmanca yamalarımız ayrıca, düşmanca etkilerin kurban üzerindeki etkisini en üst düzeye çıkarmak için en kötü durum arka plan ve sensör değişimlerine karşı dikkatlice tasarlanmıştır. Değerlendirmemiz, JackZebra'nın kurban araçları başarıyla kaçırarak orijinal rotalarından sapmalarını ve yüksek başarı oranıyla düşmanca varış noktalarında durmalarını sağlayabildiğini göstermektedir."
    }
  },
  {
    "id": "2602.07200v1",
    "title": "BadSNN: Backdoor Attacks on Spiking Neural Networks via Adversarial Spiking Neuron",
    "authors": [
      "Abdullah Arafat Miah",
      "Kevin Vu",
      "Yu Bi"
    ],
    "published_date": "2026-02-06",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.07200v1",
    "pdf_link": "https://arxiv.org/pdf/2602.07200v1",
    "content": {
      "en": "Spiking Neural Networks (SNNs) are energy-efficient counterparts of Deep Neural Networks (DNNs) with high biological plausibility, as information is transmitted through temporal spiking patterns. The core element of an SNN is the spiking neuron, which converts input data into spikes following the Leaky Integrate-and-Fire (LIF) neuron model. This model includes several important hyperparameters, such as the membrane potential threshold and membrane time constant. Both the DNNs and SNNs have proven to be exploitable by backdoor attacks, where an adversary can poison the training dataset with malicious triggers and force the model to behave in an attacker-defined manner. Yet, how an adversary can exploit the unique characteristics of SNNs for backdoor attacks remains underexplored. In this paper, we propose \\textit{BadSNN}, a novel backdoor attack on spiking neural networks that exploits hyperparameter variations of spiking neurons to inject backdoor behavior into the model. We further propose a trigger optimization process to achieve better attack performance while making trigger patterns less perceptible. \\textit{BadSNN} demonstrates superior attack performance on various datasets and architectures, as well as compared with state-of-the-art data poisoning-based backdoor attacks and robustness against common backdoor mitigation techniques. Codes can be found at https://github.com/SiSL-URI/BadSNN.",
      "tr": "İşte makale başlığının ve özetinin çevirisi:\n\n**Makale Başlığı:** BadSNN: Adversarial Spiking Neuron Aracılığıyla Spiking Neural Networks'e Arka Kapı Saldırıları\n\n**Özet:**\nSpiking Neural Networks (SNNs), bilginin zamansal spiking paternleri aracılığıyla iletildiği, yüksek biyolojik doğruluğa sahip, enerji verimli Deep Neural Networks (DNNs) muadilleridir. Bir SNN'nin temel öğesi, Leaky Integrate-and-Fire (LIF) neuron modelini takip ederek girdi verilerini spike'lara dönüştüren spiking neuron'dur. Bu model, membrane potential threshold ve membrane time constant gibi birkaç önemli hyperparameter içerir. Hem DNN'ler hem de SNN'ler, bir saldırganın eğitim veri kümesini kötü niyetli tetikleyicilerle zehirleyerek modeli saldırgan tarafından tanımlanan bir şekilde davranmaya zorlayabileceği backdoor attacks'ler tarafından istismar edilebilir hale gelmiştir. Bununla birlikte, bir saldırganın SNN'lerin benzersiz özelliklerini backdoor attacks'ler için nasıl istismar edebileceği hala yeterince araştırılmamıştır. Bu makalede, spiking neuronların hyperparameter varyasyonlarını kullanarak modele backdoor behavior enjekte eden, spiking neural networks'e yönelik yeni bir backdoor attack olan \\textit{BadSNN}'i öneriyoruz. Ayrıca, trigger pattern'leri daha az algılanabilir hale getirirken daha iyi attack performance elde etmek için bir trigger optimization process öneriyoruz. \\textit{BadSNN}, çeşitli veri kümeleri ve mimariler üzerinde üstün attack performance gösterir, ayrıca state-of-the-art data poisoning-based backdoor attacks'lerle karşılaştırıldığında ve yaygın backdoor mitigation techniques'lere karşı dayanıklılık sergiler. Kodlar https://github.com/SiSL-URI/BadSNN adresinde bulunabilir."
    }
  },
  {
    "id": "2602.07107v1",
    "title": "ShallowJail: Steering Jailbreaks against Large Language Models",
    "authors": [
      "Shang Liu",
      "Hanyu Pei",
      "Zeyan Liu"
    ],
    "published_date": "2026-02-06",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.07107v1",
    "pdf_link": "https://arxiv.org/pdf/2602.07107v1",
    "content": {
      "en": "Large Language Models(LLMs) have been successful in numerous fields. Alignment has usually been applied to prevent them from harmful purposes. However, aligned LLMs remain vulnerable to jailbreak attacks that deliberately mislead them into producing harmful outputs. Existing jailbreaks are either black-box, using carefully crafted, unstealthy prompts, or white-box, requiring resource-intensive computation. In light of these challenges, we introduce ShallowJail, a novel attack that exploits shallow alignment in LLMs. ShallowJail can misguide LLMs' responses by manipulating the initial tokens during inference. Through extensive experiments, we demonstrate the effectiveness of~\\shallow, which substantially degrades the safety of state-of-the-art LLM responses.",
      "tr": "İşte makale başlığı ve özetinin çevirisi:\n\n**Makale Başlığı:** ShallowJail: Büyük Dil Modellerine Karşı Jailbreak Saldırılarını Yönlendirme\n\n**Özet:**\nBüyük Dil Modelleri (LLMs) pek çok alanda başarılı olmuştur. Zararlı amaçları önlemek için genellikle hizalama (alignment) uygulanmaktadır. Ancak, hizalanmış LLM'ler, onları kasıtlı olarak zararlı çıktılar üretmeye yönlendiren jailbreak saldırılarına karşı hala savunmasızdır. Mevcut jailbreak yöntemleri ya dikkatlice hazırlanmış, gizli olmayan prompt'lar kullanan black-box yöntemlerdir ya da kaynak-yoğun hesaplama gerektiren white-box yöntemlerdir. Bu zorluklar ışığında, LLM'lerdeki shallow alignment'ı (sığ hizalama) kullanan yenilikçi bir saldırı yöntemi olan ShallowJail'i sunuyoruz. ShallowJail, çıkarım (inference) sırasında ilk token'ları manipüle ederek LLM'lerin yanıtlarını yanlış yönlendirebilir. Kapsamlı deneyler yoluyla, state-of-the-art LLM yanıtlarının güvenliğini önemli ölçüde bozan ShallowJail'in etkinliğini göstermekteyiz."
    }
  }
]