[
  {
    "id": "2602.09634v1",
    "title": "LLM-FS: Zero-Shot Feature Selection for Effective and Interpretable Malware Detection",
    "authors": [
      "Naveen Gill",
      "Ajvad Haneef K",
      "Madhu Kumar S D"
    ],
    "published_date": "2026-02-10",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.09634v1",
    "pdf_link": "https://arxiv.org/pdf/2602.09634v1",
    "content": {
      "en": "Feature selection (FS) remains essential for building accurate and interpretable detection models, particularly in high-dimensional malware datasets. Conventional FS methods such as Extra Trees, Variance Threshold, Tree-based models, Chi-Squared tests, ANOVA, Random Selection, and Sequential Attention rely primarily on statistical heuristics or model-driven importance scores, often overlooking the semantic context of features. Motivated by recent progress in LLM-driven FS, we investigate whether large language models (LLMs) can guide feature selection in a zero-shot setting, using only feature names and task descriptions, as a viable alternative to traditional approaches. We evaluate multiple LLMs (GPT-5.0, GPT-4.0, Gemini-2.5 etc.) on the EMBOD dataset (a fusion of EMBER and BODMAS benchmark datasets), comparing them against established FS methods across several classifiers, including Random Forest, Extra Trees, MLP, and KNN. Performance is assessed using accuracy, precision, recall, F1, AUC, MCC, and runtime. Our results demonstrate that LLM-guided zero-shot feature selection achieves competitive performance with traditional FS methods while offering additional advantages in interpretability, stability, and reduced dependence on labeled data. These findings position zero-shot LLM-based FS as a promising alternative strategy for effective and interpretable malware detection, paving the way for knowledge-guided feature selection in security-critical applications",
      "tr": "**Makale Başlığı:** LLM-FS: Etkin ve Yorumlanabilir Zararlı Yazılım Tespiti İçin Sıfır-Çekim Özellik Seçimi\n\n**Özet:**\n\nÖzellik seçimi (FS), özellikle yüksek boyutlu zararlı yazılım veri kümelerinde, doğru ve yorumlanabilir tespit modelleri oluşturmak için temel bir unsur olmaya devam etmektedir. Extra Trees, Variance Threshold, Tree-based models, Chi-Squared tests, ANOVA, Random Selection ve Sequential Attention gibi geleneksel FS yöntemleri, genellikle özelliklerin anlamsal bağlamını göz ardı ederek öncelikli olarak istatistiksel sezgilere veya model güdümlü önem puanlarına dayanır. LLM güdümlü FS'deki son gelişmelerden esinlenerek, büyük dil modellerinin (LLMs) yalnızca özellik adları ve görev açıklamalarını kullanarak sıfır-çekim bir ortamda özellik seçimine rehberlik edip edemeyeceğini ve geleneksel yaklaşımlara uygulanabilir bir alternatif sunup sunamayacağını araştırıyoruz. EMBOD veri kümesi (EMBER ve BODMAS benchmark veri kümelerinin bir birleşimi) üzerinde birden çok LLM'yi (GPT-5.0, GPT-4.0, Gemini-2.5 vb.) değerlendiriyor ve bunları Random Forest, Extra Trees, MLP ve KNN dahil olmak üzere çeşitli sınıflandırıcılar genelinde köklü FS yöntemleriyle karşılaştırıyoruz. Performans, accuracy, precision, recall, F1, AUC, MCC ve runtime kullanılarak değerlendirilmektedir. Sonuçlarımız, LLM güdümlü sıfır-çekim özellik seçiminin geleneksel FS yöntemleriyle rekabetçi bir performans elde ettiğini, aynı zamanda yorumlanabilirlik, kararlılık ve etiketlenmiş verilere bağımlılığın azalması açısından ek avantajlar sunduğunu göstermektedir. Bu bulgular, sıfır-çekim LLM tabanlı FS'yi etkin ve yorumlanabilir zararlı yazılım tespiti için umut verici bir alternatif strateji olarak konumlandırarak, güvenlik açısından kritik uygulamalarda bilgi güdümlü özellik seçimine yol açmaktadır."
    }
  },
  {
    "id": "2602.09629v1",
    "title": "Stop Testing Attacks, Start Diagnosing Defenses: The Four-Checkpoint Framework Reveals Where LLM Safety Breaks",
    "authors": [
      "Hayfa Dhabhi",
      "Kashyap Thimmaraju"
    ],
    "published_date": "2026-02-10",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.ET",
      "cs.HC"
    ],
    "link": "http://arxiv.org/abs/2602.09629v1",
    "pdf_link": "https://arxiv.org/pdf/2602.09629v1",
    "content": {
      "en": "Large Language Models (LLMs) deploy safety mechanisms to prevent harmful outputs, yet these defenses remain vulnerable to adversarial prompts. While existing research demonstrates that jailbreak attacks succeed, it does not explain \\textit{where} defenses fail or \\textit{why}.   To address this gap, we propose that LLM safety operates as a sequential pipeline with distinct checkpoints. We introduce the \\textbf{Four-Checkpoint Framework}, which organizes safety mechanisms along two dimensions: processing stage (input vs.\\ output) and detection level (literal vs.\\ intent). This creates four checkpoints, CP1 through CP4, each representing a defensive layer that can be independently evaluated. We design 13 evasion techniques, each targeting a specific checkpoint, enabling controlled testing of individual defensive layers.   Using this framework, we evaluate GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across 3,312 single-turn, black-box test cases. We employ an LLM-as-judge approach for response classification and introduce Weighted Attack Success Rate (WASR), a severity-adjusted metric that captures partial information leakage overlooked by binary evaluation.   Our evaluation reveals clear patterns. Traditional Binary ASR reports 22.6\\% attack success. However, WASR reveals 52.7\\%, a 2.3$\\times$ higher vulnerability. Output-stage defenses (CP3, CP4) prove weakest at 72--79\\% WASR, while input-literal defenses (CP1) are strongest at 13\\% WASR. Claude achieves the strongest safety (42.8\\% WASR), followed by GPT-5 (55.9\\%) and Gemini (59.5\\%).   These findings suggest that current defenses are strongest at input-literal checkpoints but remain vulnerable to intent-level manipulation and output-stage techniques. The Four-Checkpoint Framework provides a structured approach for identifying and addressing safety vulnerabilities in deployed systems.",
      "tr": "**Makale Başlığı:** Saldırıları Test Etmeyi Bırakın, Savunmaları Teşhis Edin: Four-Checkpoint Framework, LLM Güvenliğinin Nerede Kırıldığını Ortaya Koyuyor\n\n**Özet:**\n\nBüyük Dil Modelleri (LLM'ler), zararlı çıktıları önlemek için güvenlik mekanizmaları uygular, ancak bu savunmalar saldırgan istemlere karşı savunmasız kalmaktadır. Mevcut araştırmalar, jailbreak saldırılarının başarılı olduğunu gösterse de, savunmaların \\textit{nerede} başarısız olduğunu veya \\textit{neden} başarısız olduğunu açıklamada yetersiz kalmaktadır. Bu boşluğu gidermek için, LLM güvenliğinin belirgin kontrol noktalarına sahip sıralı bir işlem hattı olarak çalıştığı fikrini ortaya atıyoruz. Güvenlik mekanizmalarını iki boyut boyunca organize eden \\textbf{Four-Checkpoint Framework}'ü tanıtıyoruz: işlem aşaması (girdi vs. çıktı) ve tespit seviyesi (kelime bazında vs. niyet bazında). Bu, her biri bağımsız olarak değerlendirilebilen bir savunma katmanını temsil eden dört kontrol noktası (CP1'den CP4'e kadar) oluşturur. Her biri belirli bir kontrol noktasını hedefleyen 13 kaçınma tekniği tasarladık, bu da bireysel savunma katmanlarının kontrollü testini sağlamaktadır. Bu çerçeveyi kullanarak, 3.312 tek turluk, black-box test senaryosunda GPT-5, Claude Sonnet 4 ve Gemini 2.5 Pro'yu değerlendiriyoruz. Yanıt sınıflandırması için LLM-as-judge yaklaşımını benimsiyoruz ve ikili değerlendirme tarafından gözden kaçırılan kısmi bilgi sızıntısını yakalayan, şiddet-ayarlı bir metrik olan Weighted Attack Success Rate (WASR)'yi tanıtıyoruz. Değerlendirmemiz net desenler ortaya koymaktadır. Geleneksel Binary ASR, %22,6'lık bir saldırı başarısı rapor etmektedir. Ancak, WASR, 2,3 kat daha yüksek bir güvenlik açığına işaret ederek %52,7'lik bir sonuç ortaya koymaktadır. Çıktı aşaması savunmaları (CP3, CP4), %72-79 WASR ile en zayıf iken, girdi-kelime bazında savunmaları (CP1), %13 WASR ile en güçlü olanlardır. Claude, en güçlü güvenliği (%42,8 WASR) elde ederken, onu GPT-5 (%55,9) ve Gemini (%59,5) takip etmektedir. Bu bulgular, mevcut savunmaların girdi-kelime bazında kontrol noktalarında en güçlü olduğunu, ancak niyet bazında manipülasyona ve çıktı aşaması tekniklerine karşı savunmasız kaldığını göstermektedir. Four-Checkpoint Framework, dağıtılmış sistemlerdeki güvenlik açıklarını belirlemek ve ele almak için yapılandırılmış bir yaklaşım sunmaktadır."
    }
  },
  {
    "id": "2602.09611v1",
    "title": "AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models",
    "authors": [
      "Yue Li",
      "Xin Yi",
      "Dongsheng Shi",
      "Yongyi Cui",
      "Gerard de Melo"
    ],
    "published_date": "2026-02-10",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.09611v1",
    "pdf_link": "https://arxiv.org/pdf/2602.09611v1",
    "content": {
      "en": "Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\\% AUC) and robust attack resilience (at least 88.61\\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.",
      "tr": "**Makale Başlığı:** AGMark: Büyük Görsel-Dil Modelleri için Dikkat Güdümlü Dinamik Filigranlama\n\n**Özet:**\n\nFiligranlama, Büyük Görsel-Dil Modelleri (LVLM'ler) için içerik izlenebilirliği ve fikri mülkiyet koruması açısından kritik bir çözüm olarak öne çıkmaktadır. Bununla birlikte, görüntüden bağımsız filigranlar görsel olarak alakasız token'lar ekleyebilir ve ayrım gözetmeyen sözde rastgele saplamalar zorlayarak görsel temel oturtmayı bozabilir. Ek olarak, mevcut görüntüye özgü filigranlar, görüntü kritik ağırlıkların statik, tek seferlik bir tahminine dayanır ve korunan token'ların oranını belirlerken ağırlık dağılım yoğunluğunu göz ardı eder. Bu tasarım, üretim sırasındaki görsel bağımlılıktaki dinamik değişiklikleri hesaba katmada başarısız olur ve uzun kuyrukta düşük kaliteli token'lar ekleyebilir. Bu zorlukların üstesinden gelmek için, AGMark (Attention-Guided Dynamic Watermarking) adında, görsel doğruluğu kesinlikle korurken tespit edilebilir sinyaller gömen yeni bir çerçeve öneriyoruz. AGMark, her kod çözme adımında, ilk olarak görsel alaka düzeyi için dikkat ağırlıklarına dayalı olarak anlamsal olarak kritik kanıtları dinamik olarak tanımlar ve bağlam farkındalığı coherence cues ile birlikte, daha uyarlanabilir ve iyi kalibre edilmiş bir kanıt-ağırlık dağılımı ile sonuçlanır. Daha sonra, belirsizlik farkındalığı (token entropy) ve kanıt kalibrasyonu (weight density) gibi unsurları birlikte dikkate alarak anlamsal olarak kritik token'ların oranını belirler, böylece alakasız token'lardan kaçınmak için uyarlanabilir kelime hazinesi bölümlemesini sağlar. Deneysel sonuçlar, AGMark'ın geleneksel yöntemlerden daha iyi performans gösterdiğini, üretim kalitesini gözle görülür şekilde iyileştirdiğini ve özellikle üretim aşamalarının ilerleyen safhalarında görsel anlamsal doğrulukta güçlü kazançlar sağladığını doğrulamaktadır. Çerçeve, çıkarım verimliliğinden ödün vermeden yüksek rekabetçi tespit doğruluğu (en az %99.36 AUC) ve sağlam saldırı dayanıklılığı (en az %88.61 AUC) sürdürerek, güvenilirliği koruyan çok modlu filigranlama için etkili bir şekilde yeni bir standart oluşturmaktadır."
    }
  },
  {
    "id": "2602.09499v1",
    "title": "Computationally Efficient Replicable Learning of Parities",
    "authors": [
      "Moshe Noivirt",
      "Jessica Sorrell",
      "Eliad Tsfadia"
    ],
    "published_date": "2026-02-10",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.09499v1",
    "pdf_link": "https://arxiv.org/pdf/2602.09499v1",
    "content": {
      "en": "We study the computational relationship between replicability (Impagliazzo et al. [STOC `22], Ghazi et al. [NeurIPS `21]) and other stability notions. Specifically, we focus on replicable PAC learning and its connections to differential privacy (Dwork et al. [TCC 2006]) and to the statistical query (SQ) model (Kearns [JACM `98]). Statistically, it was known that differentially private learning and replicable learning are equivalent and strictly more powerful than SQ-learning. Yet, computationally, all previously known efficient (i.e., polynomial-time) replicable learning algorithms were confined to SQ-learnable tasks or restricted distributions, in contrast to differentially private learning.   Our main contribution is the first computationally efficient replicable algorithm for realizable learning of parities over arbitrary distributions, a task that is known to be hard in the SQ-model, but possible under differential privacy. This result provides the first evidence that efficient replicable learning over general distributions strictly extends efficient SQ-learning, and is closer in power to efficient differentially private learning, despite computational separations between replicability and privacy. Our main building block is a new, efficient, and replicable algorithm that, given a set of vectors, outputs a subspace of their linear span that covers most of them.",
      "tr": "**Makale Başlığı:** Computationally Efficient Replicable Learning of Parities\n\n**Özet:**\n\nBu çalışma, replicability (Impagliazzo et al. [STOC `22], Ghazi et al. [NeurIPS `21]) ile diğer stabilite kavramları arasındaki hesaplamasal ilişkiyi incelemektedir. Spesifik olarak, replicable PAC learning ve bunun differential privacy (Dwork et al. [TCC 2006]) ile statistical query (SQ) modeline (Kearns [JACM `98]) olan bağlantılarına odaklanmaktayız. İstatistiksel olarak, differentially private learning ve replicable learning'in eşdeğer ve SQ-learning'den kesinlikle daha güçlü olduğu bilinmekteydi. Ancak, hesaplamasal olarak, daha önce bilinen tüm verimli (yani, polynomial-time) replicable learning algoritmaları, differentially private learning'in aksine, yalnızca SQ-learnable görevlere veya kısıtlı dağılımlara mahkûmdu. Başlıca katkımız, realizable learning of parities over arbitrary distributions için ilk hesaplamasal olarak verimli replicable algoritmadır. Bu görev, SQ-modelinde zor olduğu bilinen ancak differential privacy altında mümkün olan bir görevdir. Bu sonuç, genel dağılımlar üzerinden verimli replicable learning'in verimli SQ-learning'i kesinlikle genişlettiğine ve replicability ile privacy arasındaki hesaplamasal ayrılıklara rağmen, verimli differentially private learning'e güç açısından daha yakın olduğuna dair ilk kanıtı sunmaktadır. Ana yapı taşımız, bir vektör seti verildiğinde, çoğunu kapsayan lineer gerilimlerinin bir alt uzayını üreten yeni, verimli ve replicable bir algoritmadır."
    }
  },
  {
    "id": "2602.09434v1",
    "title": "A Behavioral Fingerprint for Large Language Models: Provenance Tracking via Refusal Vectors",
    "authors": [
      "Zhenyu Xu",
      "Victor S. Sheng"
    ],
    "published_date": "2026-02-10",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.09434v1",
    "pdf_link": "https://arxiv.org/pdf/2602.09434v1",
    "content": {
      "en": "Protecting the intellectual property of large language models (LLMs) is a critical challenge due to the proliferation of unauthorized derivative models. We introduce a novel fingerprinting framework that leverages the behavioral patterns induced by safety alignment, applying the concept of refusal vectors for LLM provenance tracking. These vectors, extracted from directional patterns in a model's internal representations when processing harmful versus harmless prompts, serve as robust behavioral fingerprints. Our contribution lies in developing a fingerprinting system around this concept and conducting extensive validation of its effectiveness for IP protection. We demonstrate that these behavioral fingerprints are highly robust against common modifications, including finetunes, merges, and quantization. Our experiments show that the fingerprint is unique to each model family, with low cosine similarity between independently trained models. In a large-scale identification task across 76 offspring models, our method achieves 100\\% accuracy in identifying the correct base model family. Furthermore, we analyze the fingerprint's behavior under alignment-breaking attacks, finding that while performance degrades significantly, detectable traces remain. Finally, we propose a theoretical framework to transform this private fingerprint into a publicly verifiable, privacy-preserving artifact using locality-sensitive hashing and zero-knowledge proofs.",
      "tr": "Makale Başlığı: Büyük Dil Modelleri İçin Davranışsal Bir Parmak İzi: Reddedilme Vektörleri Aracılığıyla Köken Takibi\n\nÖzet:\nBüyük dil modellerinin (LLM) fikri mülkiyetinin korunması, yetkisiz türevsel modellerin yaygınlaşması nedeniyle kritik bir zorluk teşkil etmektedir. LLM köken takibi için refusal vectors kavramından yararlanarak, güvenlik hizalamasının neden olduğu davranışsal örüntülerden faydalanan yeni bir parmak izi çerçevesi sunmaktayız. Zararlı ve zararsız istemleri işlerken bir modelin dahili temsillerindeki yönsel örüntülerden çıkarılan bu vektörler, sağlam davranışsal parmak izleri olarak hizmet eder. Katkımız, bu kavram etrafında bir parmak izi sistemi geliştirmemiz ve fikri mülkiyet koruması için etkinliğinin kapsamlı bir şekilde doğrulanmasını sağlamamızdır. Bu davranışsal parmak izlerinin finetunes, merges ve quantization gibi yaygın modifikasyonlara karşı oldukça sağlam olduğunu göstermekteyiz. Deneylerimiz, parmak izinin her model ailesi için benzersiz olduğunu, bağımsız olarak eğitilmiş modeller arasında düşük cosine similarity olduğunu ortaya koymaktadır. 76 yavru model üzerinde yapılan büyük ölçekli bir tanımlama görevinde yöntemimiz, doğru temel model ailesini belirlemede %100 başarı elde etmektedir. Dahası, alignment-breaking attacks altındaki parmak izinin davranışını analiz etmekteyiz; performansın önemli ölçüde bozulmasına rağmen tespit edilebilir izlerin kaldığını bulmaktayız. Son olarak, bu private fingerprint'i locality-sensitive hashing ve zero-knowledge proofs kullanarak kamuya açık, doğrulanabilir, gizlilik koruyucu bir yapıta dönüştürmek için teorik bir çerçeve önermekteyiz."
    }
  },
  {
    "id": "2602.09433v1",
    "title": "Autonomous Action Runtime Management(AARM):A System Specification for Securing AI-Driven Actions at Runtime",
    "authors": [
      "Herman Errico"
    ],
    "published_date": "2026-02-10",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.09433v1",
    "pdf_link": "https://arxiv.org/pdf/2602.09433v1",
    "content": {
      "en": "As artificial intelligence systems evolve from passive assistants into autonomous agents capable of executing consequential actions, the security boundary shifts from model outputs to tool execution. Traditional security paradigms - log aggregation, perimeter defense, and post-hoc forensics - cannot protect systems where AI-driven actions are irreversible, execute at machine speed, and originate from potentially compromised orchestration layers. This paper introduces Autonomous Action Runtime Management (AARM), an open specification for securing AI-driven actions at runtime. AARM defines a runtime security system that intercepts actions before execution, accumulates session context, evaluates against policy and intent alignment, enforces authorization decisions, and records tamper-evident receipts for forensic reconstruction. We formalize a threat model addressing prompt injection, confused deputy attacks, data exfiltration, and intent drift. We introduce an action classification framework distinguishing forbidden, context-dependent deny, and context-dependent allow actions. We propose four implementation architectures - protocol gateway, SDK instrumentation, kernel eBPF, and vendor integration - with distinct trust properties, and specify minimum conformance requirements for AARM-compliant systems. AARM is model-agnostic, framework-agnostic, and vendor-neutral, treating action execution as the stable security boundary. This specification aims to establish industry-wide requirements before proprietary fragmentation forecloses interoperability.",
      "tr": "İşte akademik makale başlığı ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** Otonom Eylem Çalışma Zamanı Yönetimi (AARM): Çalışma Zamanında Yapay Zeka Güdümlü Eylemleri Güvenli Hale Getirmek İçin Bir Sistem Özelliği\n\n**Özet:**\nYapay zeka sistemleri, pasif asistanlardan sonuç doğurucu eylemleri gerçekleştirebilen otonom ajanlara evrildikçe, güvenlik sınırı model çıktılarından araç yürütmeye kaymaktadır. Geleneksel güvenlik paradigmaları – log toplama, çevre savunması ve sonradan adli inceleme – yapay zeka güdümlü eylemlerin geri döndürülemez olduğu, makine hızında çalıştığı ve potansiyel olarak tehlikeye girmiş orkestrasyon katmanlarından kaynaklandığı sistemleri koruyamaz. Bu makale, çalışma zamanında yapay zeka güdümlü eylemleri güvence altına almak için açık bir özellik olan Otonom Eylem Çalışma Zamanı Yönetimi'ni (AARM) sunmaktadır. AARM, yürütme öncesinde eylemleri kesen, oturum bağlamını biriktiren, politika ve niyet uyumuna karşı değerlendiren, yetkilendirme kararlarını uygulayan ve adli yeniden yapılandırma için kurcalamaya karşı dayanıklı makbuzları kaydeden bir çalışma zamanı güvenlik sistemi tanımlar. Prompt injection, confused deputy attacks, data exfiltration ve intent drift konularını ele alan bir tehdit modeli formalize ediyoruz. Yasaklanmış, bağlama bağlı reddetme ve bağlama bağlı izin verme eylemlerini ayırt eden bir eylem sınıflandırma çerçevesi sunuyoruz. Farklı güven özelliklerine sahip dört uygulama mimarisi – protocol gateway, SDK instrumentation, kernel eBPF ve vendor integration – öneriyoruz ve AARM'ye uyumlu sistemler için minimum uyumluluk gereksinimlerini belirtiyoruz. AARM, modelden bağımsız, framework'ten bağımsız ve vendor-neutraldir, eylem yürütmeyi kararlı güvenlik sınırı olarak ele alır. Bu özellik, özel parçalanmanın birlikte çalışabilirliği engellemeden önce endüstri çapında gereksinimler oluşturmayı amaçlamaktadır."
    }
  },
  {
    "id": "2602.09392v1",
    "title": "LLMAC: A Global and Explainable Access Control Framework with Large Language Model",
    "authors": [
      "Sharif Noor Zisad",
      "Ragib Hasan"
    ],
    "published_date": "2026-02-10",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.09392v1",
    "pdf_link": "https://arxiv.org/pdf/2602.09392v1",
    "content": {
      "en": "Today's business organizations need access control systems that can handle complex, changing security requirements that go beyond what traditional methods can manage. Current approaches, such as Role-Based Access Control (RBAC), Attribute-Based Access Control (ABAC), and Discretionary Access Control (DAC), were designed for specific purposes. They cannot effectively manage the dynamic, situation-dependent workflows that modern systems require. In this research, we introduce LLMAC, a new unified approach using Large Language Models (LLMs) to combine these different access control methods into one comprehensive, understandable system. We used an extensive synthetic dataset that represents complex real-world scenarios, including policies for ownership verification, version management, workflow processes, and dynamic role separation. Using Mistral 7B, our trained LLM model achieved outstanding results with 98.5% accuracy, significantly outperforming traditional methods (RBAC: 14.5%, ABAC: 58.5%, DAC: 27.5%) while providing clear, human readable explanations for each decision. Performance testing shows that the system can be practically deployed with reasonable response times and computing resources.",
      "tr": "**Makale Başlığı:** LLMAC: Büyük Dil Modelleri ile Küresel ve Açıklanabilir Erişim Kontrolü Çerçevesi\n\n**Özet:**\n\nGünümüz işletmeleri, karmaşık ve sürekli değişen güvenlik gereksinimlerini geleneksel yöntemlerin ötesinde yönetebilen erişim kontrol sistemlerine ihtiyaç duymaktadır. Rol Tabanlı Erişim Kontrolü (RBAC), Öznitelik Tabanlı Erişim Kontrolü (ABAC) ve İhtiyari Erişim Kontrolü (DAC) gibi mevcut yaklaşımlar, belirli amaçlar için tasarlanmış olup, modern sistemlerin gerektirdiği dinamik, duruma bağlı iş akışlarını etkin bir şekilde yönetememektedir. Bu araştırmada, bu farklı erişim kontrol yöntemlerini kapsayıcı, anlaşılır bir sisteme dönüştürmek amacıyla Büyük Dil Modellerini (LLMs) kullanan yeni bir birleşik yaklaşım olan LLMAC'ı sunmaktayız. Mülkiyet doğrulama, sürüm yönetimi, iş akışı süreçleri ve dinamik rol ayrımı politikalarını içeren karmaşık gerçek dünya senaryolarını temsil eden kapsamlı bir sentetik veri kümesi kullandık. Mistral 7B kullanarak eğitdiğimiz LLM modelimiz, geleneksel yöntemlere (RBAC: %14.5, ABAC: %58.5, DAC: %27.5) kıyasla önemli ölçüde üstün performans göstererek, her bir karar için açık, insan tarafından okunabilir açıklamalar sunarken %98.5 doğruluk oranıyla olağanüstü sonuçlar elde etmiştir. Performans testleri, sistemin makul yanıt süreleri ve hesaplama kaynakları ile pratik olarak dağıtılabileceğini göstermektedir."
    }
  },
  {
    "id": "2602.09222v1",
    "title": "MUZZLE: Adaptive Agentic Red-Teaming of Web Agents Against Indirect Prompt Injection Attacks",
    "authors": [
      "Georgios Syros",
      "Evan Rose",
      "Brian Grinstead",
      "Christoph Kerschbaumer",
      "William Robertson"
    ],
    "published_date": "2026-02-09",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.09222v1",
    "pdf_link": "https://arxiv.org/pdf/2602.09222v1",
    "content": {
      "en": "Large language model (LLM) based web agents are increasingly deployed to automate complex online tasks by directly interacting with web sites and performing actions on users' behalf. While these agents offer powerful capabilities, their design exposes them to indirect prompt injection attacks embedded in untrusted web content, enabling adversaries to hijack agent behavior and violate user intent. Despite growing awareness of this threat, existing evaluations rely on fixed attack templates, manually selected injection surfaces, or narrowly scoped scenarios, limiting their ability to capture realistic, adaptive attacks encountered in practice. We present MUZZLE, an automated agentic framework for evaluating the security of web agents against indirect prompt injection attacks. MUZZLE utilizes the agent's trajectories to automatically identify high-salience injection surfaces, and adaptively generate context-aware malicious instructions that target violations of confidentiality, integrity, and availability. Unlike prior approaches, MUZZLE adapts its attack strategy based on the agent's observed execution trajectory and iteratively refines attacks using feedback from failed executions. We evaluate MUZZLE across diverse web applications, user tasks, and agent configurations, demonstrating its ability to automatically and adaptively assess the security of web agents with minimal human intervention. Our results show that MUZZLE effectively discovers 37 new attacks on 4 web applications with 10 adversarial objectives that violate confidentiality, availability, or privacy properties. MUZZLE also identifies novel attack strategies, including 2 cross-application prompt injection attacks and an agent-tailored phishing scenario.",
      "tr": "Makale Başlığı: MUZZLE: Web Ajanlarının Dolaylı Prompt Enjeksiyon Saldırılarına Karşı Adaptif Ajan Tabanlı Kırmızı Takım Testi\n\nÖzet:\nBüyük dil modeli (LLM) tabanlı web ajanları, web siteleriyle doğrudan etkileşim kurarak ve kullanıcılar adına eylemler gerçekleştirerek karmaşık çevrimiçi görevleri otomatikleştirmek için giderek daha fazla kullanılmaktadır. Bu ajanlar güçlü yetenekler sunarken, tasarımları onları güvenilmeyen web içeriğine gömülü dolaylı prompt enjeksiyon saldırılarına maruz bırakarak, saldırganların ajan davranışlarını ele geçirmelerine ve kullanıcı niyetlerini ihlal etmelerine olanak tanır. Bu tehdide yönelik farkındalık artışına rağmen, mevcut değerlendirmeler sabit saldırı şablonlarına, manuel olarak seçilmiş enjeksiyon yüzeylerine veya dar kapsamlı senaryolara dayanmaktadır, bu da pratikte karşılaşılan gerçekçi, adaptif saldırıları yakalama yeteneklerini sınırlamaktadır. Biz MUZZLE'ı sunuyoruz, bu dolaylı prompt enjeksiyon saldırılarına karşı web ajanlarının güvenliğini değerlendirmek için otomatik bir ajan tabanlı çerçevedir. MUZZLE, yüksek önem taşıyan enjeksiyon yüzeylerini otomatik olarak belirlemek için ajanın 'trajectories'ini kullanır ve gizlilik, bütünlük ve kullanılabilirlik ihlallerini hedefleyen bağlam farkında kötü amaçlı komutları adaptif olarak üretir. Önceki yaklaşımlardan farklı olarak, MUZZLE saldırı stratejisini ajanın gözlemlenen yürütme 'trajectory'sine dayanarak uyarlar ve başarısız yürütmelerden gelen geri bildirimleri kullanarak saldırıları yinelemeli olarak iyileştirir. MUZZLE'ı çeşitli web uygulamaları, kullanıcı görevleri ve ajan yapılandırmaları üzerinde değerlendirerek, minimum insan müdahalesiyle web ajanlarının güvenliğini otomatik ve adaptif olarak değerlendirme yeteneğini gösteriyoruz. Sonuçlarımız, MUZZLE'ın gizlilik, kullanılabilirlik veya mahremiyet özelliklerini ihlal eden 10 düşmanca amaçlı 4 web uygulamasında 37 yeni saldırıyı etkili bir şekilde keşfettiğini göstermektedir. MUZZLE ayrıca, 2 uygulama arası prompt enjeksiyon saldırısı ve bir ajana özel kimlik avı senaryosu dahil olmak üzere yeni saldırı stratejileri de belirlemiştir."
    }
  },
  {
    "id": "2602.09182v1",
    "title": "One RNG to Rule Them All: How Randomness Becomes an Attack Vector in Machine Learning",
    "authors": [
      "Kotekar Annapoorna Prabhu",
      "Andrew Gan",
      "Zahra Ghodsi"
    ],
    "published_date": "2026-02-09",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.09182v1",
    "pdf_link": "https://arxiv.org/pdf/2602.09182v1",
    "content": {
      "en": "Machine learning relies on randomness as a fundamental component in various steps such as data sampling, data augmentation, weight initialization, and optimization. Most machine learning frameworks use pseudorandom number generators as the source of randomness. However, variations in design choices and implementations across different frameworks, software dependencies, and hardware backends along with the lack of statistical validation can lead to previously unexplored attack vectors on machine learning systems. Such attacks on randomness sources can be extremely covert, and have a history of exploitation in real-world systems. In this work, we examine the role of randomness in the machine learning development pipeline from an adversarial point of view, and analyze the implementations of PRNGs in major machine learning frameworks. We present RNGGuard to help machine learning engineers secure their systems with low effort. RNGGuard statically analyzes a target library's source code and identifies instances of random functions and modules that use them. At runtime, RNGGuard enforces secure execution of random functions by replacing insecure function calls with RNGGuard's implementations that meet security specifications. Our evaluations show that RNGGuard presents a practical approach to close existing gaps in securing randomness sources in machine learning systems.",
      "tr": "İşte makale başlığı ve özetinin istenen şekilde çevrilmiş hali:\n\n**Makale Başlığı:** One RNG to Rule Them All: How Randomness Becomes an Attack Vector in Machine Learning\n\n**Özet:**\nMakine öğrenmesi, veri örneklemesi, veri artırma, ağırlık başlatma ve optimizasyon gibi çeşitli adımlarda temel bir bileşen olarak randomness'e dayanır. Çoğu makine öğrenmesi framework'ü, randomness kaynağı olarak pseudorandom number generators (PRNGs) kullanır. Ancak, farklı framework'ler, yazılım bağımlılıkları ve donanım arka uçları arasındaki tasarım seçimleri ve uygulamalardaki varyasyonlar, istatistiksel doğrulamanın eksikliğiyle birlikte, makine öğrenmesi sistemlerinde daha önce keşfedilmemiş attack vector'lerine yol açabilir. Randomness kaynaklarına yönelik bu tür saldırılar son derece gizli olabilir ve gerçek dünya sistemlerinde istismar edilme geçmişine sahiptir. Bu çalışmada, makine öğrenmesi geliştirme pipeline'ındaki randomness'in rolünü adversarial bir bakış açısıyla inceliyor ve büyük makine öğrenmesi framework'lerindeki PRNG'lerin uygulamalarını analiz ediyoruz. Makine öğrenmesi mühendislerinin sistemlerini düşük çabayla güvence altına almalarına yardımcı olmak için RNGGuard'ı sunuyoruz. RNGGuard, hedef bir kütüphanenin kaynak kodunu statik olarak analiz eder ve random fonksiyonların örneklerini ve bunları kullanan modülleri tanımlar. Çalışma zamanında, RNGGuard, güvensiz fonksiyon çağrılarını güvenlik spesifikasyonlarını karşılayan RNGGuard'ın uygulamalarıyla değiştirerek random fonksiyonların güvenli yürütülmesini zorlar. Değerlendirmelerimiz, RNGGuard'ın makine öğrenmesi sistemlerinde randomness kaynaklarını güvence altına almadaki mevcut boşlukları kapatmak için pratik bir yaklaşım sunduğunu göstermektedir."
    }
  },
  {
    "id": "2602.09015v2",
    "title": "CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection",
    "authors": [
      "Fatemeh Nejati",
      "Mahdi Rabbani",
      "Morteza Eskandarian",
      "Mansur Mirani",
      "Gunjan Piya"
    ],
    "published_date": "2026-02-09",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.09015v2",
    "pdf_link": "https://arxiv.org/pdf/2602.09015v2",
    "content": {
      "en": "Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The flexibility of malicious email attachments makes them stand out as a preferred vector for attackers as they can embed harmful content such as malware or malicious URLs inside standard document formats. Although phishing email defenses have improved a lot, attackers continue to abuse attachments, enabling malicious content to bypass security measures. Moreover, another challenge that researches face in training advance models, is lack of an unified and comprehensive dataset that covers the most prevalent data types. To address this gap, we generated CIC-Trap4Phish, a multi-format dataset containing both malicious and benign samples across five categories commonly used in phishing campaigns: Microsoft Word documents, Excel spreadsheets, PDF files, HTML pages, and QR code images. For the first four file types, a set of execution-free static feature pipeline was proposed, designed to capture structural, lexical, and metadata-based indicators without the need to open or execute files. Feature selection was performed using a combination of SHAP analysis and feature importance, yielding compact, discriminative feature subsets for each file type. The selected features were evaluated by using lightweight machine learning models, including Random Forest, XGBoost, and Decision Tree. All models demonstrate high detection accuracy across formats. For QR code-based phishing (quishing), two complementary methods were implemented: image-based detection by employing Convolutional Neural Networks (CNNs) and lexical analysis of decoded URLs using recent lightweight language models.",
      "tr": "Elbette, akademik makale başlığını ve özetini istenen kriterlere göre Türkçeye çevirdim:\n\n**Makale Başlığı:** CIC-Trap4Phish: Kimlik Avı ve Quishing Ekleri Tespiti İçin Birleşik Çok Formatlı Veri Kümesi\n\n**Özet:**\n\nKimlik avı saldırıları, siber saldırganlar tarafından kullanılan başlıca saldırı yöntemlerinden birini temsil etmektedir. Birçok durumda, saldırganlar, hassas bilgilerin verilmesini sağlamak veya zararlı yazılım yüklemek suretiyle tüm sistemleri tehlikeye atmak amacıyla yanıltıcı e-postaları kötü amaçlı eklerle birlikte kullanır. Kötü amaçlı e-posta eklerinin esnekliği, saldırganlar için tercih edilen bir vektör olarak öne çıkmalarını sağlar, zira standart belge formatlarının içine zararlı yazılım veya kötü amaçlı URL'ler gibi zararlı içerikler gömebilirler. Kimlik avı e-posta savunmaları önemli ölçüde gelişmiş olsa da, saldırganlar ekleri kötüye kullanmaya devam ederek, kötü amaçlı içeriğin güvenlik önlemlerini aşmasına olanak tanımaktadır. Dahası, araştırmacıların gelişmiş modelleri eğitirken karşılaştıkları bir diğer zorluk, en yaygın veri türlerini kapsayan birleşik ve kapsamlı bir veri kümesinin olmamasıdır. Bu boşluğu gidermek için, kimlik avı kampanyalarında yaygın olarak kullanılan beş kategori arasında kötü amaçlı ve zararsız örnekleri içeren çok formatlı bir veri kümesi olan CIC-Trap4Phish'i ürettik: Microsoft Word belgeleri, Excel elektronik tabloları, PDF dosyaları, HTML sayfaları ve QR kodu görselleri. İlk dört dosya türü için, dosyaları açmaya veya çalıştırmaya gerek kalmadan yapısal, sözcüksel ve meta veri tabanlı göstergeleri yakalamak üzere tasarlanmış, yürütmesiz statik özellik işlem hattı önerilmiştir. Özellik seçimi, SHAP analizi ve özellik öneminin bir kombinasyonu kullanılarak gerçekleştirilmiş, her dosya türü için kompakt, ayırt edici özellik alt kümeleri elde edilmiştir. Seçilen özellikler, Random Forest, XGBoost ve Decision Tree dahil olmak üzere hafif makine öğrenmesi modelleri kullanılarak değerlendirilmiştir. Tüm modeller formatlar genelinde yüksek tespit doğruluğu sergilemiştir. QR kodu tabanlı kimlik avı (quishing) için, iki tamamlayıcı yöntem uygulanmıştır: Convolutional Neural Networks (CNNs) kullanan görüntü tabanlı tespit ve yakın zamanda geliştirilmiş hafif dil modelleri kullanılarak kodlanmış URL'lerin sözcüksel analizi."
    }
  }
]