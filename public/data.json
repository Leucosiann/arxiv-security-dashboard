[
  {
    "id": "2602.04653v1",
    "title": "Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates",
    "authors": [
      "Ariel Fogel",
      "Omer Hofman",
      "Eilon Cohen",
      "Roman Vainshtein"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.04653v1",
    "pdf_link": "https://arxiv.org/pdf/2602.04653v1",
    "content": {
      "en": "Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain.",
      "tr": "**Makale Başlığı:** LLM Chat Templates İçerisindeki Gizli Talimatlar Aracılığıyla Çıkarım Zamanlı Arka Kapılar\n\n**Özet:**\n\nAçık ağırlıklı dil modelleri üretim ortamlarında giderek daha fazla kullanılmakta, bu da yeni güvenlik zorlukları ortaya çıkarmaktadır. Bu bağlamdaki belirgin tehditlerden biri, saldırganların dil modellerine belirli koşullar altında aktifleşen gizli davranışlar yerleştirdiği arka kapı saldırılarıdır. Önceki çalışmalar, saldırganların eğitim ardışık düzenlerine veya dağıtım altyapısına erişiminin olduğunu varsaymıştır. Biz, bunlardan hiçbirine ihtiyaç duymayan, ancak sohbet şablonunu (chat template) kullanan yeni bir saldırı yüzeyi öneriyoruz. Sohbet şablonları, her çıkarım çağrısında (inference call) çağrılan, kullanıcı girdisi ile model işleme arasındaki ayrıcalıklı bir konumu işgal eden yürütülebilir Jinja2 programlarıdır. Zararlı bir şekilde değiştirilmiş bir şablona sahip bir model dağıtan bir saldırganın, model ağırlıklarını değiştirmeden, eğitim verilerini zehirlemeden veya çalışma zamanı altyapısını kontrol etmeden bir çıkarım zamanlı arka kapı (inference-time backdoor) yerleştirebileceğini gösteriyoruz. Bu saldırı vektörünü, iki hedefi amaçlayan şablon arka kapıları inşa ederek değerlendirdik: olgusal doğruluğu (factual accuracy) düşürmek ve saldırgan kontrolündeki URL'lerin emisyonunu sağlamak. Bu saldırıları, yedi aile ve dört çıkarım motorunu (inference engines) kapsayan on sekiz model üzerinde uyguladık. Tetiklenen koşullar altında, olgusal doğruluk ortalama olarak %90'dan %15'e düşerken, saldırgan kontrolündeki URL'ler %80'in üzerinde başarı oranlarıyla yayılmaktadır; zararsız girdilerde ise ölçülebilir bir bozulma gözlenmemiştir. Arka kapılar, çıkarım çalışma zamanları boyunca genelleme yapmakta ve en büyük açık ağırlıklı dağıtım platformu tarafından uygulanan tüm otomatik güvenlik taramalarından kaçınmaktadır. Bu sonuçlar, sohbet şablonlarını LLM tedarik zincirinde güvenilir ve şu anda savunmasız bir saldırı yüzeyi olarak belirlemektedir."
    }
  },
  {
    "id": "2602.04616v1",
    "title": "A Human-Centered Privacy Approach (HCP) to AI",
    "authors": [
      "Luyi Sun",
      "Wei Xu",
      "Zaifeng Gao"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.HC",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.04616v1",
    "pdf_link": "https://arxiv.org/pdf/2602.04616v1",
    "content": {
      "en": "As the paradigm of Human-Centered AI (HCAI) gains prominence, its benefits to society are accompanied by significant ethical concerns, one of which is the protection of individual privacy. This chapter provides a comprehensive overview of privacy within HCAI, proposing a human-centered privacy (HCP) framework, providing integrated solution from technology, ethics, and human factors perspectives. The chapter begins by mapping privacy risks across each stage of AI development lifecycle, from data collection to deployment and reuse, highlighting the impact of privacy risks on the entire system. The chapter then introduces privacy-preserving techniques such as federated learning and dif erential privacy. Subsequent chapters integrate the crucial user perspective by examining mental models, alongside the evolving regulatory and ethical landscapes as well as privacy governance. Next, advice on design guidelines is provided based on the human-centered privacy framework. After that, we introduce practical case studies across diverse fields. Finally, the chapter discusses persistent open challenges and future research directions, concluding that a multidisciplinary approach, merging technical, design, policy, and ethical expertise, is essential to successfully embed privacy into the core of HCAI, thereby ensuring these technologies advance in a manner that respects and ensures human autonomy, trust and dignity.",
      "tr": "**Makale Başlığı:** Yapay Zeka İçin İnsan Odaklı Bir Gizlilik Yaklaşımı (HCP)\n\n**Özet:**\n\nİnsan Odaklı Yapay Zeka (HCAI) paradigması öne çıkarken, toplum için getirdiği faydalar bireysel gizliliğin korunması gibi önemli etik kaygıları da beraberinde getirmektedir. Bu bölüm, HCAI'deki gizliliğe kapsamlı bir genel bakış sunmakta ve teknoloji, etik ve insan faktörleri perspektiflerinden entegre bir çözüm sağlayan bir insan odaklı gizlilik (HCP) çerçevesi önermektedir. Bölüm, veri toplama aşamasından dağıtım ve yeniden kullanıma kadar yapay zeka geliştirme yaşam döngüsünün her aşamasındaki gizlilik risklerini haritalandırarak başlamakta ve gizlilik risklerinin tüm sistem üzerindeki etkisini vurgulamaktadır. Ardından, federated learning ve differential privacy gibi gizlilik koruma tekniklerini tanıtmaktadır. Müteakip bölümler, değişen düzenleyici ve etik manzaraların yanı sıra mental models, ve privacy governance'ı inceleyerek kritik kullanıcı perspektifini entegre etmektedir. Sonrasında, insan odaklı gizlilik çerçevesine dayalı tasarım yönergeleri konusunda tavsiyeler sunulmaktadır. Takip eden bölümlerde, farklı alanlarda pratik case studies sunulmaktadır. Son olarak, bölüm devam eden açık zorlukları ve gelecekteki araştırma yönlerini tartışmakta, yapay zeka geliştirme yaşam döngüsünün her aşamasındaki gizlilik risklerini haritalandırarak, bu teknolojilerin insan özerkliğini, güvenini ve onurunu gözeten ve güvence altına alan bir şekilde ilerlemesini sağlamak için teknik, tasarım, politika ve etik uzmanlığını birleştiren disiplinlerarası bir yaklaşımın başarının temel taşı olduğunu sonucuna varmaktadır."
    }
  },
  {
    "id": "2602.04448v1",
    "title": "RASA: Routing-Aware Safety Alignment for Mixture-of-Experts Models",
    "authors": [
      "Jiacheng Liang",
      "Yuhui Wang",
      "Tanqiu Jiang",
      "Ting Wang"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.04448v1",
    "pdf_link": "https://arxiv.org/pdf/2602.04448v1",
    "content": {
      "en": "Mixture-of-Experts (MoE) language models introduce unique challenges for safety alignment due to their sparse routing mechanisms, which can enable degenerate optimization behaviors under standard full-parameter fine-tuning. In our preliminary experiments, we observe that naively applying full-parameter safety fine-tuning to MoE models can reduce attack success rates through routing or expert dominance effects, rather than by directly repairing Safety-Critical Experts. To address this challenge, we propose RASA, a routing-aware expert-level alignment framework that explicitly repairs Safety-Critical Experts while preventing routing-based bypasses. RASA identifies experts disproportionately activated by successful jailbreaks, selectively fine-tunes only these experts under fixed routing, and subsequently enforces routing consistency with safety-aligned contexts. Across two representative MoE architectures and a diverse set of jailbreak attacks, RASA achieves near-perfect robustness, strong cross-attack generalization, and substantially reduced over-refusal, while preserving general capabilities on benchmarks such as MMLU, GSM8K, and TruthfulQA. Our results suggest that robust MoE safety alignment benefits from targeted expert repair rather than global parameter updates, offering a practical and architecture-preserving alternative to prior approaches.",
      "tr": "**Makale Başlığı:** RASA: Mixture-of-Experts Modelleri İçin Yönlendirmeye Duyarlı Güvenlik Hizalaması\n\n**Özet:**\n\nMixture-of-Experts (MoE) dil modelleri, standart tam-parametre ince ayarı (full-parameter fine-tuning) altında bozulmuş optimizasyon davranışlarına yol açabilen seyrek yönlendirme (sparse routing) mekanizmaları nedeniyle güvenlik hizalaması (safety alignment) açısından benzersiz zorluklar sunmaktadır. Ön deneylerimizde, MoE modellerine naif bir şekilde tam-parametre güvenlik ince ayarı uygulamanın, Güvenlik-Kritik Uzmanları (Safety-Critical Experts) doğrudan onarmak yerine, yönlendirme (routing) veya uzman hakimiyeti (expert dominance) etkileri yoluyla saldırı başarı oranlarını düşürebildiğini gözlemledik. Bu zorluğun üstesinden gelmek için, Güvenlik-Kritik Uzmanları açıkça onaran ve yönlendirme tabanlı atlatmaları engelleyen yönlendirmeye duyarlı uzman-seviyesi bir hizalama (expert-level alignment) çerçevesi olan RASA'yı öneriyoruz. RASA, başarılı jailbreak'ler tarafından orantısız bir şekilde aktive edilen uzmanları belirler, yalnızca bu uzmanları sabit yönlendirme altında seçici olarak ince ayar yapar ve ardından güvenlik hizalı bağlamlarla yönlendirme tutarlılığını (routing consistency) zorlar. İki temsili MoE mimarisi ve çeşitli jailbreak saldırıları genelinde, RASA neredeyse mükemmel bir sağlamlık (robustness), güçlü çapraz-saldırı genellemesi (cross-attack generalization) ve önemli ölçüde azalmış aşırı reddetme (over-refusal) oranları elde ederken, MMLU, GSM8K ve TruthfulQA gibi benchmark'larda genel yetenekleri korur. Sonuçlarımız, sağlam MoE güvenlik hizalamasının, küresel parametre güncellemeleri yerine hedeflenmiş uzman onarımından (targeted expert repair) fayda sağladığını ve önceki yaklaşımlara pratik ve mimariyi koruyan bir alternatif sunduğunu göstermektedir."
    }
  },
  {
    "id": "2602.04384v1",
    "title": "Blockchain Federated Learning for Sustainable Retail: Reducing Waste through Collaborative Demand Forecasting",
    "authors": [
      "Fabio Turazza",
      "Alessandro Neri",
      "Marcello Pietri",
      "Maria Angela Butturi",
      "Marco Picone"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.04384v1",
    "pdf_link": "https://arxiv.org/pdf/2602.04384v1",
    "content": {
      "en": "Effective demand forecasting is crucial for reducing food waste. However, data privacy concerns often hinder collaboration among retailers, limiting the potential for improved predictive accuracy. In this study, we explore the application of Federated Learning (FL) in Sustainable Supply Chain Management (SSCM), with a focus on the grocery retail sector dealing with perishable goods. We develop a baseline predictive model for demand forecasting and waste assessment in an isolated retailer scenario. Subsequently, we introduce a Blockchain-based FL model, trained collaboratively across multiple retailers without direct data sharing. Our preliminary results show that FL models have performance almost equivalent to the ideal setting in which parties share data with each other, and are notably superior to models built by individual parties without sharing data, cutting waste and boosting efficiency.",
      "tr": "Makale Başlığı: Sürdürülebilir Perakendecilik İçin Blockchain Federasyonel Öğrenme: İşbirlikçi Talep Tahmini Yoluyla Atığı Azaltma\n\nÖzet:\nEtkili talep tahmini, gıda atıklarını azaltmak için kritik öneme sahiptir. Bununla birlikte, veri gizliliği endişeleri perakendeciler arasındaki işbirliğini sıklıkla engellemekte ve tahmine dayalı doğruluğun iyileştirilme potansiyelini sınırlamaktadır. Bu çalışmada, özellikle bozulabilir ürünlerle ilgilenen bakkaliye perakendecilik sektörüne odaklanarak, Sürdürülebilir Tedarik Zinciri Yönetimi (SSCM) alanında Federated Learning (FL) uygulamasını araştırmaktayız. İzole bir perakendeci senaryosunda talep tahmini ve atık değerlendirmesi için bir temel tahmine dayalı model geliştiriyoruz. Ardından, doğrudan veri paylaşımı olmaksızın birden fazla perakendeci arasında işbirlikçi olarak eğitilen Blockchain tabanlı bir FL modeli sunuyoruz. İlk sonuçlarımız, FL modellerinin, tarafların birbirleriyle veri paylaştığı ideal ayarlara neredeyse eşdeğer performansa sahip olduğunu ve veri paylaşımı olmadan bireysel taraflarca oluşturulan modellere göre belirgin şekilde üstün olduğunu, atığı azalttığını ve verimliliği artırdığını göstermektedir."
    }
  },
  {
    "id": "2602.04294v1",
    "title": "How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks",
    "authors": [
      "Yanshu Wang",
      "Shuaishuai Yang",
      "Jingjing He",
      "Tong Yang"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.04294v1",
    "pdf_link": "https://arxiv.org/pdf/2602.04294v1",
    "content": {
      "en": "Large Language Models (LLMs) face increasing threats from jailbreak attacks that bypass safety alignment. While prompt-based defenses such as Role-Oriented Prompts (RoP) and Task-Oriented Prompts (ToP) have shown effectiveness, the role of few-shot demonstrations in these defense strategies remains unclear. Prior work suggests that few-shot examples may compromise safety, but lacks investigation into how few-shot interacts with different system prompt strategies. In this paper, we conduct a comprehensive evaluation on multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods. Our key finding reveals that few-shot demonstrations produce opposite effects on RoP and ToP: few-shot enhances RoP's safety rate by up to 4.5% through reinforcing role identity, while it degrades ToP's effectiveness by up to 21.2% through distracting attention from task instructions. Based on these findings, we provide practical recommendations for deploying prompt-based defenses in real-world LLM applications.",
      "tr": "İşte akademik makalenin başlık ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks\n\n**Özet:**\n\nBüyük Dil Modelleri (LLM'ler), güvenlik uyumluluğunu aşan jailbreak saldırılarının artan tehditleriyle karşı karşıyadır. Rol Odaklı Prompt'lar (RoP) ve Görev Odaklı Prompt'lar (ToP) gibi prompt tabanlı savunma yöntemleri etkili olduğunu gösterse de, bu savunma stratejilerinde few-shot demonstrations'ın rolü belirsizliğini korumaktadır. Önceki çalışmalar, few-shot örneklerinin güvenliği tehlikeye atabileceğini öne sürmüş, ancak few-shot'un farklı sistem prompt stratejileriyle nasıl etkileşime girdiğini araştırmamıştır. Bu çalışmada, altı jailbreak saldırı yöntemi kullanarak dört güvenlik benchmark'ı (AdvBench, HarmBench, SG-Bench, XSTest) üzerinde birden fazla ana akım LLM üzerinde kapsamlı bir değerlendirme yapılmıştır. Ana bulgumuz, few-shot demonstrations'ın RoP ve ToP üzerinde zıt etkiler ürettiğini ortaya koymaktadır: few-shot, rol kimliğini pekiştirerek RoP'nin güvenlik oranını %4.5'e kadar artırırken, görev talimatlarından dikkati dağıtarak ToP'nin etkinliğini %21.2'ye kadar düşürmektedir. Bu bulgulara dayanarak, gerçek dünya LLM uygulamalarında prompt tabanlı savunmaların konuşlandırılması için pratik öneriler sunmaktayız."
    }
  },
  {
    "id": "2602.04224v1",
    "title": "RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning",
    "authors": [
      "Zeming Wei",
      "Qiaosheng Zhang",
      "Xia Hu",
      "Xingcheng Xu"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "math.OC"
    ],
    "link": "http://arxiv.org/abs/2602.04224v1",
    "pdf_link": "https://arxiv.org/pdf/2602.04224v1",
    "content": {
      "en": "Large Reasoning Models (LRMs) have achieved tremendous success with their chain-of-thought (CoT) reasoning, yet also face safety issues similar to those of basic language models. In particular, while algorithms are designed to guide them to deliberately refuse harmful prompts with safe reasoning, this process often fails to generalize against diverse and complex jailbreak attacks. In this work, we attribute these failures to the generalization of the safe reasoning process, particularly their insufficiency against complex attack prompts. We provide both theoretical and empirical evidence to show the necessity of a more sufficient safe reasoning process to defend against advanced attack prompts. Building on this insight, we propose a Risk-Aware Preference Optimization (RAPO) framework that enables LRM to adaptively identify and address the safety risks with appropriate granularity in its thinking content. Extensive experiments demonstrate that RAPO successfully generalizes multiple LRMs' safe reasoning adaptively across diverse attack prompts whilst preserving general utility, contributing a robust alignment technique for LRM safety. Our code is available at https://github.com/weizeming/RAPO.",
      "tr": "Elbette, makale başlığını ve özetini istediğiniz şekilde Türkçeye çevirdim:\n\n**Makale Başlığı:** RAPO: Genelleştirilebilir Güvenli Reasoning için Risk-Bilinçli Tercih Optimizasyonu\n\n**Özet:**\nLarge Reasoning Models (LRMs), chain-of-thought (CoT) reasoning yetenekleri ile muazzam başarılar elde etmişlerdir; ancak, temel dil modellerine benzer şekilde güvenlik sorunlarıyla da karşılaşmaktadırlar. Özellikle, algoritmalar bu modelleri güvenli reasoning yoluyla kasıtlı olarak zararlı yönlendirmeleri reddetmeye yönlendirecek şekilde tasarlanmış olsa da, bu süreç genellikle çeşitli ve karmaşık jailbreak saldırılarına karşı genelleştirilememektedir. Bu çalışmada, bu başarısızlıkları güvenli reasoning sürecinin genelleştirilebilirliği, özellikle de karmaşık saldırı yönlendirmelerine karşı yetersizlikleri ile ilişkilendiriyoruz. Gelişmiş saldırı yönlendirmelerine karşı savunma için daha yeterli bir güvenli reasoning sürecinin gerekliliğini hem teorik hem de ampirik kanıtlarla göstermekteyiz. Bu içgörüye dayanarak, LRMs'in düşünce içeriğindeki uygun detay seviyesi ile güvenlik risklerini uyarlanabilir bir şekilde tanımlamasını ve ele almasını sağlayan bir Risk-Aware Preference Optimization (RAPO) çerçevesi öneriyoruz. Kapsamlı deneyler, RAPO'nun genel faydayı korurken, çeşitli saldırı yönlendirmeleri boyunca birden fazla LRM'nin güvenli reasoning yeteneğini uyarlanabilir bir şekilde başarıyla genelleştirdiğini göstermektedir. Bu, LRM güvenliği için sağlam bir alignment tekniği sunmaktadır. Kodumuz https://github.com/weizeming/RAPO adresinde mevcuttur."
    }
  },
  {
    "id": "2602.04113v1",
    "title": "ZKBoost: Zero-Knowledge Verifiable Training for XGBoost",
    "authors": [
      "Nikolas Melissaris",
      "Jiayi Xu",
      "Antigoni Polychroniadou",
      "Akira Takahashi",
      "Chenkai Weng"
    ],
    "published_date": "2026-02-04",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.04113v1",
    "pdf_link": "https://arxiv.org/pdf/2602.04113v1",
    "content": {
      "en": "Gradient boosted decision trees, particularly XGBoost, are among the most effective methods for tabular data. As deployment in sensitive settings increases, cryptographic guarantees of model integrity become essential. We present ZKBoost, the first zero-knowledge proof of training (zkPoT) protocol for XGBoost, enabling model owners to prove correct training on a committed dataset without revealing data or parameters. We make three key contributions: (1) a fixed-point XGBoost implementation compatible with arithmetic circuits, enabling instantiation of efficient zkPoT, (2) a generic template of zkPoT for XGBoost, which can be instantiated with any general-purpose ZKP backend, and (3) vector oblivious linear evaluation (VOLE)-based instantiation resolving challenges in proving nonlinear fixed-point operations. Our fixed-point implementation matches standard XGBoost accuracy within 1\\% while enabling practical zkPoT on real-world datasets.",
      "tr": "İşte akademik makalenin başlığı ve özetinin Türkçeye çevirisi:\n\n**Makale Başlığı:** ZKBoost: XGBoost için Sıfır Bilgi Doğrulanabilir Eğitim\n\n**Özet:**\n\nEğim artırımlı karar ağaçları, özellikle XGBoost, tablo verileri için en etkili yöntemler arasındadır. Hassas ortamlarda kullanımı arttıkça, model bütünlüğünün kriptografik garantileri vazgeçilmez hale gelmektedir. Biz, XGBoost için ilk sıfır-bilgi kanıtı (zero-knowledge proof of training - zkPoT) protokolü olan ZKBoost'u sunuyoruz. Bu protokol, model sahiplerinin veri veya parametreleri açıklamadan, kayıtlı bir veri kümesi üzerinde doğru eğitimi kanıtlamalarına olanak tanır. Üç temel katkımız bulunmaktadır: (1) verimli zkPoT'un oluşturulmasını sağlayan, aritmetik devrelerle uyumlu bir sabit nokta (fixed-point) XGBoost uygulaması; (2) herhangi bir genel amaçlı ZKP arka ucu ile oluşturulabilen, XGBoost için jenerik bir zkPoT şablonu; ve (3) doğrusal olmayan sabit nokta (nonlinear fixed-point) işlemlerinin kanıtlanmasındaki zorlukları çözen, vektör kör doğrusal değerlendirme (vector oblivious linear evaluation - VOLE) tabanlı bir oluşturma. Sabit nokta uygulamamız, standart XGBoost doğruluğunu %1'in içinde eşleştirmekte ve gerçek dünya veri kümeleri üzerinde pratik zkPoT'u mümkün kılmaktadır."
    }
  },
  {
    "id": "2602.04027v1",
    "title": "A Consensus-Bayesian Framework for Detecting Malicious Activity in Enterprise Directory Access Graphs",
    "authors": [
      "Pratyush Uppuluri",
      "Shilpa Noushad",
      "Sajan Kumar"
    ],
    "published_date": "2026-02-03",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.04027v1",
    "pdf_link": "https://arxiv.org/pdf/2602.04027v1",
    "content": {
      "en": "This work presents a consensus-based Bayesian framework to detect malicious user behavior in enterprise directory access graphs. By modeling directories as topics and users as agents within a multi-level interaction graph, we simulate access evolution using influence-weighted opinion dynamics. Logical dependencies between users are encoded in dynamic matrices Ci, and directory similarity is captured via a shared influence matrix W. Malicious behavior is injected as cross-component logical perturbations that violate structural norms of strongly connected components(SCCs). We apply theoretical guarantees from opinion dynamics literature to determine topic convergence and detect anomaly via scaled opinion variance. To quantify uncertainty, we introduce a Bayesian anomaly scoring mechanism that evolves over time, using both static and online priors. Simulations over synthetic access graphs validate our method, demonstrating its sensitivity to logical inconsistencies and robustness under dynamic perturbation.",
      "tr": "İşte akademik makale başlığı ve özetinin çevirisi:\n\n**Makale Başlığı: Kurumsal Dizin Erişim Grafiklerindeki Zararlı Faaliyetlerin Tespiti İçin Bir Consensus-Bayesian Çerçeve**\n\n**Özet:**\nBu çalışma, kurumsal dizin erişim grafiklerindeki zararlı kullanıcı davranışlarını tespit etmek için bir consensus-based Bayesian çerçeve sunmaktadır. Dizinleri konular (topics) ve kullanıcıları çok katmanlı bir etkileşim grafiği içindeki ajanlar (agents) olarak modelleyerek, etki ağırlıklı (influence-weighted) görüş dinamikleri (opinion dynamics) aracılığıyla erişim evrimini simüle ediyoruz. Kullanıcılar arasındaki mantıksal bağımlılıklar dinamik matrisler Ci'ye kodlanmakta ve dizin benzerliği paylaşılan bir etki matrisi (shared influence matrix) W aracılığıyla yakalanmaktadır. Zararlı davranışlar, strongly connected components (SCCs) yapısının normlarını ihlal eden çapraz bileşen mantıksal pertürbasyonlar (cross-component logical perturbations) olarak enjekte edilmektedir. Konu yakınsaması (topic convergence) belirlemek ve scaled opinion variance yoluyla anomalileri tespit etmek için görüş dinamikleri literatüründen teorik güvenceler (theoretical guarantees) uyguluyoruz. Belirsizliği nicelleştirmek için, hem statik hem de online önsel bilgiler (online priors) kullanarak zamanla evrilen bir Bayesian anomali skorlama mekanizması (Bayesian anomaly scoring mechanism) tanıtıyoruz. Sentetik erişim grafikleri üzerinde yapılan simülasyonlar, mantıksal tutarsızlıklara karşı hassasiyetini ve dinamik pertürbasyon altında sağlamlığını (robustness) göstererek yöntemimizi doğrulamaktadır."
    }
  },
  {
    "id": "2602.03948v1",
    "title": "Privacy utility trade offs for parameter estimation in degree heterogeneous higher order networks",
    "authors": [
      "Bibhabasu Mandal",
      "Sagnik Nandy"
    ],
    "published_date": "2026-02-03",
    "tags": [
      "stat.ML",
      "cs.CR",
      "cs.LG",
      "cs.SI",
      "math.ST"
    ],
    "link": "http://arxiv.org/abs/2602.03948v1",
    "pdf_link": "https://arxiv.org/pdf/2602.03948v1",
    "content": {
      "en": "In sensitive applications involving relational datasets, protecting information about individual links from adversarial queries is of paramount importance. In many such settings, the available data are summarized solely through the degrees of the nodes in the network. We adopt the $β$ model, which is the prototypical statistical model adopted for this form of aggregated relational information, and study the problem of minimax-optimal parameter estimation under both local and central differential privacy constraints. We establish finite sample minimax lower bounds that characterize the precise dependence of the estimation risk on the network size and the privacy parameters, and we propose simple estimators that achieve these bounds up to constants and logarithmic factors under both local and central differential privacy frameworks. Our results provide the first comprehensive finite sample characterization of privacy utility trade offs for parameter estimation in $β$ models, addressing the classical graph case and extending the analysis to higher order hypergraph models. We further demonstrate the effectiveness of our methods through experiments on synthetic data and a real world communication network.",
      "tr": "Elbette, istediğiniz çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** Derece Heterojenliği Gösteren Yüksek Dereceli Ağlarda Parametre Tahmini İçin Gizlilik Fayda Ödünleşmeleri\n\n**Özet:**\n\nİlişkisel veri kümelerini içeren hassas uygulamalarda, bireysel bağlantılar hakkındaki bilgilerin kötü niyetli sorgulardan korunması büyük önem taşımaktadır. Bu tür birçok ortamda, mevcut veriler yalnızca ağdaki düğümlerin dereceleri aracılığıyla özetlenir. Bu tür toplu ilişkisel bilgi için prototipik istatistiksel model olan $β$ modelini benimsiyor ve hem yerel hem de merkezi differential privacy kısıtlamaları altında minimax-optimal parametre tahmini problemini inceliyoruz. Tahmin riskinin ağ boyutu ve gizlilik parametreleri üzerindeki kesin bağımlılığını karakterize eden sonlu örneklem minimax alt sınırlarını belirliyor ve hem yerel hem de merkezi differential privacy çerçeveleri altında bu sınırları sabitler ve logaritmik faktörlere kadar elde eden basit tahminciler öneriyoruz. Sonuçlarımız, $β$ modellerinde parametre tahmini için gizlilik fayda ödünleşmelerinin ilk kapsamlı sonlu örneklem karakterizasyonunu sunarak, klasik graf durumunu ele almakta ve analizi yüksek dereceli hipergraf modellerine genişletmektedir. Ayrıca, yöntemlerimizin etkinliğini sentetik veriler ve gerçek dünya bir iletişim ağı üzerindeki deneylerle gösteriyoruz."
    }
  },
  {
    "id": "2602.03792v1",
    "title": "WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents",
    "authors": [
      "Xilong Wang",
      "Yinuo Liu",
      "Zhun Wang",
      "Dawn Song",
      "Neil Gong"
    ],
    "published_date": "2026-02-03",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2602.03792v1",
    "pdf_link": "https://arxiv.org/pdf/2602.03792v1",
    "content": {
      "en": "Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user's intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing prompt injection attacks in webpages. Given a webpage, Step I extracts \\emph{segments of interest} that may be contaminated, and Step II evaluates each segment by checking its consistency with the webpage content as context. We show that WebSentinel is highly effective, substantially outperforming baseline methods across multiple datasets of both contaminated and clean webpages that we collected. Our code is available at: https://github.com/wxl-lxw/WebSentinel.",
      "tr": "Makale Başlığı: WebSentinel: Web Ajanları İçin Prompt Injection Saldırılarını Tespit Etme ve Konumlandırma\n\nÖzet:\nPrompt injection saldırıları, web ajanslarının kullanıcının amaçladığı görevler yerine saldırgan tarafından belirtilen görevleri yürütmesine neden olmak için web sayfası içeriğini manipüle eder. Bu tür saldırıları tespit etme ve konumlandırma için mevcut yöntemler, web-ajanı ortamında geçerli olmayan temel varsayımları nedeniyle sınırlı bir etkililik sergilemektedir. Bu çalışmada, web sayfalarındaki prompt injection saldırılarını tespit etmek ve konumlandırmak için iki aşamalı bir yaklaşım olan WebSentinel'ı öneriyoruz. Bir web sayfası verildiğinde, Adım I potansiyel olarak kirletilmiş olan \\emph{ilgi alanlarını} (segments of interest) çıkarır ve Adım II, her bir segmenti web sayfası içeriği ile olan tutarlılığını bağlam olarak kontrol ederek değerlendirir. WebSentinel'ın yüksek derecede etkili olduğunu, topladığımız hem kirletilmiş hem de temiz web sayfalarından oluşan birden çok veri kümesinde temel (baseline) yöntemlerden önemli ölçüde daha iyi performans gösterdiğini gösteriyoruz. Kodumuz şu adreste mevcuttur: https://github.com/wxl-lxw/WebSentinel."
    }
  }
]