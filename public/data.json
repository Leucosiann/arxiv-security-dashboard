[
  {
    "id": "2601.08623v1",
    "title": "SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models",
    "authors": [
      "Renyang Liu",
      "Kangjie Chen",
      "Han Qiu",
      "Jie Zhang",
      "Kwok-Yan Lam"
    ],
    "published_date": "2026-01-13",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.08623v1",
    "pdf_link": "https://arxiv.org/pdf/2601.08623v1",
    "content": {
      "en": "Image generation models (IGMs), while capable of producing impressive and creative content, often memorize a wide range of undesirable concepts from their training data, leading to the reproduction of unsafe content such as NSFW imagery and copyrighted artistic styles. Such behaviors pose persistent safety and compliance risks in real-world deployments and cannot be reliably mitigated by post-hoc filtering, owing to the limited robustness of such mechanisms and a lack of fine-grained semantic control. Recent unlearning methods seek to erase harmful concepts at the model level, which exhibit the limitations of requiring costly retraining, degrading the quality of benign generations, or failing to withstand prompt paraphrasing and adversarial attacks. To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection. Without modifying the underlying IGMs, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework comprises two core components: a latent-aware multi-modal safety classifier for identifying unsafe generation trajectories, and a token-level delta generator for precise semantic redirection, equipped with auxiliary predictors for token masking and adaptive scaling to localize and regulate the intervention. Empirical results across multiple representative unlearning tasks demonstrate that SafeRedir achieves effective unlearning capability, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks. Furthermore, SafeRedir generalizes effectively across a variety of diffusion backbones and existing unlearned models, validating its plug-and-play compatibility and broad applicability. Code and data are available at https://github.com/ryliu68/SafeRedir.",
      "tr": "Kesinlikle, işte istenen akademik makale başlığı ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** SafeRedir: Image Generation Modellerinde Robust Unlearning İçin Prompt Embedding Redirection\n\n**Özet:**\n\nImage generation models (IGMs), etkileyici ve yaratıcı içerikler üretme yeteneğine sahip olmalarına rağmen, eğitim verilerinden geniş bir yelpazede istenmeyen kavramları ezberleyebilirler. Bu durum, NSFW (Not Safe For Work) görseller ve telif hakkıyla korunan sanatsal tarzlar gibi güvensiz içeriklerin yeniden üretilmesine yol açar. Bu tür davranışlar, gerçek dünya uygulamalarında kalıcı güvenlik ve uyumluluk riskleri oluşturur ve bu mekanizmaların sınırlı sağlamlığı ile ince taneli anlamsal kontrolün eksikliği nedeniyle, sonradan uygulanan filtreleme ile güvenilir bir şekilde azaltılamaz. Son unlearning yöntemleri, model düzeyinde zararlı kavramları silmeyi amaçlar; ancak bu yöntemler, maliyetli yeniden eğitim gerektirme, iyi niyetli üretimlerin kalitesini düşürme veya prompt paraphrasing ve adversarial attacks'e karşı koyamama gibi sınırlamalar sergiler. Bu zorlukların üstesinden gelmek için, prompt embedding redirection aracılığıyla robust unlearning için hafif bir inference-time framework olan SafeRedir'ı tanıtıyoruz. Temel IGMs'leri değiştirmeden, SafeRedir, embedding space'de token-level interventions aracılığıyla güvensiz promptları güvenli anlamsal bölgelere adaptif olarak yönlendirir. Framework, iki temel bileşenden oluşur: güvensiz üretim yörüngelerini belirlemek için latent-aware multi-modal safety classifier ve token masking ve adaptive scaling için yardımcı öngörücülerle donatılmış, müdahalenin yerini belirleyen ve düzenleyen token-level delta generator. Birden çok temsili unlearning görevi üzerindeki ampirik sonuçlar, SafeRedir'ın etkili unlearning kabiliyeti, yüksek anlamsal ve algısal koruma, sağlam görüntü kalitesi ve adversarial attacks'e karşı geliştirilmiş direnç sağladığını göstermektedir. Ayrıca, SafeRedir, plug-and-play uyumluluğunu ve geniş uygulanabilirliğini doğrulayarak, çeşitli diffusion backbones ve mevcut unlearned modeller arasında etkili bir şekilde genelleme yapar. Kod ve veriler https://github.com/ryliu68/SafeRedir adresinde mevcuttur."
    }
  },
  {
    "id": "2601.08511v1",
    "title": "STAR: Detecting Inference-time Backdoors in LLM Reasoning via State-Transition Amplification Ratio",
    "authors": [
      "Seong-Gyu Park",
      "Sohee Park",
      "Jisu Lee",
      "Hyunsik Na",
      "Daeseon Choi"
    ],
    "published_date": "2026-01-13",
    "tags": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.08511v1",
    "pdf_link": "https://arxiv.org/pdf/2601.08511v1",
    "content": {
      "en": "Recent LLMs increasingly integrate reasoning mechanisms like Chain-of-Thought (CoT). However, this explicit reasoning exposes a new attack surface for inference-time backdoors, which inject malicious reasoning paths without altering model parameters. Because these attacks generate linguistically coherent paths, they effectively evade conventional detection. To address this, we propose STAR (State-Transition Amplification Ratio), a framework that detects backdoors by analyzing output probability shifts. STAR exploits the statistical discrepancy where a malicious input-induced path exhibits high posterior probability despite a low prior probability in the model's general knowledge. We quantify this state-transition amplification and employ the CUSUM algorithm to detect persistent anomalies. Experiments across diverse models (8B-70B) and five benchmark datasets demonstrate that STAR exhibits robust generalization capabilities, consistently achieving near-perfect performance (AUROC $\\approx$ 1.0) with approximately $42\\times$ greater efficiency than existing baselines. Furthermore, the framework proves robust against adaptive attacks attempting to bypass detection.",
      "tr": "**Makale Başlığı:** STAR: Durum-Geçiş Amplifikasyon Oranı ile LLM Muhakemesinde Çıkarım Zamanı Arka Kapılarını Tespit Etme\n\n**Özet:**\nSon zamanlardaki LLM'ler, Chain-of-Thought (CoT) gibi muhakeme mekanizmalarını giderek daha fazla entegre etmektedir. Ancak bu açık muhakeme, çıkarım zamanı arka kapıları için yeni bir saldırı yüzeyi oluşturmaktadır; bu arka kapılar, model parametrelerini değiştirmeden zararlı muhakeme yolları enjekte eder. Bu saldırılar dilbilimsel olarak tutarlı yollar ürettiği için geleneksel tespitleri etkili bir şekilde atlatırlar. Bunu ele almak için, çıktının olasılık kaymalarını analiz ederek arka kapıları tespit eden bir çerçeve olan STAR (State-Transition Amplification Ratio)'ı öneriyoruz. STAR, kötü niyetli bir girdinin neden olduğu yolun, modelin genel bilgisindeki düşük bir ön olasılığa rağmen yüksek bir son olasılık sergilediği istatistiksel tutarsızlıktan yararlanır. Bu durum-geçiş amplifikasyonunu ölçüyor ve kalıcı anomalileri tespit etmek için CUSUM algoritmasını kullanıyoruz. Çeşitli modeller (8B-70B) ve beş karşılaştırma veri kümesi üzerinde yapılan deneyler, STAR'ın sağlam genelleme yetenekleri sergilediğini, sürekli olarak mükemmel performans yakınında (AUROC $\\approx$ 1.0) ve mevcut temel çizgilere göre yaklaşık $42\\times$ daha yüksek verimlilikle ulaştığını göstermektedir. Ayrıca, çerçeve, tespitleri atlatmaya çalışan adaptif saldırılara karşı da dayanıklı olduğunu kanıtlamaktadır."
    }
  },
  {
    "id": "2601.08406v1",
    "title": "WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents",
    "authors": [
      "Xinyi Wu",
      "Jiagui Chen",
      "Geng Hong",
      "Jiayi Dong",
      "Xudong Pan"
    ],
    "published_date": "2026-01-13",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.08406v1",
    "pdf_link": "https://arxiv.org/pdf/2601.08406v1",
    "content": {
      "en": "Web Agents are increasingly deployed to perform complex tasks in real web environments, yet their security evaluation remains fragmented and difficult to standardize. We present WebTrap Park, an automated platform for systematic security evaluation of Web Agents through direct observation of their concrete interactions with live web pages. WebTrap Park instantiates three major sources of security risk into 1,226 executable evaluation tasks and enables action based assessment without requiring agent modification. Our results reveal clear security differences across agent frameworks, highlighting the importance of agent architecture beyond the underlying model. WebTrap Park is publicly accessible at https://security.fudan.edu.cn/webagent and provides a scalable foundation for reproducible Web Agent security evaluation.",
      "tr": "Aşağıdaki akademik makale başlığı ve özetinin Türkçe çevirisi, belirtilen teknik terimlerin İngilizce bırakılması ve resmi, akademik bir dil kullanılarak aşağıda sunulmuştur:\n\n**Makale Başlığı:** WebTrap Park: Web Ajanlarının Sistematik Güvenlik Değerlendirmesi İçin Otomatik Bir Platform\n\n**Özet:**\nWeb ajanları, gerçek web ortamlarında karmaşık görevleri yerine getirmek üzere giderek daha fazla konuşlandırılmaktadır, ancak güvenlik değerlendirmeleri parçalı kalmakta ve standartlaştırılması zorlaşmaktadır. WebTrap Park'ı sunuyoruz; bu, Web ajanlarının canlı web sayfalarıyla olan somut etkileşimlerinin doğrudan gözlemlenmesi yoluyla sistematik güvenlik değerlendirmesi için otomatik bir platformdur. WebTrap Park, ajan modifikasyonu gerektirmeksizin harekete dayalı bir değerlendirme sağlayarak, 1.226 çalıştırılabilir değerlendirme görevine üç ana güvenlik riski kaynağını somutlaştırmaktadır. Sonuçlarımız, ajan çerçeveleri arasında belirgin güvenlik farklılıklarını ortaya koymakta, temel modelin ötesinde ajan mimarisinin önemini vurgulamaktadır. WebTrap Park, https://security.fudan.edu.cn/webagent adresinde kamuya açık olarak erişilebilmekte ve tekrarlanabilir Web Ajan güvenlik değerlendirmesi için ölçeklenebilir bir temel sunmaktadır."
    }
  },
  {
    "id": "2601.08223v1",
    "title": "DNF: Dual-Layer Nested Fingerprinting for Large Language Model Intellectual Property Protection",
    "authors": [
      "Zhenhua Xu",
      "Yiran Zhao",
      "Mengting Zhong",
      "Dezhang Kong",
      "Changting Lin"
    ],
    "published_date": "2026-01-13",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.08223v1",
    "pdf_link": "https://arxiv.org/pdf/2601.08223v1",
    "content": {
      "en": "The rapid growth of large language models raises pressing concerns about intellectual property protection under black-box deployment. Existing backdoor-based fingerprints either rely on rare tokens -- leading to high-perplexity inputs susceptible to filtering -- or use fixed trigger-response mappings that are brittle to leakage and post-hoc adaptation. We propose \\textsc{Dual-Layer Nested Fingerprinting} (DNF), a black-box method that embeds a hierarchical backdoor by coupling domain-specific stylistic cues with implicit semantic triggers. Across Mistral-7B, LLaMA-3-8B-Instruct, and Falcon3-7B-Instruct, DNF achieves perfect fingerprint activation while preserving downstream utility. Compared with existing methods, it uses lower-perplexity triggers, remains undetectable under fingerprint detection attacks, and is relatively robust to incremental fine-tuning and model merging. These results position DNF as a practical, stealthy, and resilient solution for LLM ownership verification and intellectual property protection.",
      "tr": "Elbette, istenen çeviri aşağıdadır:\n\n**Makale Başlığı:** DNF: Büyük Dil Modeli Fikri Mülkiyet Koruması İçin İki Katmanlı İç İçe Parmak İzi\n\n**Özet:**\nBüyük dil modellerinin hızlı büyümesi, black-box dağıtım altında fikri mülkiyet korumasıyla ilgili acil endişeleri gündeme getirmektedir. Mevcut backdoor tabanlı parmak izleri ya nadir token'lara dayanır (bu da filtrelenmeye yatkın yüksek perplexity'li girdilere yol açar) ya da sızıntılara ve sonradan adaptasyona karşı dayanıksız olan sabit tetikleyici-yanıt eşleştirmeleri kullanır. Biz, alan-spesifik stilistik ipuçlarını örtük anlamsal tetikleyicilerle eşleştirerek hiyerarşik bir backdoor yerleştiren bir black-box yöntemi olan \\textsc{Dual-Layer Nested Fingerprinting} (DNF)'yi öneriyoruz. Mistral-7B, LLaMA-3-8B-Instruct ve Falcon3-7B-Instruct genelinde, DNF aşağı akış kullanımını koruyarak mükemmel fingerprint aktivasyonu sağlamaktadır. Mevcut yöntemlerle karşılaştırıldığında, daha düşük perplexity'li tetikleyiciler kullanır, fingerprint detection attacks altında tespit edilemez kalır ve artımlı fine-tuning ve model merging'e nispeten dayanıklıdır. Bu sonuçlar, DNF'yi LLM sahiplik doğrulaması ve fikri mülkiyet koruması için pratik, gizli ve dirençli bir çözüm olarak konumlandırmaktadır."
    }
  },
  {
    "id": "2601.08216v1",
    "title": "One-Shot Federated Ridge Regression: Exact Recovery via Sufficient Statistic Aggregation",
    "authors": [
      "Zahir Alsulaimawi"
    ],
    "published_date": "2026-01-13",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.08216v1",
    "pdf_link": "https://arxiv.org/pdf/2601.08216v1",
    "content": {
      "en": "Federated learning protocols require repeated synchronization between clients and a central server, with convergence rates depending on learning rates, data heterogeneity, and client sampling. This paper asks whether iterative communication is necessary for distributed linear regression. We show it is not. We formulate federated ridge regression as a distributed equilibrium problem where each client computes local sufficient statistics -- the Gram matrix and moment vector -- and transmits them once. The server reconstructs the global solution through a single matrix inversion. We prove exact recovery: under a coverage condition on client feature matrices, one-shot aggregation yields the centralized ridge solution, not an approximation. For heterogeneous distributions violating coverage, we derive non-asymptotic error bounds depending on spectral properties of the aggregated Gram matrix. Communication reduces from $\\mathcal{O}(Rd)$ in iterative methods to $\\mathcal{O}(d^2)$ total; for high-dimensional settings, we propose and experimentally validate random projection techniques reducing this to $\\mathcal{O}(m^2)$ where $m \\ll d$. We establish differential privacy guarantees where noise is injected once per client, eliminating the composition penalty that degrades privacy in multi-round protocols. We further address practical considerations including client dropout robustness, federated cross-validation for hyperparameter selection, and comparison with gradient-based alternatives. Comprehensive experiments on synthetic heterogeneous regression demonstrate that one-shot fusion matches FedAvg accuracy while requiring up to $38\\times$ less communication. The framework applies to kernel methods and random feature models but not to general nonlinear architectures.",
      "tr": "**Makale Başlığı:** Tek Atımlı Federe Ridge Regresyonu: Yeterli İstatistik Toplama Yoluyla Tam Kurtarma\n\n**Özet:**\n\nFedere öğrenme protokolleri, istemciler ve merkezi sunucu arasında tekrarlanan senkronizasyon gerektirir; yakınsama oranları öğrenme oranlarına, veri heterojenliğine ve istemci örneklemesine bağlıdır. Bu makale, dağıtılmış lineer regresyon için iteratif iletişimin gerekli olup olmadığını sorgulamaktadır. Gerekli olmadığını göstermekteyiz. Federe ridge regresyonunu, her istemcinin yerel yeterli istatistikleri -- Gram matrisi ve moment vektörü -- hesaplayıp bunları bir kez ilettiği dağıtılmış bir denge problemi olarak formüle ediyoruz. Sunucu, tek bir matris tersi alma yoluyla global çözümü yeniden yapılandırır. İstemci özellik matrisleri üzerinde bir kapsama koşulu altında, tek atımlı toplamanın bir yaklaşım değil, merkezileştirilmiş ridge çözümünü verdiği tam kurtarma teoremini ispatlıyoruz. Kapsamayı ihlal eden heterojen dağılımlar için, toplanmış Gram matrisinin spektral özelliklerine bağlı olarak asimptotik olmayan hata sınırları türetiyoruz. İletişim, iteratif yöntemlerde $\\mathcal{O}(Rd)$'den toplamda $\\mathcal{O}(d^2)$'ye düşer; yüksek boyutlu ayarlarda, bunun $\\mathcal{O}(m^2)$'ye indirgenmesi için rastgele projeksiyon teknikleri önerip deneysel olarak doğrulamaktayız, burada $m \\ll d$. Gürültünün istemci başına bir kez enjekte edildiği diferansiyel gizlilik garantileri kuruyoruz, bu da çok turlu protokollerde gizliliği bozan bileşim cezasını ortadan kaldırır. Ayrıca, istemci düşüşü dayanıklılığı, hiperparametre seçimi için federe çapraz doğrulama ve gradyan tabanlı alternatiflerle karşılaştırma gibi pratik hususları ele alıyoruz. Sentetik heterojen regresyon üzerindeki kapsamlı deneyler, tek atımlı füzyonun FedAvg doğruluğuyla eşleştiğini ve bunun için $38\\times$ daha az iletişim gerektirdiğini göstermektedir. Bu çerçeve, kernel yöntemleri ve rastgele özellik modelleri için geçerlidir, ancak genel doğrusal olmayan mimariler için geçerli değildir."
    }
  },
  {
    "id": "2601.08196v1",
    "title": "Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis",
    "authors": [
      "Da Song",
      "Yuheng Huang",
      "Boqi Chen",
      "Tianshuo Cong",
      "Randy Goebel"
    ],
    "published_date": "2026-01-13",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LO",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2601.08196v1",
    "pdf_link": "https://arxiv.org/pdf/2601.08196v1",
    "content": {
      "en": "The integration of large language models (LLMs) into autonomous agents has enabled complex tool use, yet in high-stakes domains, these systems must strictly adhere to regulatory standards beyond simple functional correctness. However, existing benchmarks often overlook implicit regulatory compliance, thus failing to evaluate whether LLMs can autonomously enforce mandatory safety constraints. To fill this gap, we introduce LogiSafetyGen, a framework that converts unstructured regulations into Linear Temporal Logic oracles and employs logic-guided fuzzing to synthesize valid, safety-critical traces. Building on this framework, we construct LogiSafetyBench, a benchmark comprising 240 human-verified tasks that require LLMs to generate Python programs that satisfy both functional objectives and latent compliance rules. Evaluations of 13 state-of-the-art (SOTA) LLMs reveal that larger models, despite achieving better functional correctness, frequently prioritize task completion over safety, which results in non-compliant behavior.",
      "tr": "Makale Başlığı: Mantık Güdümlü Sentez Yoluyla LLM Araç Çağırmada Örtük Düzenleyici Uyumluluğun Değerlendirilmesi\n\nÖzet:\nBüyük dil modellerinin (LLM'ler) otonom aracılara entegrasyonu, karmaşık araç kullanımını mümkün kılmış olsa da, yüksek riskli alanlarda bu sistemlerin yalnızca basit işlevsel doğruluktan daha fazlasını, düzenleyici standartlara sıkı sıkıya uyması gerekmektedir. Bununla birlikte, mevcut benchmarklar genellikle örtük düzenleyici uyumluluğu göz ardı etmekte, dolayısıyla LLM'lerin zorunlu güvenlik kısıtlamalarını otonom olarak uygulayıp uygulayamayacağını değerlendirememektedir. Bu boşluğu doldurmak için, yapılandırılmamış düzenlemeleri Linear Temporal Logic oracle'larına dönüştüren ve geçerli, güvenliğe duyarlı izleri sentezlemek için logic-guided fuzzing kullanan bir framework olan LogiSafetyGen'i tanıtıyoruz. Bu framework üzerine inşa ederek, LLM'lerin hem işlevsel hedefleri hem de gizli uyumluluk kurallarını karşılayan Python programları üretmesini gerektiren 240 insan tarafından doğrulanmış görevi içeren bir benchmark olan LogiSafetyBench'i oluşturuyoruz. 13 adet state-of-the-art (SOTA) LLM'nin değerlendirmeleri, daha büyük modellerin, daha iyi işlevsel doğruluk elde etmelerine rağmen, sıklıkla görev tamamlama önceliğini güvenliğe vermekte olduğunu ve bunun da uyumsuz davranışlara yol açtığını ortaya koymaktadır."
    }
  },
  {
    "id": "2601.08189v1",
    "title": "ForgetMark: Stealthy Fingerprint Embedding via Targeted Unlearning in Language Models",
    "authors": [
      "Zhenhua Xu",
      "Haobo Zhang",
      "Zhebo Wang",
      "Qichen Liu",
      "Haitao Xu"
    ],
    "published_date": "2026-01-13",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.08189v1",
    "pdf_link": "https://arxiv.org/pdf/2601.08189v1",
    "content": {
      "en": "Existing invasive (backdoor) fingerprints suffer from high-perplexity triggers that are easily filtered, fixed response patterns exposed by heuristic detectors, and spurious activations on benign inputs. We introduce \\textsc{ForgetMark}, a stealthy fingerprinting framework that encodes provenance via targeted unlearning. It builds a compact, human-readable key--value set with an assistant model and predictive-entropy ranking, then trains lightweight LoRA adapters to suppress the original values on their keys while preserving general capabilities. Ownership is verified under black/gray-box access by aggregating likelihood and semantic evidence into a fingerprint success rate. By relying on probabilistic forgetting traces rather than fixed trigger--response patterns, \\textsc{ForgetMark} avoids high-perplexity triggers, reduces detectability, and lowers false triggers. Across diverse architectures and settings, it achieves 100\\% ownership verification on fingerprinted models while maintaining standard performance, surpasses backdoor baselines in stealthiness and robustness to model merging, and remains effective under moderate incremental fine-tuning. Our code and data are available at \\href{https://github.com/Xuzhenhua55/ForgetMark}{https://github.com/Xuzhenhua55/ForgetMark}.",
      "tr": "İşte akademik makale başlığı ve özetinin çevirisi:\n\n**Makale Başlığı:** ForgetMark: Dil Modellerinde Hedeflenmiş Unlearning Yoluyla Gizli Parmak İzi Gömme\n\n**Özet:**\nMevcut istilacı (arka kapı) parmak izleri, kolayca filtrelenen yüksek perplexity tetikleyicilerinden, sezgisel dedektörler tarafından ortaya çıkarılan sabit yanıt modellerinden ve iyi huylu girdilerdeki yapay etkinleşmelerden muzdariptir. Hedeflenmiş unlearning yoluyla kökeni kodlayan gizli bir fingerprinting framework'ü olan \\textsc{ForgetMark}'ı tanıtıyoruz. Bir asistan model ve predictive-entropy ranking ile kompakt, insan tarafından okunabilir bir key--value set oluşturur, ardından orijinal değerleri anahtarları üzerindeyken bastırmak için hafif LoRA adapter'lar eğitir, genel yetenekleri koruyarak. Sahiplik, black/gray-box erişimi altında likelihood ve semantik kanıtı bir fingerprint success rate'e toplamak suretiyle doğrulanır. Sabit tetikleyici-yanıt modelleri yerine olasılıksal unutma izlerine dayanarak, \\textsc{ForgetMark} yüksek perplexity tetikleyicilerden kaçınır, tespit edilebilirliği azaltır ve yanlış tetikleyicileri düşürür. Çeşitli mimariler ve ayarlarda, parmak izli modellerde %100 sahiplik doğrulaması elde ederken standart performansı korur, gizlilik ve model birleştirmeye karşı sağlamlık açısından arka kapı temel çizgilerini geride bırakır ve orta düzeyde artımlı ince ayar altında etkili kalır. Kodumuz ve verilerimiz \\href{https://github.com/Xuzhenhua55/ForgetMark}{https://github.com/Xuzhenhua55/ForgetMark} adresinde mevcuttur."
    }
  },
  {
    "id": "2601.07654v1",
    "title": "Towards Automating Blockchain Consensus Verification with IsabeLLM",
    "authors": [
      "Elliot Jones",
      "William Knottenbelt"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.07654v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07654v1",
    "content": {
      "en": "Consensus protocols are crucial for a blockchain system as they are what allow agreement between the system's nodes in a potentially adversarial environment. For this reason, it is paramount to ensure their correct design and implementation to prevent such adversaries from carrying out malicious behaviour. Formal verification allows us to ensure the correctness of such protocols, but requires high levels of effort and expertise to carry out and thus is often omitted in the development process. In this paper, we present IsabeLLM, a tool that integrates the proof assistant Isabelle with a Large Language Model to assist and automate proofs. We demonstrate the effectiveness of IsabeLLM by using it to develop a novel model of Bitcoin's Proof of Work consensus protocol and verify its correctness. We use the DeepSeek R1 API for this demonstration and found that we were able to generate correct proofs for each of the non-trivial lemmas present in the verification.",
      "tr": "**Akademik Makale Başlığı:** IsabeLLM ile Blockchain Konsensüs Doğrulamasının Otomasyonuna Doğru\n\n**Özet:**\n\nBlockchain sistemlerinde, potansiyel olarak hasmane bir ortamda sistemin düğümleri arasında anlaşmayı sağlayan konsensüs protokolleri hayati önem taşımaktadır. Bu nedenle, bu tür hasmane aktörlerin kötü niyetli davranışlar sergilemesini önlemek için tasarımlarının ve uygulamalarının doğruluğunu sağlamak büyük önem arz etmektedir. Formel doğrulama, bu tür protokollerin doğruluğunu güvence altına almamızı sağlarken, yüksek düzeyde çaba ve uzmanlık gerektirmekte olup, bu durum geliştirme sürecinde sıklıkla ihmal edilmesine yol açmaktadır. Bu çalışmada, ispat yardımcısı Isabelle'i bir Large Language Model ile entegre ederek ispatları otomatikleştirmeye ve kolaylaştırmaya yardımcı olan bir araç olan IsabeLLM'i sunmaktayız. IsabeLLM'in etkinliğini, Bitcoin'in Proof of Work konsensüs protokolüne dair yeni bir model geliştirmek ve doğruluğunu teyit etmek amacıyla kullanarak göstermekteyiz. Bu gösterim için DeepSeek R1 API'sini kullanmış olup, doğrulama sürecinde yer alan her bir aşamada doğru ispatlar üretebildiğimizi gözlemlemiş bulunmaktayız."
    }
  },
  {
    "id": "2601.07395v1",
    "title": "MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP",
    "authors": [
      "Ruiqi Li",
      "Zhiqiang Wang",
      "Yunhao Yao",
      "Xiang-Yang Li"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.07395v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07395v1",
    "content": {
      "en": "To standardize interactions between LLM-based agents and their environments, the Model Context Protocol (MCP) was proposed and has since been widely adopted. However, integrating external tools expands the attack surface, exposing agents to tool poisoning attacks. In such attacks, malicious instructions embedded in tool metadata are injected into the agent context during MCP registration phase, thereby manipulating agent behavior. Prior work primarily focuses on explicit tool poisoning or relied on manually crafted poisoned tools. In contrast, we focus on a particularly stealthy variant: implicit tool poisoning, where the poisoned tool itself remains uninvoked. Instead, the instructions embedded in the tool metadata induce the agent to invoke a legitimate but high-privilege tool to perform malicious operations. We propose MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. MCP-ITP formulates poisoned tool generation as a black-box optimization problem and employs an iterative optimization strategy that leverages feedback from both an evaluation LLM and a detection LLM to maximize Attack Success Rate (ASR) while evading current detection mechanisms. Experimental results on the MCPTox dataset across 12 LLM agents demonstrate that MCP-ITP consistently outperforms the manually crafted baseline, achieving up to 84.2% ASR while suppressing the Malicious Tool Detection Rate (MDR) to as low as 0.3%.",
      "tr": "**Makale Başlığı:** MCP-ITP: MCP'de Örtük Araç Zehirlemesi İçin Otomatik Bir Çerçeve\n\n**Özet:**\nLLM tabanlı ajanlar ve ortamları arasındaki etkileşimleri standartlaştırmak amacıyla Model Context Protocol (MCP) önerilmiş ve o zamandan beri yaygın olarak benimsenmiştir. Ancak, harici araçların entegrasyonu saldırı yüzeyini genişleterek ajanları araç zehirlemesi saldırılarına maruz bırakır. Bu tür saldırılarda, araç meta verilerine gömülü kötü niyetli talimatlar, MCP kayıt aşaması sırasında ajan bağlamına enjekte edilerek ajan davranışını manipüle eder. Önceki çalışmalar ağırlıklı olarak açık araç zehirlemesine odaklanmış veya manuel olarak hazırlanmış zehirli araçlara dayanmıştır. Buna karşılık, biz özellikle gizli bir varyanta odaklanıyoruz: örtük araç zehirlemesi, burada zehirli araçın kendisi çağrılmaz. Bunun yerine, araç meta verilerindeki talimatlar, ajanı yasal ancak yüksek ayrıcalıklı bir aracı kötü niyetli işlemler gerçekleştirmek üzere çağırmaya yönlendirir. MCP ekosistemi içinde örtük araç zehirlemesi için ilk otomatik ve uyarlanabilir çerçeve olan MCP-ITP'yi öneriyoruz. MCP-ITP, zehirli araç üretimini bir black-box optimization problemi olarak formüle eder ve Attack Success Rate (ASR)'yi maksimize ederken mevcut detection mechanisms'ten kaçınmak için hem bir evaluation LLM hem de bir detection LLM'den gelen geri bildirimleri kullanan iteratif bir optimization strategy kullanır. 12 LLM ajanı üzerinde MCPTox veri kümesi üzerinde yapılan deneysel sonuçlar, MCP-ITP'nin manuel olarak hazırlanmış baseline'dan sürekli olarak daha iyi performans gösterdiğini, %84.2'ye kadar ASR elde ederken Malicious Tool Detection Rate (MDR)'i %0.3 gibi düşük bir seviyeye bastırdığını göstermektedir."
    }
  },
  {
    "id": "2601.07276v1",
    "title": "A High-Recall Cost-Sensitive Machine Learning Framework for Real-Time Online Banking Transaction Fraud Detection",
    "authors": [
      "Karthikeyan V. R.",
      "Premnath S.",
      "Kavinraaj S.",
      "J. Sangeetha"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.07276v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07276v1",
    "content": {
      "en": "Fraudulent activities on digital banking services are becoming more intricate by the day, challenging existing defenses. While older rule driven methods struggle to keep pace, even precision focused algorithms fall short when new scams are introduced. These tools typically overlook subtle shifts in criminal behavior, missing crucial signals. Because silent breaches cost institutions far more than flagged but legitimate actions, catching every possible case is crucial. High sensitivity to actual threats becomes essential when oversight leads to heavy losses. One key aim here involves reducing missed fraud cases without spiking incorrect alerts too much. This study builds a system using group learning methods adjusted through smart threshold choices. Using real world transaction records shared openly, where cheating acts rarely appear among normal activities, tests are run under practical skewed distributions. The outcomes reveal that approximately 91 percent of actual fraud is detected, outperforming standard setups that rely on unchanging rules when dealing with uneven examples across classes. When tested in live settings, the fraud detection system connects directly to an online banking transaction flow, stopping questionable activities before they are completed. Alongside this setup, a browser add on built for Chrome is designed to flag deceptive web links and reduce threats from harmful sites. These results show that adjusting decisions by cost impact and validating across entire systems makes deployment more stable and realistic for today's digital banking platforms.",
      "tr": "İşte akademik makale başlığının ve özetinin resmi ve akademik bir dille yapılmış Türkçe çevirisi:\n\n**Makale Başlığı:** Gerçek Zamanlı Çevrimiçi Bankacılık İşlemleri Dolandırıcılık Tespiti İçin Yüksek `Recall` Maliyet-Hassasiyetli Makine Öğrenmesi Çerçevesi\n\n**Özet:**\n\nDijital bankacılık hizmetlerindeki dolandırıcılık faaliyetleri her geçen gün daha karmaşık hale gelmekte ve mevcut savunma mekanizmalarını zorlamaktadır. Eski kural tabanlı yöntemler bu hıza ayak uydurmakta zorlanırken, hatta hassasiyete odaklanan algoritmalar bile yeni dolandırıcılık yöntemleri ortaya çıktığında yetersiz kalmaktadır. Bu araçlar tipik olarak suçlu davranışlarındaki ince değişimleri göz ardı ederek kritik sinyalleri kaçırmaktadır. Sessiz ihlaller kurumlara işaretlenmiş ancak meşru eylemlerden çok daha pahalıya mal olduğundan, mümkün olan her vakayı yakalamak çok önemlidir. Gözetim ağır kayıplara yol açtığında gerçek tehditlere karşı yüksek hassasiyet esas haline gelir. Buradaki temel amaçlardan biri, yanlış uyarıları aşırı derecede artırmadan kaçırılan dolandırıcılık vakalarını azaltmaktır. Bu çalışma, akıllı eşik (threshold) seçimleriyle ayarlanmış grup öğrenme yöntemleri kullanan bir sistem inşa etmektedir. Hileli eylemlerin normal faaliyetler arasında nadiren göründüğü gerçek dünya işlem kayıtları açıkça paylaşıldığında, testler pratik eğri (skewed) dağılımlar altında gerçekleştirilir. Sonuçlar, sınıf genelindeki dengesiz örneklerle başa çıkmada değişmeyen kurallara dayanan standart kurulumlardan daha iyi performans göstererek, gerçek dolandırıcılığın yaklaşık yüzde 91'inin tespit edildiğini ortaya koymaktadır. Canlı ortamlarda test edildiğinde, dolandırıcılık tespit sistemi doğrudan bir çevrimiçi bankacılık işlem akışına bağlanarak, şüpheli etkinlikleri tamamlanmadan durdurmaktadır. Bu kurulumun yanı sıra, Chrome için tasarlanmış bir tarayıcı eklentisi, yanıltıcı web bağlantılarını işaretlemek ve zararlı sitelerden kaynaklanan tehditleri azaltmak üzere tasarlanmıştır. Bu sonuçlar, kararları maliyet etkisine göre ayarlamanın ve tüm sistemler genelinde doğrulama yapmanın, günümüzün dijital bankacılık platformları için dağıtımı daha istikrarlı ve gerçekçi hale getirdiğini göstermektedir."
    }
  }
]