[
  {
    "id": "2512.23557v1",
    "title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks",
    "authors": [
      "Toqeer Ali Syed",
      "Mishal Ateeq Almutairi",
      "Mahmoud Abdel Moaty"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.23557v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23557v1",
    "content": {
      "en": "Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.",
      "tr": "Makale Başlığı: Güvenilir Ajan Temelli Yapay Zeka'ya Doğru: Prompt Injection Saldırılarını Önlemek İçin Çok Modlu Bir Çerçeve\n\nÖzet:\nLarge Language Models (LLMs), Vision-Language Models (VLMs) ve LangChain ile GraphChain gibi yeni ajan temelli yapay zeka sistemleri sayesinde, çok sayıda araç ve ajan kullanarak ve aralarında akıl yürütme, planlama ve konuşma yapabilen güçlü otonom sistemler mümkün hale gelmiştir. Bununla birlikte, bu ajan temelli ortam, metin, resim, metadata veya ajanlar arası mesajlarda taşınan gizli veya kötü niyetli talimatların grafikte yayılmasına ve istenmeyen davranışlara, politika ihlaline veya durum bozulmasına yol açabilen çok modlu prompt injection (PI) saldırılarının meydana gelme olasılığını artırmaktadır. Bu riskleri azaltmak amacıyla bu çalışma, ister kullanıcı tarafından oluşturulmuş ister upstream ajanlar tarafından üretilmiş olsun tüm prompt'ların temizlendiği ve bir LLM tarafından üretilen tüm çıktıların, downstream düğümlere gönderilmeden önce bağımsız olarak doğrulandığı bir Cross-Agent Multimodal Provenanc- Aware Defense Framework önermektedir. Bu çerçeve, tüm ajan ağı boyunca modality, kaynak ve trust level'ın metadata'sını tutan bir provenance ledger tarafından koordine edilen bir Text sanitizer agent, visual sanitizer agent ve output validator agent içermektedir. Bu mimari, ajanlar arası iletişimin açık trust frames'e uyulmasını sağlayarak, injected instruction'ların LangChain veya GraphChain-style-workflows'a yayılmamasını temin eder. Deneysel değerlendirmeler, çok modlu injection detection accuracy'nin önemli ölçüde arttığını, cross-agent trust leakage'ın minimize edildiğini ve ajan temelli execution pathways'in stabil hale geldiğini göstermektedir. Bu çerçeve, provenance tracking ve validation kavramını çoklu ajan orkestrasyonuna genişleterek, güvenli, anlaşılabilir ve güvenilir ajan temelli yapay zeka sistemlerinin kurulmasını güçlendirmektedir."
    }
  },
  {
    "id": "2512.23480v1",
    "title": "Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation",
    "authors": [
      "Toqeer Ali Syed",
      "Mohammad Riyaz Belgaum",
      "Salman Jan",
      "Asadullah Abdullah Khan",
      "Saad Said Alqahtani"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.23480v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23480v1",
    "content": {
      "en": "The software supply chain attacks are becoming more and more focused on trusted development and delivery procedures, so the conventional post-build integrity mechanisms cannot be used anymore. The available frameworks like SLSA, SBOM and in toto are majorly used to offer provenance and traceability but do not have the capabilities of actively identifying and removing vulnerabilities in software production. The current paper includes an example of agentic artificial intelligence (AI) based on autonomous software supply chain security that combines large language model (LLM)-based reasoning, reinforcement learning (RL), and multi-agent coordination. The suggested system utilizes specialized security agents coordinated with the help of LangChain and LangGraph, communicates with actual CI/CD environments with the Model Context Protocol (MCP), and documents all the observations and actions in a blockchain security ledger to ensure integrity and auditing. Reinforcement learning can be used to achieve adaptive mitigation strategies that consider the balance between security effectiveness and the operational overhead, and LLMs can be used to achieve semantic vulnerability analysis, as well as explainable decisions. This framework is tested based on simulated pipelines, as well as, actual world CI/CD integrations on GitHub Actions and Jenkins, including injection attacks, insecure deserialization, access control violations, and configuration errors. Experimental outcomes indicate better detection accuracy, shorter mitigation latency and reasonable build-time overhead than rule-based, provenance only and RL only baselines. These results show that agentic AI can facilitate the transition to self defending, proactive software supply chains rather than reactive verification ones.",
      "tr": "**Makale Başlığı:** Yazılım Tedarik Zinciri Güvenliğinde Otonom Savunma için Agentic AI: Provenance'ın Ötesinde Zafiyet Azaltma\n\n**Özet:**\n\nYazılım tedarik zinciri saldırıları, güvenilir geliştirme ve teslimat prosedürlerine giderek daha fazla odaklandığından, geleneksel derleme sonrası bütünlük mekanizmaları artık kullanılamamaktadır. SLSA, SBOM ve in toto gibi mevcut çerçeveler çoğunlukla provenance ve izlenebilirlik sunmak için kullanılır, ancak yazılım üretiminde zafiyetleri aktif olarak tanımlama ve kaldırma yeteneklerine sahip değildir. Mevcut makale, otonom yazılım tedarik zinciri güvenliğine dayalı agentic artificial intelligence (AI) örneği içermektedir; bu örnek, large language model (LLM)-based reasoning, reinforcement learning (RL) ve multi-agent coordination'ı birleştirmektedir. Önerilen sistem, LangChain ve LangGraph yardımıyla koordine edilen özel güvenlik ajanlarını kullanır, Model Context Protocol (MCP) ile gerçek CI/CD ortamlarıyla iletişim kurar ve bütünlük ile denetimi sağlamak için tüm gözlemleri ve eylemleri bir blockchain security ledger'da belgeler. Reinforcement learning, güvenlik etkinliği ile operasyonel yük arasındaki dengeyi gözeten uyarlanabilir azaltma stratejileri elde etmek için kullanılabilir ve LLMs, anlamsal zafiyet analizi ve açıklanabilir kararlar elde etmek için kullanılabilir. Bu çerçeve, simüle edilmiş pipeline'lar yanı sıra GitHub Actions ve Jenkins üzerinde gerçek dünya CI/CD entegrasyonlarına dayanarak test edilmiştir; bu testler injection attacks, insecure deserialization, access control violations ve configuration errors'ı içermektedir. Deneysel sonuçlar, kural tabanlı, yalnızca provenance ve yalnızca RL tabanlı karşılaştırmalara göre daha iyi tespit doğruluğu, daha kısa azaltma gecikmesi ve makul derleme süresi yükü göstermektedir. Bu sonuçlar, agentic AI'nin reaktif doğrulama zincirleri yerine kendi kendini savunan, proaktif yazılım tedarik zincirlerine geçişi kolaylaştırabileceğini göstermektedir."
    }
  },
  {
    "id": "2512.23385v1",
    "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?",
    "authors": [
      "The Anh Nguyen",
      "Triet Huynh Minh Le",
      "M. Ali Babar"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.HC"
    ],
    "link": "http://arxiv.org/abs/2512.23385v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23385v1",
    "content": {
      "en": "The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.",
      "tr": "Elbette, akademik makale başlığını ve özetini istenen şekilde Türkçeye çevirdim:\n\n**Makale Başlığı:** Yapay Zeka Tedarik Zincirini Güvence Altına Almak: Yapay Zeka Projelerindeki Geliştirici Tarafından Rapor Edilen Güvenlik Sorunları ve Çözümlerinden Neler Öğrenebiliriz?\n\n**Özet:**\n\nYapay Zeka (AI) modellerinin ve uygulamalarının hızlı büyümesi, giderek karmaşıklaşan bir güvenlik ortamına yol açmıştır. Yapay Zeka projelerinin geliştiricileri, yalnızca geleneksel yazılım tedarik zinciri sorunlarıyla değil, aynı zamanda yeni, Yapay Zeka'ya özgü güvenlik tehditleriyle de mücadele etmek zorundadır. Ancak, pratikte hangi güvenlik sorunlarının yaygın olarak karşılaşıldığı ve bunların nasıl çözüldüğü hakkında çok az bilgi mevcuttur. Bu boşluk, Yapay Zeka tedarik zincirinin her bir bileşeni için etkili güvenlik önlemlerinin geliştirilmesini engellemektedir. Biz bu boşluğu, Hugging Face ve GitHub'daki tartışmalara dayanarak, geliştiriciler tarafından rapor edilen sorunlar ve çözümler üzerine ampirik bir inceleme yaparak dolduruyoruz. Güvenlikle ilgili tartışmaları belirlemek için, anahtar kelime eşleştirmesini, çeşitli derin öğrenme ve büyük dil modelleri üzerinde yaptığımız kapsamlı karşılaştırmada en iyi performansı gösteren, optimal olarak fine-tuned edilmiş bir distilBERT sınıflandırıcısı ile birleştiren bir pipeline geliştiriyoruz. Bu pipeline, Yapay Zeka uygulamaları ve projelerinin güvenlik raporlama pratiklerine dair içgörüler sunan 312.868 güvenlik tartışmasından oluşan bir veri kümesi üretmektedir. Veri kümemizden örneklenmiş 753 gönderinin tematik analizini gerçekleştirerek, dört tema altında 32 güvenlik sorunu ve 24 çözümden oluşan ince taneli bir taksonomi ortaya koyuyoruz: (1) Sistem ve Yazılım, (2) Harici Araçlar ve Ekosistem, (3) Model ve (4) Veri. Yapay Zeka bileşenlerinin karmaşık bağımlılıkları ve black-box doğası nedeniyle birçok güvenlik sorununun ortaya çıktığını ortaya koyuyoruz. Özellikle, Model ve Veri ile ilgili zorluklar genellikle somut çözümlerden yoksundur. Bulgularımız, geliştiricilere ve araştırmacılara Yapay Zeka tedarik zinciri boyunca gerçek dünya güvenlik tehditlerini ele almak için kanıta dayalı rehberlik sunabilir."
    }
  },
  {
    "id": "2512.23173v1",
    "title": "EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion",
    "authors": [
      "Zhen Liang",
      "Hai Huang",
      "Zhengkui Chen"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.23173v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23173v1",
    "content": {
      "en": "Large language models (LLMs), such as ChatGPT, have achieved remarkable success across a wide range of fields. However, their trustworthiness remains a significant concern, as they are still susceptible to jailbreak attacks aimed at eliciting inappropriate or harmful responses. However, existing jailbreak attacks mainly operate at the natural language level and rely on a single attack strategy, limiting their effectiveness in comprehensively assessing LLM robustness. In this paper, we propose Equacode, a novel multi-strategy jailbreak approach for large language models via equation-solving and code completion. This approach transforms malicious intent into a mathematical problem and then requires the LLM to solve it using code, leveraging the complexity of cross-domain tasks to divert the model's focus toward task completion rather than safety constraints. Experimental results show that Equacode achieves an average success rate of 91.19% on the GPT series and 98.65% across 3 state-of-the-art LLMs, all with only a single query. Further, ablation experiments demonstrate that EquaCode outperforms either the mathematical equation module or the code module alone. This suggests a strong synergistic effect, thereby demonstrating that multi-strategy approach yields results greater than the sum of its parts.",
      "tr": "**Makale Başlığı:** EquaCode: Denklem Çözme ve Kod Tamamlama Yoluyla Büyük Dil Modelleri İçin Çok Stratejili Jailbreak Yaklaşımı\n\n**Özet:**\n\nChatGPT gibi büyük dil modelleri (LLM'ler), geniş bir yelpazede olağanüstü başarılar elde etmiştir. Bununla birlikte, güvenilirlikleri önemli bir endişe kaynağı olmaya devam etmektedir, zira bu modeller hala uygunsuz veya zararlı yanıtlar ortaya çıkarmayı hedefleyen jailbreak saldırılarına karşı savunmasızdır. Mevcut jailbreak saldırıları esasen doğal dil düzeyinde işlemekte ve tek bir saldırı stratejisine dayanmaktadır, bu da LLM sağlamlığını kapsamlı bir şekilde değerlendirmedeki etkinliklerini sınırlamaktadır. Bu makalede, denklem çözme ve kod tamamlama yoluyla büyük dil modelleri için yeni bir çok stratejili jailbreak yaklaşımı olan EquaCode'u öneriyoruz. Bu yaklaşım, kötü niyetli niyeti bir matematik problemine dönüştürür ve ardından LLM'den kodu kullanarak bu problemi çözmesini talep eder. Bu şekilde, çapraz alan görevlerinin karmaşıklığından yararlanarak modelin odağını güvenlik kısıtlamalarından çok görev tamamlama yönüne kaydırır. Deneysel sonuçlar, EquaCode'un tek bir sorguyla GPT serisinde ortalama %91,19 başarı oranı ve 3 adet son teknoloji LLM'de %98,65 başarı oranı elde ettiğini göstermektedir. Ayrıca, ablasyon deneyleri EquaCode'un yalnızca matematiksel denklem modülünden veya yalnızca kod modülünden daha üstün olduğunu ortaya koymuştur. Bu durum, çok stratejili yaklaşımın bileşenlerinin toplamından daha büyük sonuçlar verdiği güçlü bir sinerjistik etki olduğunu göstermektedir."
    }
  },
  {
    "id": "2512.23171v1",
    "title": "Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning",
    "authors": [
      "Yu Jiang",
      "Xindi Tong",
      "Ziyao Liu",
      "Xiaoxi Zhang",
      "Kwok-Yan Lam"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.23171v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23171v1",
    "content": {
      "en": "Federated unlearning has become an attractive approach to address privacy concerns in collaborative machine learning, for situations when sensitive data is remembered by AI models during the machine learning process. It enables the removal of specific data influences from trained models, aligning with the growing emphasis on the \"right to be forgotten.\" While extensively studied in horizontal federated learning, unlearning in vertical federated learning (VFL) remains challenging due to the distributed feature architecture. VFL unlearning includes sample unlearning that removes specific data points' influence and label unlearning that removes entire classes. Since different parties hold complementary features of the same samples, unlearning tasks require cross-party coordination, creating computational overhead and complexities from feature interdependencies. To address such challenges, we propose FedORA (Federated Optimization for data Removal via primal-dual Algorithm), designed for sample and label unlearning in VFL. FedORA formulates the removal of certain samples or labels as a constrained optimization problem solved using a primal-dual framework. Our approach introduces a new unlearning loss function that promotes classification uncertainty rather than misclassification. An adaptive step size enhances stability, while an asymmetric batch design, considering the prior influence of the remaining data on the model, handles unlearning and retained data differently to efficiently reduce computational costs. We provide theoretical analysis proving that the model difference between FedORA and Train-from-scratch is bounded, establishing guarantees for unlearning effectiveness. Experiments on tabular and image datasets demonstrate that FedORA achieves unlearning effectiveness and utility preservation comparable to Train-from-scratch with reduced computation and communication overhead.",
      "tr": "**Makale Başlığı:** Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning\n\n**Özet:**\n\nFederated unlearning, makine öğrenmesi süreci boyunca AI modelleri tarafından hatırlanan hassas veriler söz konusu olduğunda, işbirlikçi makine öğrenmesindeki gizlilik endişelerini gidermek için çekici bir yaklaşım haline gelmiştir. \"Unutulma hakkına\" yönelik artan vurguyla uyumlu olarak, eğitilmiş modellerden belirli veri etkilerinin kaldırılmasına olanak tanır. Yatay federatif öğrenmede (horizontal federated learning) kapsamlı bir şekilde incelenmiş olmasına rağmen, dikey federatif öğrenmede (vertical federated learning - VFL) unlearning, dağıtık özellik mimarisi nedeniyle zorlu olmaya devam etmektedir. VFL unlearning, belirli veri noktalarının etkisini kaldıran sample unlearning ve tüm sınıfları kaldıran label unlearning'i içerir. Farklı taraflar aynı örneklerin tamamlayıcı özelliklerini tuttukları için, unlearning görevleri partiler arası koordinasyon gerektirir ve özellik bağımlılıklarından kaynaklanan hesaplama yükü ve karmaşıklıklar oluşturur. Bu zorlukları ele almak için, VFL'de sample ve label unlearning için tasarlanmış olan FedORA'yı (Federated Optimization for data Removal via primal-dual Algorithm) öneriyoruz. FedORA, primal-dual bir çerçeve kullanılarak çözülen kısıtlı bir optimizasyon problemi olarak belirli örneklerin veya etiketlerin kaldırılmasını formüle eder. Yaklaşımımız, yanlış sınıflandırma yerine sınıflandırma belirsizliğini destekleyen yeni bir unlearning loss function'ı tanıtır. Adaptif bir step size, kararlılığı artırırken, kalan verinin model üzerindeki önsel etkisini dikkate alan asimetrik bir batch tasarımı, unlearning ve tutulan veriyi farklı şekilde ele alarak hesaplama maliyetlerini verimli bir şekilde azaltır. FedORA ve Train-from-scratch arasındaki model farkının sınırlı olduğunu kanıtlayan teorik analiz sunarak, unlearning etkinliği için garantiler sağlıyoruz. Tabular ve görüntü veri kümeleri üzerindeki deneyler, FedORA'nın, azalan hesaplama ve iletişim yükü ile Train-from-scratch'e kıyasla unlearning etkinliği ve utility preservation'ı karşılaştırdığını göstermektedir."
    }
  },
  {
    "id": "2512.23132v1",
    "title": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems",
    "authors": [
      "Armstrong Foundjem",
      "Lionel Nganyewou Tidjon",
      "Leuson Da Silva",
      "Foutse Khomh"
    ],
    "published_date": "2025-12-29",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2512.23132v1",
    "pdf_link": "https://arxiv.org/pdf/2512.23132v1",
    "content": {
      "en": "Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.",
      "tr": "**Makale Başlığı: Yapay Zeka Tabanlı Sistemlerde Tehdit Azaltma ve Dirençlilik İçin Çoklu Ajan Çerçevesi**\n\n**Özet:**\nMakine öğrenmesi (ML), finans, sağlık ve kritik altyapıdaki temel modellerin temelini oluşturmakta olup, bu modelleri data poisoning, model extraction, prompt injection, otomatik jailbreaking ve model karşılaştırmalarını istismar eden preference-guided black-box attacks gibi saldırılara hedef hale getirmektedir. Daha büyük modeller, introspection-driven jailbreaks ve cross-modal manipulation'a karşı daha savunmasız olabilir. Geleneksel siber güvenlik, temel, multimodal ve RAG sistemleri için ML'ye özgü tehdit modellemesinden yoksundur. Amaç: Hakim TTP'leri, zafiyetleri ve hedeflenen yaşam döngüsü aşamalarını belirleyerek ML güvenlik risklerini karakterize etmek. Yöntemler: MITRE ATLAS'tan (26), AI Incident Database'den (12) ve literatürden (55) 93 tehdit çıkarıp, 854 GitHub/Python deposunu analiz ettik. Çoklu ajanlı bir RAG sistemi (ChatGPT-4o, temp 0.4), TTP'leri, zafiyetleri ve aşamaları birbirine bağlayan ontoloji güdümlü bir tehdit grafiği oluşturmak için 300'den fazla makaleyi taradı. Sonuçlar: Ticari LLM API model çalınması, parameter memorization leakage ve preference-guided text-only jailbreaks dahil olmak üzere raporlanmamış tehditler tespit ettik. Hakim TTP'ler arasında MASTERKEY tarzı jailbreaking, federated poisoning, diffusion backdoors ve preference optimization leakage yer almakta olup, bunlar ağırlıklı olarak pre-training ve inference'ı etkilemektedir. Graf analizi, zayıf yama yayılımına sahip kütüphanelerde yoğun zafiyet kümeleri ortaya koymuştur. Sonuç: Bağımlılık hijyeni, tehdit istihbaratı ve izlemeyi birleştiren uyarlanabilir, ML'ye özgü güvenlik çerçeveleri, ML yaşam döngüsü boyunca supply-chain ve inference risklerini azaltmak için esastır."
    }
  },
  {
    "id": "2512.22894v1",
    "title": "DECEPTICON: How Dark Patterns Manipulate Web Agents",
    "authors": [
      "Phil Cuvin",
      "Hao Zhu",
      "Diyi Yang"
    ],
    "published_date": "2025-12-28",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.22894v1",
    "pdf_link": "https://arxiv.org/pdf/2512.22894v1",
    "content": {
      "en": "Deceptive UI designs, widely instantiated across the web and commonly known as dark patterns, manipulate users into performing actions misaligned with their goals. In this paper, we show that dark patterns are highly effective in steering agent trajectories, posing a significant risk to agent robustness. To quantify this risk, we introduce DECEPTICON, an environment for testing individual dark patterns in isolation. DECEPTICON includes 700 web navigation tasks with dark patterns -- 600 generated tasks and 100 real-world tasks, designed to measure instruction-following success and dark pattern effectiveness. Across state-of-the-art agents, we find dark patterns successfully steer agent trajectories towards malicious outcomes in over 70% of tested generated and real-world tasks -- compared to a human average of 31%. Moreover, we find that dark pattern effectiveness correlates positively with model size and test-time reasoning, making larger, more capable models more susceptible. Leading countermeasures against adversarial attacks, including in-context prompting and guardrail models, fail to consistently reduce the success rate of dark pattern interventions. Our findings reveal dark patterns as a latent and unmitigated risk to web agents, highlighting the urgent need for robust defenses against manipulative designs.",
      "tr": "Makale Başlığı: DECEPTICON: Karanlık Kalıplar Web Ajanlarını Nasıl Manipüle Ediyor\n\nÖzet:\nWeb genelinde yaygın olarak rastlanan ve genellikle karanlık kalıplar olarak bilinen yanıltıcı kullanıcı arayüzü tasarımları, kullanıcıları hedefleriyle uyumsuz eylemler gerçekleştirmeye yönlendirir. Bu makalede, karanlık kalıpların ajan yörüngelerini yönlendirmede son derece etkili olduğunu ve ajan sağlamlığı için önemli bir risk oluşturduğunu gösteriyoruz. Bu riski ölçmek için, bireysel karanlık kalıpları izole edilmiş olarak test etmek amacıyla bir ortam olan DECEPTICON'u tanıtıyoruz. DECEPTICON, karanlık kalıplar içeren 700 web navigasyon görevi içerir; bunların 600'ü üretilmiş ve 100'ü gerçek dünya görevleridir. Bu görevler, talimat takip başarısını ve karanlık kalıp etkinliğini ölçmek için tasarlanmıştır. Son teknoloji ajanlar üzerinde yaptığımız incelemelerde, karanlık kalıpların test edilen üretilmiş ve gerçek dünya görevlerinin %70'inden fazlasında ajan yörüngelerini başarılı bir şekilde kötü niyetli sonuçlara yönlendirdiğini bulduk; bu oran insan ortalaması olan %31 ile karşılaştırıldığında oldukça yüksektir. Dahası, karanlık kalıp etkinliğinin model boyutu ve test zamanı reasoning ile pozitif korelasyon gösterdiğini, bu da daha büyük ve daha yetenekli modellerin daha hassas hale geldiğini tespit ettik. Saldırgan saldırılara karşı önde gelen karşı önlemler, in-context prompting ve guardrail modelleri dahil olmak üzere, karanlık kalıp müdahalelerinin başarı oranını tutarlı bir şekilde azaltamamıştır. Bulgularımız, karanlık kalıpları web ajanları için gizli ve hafifletilmemiş bir risk olarak ortaya koymakta ve manipülatif tasarımlara karşı sağlam savunma mekanizmalarına olan acil ihtiyacı vurgulamaktadır."
    }
  },
  {
    "id": "2512.22883v1",
    "title": "Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations",
    "authors": [
      "Tao Li",
      "Quanyan Zhu"
    ],
    "published_date": "2025-12-28",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.22883v1",
    "pdf_link": "https://arxiv.org/pdf/2512.22883v1",
    "content": {
      "en": "Cybersecurity is being fundamentally reshaped by foundation-model-based artificial intelligence. Large language models now enable autonomous planning, tool orchestration, and strategic adaptation at scale, challenging security architectures built on static rules, perimeter defenses, and human-centered workflows. This chapter argues for a shift from prevention-centric security toward agentic cyber resilience. Rather than seeking perfect protection, resilient systems must anticipate disruption, maintain critical functions under attack, recover efficiently, and learn continuously. We situate this shift within the historical evolution of cybersecurity paradigms, culminating in an AI-augmented paradigm where autonomous agents participate directly in sensing, reasoning, action, and adaptation across cyber and cyber-physical systems. We then develop a system-level framework for designing agentic AI workflows. A general agentic architecture is introduced, and attacker and defender workflows are analyzed as coupled adaptive processes, and game-theoretic formulations are shown to provide a unifying design language for autonomy allocation, information flow, and temporal composition. Case studies in automated penetration testing, remediation, and cyber deception illustrate how equilibrium-based design enables system-level resiliency design.",
      "tr": "Aşağıdaki akademik makale başlığı ve özetinin Türkçe çevirisi aşağıdadır:\n\n**Makale Başlığı:** Siber Dirençlilik için Agentic AI: Yeni Bir Güvenlik Paradigması ve Sistematik Temelleri\n\n**Özet:**\nSiber güvenlik, temel model tabanlı yapay zeka tarafından kökten yeniden şekillendirilmektedir. Büyük dil modelleri artık statik kurallar, çevre savunmaları ve insan merkezli iş akışlarına dayalı güvenlik mimarilerini zorlayarak, otonom planlama, araç düzenleme ve ölçekte stratejik uyum sağlamayı mümkün kılmaktadır. Bu bölüm, önleme odaklı güvenlikten agentic siber dirence doğru bir kaymayı savunmaktadır. Mükemmel koruma arayışındansa, dirençli sistemler kesintileri öngörmeli, saldırı altında kritik işlevleri sürdürmeli, verimli bir şekilde iyileşmeli ve sürekli öğrenmelidir. Bu değişimi siber güvenlik paradigmalarının tarihsel evrimi içinde konumlandırıyor, otonom ajanların siber ve siber-fiziksel sistemlerde algılama, reasoning, eylem ve adaptasyona doğrudan katıldığı yapay zeka destekli bir paradigma ile sonuçlandırıyoruz. Ardından, agentic AI iş akışlarını tasarlamak için bir sistem seviyesi çerçeve geliştiriyoruz. Genel bir agentic mimari tanıtılır ve saldırgan ile savunmacı iş akışları bağlı adaptif süreçler olarak analiz edilir; ayrıca game-theoretic formülasyonların otonomi tahsisi, bilgi akışı ve zamansal bileşim için birleştirici bir tasarım dili sağladığı gösterilir. Otomatik sızma testi, düzeltme ve siber aldatma alanlarındaki vaka çalışmaları, denge tabanlı tasarımın sistem seviyesinde direnilçilik tasarımını nasıl sağladığını örneklemektedir."
    }
  },
  {
    "id": "2512.22860v1",
    "title": "Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks",
    "authors": [
      "Soham Padia",
      "Dhananjay Vaidya",
      "Ramchandra Mangrulkar"
    ],
    "published_date": "2025-12-28",
    "tags": [
      "cs.CR",
      "cs.LG",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2512.22860v1",
    "pdf_link": "https://arxiv.org/pdf/2512.22860v1",
    "content": {
      "en": "Securing blockchain-enabled IoT networks against sophisticated adversarial attacks remains a critical challenge. This paper presents a trust-based delegated consensus framework integrating Fully Homomorphic Encryption (FHE) with Attribute-Based Access Control (ABAC) for privacy-preserving policy evaluation, combined with learning-based defense mechanisms. We systematically compare three reinforcement learning approaches -- tabular Q-learning (RL), Deep RL with Dueling Double DQN (DRL), and Multi-Agent RL (MARL) -- against five distinct attack families: Naive Malicious Attack (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine Fault Injection (BFI), and Time-Delayed Poisoning (TDP). Experimental results on a 16-node simulated IoT network reveal significant performance variations: MARL achieves superior detection under collusive attacks (F1=0.85 vs. DRL's 0.68 and RL's 0.50), while DRL and MARL both attain perfect detection (F1=1.00) against adaptive attacks where RL fails (F1=0.50). All agents successfully defend against Byzantine attacks (F1=1.00). Most critically, the Time-Delayed Poisoning attack proves catastrophic for all agents, with F1 scores dropping to 0.11-0.16 after sleeper activation, demonstrating the severe threat posed by trust-building adversaries. Our findings indicate that coordinated multi-agent learning provides measurable advantages for defending against sophisticated trust manipulation attacks in blockchain IoT environments.",
      "tr": "Elbette, istediğiniz çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** Blockchain IoT için Adaptif Güven Konsensüsü: RL, DRL ve MARL'nin Naive, Collusive, Adaptive, Byzantine ve Sleeper Saldırılarına Karşı Karşılaştırılması\n\n**Özet:**\nGelişmiş düşmanca saldırılara karşı blockchain tabanlı IoT ağlarını güvence altına almak kritik bir zorluk olmaya devam etmektedir. Bu çalışma, gizliliği koruyan politika değerlendirmesi için Attribute-Based Access Control (ABAC) ile Fully Homomorphic Encryption (FHE) entegrasyonunu ve öğrenme tabanlı savunma mekanizmalarını birleştiren, güvene dayalı bir delegasyonlu konsensüs çerçevesi sunmaktadır. Üç pekiştirmeli öğrenme yaklaşımını – tabular Q-learning (RL), Dueling Double DQN (DRL) ile Deep RL ve Multi-Agent RL (MARL) – beş farklı saldırı ailesine karşı sistematik olarak karşılaştırıyoruz: Naive Malicious Attack (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine Fault Injection (BFI) ve Time-Delayed Poisoning (TDP). 16 düğümlü simüle edilmiş bir IoT ağı üzerinde yapılan deneysel sonuçlar, önemli performans farklılıklarını ortaya koymaktadır: MARL, işbirlikçi saldırılar altında üstün tespit sağlamaktadır (F1=0.85, DRL'nin 0.68 ve RL'nin 0.50'sine karşılık), DRL ve MARL ise RL'nin başarısız olduğu (F1=0.50) adaptif saldırılara karşı mükemmel tespit (F1=1.00) elde etmektedir. Tüm ajanlar Byzantine saldırılarına karşı başarılı bir şekilde savunma yapmaktadır (F1=1.00). En kritik olarak, Time-Delayed Poisoning saldırısı, sleeper aktivasyonundan sonra F1 skorlarının 0.11-0.16'ya düşmesiyle tüm ajanlar için yıkıcı olduğunu kanıtlamakta, güven oluşturan düşmanların oluşturduğu ciddi tehdidi göstermektedir. Bulgularımız, koordineli çoklu ajan öğrenmesinin, blockchain IoT ortamlarında gelişmiş güven manipülasyon saldırılarına karşı savunma için ölçülebilir avantajlar sağladığını göstermektedir."
    }
  },
  {
    "id": "2512.22307v1",
    "title": "LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators",
    "authors": [
      "You Li",
      "Guannan Zhao",
      "Yuhao Ju",
      "Yunqi He",
      "Jie Gu"
    ],
    "published_date": "2025-12-26",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.22307v1",
    "pdf_link": "https://arxiv.org/pdf/2512.22307v1",
    "content": {
      "en": "We introduce LLA, an effective intellectual property (IP) protection scheme for generative AI models. LLA leverages the synergy between hardware and software to defend against various supply chain threats, including model theft, model corruption, and information leakage. On the software side, it embeds key bits into neurons that can trigger outliers to degrade performance and applies invariance transformations to obscure the key values. On the hardware side, it integrates a lightweight locking module into the AI accelerator while maintaining compatibility with various dataflow patterns and toolchains. An accelerator with a pre-stored secret key acts as a license to access the model services provided by the IP owner. The evaluation results show that LLA can withstand a broad range of oracle-guided key optimization attacks, while incurring a minimal computational overhead of less than 0.1% for 7,168 key bits.",
      "tr": "**Makale Başlığı:** LLA: Mantık Kilitli Hızlandırıcılarla Üretken Modeller için Güvenlik ve Gizliliği Geliştirme\n\n**Özet:**\n\nBu çalışmada, üretken yapay zeka modelleri için etkili bir fikri mülkiyet (IP) koruma şeması olan LLA'yı sunuyoruz. LLA, model hırsızlığı, model bozulması ve bilgi sızıntısı dahil olmak üzere çeşitli tedarik zinciri tehditlerine karşı savunma yapmak için donanım ve yazılım arasındaki sinerjiden yararlanır. Yazılım tarafında, performansı düşürmek için outlierları tetikleyebilen ve anahtar değerlerini gizlemek için invariance transformation uygulayan key bitlerini nöronlara gömer. Donanım tarafında ise, çeşitli veri akışı desenleri ve toolchain'leri ile uyumluluğu korurken, yapay zeka hızlandırıcısına hafif bir locking module entegre eder. Önceden saklanmış bir gizli anahtara sahip bir hızlandırıcı, IP sahibinin sağladığı model hizmetlerine erişim için bir lisans görevi görür. Değerlendirme sonuçları, LLA'nın geniş bir oracle-guided key optimization attacks yelpazesine dayanabileceğini ve 7,168 key bit için %0.1'in altında minimal bir hesaplama yükü getirdiğini göstermektedir."
    }
  }
]