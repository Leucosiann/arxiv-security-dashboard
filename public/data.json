[
  {
    "id": "2512.21241v1",
    "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
    "authors": [
      "Xinjie Xu",
      "Shuyu Cheng",
      "Dongwei Xu",
      "Qi Xuan",
      "Chen Ma"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2512.21241v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21241v1",
    "content": {
      "en": "In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum $\\ell_2$-norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, which proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum. We provide a theoretical analysis of its convergence behavior, showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization. To further accelerate convergence, we incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance. The superiority of our approach is supported by theoretical guarantees under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that our method surpasses 13 state-of-the-art approaches in query efficiency.",
      "tr": "**Makale Başlığı:** Sorgu-Verimli Zor Etiket Saldırıları İçin Işın Arama Optimizasyonunun Yakınsama Oranının İyileştirilmesi\n\n**Özet:**\n\nZor etiketli kara kutu saldırılarında, yalnızca en üst sıradaki tahmin edilen etiketin erişilebilir olduğu durumlarda, aşırı sorgu karmaşıklığı pratik dağıtımın önünde büyük bir engel teşkil etmektedir. Bu makalede, zararsız bir görüntüyü düşmanca bölgeye taşımak için gereken minimum $\\ell_2$-norm pertürbasyonunu veren optimal ışın yönünü arayan, saldırıların temsili bir sınıfının optimizasyonuna odaklanıyoruz. Nesterov's Accelerated Gradient (NAG)'den esinlenerek, bir momentum-temelli algoritma olan ARS-OPT'yi öneriyoruz. Bu algoritma, birikmiş momentumdan çıkarılan gelecekteki bir ışın yönüne ilişkin gradyanı proaktif olarak tahmin eder. Yakınsama davranışı üzerine teorik bir analiz sunarak, ARS-OPT'nin daha doğru yönsel güncellemeler sağladığını ve daha hızlı, daha kararlı optimizasyon elde ettiğini gösteriyoruz. Yakınsamayı daha da hızlandırmak için, PARS-OPT ile sonuçlanan ve performansı artırılmış ARS-OPT'nin gradyan tahminine surrogate-model priors entegre ediyoruz. Yaklaşımımızın üstünlüğü, standart varsayımlar altında teorik garantilerle desteklenmektedir. ImageNet ve CIFAR-10 üzerindeki kapsamlı deneyler, yöntemimizin sorgu verimliliği açısından 13 adet state-of-the-art yaklaşımı geride bıraktığını göstermektedir."
    }
  },
  {
    "id": "2512.21238v1",
    "title": "Assessing the Software Security Comprehension of Large Language Models",
    "authors": [
      "Mohammed Latif Siddiq",
      "Natalie Sekerak",
      "Antonio Karam",
      "Maria Leal",
      "Arvin Islam-Gomes"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.SE",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.21238v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21238v1",
    "content": {
      "en": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.",
      "tr": "**Makale Başlığı:** Büyük Dil Modellerinin Yazılım Güvenliği Kavrayışının Değerlendirilmesi\n\n**Özet:**\n\nBüyük Dil Modelleri (LLMs), yazılım geliştirmede giderek daha fazla kullanılmaktadır, ancak yazılım güvenliği alanındaki uzmanlık düzeyleri belirsizliğini korumaktadır. Bu çalışma, Blooms Taksonomisi'ni bir çerçeve olarak kullanarak beş önde gelen LLM'nin (GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1 ve Qwen-2.5) güvenlik kavrayışını sistematik olarak değerlendirmektedir. Altı bilişsel boyutu değerlendiriyoruz: hatırlama, anlama, uygulama, analiz etme, değerlendirme ve oluşturma. Metodolojimiz, derlenmiş çoktan seçmeli soruları, güvenlik açığı içeren kod parçacıklarını (SALLM), Bir Yazılım Güvenliğine Giriş dersinden alınan ders değerlendirmelerini, gerçek dünya vaka çalışmalarını (XBOW) ve Güvenli Yazılım Mühendisliği dersinden elde edilen proje tabanlı oluşturma görevlerini içeren çeşitli veri kümelerini entegre etmektedir. Sonuçlar, LLM'lerin hatırlama ve bilinen güvenlik açıklarını belirleme gibi alt düzey bilişsel görevlerde iyi performans gösterdiğini, ancak reasoning, mimari değerlendirme ve güvenli sistem oluşturma gerektiren üst düzey görevlerde performanslarının önemli ölçüde düştüğünü göstermektedir. Toplam doğruluğu raporlamanın ötesinde, bir modelin güvenilir performansı sürekli olarak sürdürdüğü en yüksek bilişsel düzeyi belirleyen bir yazılım güvenlik bilgi sınırı sunuyoruz. Ek olarak, Blooms seviyelerinde LLM'lerin sergilediği 51 tekrarlayan yanlış anlama paterni belirliyoruz."
    }
  },
  {
    "id": "2512.21236v1",
    "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
    "authors": [
      "Yifan Huang",
      "Xiaojun Jia",
      "Wenbo Guo",
      "Yuqiang Sun",
      "Yihao Huang"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2512.21236v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21236v1",
    "content": {
      "en": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",
      "tr": "Makale Başlığı: SPELL'i Kullanmak: LLM Sınırlamalarını Aşmak İçin Cümle Eşleştirme Keşfi\n\nÖzet:\nBüyük dil modelleri (LLM), AI destekli kodlama araçları aracılığıyla yazılım geliştirme alanında devrim yaratmış, sınırlı programlama uzmanlığına sahip geliştiricilerin karmaşık uygulamalar oluşturmasını sağlamıştır. Ancak, bu erişilebilirlik, bu güçlü araçları zararlı yazılımlar üretmek için istismar edebilecek kötü niyetli aktörlere de genişlemektedir. Mevcut jailbreaking araştırmaları, ağırlıklı olarak LLM'lere yönelik genel saldırı senaryolarına odaklanmakta ve malicious code generation'ı bir jailbreak hedefi olarak keşfetme konusunda sınırlı kalmaktadır. Bu boşluğu gidermek için, malicious code generation'daki security alignment'ın zayıflıklarını değerlendirmek üzere özel olarak tasarlanmış kapsamlı bir testing framework olan SPELL'i öneriyoruz. Framework'ümüz, novel attack patterns keşfi ile başarılı tekniklerin istismarını dengeleyerek, önceden sahip olunan bir knowledge dataset'ten cümleleri akıllıca birleştirerek jailbreaking prompt'ları sistematik olarak oluşturan bir time-division selection strategy kullanır. Üç gelişmiş code model (GPT-4.1, Claude-3.5 ve Qwen2.5-Coder) üzerinde yapılan kapsamlı değerlendirme, SPELL'in etkinliğini göstermekte ve sekiz malicious code kategorisinde sırasıyla %83,75, %19,38 ve %68,12 oranlarında attack success rates elde etmektedir. Üretilen prompt'lar, Cursor gibi gerçek dünya AI development tools'larında başarılı bir şekilde malicious code üretmeyi başarmış, çıktıları state-of-the-art detection systems tarafından %73'ü aşan oranlarda malicious olarak doğrulanmıştır. Bu bulgular, mevcut LLM implementations'daki önemli güvenlik açıklarını ortaya koymakta ve code generation applications'da AI safety alignment'ı iyileştirmek için değerli içgörüler sunmaktadır."
    }
  },
  {
    "id": "2512.21132v1",
    "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
    "authors": [
      "Tobias von Arx",
      "Niels Mündler",
      "Mark Vero",
      "Maximilian Baader",
      "Martin Vechev"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "link": "http://arxiv.org/abs/2512.21132v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21132v1",
    "content": {
      "en": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.",
      "tr": "İşte akademik makale başlığı ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** AutoBaxBuilder: Kod Güvenliği Kıyaslamasını Önyükleme Yöntemi\n\n**Özet:**\n\nYazılım mühendisliğinde büyük dil modellerinin (LLM'ler) yaygın olarak benimsenmesiyle birlikte, LLM tarafından üretilen kodun doğruluğunun ve güvenliğinin güvenilir bir şekilde değerlendirilmesi kritik önem taşımaktadır. Özellikle, önceki çalışmalar güvenliğin sıklıkla göz ardı edildiğini ve LLM'lerin güvenlik açıkları içeren kod üretme eğiliminde olduğunu ortaya koymuştur. Bu bulgular, güvenlik uzmanları tarafından önemli manuel çaba harcanarak oluşturulan özel kıyaslama araçları sayesinde mümkün olmuştur. Ancak, manuel olarak oluşturulan kıyaslama araçlarına dayanmak uzun vadede yetersizdir, çünkü kıyaslama araçları (i) doğal olarak eğitim verilerini kirletir, (ii) daha kapsamlı bir resim sunmak için yeni görevlere genişletilmeli ve (iii) daha yetenekli LLM'leri zorlamak için zorluk seviyeleri artırılmalıdır. Bu çalışmada, bu zorluklarla başa çıkıyor ve kod güvenliği kıyaslaması için görevler ve testler üreten bir framework olan AutoBaxBuilder'ı sunuyoruz. LLM'lerin kod anlama yeteneklerinden yararlanarak işlevsellik testleri ve uçtan uca güvenlik yoklaması exploit'leri oluşturmak için ince ayarlanmış uygunluk kontrolleriyle sağlam bir pipeline sunuyoruz. Üretilen kıyaslamanın kalitesini doğrulamak için, hem niteliksel bir analiz yürütüyor hem de insanlar tarafından oluşturulan görevlerle karşılaştırarak niceliksel deneyler yapıyoruz. AutoBaxBuilder'ı tamamen yeni görevler oluşturmak için kullanıyor ve bunları AutoBaxBench olarak halka sunuyoruz, aynı zamanda LLM'lerin bu görevlerdeki güvenlik yeteneklerinin kapsamlı bir değerlendirmesini de yapıyoruz. Yeni bir görevin 2 saatten kısa sürede ve 10 USD'den az bir maliyetle üretilebileceğini bulduk."
    }
  },
  {
    "id": "2512.21110v1",
    "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent",
    "authors": [
      "Ahmed M. Hussain",
      "Salahuddin Salahuddin",
      "Panos Papadimitratos"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ],
    "link": "http://arxiv.org/abs/2512.21110v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21110v1",
    "content": {
      "en": "Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.",
      "tr": "İşte makale başlığı ve özetinin istenen şekilde çevirisi:\n\n**Makale Başlığı:** Bağlamın Ötesinde: Büyük Dil Modellerinin Kullanıcı Niyetini Kavramadaki Başarısızlığı\n\n**Özet:**\nMevcut Büyük Dil Modelleri (LLM'ler) güvenlik yaklaşımları, açıkça zararlı içeriklere odaklanırken kritik bir zafiyeti göz ardı etmektedir: bağlamı anlama ve kullanıcı niyetini tanıma konusundaki yetersizlik. Bu durum, kötü niyetli kullanıcıların güvenlik mekanizmalarını atlatmak için sistematik olarak faydalanabileceği, istismar edilebilir zafiyetler yaratmaktadır. ChatGPT, Claude, Gemini ve DeepSeek dahil olmak üzere, birden fazla en son teknoloji LLM'yi ampirik olarak değerlendirmekteyiz. Analizimiz, duygusal çerçeveleme, aşamalı ifşa ve akademik gerekçelendirme teknikleri aracılığıyla güvenilir güvenlik mekanizmalarının atlatılmasını göstermektedir. Özellikle, reasoning-enabled konfigürasyonlar, altta yatan niyeti sorgulamadan olgusal kesinliği artırarak, istismarın etkinliğini azaltmak yerine artırmıştır. İstisna, bazı kullanım durumlarında bilgi sağlamaya göre niyet tespitini önceliklendiren Claude Opus 4.1 olmuştur. Bu örüntü, mevcut mimari tasarımların sistematik zafiyetler yarattığını ortaya koymaktadır. Bu sınırlamalar, sonradan koruyucu mekanizmalar yerine, bağlamsal anlama ve niyet tanıma yönünde paradigmatik değişimleri temel güvenlik yetenekleri olarak gerektirmektedir."
    }
  },
  {
    "id": "2512.21048v1",
    "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
    "authors": [
      "Savvy Sharma",
      "George Petrovic",
      "Sarthak Kaushik"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.21048v1",
    "pdf_link": "https://arxiv.org/pdf/2512.21048v1",
    "content": {
      "en": "Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.",
      "tr": "Makale Başlığı: zkFL-Health: Tıbbi Yapay Zeka Gizliliği için Blockchain Tabanlı Sıfır Bilgi Birleşik Öğrenme\n\nÖzet:\nSağlık yapay zekası, büyük ve çeşitli veri kümelerine ihtiyaç duyar; ancak katı gizlilik ve yönetim kısıtlamaları, kurumlar arasında ham veri paylaşımını engellemektedir. Birleşik öğrenme (FL), verinin bulunduğu yerde eğitim yaparak ve yalnızca model güncellemelerini alıp vererek bu durumu hafifletir, ancak pratik uygulamalar hala iki temel riskle karşı karşıyadır: (1) gizlilik sızıntısı (membership inference, gradient inversion gibi) yoluyla gradyanlar veya güncellemeler ve (2) tek bir hata noktası olan ve katkıları tespit edilmeden düşürebilen, değiştirebilen veya enjekte edebilen toplayıcıya duyulan güven. Biz, tıbbi yapay zeka için gizlilik korumalı, doğrulanabilir şekilde doğru işbirlikçi eğitimi sağlamak üzere FL'yi zero-knowledge proofs (ZKPs) ve Trusted Execution Environments (TEEs) ile birleştiren bir mimari olan zkFL-Health'i sunuyoruz. İstemciler yerel olarak eğitim yapar ve güncellemelerini commitleder; toplayıcı, küresel güncellemeyi hesaplamak için bir TEE içinde çalışır ve herhangi bir istemci güncellemesini ana bilgisayara ifşa etmeden, tam olarak commitletilmiş girdileri ve doğru aggregation rule'u kullandığını gösteren kısa bir ZK proof (Halo2/Nova aracılığıyla) üretir. Doğrulayıcı düğümler proof'u doğrular ve on-chain üzerinde kriptografik commitment'ları kaydeder, bu da değiştirilemez bir denetim izi sağlar ve tek bir tarafa güvenme ihtiyacını ortadan kaldırır. Sağlık hizmetlerine özel sistem ve tehdit modellerini, zkFL-Health protokolünü, güvenlik/gizlilik garantilerini ve doğruluk, gizlilik riski, gecikme ve maliyetleri kapsayan bir performans değerlendirme planını özetliyoruz. Bu çerçeve, güçlü gizlilik, bütünlük ve denetlenebilirlik özellikleriyle, klinik benimseme ve düzenleyici uyumluluk için anahtar özellikler olan kurumlar arası tıbbi yapay zeka kullanımını mümkün kılar."
    }
  },
  {
    "id": "2512.20872v1",
    "title": "Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification",
    "authors": [
      "Jakir Hossain",
      "Gurvinder Singh",
      "Lukasz Ziarek",
      "Ahmet Erdem Sarıyüce"
    ],
    "published_date": "2025-12-24",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.20872v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20872v1",
    "content": {
      "en": "Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.",
      "tr": "**Makale Başlığı:** Better Call Graphs: Kötü Amaçlı Yazılım Sınıflandırması İçin Yeni Bir Fonksiyon Çağrı Grafikleri Veri Kümesi\n\n**Özet:**\n\nFonksiyon Çağrı Grafikleri (FCGs), uygulamaların yüzey düzeyindeki imzaların ötesindeki davranışsal yapısını yakalayarak kötü amaçlı yazılım tespiti için güçlü bir soyutlama olarak öne çıkmaktadır. Geleneksel program analizindeki faydaları, kötü amaçlı yazılımların etkili bir şekilde sınıflandırılmasına ve analiz edilmesine olanak tanıyarak iyi bir şekilde kurulmuştur. Mobil alanda, özellikle Android ekosisteminde, FCG tabanlı kötü amaçlı yazılım sınıflandırması, platformun yaygın benimsenmesi ve Android uygulamalarının karmaşık, bileşen tabanlı yapısı nedeniyle özellikle kritiktir. Ancak, bu yöndeki ilerleme, büyük ölçekli, yüksek kaliteli Android'e özgü FCG veri kümelerinin eksikliği ile engellenmektedir. Mevcut veri kümeleri genellikle güncel değildir, uygulama yeniden paketlenmesinden kaynaklanan küçük veya yinelenen grafikler tarafından domine edilir ve gerçek dünya kötü amaçlı yazılımlarının çeşitliliğini yansıtmakta başarısız olurlar. Bu sınırlamalar, grafik tabanlı sınıflandırma yöntemlerinin aşırı uyumlanmasına (overfitting) ve güvenilmez bir şekilde değerlendirilmesine yol açar. Bu boşluğu gidermek için, güncel Android uygulama paketlerinden (APKs) çıkarılan büyük ve benzersiz FCG'lerden oluşan kapsamlı bir veri kümesi olan Better Call Graphs'ı (BCG) sunuyoruz. BCG, çeşitli aileleri ve türleri kapsayan hem zararsız hem de kötü amaçlı örnekleri ve her APK için grafik düzeyinde özellikleri içermektedir. Temel sınıflandırıcılar (baseline classifiers) kullanarak yapılan kapsamlı deneyler aracılığıyla, BCG'nin mevcut veri kümelerine kıyasla gerekliliğini ve değerini göstermekteyiz. BCG, https://erdemub.github.io/BCG-dataset adresinde kamuya açıktır."
    }
  },
  {
    "id": "2512.20712v1",
    "title": "Real-World Adversarial Attacks on RF-Based Drone Detectors",
    "authors": [
      "Omer Gazit",
      "Yael Itzhakev",
      "Yuval Elovici",
      "Asaf Shabtai"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.20712v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20712v1",
    "content": {
      "en": "Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.",
      "tr": "**Makale Başlığı:** RF Tabanlı Drone Dedektörlerine Yönelik Gerçek Dünya Çekişmeli Saldırılar\n\n**Özet:**\n\nRadyo frekansı (RF) tabanlı sistemler, drone'ların RF sinyal modellerini analiz ederek, bunları nesne tespit modelleri tarafından işlenen spektrogram görüntülerine dönüştürerek giderek daha fazla kullanılmaktadır. Görüntü tabanlı modellere yönelik mevcut RF saldırıları, dijital özellikleri değiştirerek, dijital pertürbasyonların senkronizasyon hataları ve parazitlere neden olabilecek iletilebilir dalga formlarına dönüştürülmesindeki zorluklar ve donanım sınırlamaları nedeniyle havadan (OTA) uygulamayı zorlaştırmaktadır. İlk fiziksel saldırıyı RF görüntü tabanlı drone dedektörlerine sunuyoruz; bu saldırı, standart iletişimle birlikte iletilen sınıf-spesifik universal complex baseband (I/Q) pertürbasyon dalga formlarını optimize etmektedir. Saldırıyı RF kayıtları ve dört farklı drone tipiyle yapılan OTA deneyleri aracılığıyla değerlendirdik. Sonuçlarımız, mütevazı, yapılandırılmış I/Q pertürbasyonlarının standart RF zincirleriyle uyumlu olduğunu ve hedef drone tespitini güvenilir bir şekilde azalttığını, aynı zamanda standart drone tespitini koruduğunu göstermektedir."
    }
  },
  {
    "id": "2512.20423v1",
    "title": "Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit",
    "authors": [
      "Adam Elaoumari"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2512.20423v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20423v1",
    "content": {
      "en": "The purpose of this project is to assess how well defenders can detect DNS-over-HTTPS (DoH) file exfiltration, and which evasion strategies can be used by attackers. While providing a reproducible toolkit to generate, intercept and analyze DoH exfiltration, and comparing Machine Learning vs threshold-based detection under adversarial scenarios. The originality of this project is the introduction of an end-to-end, containerized pipeline that generates configurable file exfiltration over DoH using several parameters (e.g., chunking, encoding, padding, resolver rotation). It allows for file reconstruction at the resolver side, while extracting flow-level features using a fork of DoHLyzer. The pipeline contains a prediction side, which allows the training of machine learning models based on public labelled datasets and then evaluates them side-by-side with threshold-based detection methods against malicious and evasive DNS-Over-HTTPS traffic. We train Random Forest, Gradient Boosting and Logistic Regression classifiers on a public DoH dataset and benchmark them against evasive DoH exfiltration scenarios. The toolkit orchestrates traffic generation, file capture, feature extraction, model training and analysis. The toolkit is then encapsulated into several Docker containers for easy setup and full reproducibility regardless of the platform it is run on. Future research regarding this project is directed at validating the results on mixed enterprise traffic, extending the protocol coverage to HTTP/3/QUIC request, adding a benign traffic generation, and working on real-time traffic evaluation. A key objective is to quantify when stealth constraints make DoH exfiltration uneconomical and unworthy for the attacker.",
      "tr": "Elbette, makale başlığını ve özetini istenen şekilde çevirelim:\n\n**Makale Başlığı:** DNS-over-HTTPS Veri Sızdırma için Kaçınmaya Dayanıklı Tespit: Pratik Bir Değerlendirme ve Araç Seti\n\n**Özet:**\n\nBu projenin amacı, savunmacıların DNS-over-HTTPS (DoH) dosya sızdırma yeteneğini ne kadar iyi tespit edebildiğini ve saldırganlar tarafından hangi kaçınma stratejilerinin kullanılabileceğini değerlendirmektir. Adversarial senaryolar altında Machine Learning ile eşik tabanlı tespiti karşılaştırırken, DoH sızdırma üreten, engelleyen ve analiz eden tekrarlanabilir bir araç seti sunulmaktadır. Bu projenin özgünlüğü, çeşitli parametreler (örneğin, chunking, encoding, padding, resolver rotation) kullanarak DoH üzerinden yapılandırılabilir dosya sızdırmayı üreten uçtan uca, containerized bir pipeline'ın tanıtılmasıdır. Bu pipeline, DoHLyzer'ın bir fork'unu kullanarak flow-level features'ları çıkarırken, resolver tarafında dosya yeniden yapılandırmasına olanak tanır. Pipeline, public labelled datasets'lere dayalı olarak makine öğrenmesi modellerinin eğitimini sağlayan ve ardından malicious ve evasive DNS-Over-HTTPS trafiğine karşı eşik tabanlı tespit yöntemleriyle yan yana değerlendiren bir prediction side'ı içermektedir. Random Forest, Gradient Boosting ve Logistic Regression classifier'ları, public bir DoH dataset'i üzerinde eğitilmiş olup, evasive DoH sızdırma senaryolarına karşı benchmark edilmiştir. Araç seti, trafik üretimi, dosya yakalama, feature extraction, model eğitimi ve analizi orkestre etmektedir. Araç seti daha sonra, üzerinde çalıştırıldığı platformdan bağımsız olarak kolay kurulum ve tam tekrarlanabilirlik için çeşitli Docker container'larına kapsüllenmiştir. Bu projeyle ilgili gelecek araştırmalar, sonuçların karma kurumsal trafik üzerinde doğrulanması, protocol kapsamının HTTP/3/QUIC request'lerine genişletilmesi, benign trafik üretimi eklenmesi ve real-time trafik değerlendirmesi üzerine çalışılması yönündedir. Temel bir amaç, stealth constraints'lerin DoH sızdırmayı saldırgan için ekonomik olmayan ve değersiz hale getirdiği noktaları nicel olarak belirlemektir."
    }
  },
  {
    "id": "2512.20396v1",
    "title": "Symmaries: Automatic Inference of Formal Security Summaries for Java Programs",
    "authors": [
      "Narges Khakpour",
      "Nicolas Berthier"
    ],
    "published_date": "2025-12-23",
    "tags": [
      "cs.CR",
      "cs.FL",
      "cs.PL",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2512.20396v1",
    "pdf_link": "https://arxiv.org/pdf/2512.20396v1",
    "content": {
      "en": "We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.",
      "tr": "Makale Başlığı: Symmaries: Java Programları İçin Formal Güvenlik Özetlerinin Otomatik Çıkarımı\n\nÖzet:\nJava bytecode programları için yöntem özetleri şeklinde formal güvenlik spesifikasyonları oluşturan ölçeklenebilir, modüler ve sağlam bir yaklaşım sunuyoruz. Bir özet, yöntemin güvenli bir şekilde çağrılabileceği koşulları ve bilgi akışı ile aliasing güncellemelerinin spesifikasyonlarını içeren yöntemin güvenlik davranışının soyut bir temsilini sağlar. Bu tür özetler, statik kod analizi araçları tarafından tüketilebilir ve aynı zamanda kütüphaneler gibi kod bölümlerinin davranışlarını anlamalarına yardımcı olarak, uygulamalarda yeniden kullanıldıklarında güvenlik etkilerini değerlendirmelerine olanak tanır. Yaklaşımımız, güvenlik özetlerinin oluşturulmasını otomatikleştiren Symmaries adlı bir araçta uygulanmıştır. Symmaries'i Java API kütüphanelerine uygulayarak güvenlik spesifikasyonlarını çıkardık ve ölçeklenebilirliğini değerlendirmek için büyük gerçek dünya uygulamalarına uyguladık. Sonuçlarımız, aracın yüz binlerce satır kod içeren uygulamaları analiz etmek için başarılı bir şekilde ölçeklendiğini ve Symmaries'in kullanılan heap modeline bağlı olarak umut verici bir hassasiyet elde ettiğini göstermektedir. Yaklaşımımızın, termination-insensitive non-interference'ı garanti etme açısından sağlamlığını kanıtlıyoruz."
    }
  }
]