[
  {
    "id": "2601.04940v1",
    "title": "CurricuLLM: Designing Personalized and Workforce-Aligned Cybersecurity Curricula Using Fine-Tuned LLMs",
    "authors": [
      "Arthur Nijdam",
      "Harri Kähkönen",
      "Valtteri Niemi",
      "Paul Stankovski Wagner",
      "Sara Ramezanian"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.04940v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04940v1",
    "content": {
      "en": "The cybersecurity landscape is constantly evolving, driven by increased digitalization and new cybersecurity threats. Cybersecurity programs often fail to equip graduates with skills demanded by the workforce, particularly concerning recent developments in cybersecurity, as curriculum design is costly and labor-intensive. To address this misalignment, we present a novel Large Language Model (LLM)-based framework for automated design and analysis of cybersecurity curricula, called CurricuLLM. Our approach provides three key contributions: (1) automation of personalized curriculum design, (2) a data-driven pipeline aligned with industry demands, and (3) a comprehensive methodology for leveraging fine-tuned LLMs in curriculum development.   CurricuLLM utilizes a two-tier approach consisting of PreprocessLM, which standardizes input data, and ClassifyLM, which assigns course content to nine Knowledge Areas in cybersecurity. We systematically evaluated multiple Natural Language Processing (NLP) architectures and fine-tuning strategies, ultimately selecting the Bidirectional Encoder Representations from Transformers (BERT) model as ClassifyLM, fine-tuned on foundational cybersecurity concepts and workforce competencies.   We are the first to validate our method with human experts who analyzed real-world cybersecurity curricula and frameworks, motivating that CurricuLLM is an efficient solution to replace labor-intensive curriculum analysis. Moreover, once course content has been classified, it can be integrated with established cybersecurity role-based weights, enabling alignment of the educational program with specific job roles, workforce categories, or general market needs. This lays the foundation for personalized, workforce-aligned cybersecurity curricula that prepare students for the evolving demands in cybersecurity.",
      "tr": "**Makale Başlığı:** CurricuLLM: Fine-Tuned LLM'ler Kullanılarak Kişiselleştirilmiş ve İşgücüyle Uyumlu Siber Güvenlik Müfredatlarının Tasarlanması\n\n**Özet:**\n\nDijitalleşmenin artması ve yeni siber güvenlik tehditlerinin ortaya çıkmasıyla siber güvenlik alanı sürekli evrilmektedir. Siber güvenlik programları, müfredat tasarımının maliyetli ve yoğun emek gerektiren bir süreç olması nedeniyle, özellikle siber güvenliğin son gelişmelerine ilişkin işgücünün talep ettiği becerileri mezunlara kazandırmakta sıklıkla yetersiz kalmaktadır. Bu uyumsuzluğa çözüm bulmak amacıyla, siber güvenlik müfredatlarının otomatik tasarım ve analizi için CurricuLLM adını verdiğimiz yenilikçi bir Large Language Model (LLM)-tabanlı çerçeve sunuyoruz. Yaklaşımımız üç ana katkı sağlamaktadır: (1) kişiselleştirilmiş müfredat tasarımının otomasyonu, (2) endüstri talepleriyle uyumlu veri güdümlü bir pipeline ve (3) müfredat geliştirilmesinde fine-tuned LLM'lerden yararlanmak için kapsamlı bir metodoloji. CurricuLLM, girdi verilerini standartlaştıran PreprocessLM ve ders içeriğini siber güvenliğin dokuz Knowledge Areas'ına atayan ClassifyLM olmak üzere iki katmanlı bir yaklaşım kullanır. Birden çok Natural Language Processing (NLP) mimarisini ve fine-tuning stratejilerini sistematik olarak değerlendirdik ve nihayetında, temel siber güvenlik kavramları ve işgücü yetkinlikleri üzerinde fine-tuned edilmiş Bidirectional Encoder Representations from Transformers (BERT) modelini ClassifyLM olarak seçtik. Yöntemimizi, gerçek dünya siber güvenlik müfredatlarını ve çerçevelerini analiz eden insan uzmanlarla doğrulayarak, CurricuLLM'in yoğun emek gerektiren müfredat analizini değiştirmek için verimli bir çözüm olduğunu gösteriyoruz. Dahası, ders içeriği sınıflandırıldıktan sonra, belirli iş rolleri, işgücü kategorileri veya genel pazar ihtiyaçlarıyla eğitim programının uyumunu sağlayarak, yerleşik siber güvenlik rol-temelli ağırlıklarla entegre edilebilir. Bu, öğrencileri siber güvenlikteki gelişen taleplere hazırlayan, kişiselleştirilmiş, işgücüyle uyumlu siber güvenlik müfredatlarının temelini oluşturmaktadır."
    }
  },
  {
    "id": "2601.04795v1",
    "title": "Defense Against Indirect Prompt Injection via Tool Result Parsing",
    "authors": [
      "Qiang Yu",
      "Xinran Cheng",
      "Chuanyi Liu"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.MA"
    ],
    "link": "http://arxiv.org/abs/2601.04795v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04795v1",
    "content": {
      "en": "As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.",
      "tr": "İşte akademik makale başlığı ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** Tool Result Parsing Aracılığıyla Dolaylı Prompt Enjeksiyonuna Karşı Savunma\n\n**Özet:**\n\nLLM ajanları, dijital asistanlardan otonom sistemler ve robotikte fiziksel kontrolcülere doğru geçiş yaparken, dolaylı prompt enjeksiyonundan kaynaklanan artan bir tehditle karşı karşıyadır. Saldırganlar, tool call sonuçlarına düşmanca talimatlar gömerek, ajanın karar verme sürecini ele geçirip yetkisiz eylemleri gerçekleştirebilirler. Ajanlar fiziksel ortamlar üzerinde daha doğrudan kontrol kazandıkça, bu güvenlik açığı önemli bir risk oluşturmaktadır. Dolaylı Prompt Enjeksiyonu (IPI) karşı mevcut savunma mekanizmaları genellikle iki kategoriye ayrılır. Birincisi, özel tespit modelleri eğitmeyi içerir; ancak bu yaklaşım, hem eğitim hem de çıkarım için yüksek hesaplama yükü gerektirir ve gelişen saldırı vektörleriyle başa çıkmak için sık güncellemeler gerektirir. Alternatif olarak, prompt tabanlı yöntemler, prompt mühendisliği yoluyla kötü niyetli talimatları tespit etmek veya göz ardı etmek için LLM'lerin doğasında bulunan yeteneklerden yararlanır. Esnekliklerine rağmen, mevcut prompt tabanlı savunmaların çoğu, yüksek Saldırı Başarı Oranları (ASR) ile karakterize edilir ve sofistike enjeksiyon saldırılarına karşı sınırlı bir sağlamlık gösterir. Bu makalede, tool result parsing aracılığıyla LLM'lere hassas veriler sağlayan ve aynı zamanda enjekte edilen kötü niyetli kodu etkili bir şekilde filtreleyen yeni bir yöntem öneriyoruz. Yaklaşımımız, en düşük Saldırı Başarı Oranı (ASR) seviyesini korurken rekabetçi Saldırı Altında Fayda (UA) elde eder ve mevcut yöntemlerden önemli ölçüde daha iyi performans gösterir. Kod GitHub'da mevcuttur."
    }
  },
  {
    "id": "2601.04666v1",
    "title": "Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning",
    "authors": [
      "Zhiyuan Chang",
      "Mingyang Li",
      "Yuekai Huang",
      "Ziyou Jiang",
      "Xiaojun Jia"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.04666v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04666v1",
    "content": {
      "en": "Large language model (LLM)-integrated applications have become increasingly prevalent, yet face critical security vulnerabilities from prompt injection (PI) attacks. Defending against PI attacks faces two major issues: malicious instructions can be injected through diverse vectors, and injected instructions often lack clear semantic boundaries from the surrounding context, making them difficult to identify. To address these issues, we propose InstruCoT, a model enhancement method for PI defense that synthesizes diverse training data and employs instruction-level chain-of-thought fine-tuning, enabling LLMs to effectively identify and reject malicious instructions regardless of their source or position in the context. We evaluate InstruCoT across three critical dimensions: Behavior Deviation, Privacy Leakage, and Harmful Output. Experimental results across four LLMs demonstrate that InstruCoT significantly outperforms baselines in all dimensions while maintaining utility performance without degradation",
      "tr": "**Makale Başlığı:** Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning\n\n**Özet:**\n\nBüyük dil modeli (LLM) entegre uygulamaların yaygınlığı giderek artmaktadır, ancak prompt injection (PI) saldırılarından kaynaklanan kritik güvenlik açıklarıyla karşı karşıyadır. PI saldırılarına karşı savunma iki büyük sorunla karşı karşıyadır: kötü niyetli talimatlar çeşitli vektörler aracılığıyla enjekte edilebilir ve enjekte edilen talimatlar genellikle çevredeki bağlamdan net semantik sınırlardan yoksundur, bu da onları tanımlamayı zorlaştırır. Bu sorunları ele almak için, çeşitli eğitim verileri sentezleyen ve instruction-level chain-of-thought fine-tuning kullanan, böylece LLM'lerin kaynakları veya bağlamdaki konumları ne olursa olsun kötü niyetli talimatları etkili bir şekilde tanımlamasını ve reddetmesini sağlayan bir PI savunması için model geliştirme yöntemi olan InstruCoT'u öneriyoruz. InstruCoT'u üç kritik boyutta değerlendiriyoruz: Behavior Deviation, Privacy Leakage ve Harmful Output. Dört LLM üzerinde yapılan deneysel sonuçlar, InstruCoT'un herhangi bir bozulma olmaksızın fayda performansını korurken, tüm boyutlarda temellere kıyasla önemli ölçüde daha iyi performans gösterdiğini ortaya koymaktadır."
    }
  },
  {
    "id": "2601.04641v1",
    "title": "DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization",
    "authors": [
      "Lionel Z. Wang",
      "Yusheng Zhao",
      "Jiabin Luo",
      "Xinfeng Li",
      "Lixu Wang"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.04641v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04641v1",
    "content": {
      "en": "The deployment of Machine-Generated Text (MGT) detection systems necessitates processing sensitive user data, creating a fundamental conflict between authorship verification and privacy preservation. Standard anonymization techniques often disrupt linguistic fluency, while rigorous Differential Privacy (DP) mechanisms typically degrade the statistical signals required for accurate detection. To resolve this dilemma, we propose \\textbf{DP-MGTD}, a framework incorporating an Adaptive Differentially Private Entity Sanitization algorithm. Our approach utilizes a two-stage mechanism that performs noisy frequency estimation and dynamically calibrates privacy budgets, applying Laplace and Exponential mechanisms to numerical and textual entities respectively. Crucially, we identify a counter-intuitive phenomenon where the application of DP noise amplifies the distinguishability between human and machine text by exposing distinct sensitivity patterns to perturbation. Extensive experiments on the MGTBench-2.0 dataset show that our method achieves near-perfect detection accuracy, significantly outperforming non-private baselines while satisfying strict privacy guarantees.",
      "tr": "**Makale Başlığı:** DP-MGTD: Adaptif Diferansiyel Gizliliğe Sahip Varlık Sanitizasyonu Yoluyla Makine Tarafından Üretilen Metin Tespiti İçin Gizliliği Koruyan Yaklaşım\n\n**Özet:**\n\nMakine Tarafından Üretilen Metin (MGT) tespit sistemlerinin dağıtımı, hassas kullanıcı verilerinin işlenmesini gerektirerek, yazarlık doğrulama ile gizlilik koruma arasında temel bir çatışma yaratmaktadır. Standart anonimleştirme teknikleri genellikle dilsel akıcılığı bozarken, titiz Diferansiyel Gizlilik (DP) mekanizmaları tipik olarak doğru tespit için gereken istatistiksel sinyalleri düşürmektedir. Bu ikilemi çözmek için, Adaptif Diferansiyel Gizliliğe Sahip Varlık Sanitizasyonu algoritmasını içeren bir çerçeve olan \\textbf{DP-MGTD}'yi öneriyoruz. Yaklaşımımız, gürültülü frekans tahmini gerçekleştiren ve gizlilik bütçelerini dinamik olarak kalibre eden, sayısal ve metinsel varlıklara sırasıyla Laplace ve Exponential mekanizmalarını uygulayan iki aşamalı bir mekanizma kullanmaktadır. Kritik öneme sahip olarak, DP gürültüsünün uygulanmasının, pertürbasyona karşı farklı hassasiyet kalıplarını ortaya çıkararak insan ve makine metni arasındaki ayırt edilebilirliği artırdığı sezgilere aykırı bir olgu tespit ediyoruz. MGTBench-2.0 veri kümesi üzerinde yapılan kapsamlı deneyler, yöntemimizin katı gizlilik garantilerini sağlarken, özel olmayan taban çizgisi modellerinden önemli ölçüde daha iyi performans göstererek neredeyse mükemmel bir tespit doğruluğu elde ettiğini göstermektedir."
    }
  },
  {
    "id": "2601.04603v1",
    "title": "Constitutional Classifiers++: Efficient Production-Grade Defenses against Universal Jailbreaks",
    "authors": [
      "Hoagy Cunningham",
      "Jerry Wei",
      "Zihan Wang",
      "Andrew Persic",
      "Alwin Peng"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.04603v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04603v1",
    "content": {
      "en": "We introduce enhanced Constitutional Classifiers that deliver production-grade jailbreak robustness with dramatically reduced computational costs and refusal rates compared to previous-generation defenses. Our system combines several key insights. First, we develop exchange classifiers that evaluate model responses in their full conversational context, which addresses vulnerabilities in last-generation systems that examine outputs in isolation. Second, we implement a two-stage classifier cascade where lightweight classifiers screen all traffic and escalate only suspicious exchanges to more expensive classifiers. Third, we train efficient linear probe classifiers and ensemble them with external classifiers to simultaneously improve robustness and reduce computational costs. Together, these techniques yield a production-grade system achieving a 40x computational cost reduction compared to our baseline exchange classifier, while maintaining a 0.05% refusal rate on production traffic. Through extensive red-teaming comprising over 1,700 hours, we demonstrate strong protection against universal jailbreaks -- no attack on this system successfully elicited responses to all eight target queries comparable in detail to an undefended model. Our work establishes Constitutional Classifiers as practical and efficient safeguards for large language models.",
      "tr": "**Makale Başlığı:** Constitutional Classifiers++: Evrensel Jailbreak'lere Karşı Verimli Üretim Sınıfı Savunmalar\n\n**Özet:**\n\nDaha önceki nesil savunmalara kıyasla dramatik şekilde azaltılmış hesaplama maliyetleri ve reddetme oranları ile üretim sınıfı jailbreak dayanıklılığı sağlayan geliştirilmiş Constitutional Classifiers'ı sunuyoruz. Sistemimiz birkaç temel içgörüyü birleştirmektedir. İlk olarak, model yanıtlarını tam konuşma bağlamında değerlendiren exchange classifiers geliştiriyoruz; bu, çıktıları izole bir şekilde inceleyen son nesil sistemlerdeki savunmasızlıkları ele almaktadır. İkinci olarak, hafif classifiers'ların tüm trafiği taradığı ve yalnızca şüpheli exchanges'leri daha pahalı classifiers'lara yükselttiği iki aşamalı bir classifier cascade uyguluyoruz. Üçüncü olarak, verimli linear probe classifiers eğitiyor ve bunları harici classifiers ile birlikte kullanarak hem dayanıklılığı artırıyor hem de hesaplama maliyetlerini düşürüyoruz. Birlikte, bu teknikler, temel exchange classifier'ımızla karşılaştırıldığında 40 kat hesaplama maliyeti azaltımı sağlayan ve üretim trafiğinde %0,05 reddetme oranını koruyan üretim sınıfı bir sistem ortaya koymaktadır. 1.700 saati aşan kapsamlı red-teaming çalışmaları aracılığıyla, evrensel jailbreak'lere karşı güçlü bir koruma gösteriyoruz – bu sisteme yönelik hiçbir saldırı, savunmasız bir modelle karşılaştırılabilir ayrıntıda sekiz hedef sorgunun tümüne başarılı bir şekilde yanıt üretemedi. Çalışmamız, Constitutional Classifiers'ı büyük dil modelleri için pratik ve verimli korumalar olarak tesis etmektedir."
    }
  },
  {
    "id": "2601.04486v1",
    "title": "Decision-Aware Trust Signal Alignment for SOC Alert Triage",
    "authors": [
      "Israt Jahan Chowdhury",
      "Md Abu Yousuf Tanvir"
    ],
    "published_date": "2026-01-08",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "link": "http://arxiv.org/abs/2601.04486v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04486v1",
    "content": {
      "en": "Detection systems that utilize machine learning are progressively implemented at Security Operations Centers (SOCs) to help an analyst to filter through high volumes of security alerts. Practically, such systems tend to reveal probabilistic results or confidence scores which are ill-calibrated and hard to read when under pressure. Qualitative and survey based studies of SOC practice done before reveal that poor alert quality and alert overload greatly augment the burden on the analyst, especially when tool outputs are not coherent with decision requirements, or signal noise. One of the most significant limitations is that model confidence is usually shown without expressing that there are asymmetric costs in decision making where false alarms are much less harmful than missed attacks. The present paper presents a decision-sensitive trust signal correspondence scheme of SOC alert triage. The framework combines confidence that has been calibrated, lightweight uncertainty cues, and cost-sensitive decision thresholds into coherent decision-support layer, instead of making changes to detection models. To enhance probabilistic consistency, the calibration is done using the known post-hoc methods and the uncertainty cues give conservative protection in situations where model certainty is low. To measure the model-independent performance of the suggested model, we apply the Logistic Regression and the Random Forest classifiers to the UNSW-NB15 intrusion detection benchmark. According to simulation findings, false negatives are greatly amplified by the presence of misaligned displays of confidence, whereas cost weighted loss decreases by orders of magnitude between models with decision aligned trust signals. Lastly, we describe a human-in-the-loop study plan that would allow empirically assessing the decision-making of the analysts with aligned and misaligned trust interfaces.",
      "tr": "**Makale Başlığı:** SOC Alert Triage için Karar Odaklı Güven Sinyali Hizalaması\n\n**Özet:**\n\nGüvenlik Operasyon Merkezlerinde (SOC) makine öğrenmesi tabanlı tespit sistemleri, analistlerin büyük hacimli güvenlik uyarılarını filtrelemesine yardımcı olmak amacıyla giderek daha fazla kullanılmaktadır. Pratikte, bu tür sistemler olasılıksal sonuçlar veya güven skorları sunma eğilimindedir, ancak bu skorlar baskı altında okunması zor, iyi kalibre edilmemiş çıktılardır. Daha önceki nitel ve anket tabanlı SOC uygulaması çalışmaları, düşük uyarı kalitesi ve uyarı yükünün analist üzerindeki yükünü önemli ölçüde artırdığını ortaya koymuştur; özellikle araç çıktıları karar gereksinimleriyle tutarlı değilse veya sinyal gürültüsü varsa bu durum daha da kötüleşir. En önemli sınırlılıklardan biri, model güveninin genellikle karar vermede asimetrik maliyetlerin olduğu, yanlış alarmların kaçırılan saldırılardan çok daha az zararlı olduğu gerçeği ifade edilmeden gösterilmesidir. Bu makale, SOC uyarı sınıflandırması için karar-hassas bir güven sinyali eşleştirme planı sunmaktadır. Çerçeve, tespit modellerinde değişiklik yapmak yerine, kalibre edilmiş güveni, hafif belirsizlik ipuçlarını ve maliyet-duyarlı karar eşiklerini tutarlı bir karar destek katmanına entegre eder. Olasılıksal tutarlılığı artırmak için kalibrasyon, bilinen post-hoc yöntemler kullanılarak yapılır ve belirsizlik ipuçları, model kesinliğinin düşük olduğu durumlarda konservatif koruma sağlar. Önerilen modelin modele bağımlı olmayan performansını ölçmek için UNSW-NB15 izinsiz giriş tespit referansına Logistic Regression ve Random Forest sınıflandırıcılarını uyguluyoruz. Simülasyon bulgularına göre, yanlış negatifler yanlış hizalanmış güven gösterimlerinin varlığıyla büyük ölçüde artarken, maliyet ağırlıklı kayıp, karar hizalanmış güven sinyallerine sahip modeller arasında büyüklük mertebeleriyle azalır. Son olarak, analistlerin hizalanmış ve yanlış hizalanmış güven arayüzleriyle karar verme süreçlerini ampirik olarak değerlendirmeye olanak tanıyacak bir human-in-the-loop çalışma planını açıklıyoruz."
    }
  },
  {
    "id": "2601.04443v1",
    "title": "Large Language Models for Detecting Cyberattacks on Smart Grid Protective Relays",
    "authors": [
      "Ahmad Mohammad Saber",
      "Saeed Jafari",
      "Zhengmao Ouyang",
      "Paul Budnarain",
      "Amr Youssef"
    ],
    "published_date": "2026-01-07",
    "tags": [
      "cs.CR",
      "cs.LG",
      "eess.SP"
    ],
    "link": "http://arxiv.org/abs/2601.04443v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04443v1",
    "content": {
      "en": "This paper presents a large language model (LLM)-based framework for detecting cyberattacks on transformer current differential relays (TCDRs), which, if undetected, may trigger false tripping of critical transformers. The proposed approach adapts and fine-tunes compact LLMs such as DistilBERT to distinguish cyberattacks from actual faults using textualized multidimensional TCDR current measurements recorded before and after tripping. Our results demonstrate that DistilBERT detects 97.6% of cyberattacks without compromising TCDR dependability and achieves inference latency below 6 ms on a commercial workstation. Additional evaluations confirm the framework's robustness under combined time-synchronization and false-data-injection attacks, resilience to measurement noise, and stability across prompt formulation variants. Furthermore, GPT-2 and DistilBERT+LoRA achieve comparable performance, highlighting the potential of LLMs for enhancing smart grid cybersecurity. We provide the full dataset used in this study for reproducibility.",
      "tr": "**Makale Başlığı:** Akıllı Şebeke Koruyucu Rölelerine Yönelik Siber Saldırıların Tespiti İçin Büyük Dil Modelleri\n\n**Özet:**\n\nBu makale, trafo akım diferansiyel röleleri (TCDRs) üzerindeki siber saldırıların tespiti için büyük dil modeli (LLM) tabanlı bir çerçeve sunmaktadır. Tespit edilmeyen bu saldırılar, kritik trafoların yanlış açılmasına (false tripping) neden olabilir. Önerilen yaklaşım, DistilBERT gibi kompakt LLM'leri uyarlayarak ve ince ayarlar yaparak, metinleştirilmiş çok boyutlu TCDR akım ölçümlerini kullanarak siber saldırıları gerçek arızalardan ayırt etmektedir. Bu ölçümler, açma (tripping) öncesi ve sonrası kaydedilmiştir. Sonuçlarımız, DistilBERT'in TCDR'nin güvenilirliğini (dependability) tehlikeye atmadan siber saldırıların %97.6'sını tespit ettiğini ve ticari bir iş istasyonunda 6 ms altında çıkarım gecikmesi (inference latency) sağladığını göstermektedir. Ek değerlendirmeler, çerçevenin birleşik zaman senkronizasyon (time-synchronization) ve yanlış veri enjeksiyonu (false-data-injection) saldırıları altındaki sağlamlığını (robustness), ölçüm gürültüsüne karşı direncini (resilience) ve çeşitli prompt formülasyonları (prompt formulation variants) boyunca istikrarını (stability) doğrulamaktadır. Dahası, GPT-2 ve DistilBERT+LoRA benzer performanslar elde ederek akıllı şebeke siber güvenliğini artırmada LLM'lerin potansiyelini vurgulamaktadır. Reproducibility için bu çalışmada kullanılan tam veri setini sağlıyoruz."
    }
  },
  {
    "id": "2601.04034v1",
    "title": "HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense",
    "authors": [
      "Siyuan Li",
      "Xi Lin",
      "Jun Wu",
      "Zehao Liu",
      "Haoyu Li"
    ],
    "published_date": "2026-01-07",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.04034v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04034v1",
    "content": {
      "en": "Jailbreak attacks pose significant threats to large language models (LLMs), enabling attackers to bypass safeguards. However, existing reactive defense approaches struggle to keep up with the rapidly evolving multi-turn jailbreaks, where attackers continuously deepen their attacks to exploit vulnerabilities. To address this critical challenge, we propose HoneyTrap, a novel deceptive LLM defense framework leveraging collaborative defenders to counter jailbreak attacks. It integrates four defensive agents, Threat Interceptor, Misdirection Controller, Forensic Tracker, and System Harmonizer, each performing a specialized security role and collaborating to complete a deceptive defense. To ensure a comprehensive evaluation, we introduce MTJ-Pro, a challenging multi-turn progressive jailbreak dataset that combines seven advanced jailbreak strategies designed to gradually deepen attack strategies across multi-turn attacks. Besides, we present two novel metrics: Mislead Success Rate (MSR) and Attack Resource Consumption (ARC), which provide more nuanced assessments of deceptive defense beyond conventional measures. Experimental results on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1 demonstrate that HoneyTrap achieves an average reduction of 68.77% in attack success rates compared to state-of-the-art baselines. Notably, even in a dedicated adaptive attacker setting with intensified conditions, HoneyTrap remains resilient, leveraging deceptive engagement to prolong interactions, significantly increasing the time and computational costs required for successful exploitation. Unlike simple rejection, HoneyTrap strategically wastes attacker resources without impacting benign queries, improving MSR and ARC by 118.11% and 149.16%, respectively.",
      "tr": "Makale Başlığı: HoneyTrap: Büyük Dil Modeli Saldırganlarını Honeypot Tuzaklarına Çekerek Aldatma ve Dayanıklı Çoklu Ajan Savunması\n\nÖzet:\nJailbreak saldırıları, büyük dil modellerine (LLM) yönelik ciddi tehditler oluşturarak saldırganların koruma mekanizmalarını aşmalarına olanak tanır. Ancak, mevcut reaktif savunma yaklaşımları, saldırganların güvenlik açıklarından yararlanmak için saldırılarını sürekli derinleştirdiği hızla gelişen multi-turn jailbreak'lerle başa çıkmakta zorlanmaktadır. Bu kritik zorluğun üstesinden gelmek için, jailbreak saldırılarına karşı koymak üzere işbirlikçi savunuculardan yararlanan yenilikçi bir aldatıcı LLM savunma çerçevesi olan HoneyTrap'i öneriyoruz. HoneyTrap, her biri özel bir güvenlik rolü üstlenen ve aldatıcı bir savunmayı tamamlamak üzere işbirliği yapan dört savunma ajanını entegre eder: Threat Interceptor, Misdirection Controller, Forensic Tracker ve System Harmonizer. Kapsamlı bir değerlendirme sağlamak amacıyla, çoklu tur saldırıları boyunca saldırı stratejilerini giderek derinleştirmek üzere tasarlanmış yedi gelişmiş jailbreak stratejisini birleştiren zorlu bir multi-turn progressive jailbreak veri seti olan MTJ-Pro'yu tanıtıyoruz. Bunun yanı sıra, geleneksel ölçütlerin ötesinde aldatıcı savunmanın daha incelikli değerlendirmelerini sağlayan iki yeni metrik sunuyoruz: Mislead Success Rate (MSR) ve Attack Resource Consumption (ARC). GPT-4, GPT-3.5-turbo, Gemini-1.5-pro ve LLaMa-3.1 üzerindeki deneysel sonuçlar, HoneyTrap'in en gelişmiş temel yöntemlere kıyasla saldırı başarı oranlarında ortalama %68.77'lik bir azalma sağladığını göstermektedir. Özellikle, yoğunlaştırılmış koşullara sahip özel bir adaptif saldırgan ayarında bile HoneyTrap, etkileşimleri uzatmak için aldatıcı etkileşimden yararlanarak, başarılı bir şekilde sömürülmesi için gereken süreyi ve hesaplama maliyetlerini önemli ölçüde artırarak dirençli kalır. Basit reddetmenin aksine, HoneyTrap zararsız sorguları etkilemeden saldırgan kaynaklarını stratejik olarak boşa harcar, MSR ve ARC'yi sırasıyla %118.11 ve %149.16 oranında iyileştirir."
    }
  },
  {
    "id": "2601.04278v1",
    "title": "From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning",
    "authors": [
      "Xiaoyu Xu",
      "Minxin Du",
      "Zitong Li",
      "Zi Liang",
      "Zhibiao Guo"
    ],
    "published_date": "2026-01-07",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.04278v1",
    "pdf_link": "https://arxiv.org/pdf/2601.04278v1",
    "content": {
      "en": "Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true \"forgetting scope\" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\\sim}20$ and diversity by ${\\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.",
      "tr": "Makale Başlığı: Alanlardan Örneklemelere: LLM Unlearning için Çift Granülerlikte Veri Sentezi\n\nÖzet:\nMakine unlearning, LLM'lerden özel, zararlı veya telif hakkıyla korunan içeriğin kaldırılması için önemli olsa da, mevcut kıyaslamalar genellikle modelin öğrendiği gerçek \"unutma kapsamını\" doğru bir şekilde temsil etmekte yetersiz kalmaktadır. Biz iki farklı unlearning granülerliğini, alan-düzeyi (domain-level) ve örneklem-düzeyi (instance-level) olmak üzere biçimlendiriyoruz ve yüksek kaliteli unutulmuş veri kümeleri (forget sets) sentezlemek için otomatik bir çerçeve olan BiForget'i öneriyoruz. Harici üreticilere dayanan önceki çalışmalardan farklı olarak, BiForget, tohum-rehberli (seed-guided) ve düşmanca yönlendirme (adversarial prompting) yoluyla modelin içsel bilgi dağılımıyla eşleşen verileri ortaya çıkarmak için doğrudan hedef modeli kullanmaktadır. Çeşitli kıyaslamalarımızdaki deneylerimiz, alaka düzeyi, çeşitlilik ve verimlilik açısından üstün bir denge sağladığını göstermektedir. Nicel olarak, Harry Potter alanında, SOTA'lara kıyasla toplam veri boyutunu yarıya indirirken alaka düzeyini yaklaşık %20 ve çeşitliliği yaklaşık 0.05 oranında iyileştirmektedir. Nihayetinde, daha sağlam bir unutma ve daha iyi bir fayda koruması sağlayarak LLM unlearning'i değerlendirmek için daha titiz bir temel oluşturmaktadır."
    }
  },
  {
    "id": "2601.03868v1",
    "title": "What Matters For Safety Alignment?",
    "authors": [
      "Xing Li",
      "Hui-Ling Zhen",
      "Lihao Yin",
      "Xianzhi Yu",
      "Zhenhua Dong"
    ],
    "published_date": "2026-01-07",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.03868v1",
    "pdf_link": "https://arxiv.org/pdf/2601.03868v1",
    "content": {
      "en": "This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spanning a parameter scale from 3B to 235B. The assessment leverages five established safety datasets and probes model vulnerabilities with 56 jailbreak techniques and four CoT attack strategies, resulting in 4.6M API calls. Our key empirical findings are fourfold. First, we identify the LRMs GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, and GPT-OSS-120B as the top-three safest models, which substantiates the significant advantage of integrated reasoning and self-reflection mechanisms for robust safety alignment. Second, post-training and knowledge distillation may lead to a systematic degradation of safety alignment. We thus argue that safety must be treated as an explicit constraint or a core optimization objective during these stages, not merely subordinated to the pursuit of general capability. Third, we reveal a pronounced vulnerability: employing a CoT attack via a response prefix can elevate the attack success rate by 3.34x on average and from 0.6% to 96.3% for Seed-OSS-36B-Instruct. This critical finding underscores the safety risks inherent in text-completion interfaces and features that allow user-defined response prefixes in LLM services, highlighting an urgent need for architectural and deployment safeguards. Fourth, roleplay, prompt injection, and gradient-based search for adversarial prompts are the predominant methodologies for eliciting unaligned behaviors in modern models.",
      "tr": "Makale Başlığı: Güvenlik Hizalaması İçin Neler Önemlidir?\n\nÖzet:\nBu makale, güvenlik hizalaması (safety alignment) yetenekleri üzerine kapsamlı bir ampirik çalışma sunmaktadır. Daha güvenli ve güvenilir yapay zeka sistemleri geliştirme konusunda temel içgörüler sağlamak amacıyla, büyük dil modellerinde (LLMs) ve büyük dil modellerinde (LRMs) güvenlik hizalaması için nelerin önemli olduğunu değerlendiriyoruz. Altı kritik içsel model karakteristiklerinin ve üç dış saldırı tekniğinin etkisini sistematik olarak araştırıyoruz ve karşılaştırıyoruz. Büyük ölçekli değerlendirmemiz, 3B'den 235B'ye kadar değişen parametre ölçeklerinde, on üç farklı model ailesinden 32 yeni, popüler LLM ve LRM kullanılarak gerçekleştirilmiştir. Değerlendirme, beş yerleşik güvenlik veri kümesinden yararlanmakta ve 56 jailbreak tekniği ve dört CoT saldırı stratejisi ile modelin güvenlik açıklarını araştırmaktadır; bu da 4.6 milyon API çağrısı ile sonuçlanmıştır. Temel ampirik bulgularımız dört yönlüdür. Birincisi, LRM'ler GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking ve GPT-OSS-120B'yi en güvenli üç model olarak belirledik; bu da sağlam güvenlik hizalaması için entegre reasoning ve self-reflection mekanizmalarının önemli avantajını desteklemektedir. İkincisi, post-training ve knowledge distillation, güvenlik hizalamasında sistematik bir bozulmaya yol açabilir. Bu nedenle, güvenliğin genel yetenek arayışının sadece ikincil planda bırakılmak yerine, bu aşamalarda açık bir kısıtlama veya temel bir optimizasyon hedefi olarak ele alınması gerektiğini savunuyoruz. Üçüncüsü, belirgin bir güvenlik açığını ortaya koyuyoruz: bir response prefix aracılığıyla CoT saldırısı kullanmak, ortalama olarak saldırı başarı oranını 3.34 kat artırabilmekte ve Seed-OSS-36B-Instruct modelinde %0.6'dan %96.3'e çıkarmaktadır. Bu kritik bulgu, text-completion arayüzlerinde ve LLM hizmetlerinde kullanıcı tanımlı response prefix'lere izin veren özelliklerde mevcut olan güvenlik risklerini vurgulamakta ve mimari ile dağıtım güvencelerine acil bir ihtiyaç olduğunu göstermektedir. Dördüncüsü, roleplay, prompt injection ve adversarial prompts için gradient-based search, modern modellerde uyumsuz davranışları ortaya çıkarmada hakim metodolojilerdir."
    }
  }
]