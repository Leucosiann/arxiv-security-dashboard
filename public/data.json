[
  {
    "id": "2601.07654v1",
    "title": "Towards Automating Blockchain Consensus Verification with IsabeLLM",
    "authors": [
      "Elliot Jones",
      "William Knottenbelt"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.07654v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07654v1",
    "content": {
      "en": "Consensus protocols are crucial for a blockchain system as they are what allow agreement between the system's nodes in a potentially adversarial environment. For this reason, it is paramount to ensure their correct design and implementation to prevent such adversaries from carrying out malicious behaviour. Formal verification allows us to ensure the correctness of such protocols, but requires high levels of effort and expertise to carry out and thus is often omitted in the development process. In this paper, we present IsabeLLM, a tool that integrates the proof assistant Isabelle with a Large Language Model to assist and automate proofs. We demonstrate the effectiveness of IsabeLLM by using it to develop a novel model of Bitcoin's Proof of Work consensus protocol and verify its correctness. We use the DeepSeek R1 API for this demonstration and found that we were able to generate correct proofs for each of the non-trivial lemmas present in the verification.",
      "tr": "İşte akademik makale başlığının ve özetinin çevirisi:\n\n**Makale Başlığı:** IsabeLLM ile Blok Zinciri Uzlaşma Doğrulamayı Otomatikleştirme Yönünde\n\n**Özet:**\nUzlaşma protokolleri, bir blok zinciri sistemi için hayati öneme sahiptir çünkü potansiyel olarak düşmanca bir ortamda sistemin düğümleri arasındaki anlaşmayı sağlarlar. Bu nedenle, bu tür düşmanların kötü niyetli davranışlar sergilemesini önlemek için tasarımlarının ve uygulamalarının doğruluğunu sağlamak büyük önem taşımaktadır. Formal verification (resmi doğrulama), bu tür protokollerin doğruluğunu sağlamamıza olanak tanır, ancak gerçekleştirilmesi yüksek düzeyde çaba ve uzmanlık gerektirir ve bu nedenle geliştirme sürecinde sıklıkla atlanmaktadır. Bu makalede, ispat yardımcısını (proof assistant) Isabelle'i bir Large Language Model (LLM) ile entegre ederek ispatları otomatikleştirmeye ve bu sürece yardımcı olmaya yönelik bir araç olan IsabeLLM'i sunuyoruz. IsabeLLM'in etkinliğini, Bitcoin'in Proof of Work uzlaşma protokolünün yeni bir modelini geliştirmek ve doğruluğunu doğrulamak için kullanarak gösteriyoruz. Bu gösterim için DeepSeek R1 API'sini kullanıyoruz ve doğrulamada bulunan önemsiz olmayan her lemma için doğru ispatlar üretebildiğimizi tespit ettik."
    }
  },
  {
    "id": "2601.07395v1",
    "title": "MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP",
    "authors": [
      "Ruiqi Li",
      "Zhiqiang Wang",
      "Yunhao Yao",
      "Xiang-Yang Li"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.07395v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07395v1",
    "content": {
      "en": "To standardize interactions between LLM-based agents and their environments, the Model Context Protocol (MCP) was proposed and has since been widely adopted. However, integrating external tools expands the attack surface, exposing agents to tool poisoning attacks. In such attacks, malicious instructions embedded in tool metadata are injected into the agent context during MCP registration phase, thereby manipulating agent behavior. Prior work primarily focuses on explicit tool poisoning or relied on manually crafted poisoned tools. In contrast, we focus on a particularly stealthy variant: implicit tool poisoning, where the poisoned tool itself remains uninvoked. Instead, the instructions embedded in the tool metadata induce the agent to invoke a legitimate but high-privilege tool to perform malicious operations. We propose MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. MCP-ITP formulates poisoned tool generation as a black-box optimization problem and employs an iterative optimization strategy that leverages feedback from both an evaluation LLM and a detection LLM to maximize Attack Success Rate (ASR) while evading current detection mechanisms. Experimental results on the MCPTox dataset across 12 LLM agents demonstrate that MCP-ITP consistently outperforms the manually crafted baseline, achieving up to 84.2% ASR while suppressing the Malicious Tool Detection Rate (MDR) to as low as 0.3%.",
      "tr": "İşte akademik makale başlığının ve özetinin teknik terimler korunarak yapılan Türkçe çevirisi:\n\n**Makale Başlığı:** MCP-ITP: MCP'de Örtük Araç Zehirlemesi İçin Otomatik Bir Çerçeve\n\n**Özet:**\nModel Context Protocol (MCP) tarafından LLM tabanlı ajanlar ve çevreleri arasındaki etkileşimleri standartlaştırmak için önerilen protokol, o zamandan beri yaygın olarak benimsenmiştir. Ancak, harici araçların entegrasyonu saldırı yüzeyini genişleterek ajanları araç zehirlemesi saldırılarına maruz bırakmaktadır. Bu tür saldırılarda, araç meta verilerine gömülü kötü amaçlı talimatlar, MCP kayıt aşamasında ajan bağlamına enjekte edilerek ajanın davranışını manipüle eder. Önceki çalışmalar ağırlıklı olarak açık araç zehirlemesine odaklanmış veya manuel olarak hazırlanmış zehirlenmiş araçlara dayanmıştır. Buna karşılık, biz özellikle gizli bir varyanta odaklanıyoruz: örtük araç zehirlemesi, burada zehirlenmiş araç kendisi çağrılmaz. Bunun yerine, araç meta verilerindeki talimatlar, ajanı kötü amaçlı operasyonlar gerçekleştirmek için meşru ancak yüksek ayrıcalıklı bir aracı çağırmaya teşvik eder. MCP ekosistemi içinde örtük araç zehirlemesi için ilk otomatik ve uyarlanabilir çerçeve olan MCP-ITP'yi öneriyoruz. MCP-ITP, zehirlenmiş araç üretimini bir black-box optimization problemi olarak formüle eder ve Attack Success Rate (ASR) maksimize ederken mevcut tespit mekanizmalarından kaçınmak için hem bir evaluation LLM hem de bir detection LLM'den gelen geri bildirimlerden yararlanan iteratif bir optimization stratejisi kullanır. 12 LLM ajanı üzerindeki MCPTox veri kümesinde yapılan deneysel sonuçlar, MCP-ITP'nin manuel olarak hazırlanmış baseline'dan tutarlı bir şekilde daha iyi performans gösterdiğini, %84.2'ye varan ASR elde ettiğini ve Malicious Tool Detection Rate (MDR)'yi %0.3'e kadar bastırdığını göstermektedir."
    }
  },
  {
    "id": "2601.07276v1",
    "title": "A High-Recall Cost-Sensitive Machine Learning Framework for Real-Time Online Banking Transaction Fraud Detection",
    "authors": [
      "Karthikeyan V. R.",
      "Premnath S.",
      "Kavinraaj S.",
      "J. Sangeetha"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.07276v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07276v1",
    "content": {
      "en": "Fraudulent activities on digital banking services are becoming more intricate by the day, challenging existing defenses. While older rule driven methods struggle to keep pace, even precision focused algorithms fall short when new scams are introduced. These tools typically overlook subtle shifts in criminal behavior, missing crucial signals. Because silent breaches cost institutions far more than flagged but legitimate actions, catching every possible case is crucial. High sensitivity to actual threats becomes essential when oversight leads to heavy losses. One key aim here involves reducing missed fraud cases without spiking incorrect alerts too much. This study builds a system using group learning methods adjusted through smart threshold choices. Using real world transaction records shared openly, where cheating acts rarely appear among normal activities, tests are run under practical skewed distributions. The outcomes reveal that approximately 91 percent of actual fraud is detected, outperforming standard setups that rely on unchanging rules when dealing with uneven examples across classes. When tested in live settings, the fraud detection system connects directly to an online banking transaction flow, stopping questionable activities before they are completed. Alongside this setup, a browser add on built for Chrome is designed to flag deceptive web links and reduce threats from harmful sites. These results show that adjusting decisions by cost impact and validating across entire systems makes deployment more stable and realistic for today's digital banking platforms.",
      "tr": "Elbette, makale başlığı ve özetinin teknik terimler korunarak yapılan Türkçe çevirisi aşağıdadır:\n\n**Makale Başlığı:** Gerçek Zamanlı Çevrimiçi Bankacılık İşlemleri Dolandırıcılık Tespiti İçin Yüksek-Recall Maliyet-Hassasiyetli Bir Makine Öğrenmesi Çerçevesi\n\n**Özet:**\nDijital bankacılık hizmetlerindeki dolandırıcılık faaliyetleri her geçen gün daha karmaşık hale gelmekte ve mevcut savunmaları zorlamaktadır. Eski kural tabanlı yöntemler ayak uydurmakta zorlanırken, hassasiyet odaklı algoritmalar bile yeni dolandırıcılık türleri tanıtıldığında yetersiz kalmaktadır. Bu araçlar tipik olarak suçlu davranışlarındaki ince değişimleri göz ardı eder ve kritik sinyalleri kaçırır. Sessiz ihlaller kurumlara işaretlenen ancak meşru eylemlerden çok daha pahalıya mal olduğu için, olası her vakayı yakalamak esastır. Gözetim ağır kayıplara yol açtığında gerçek tehditlere karşı yüksek hassasiyet önemlidir. Buradaki temel amaçlardan biri, yanlış alarmları aşırı derecede artırmadan gözden kaçan dolandırıcılık vakalarını azaltmaktır. Bu çalışma, akıllı eşik seçimleriyle ayarlanan grup öğrenme yöntemlerini kullanarak bir sistem inşa etmektedir. Dolandırıcılık eylemlerinin normal faaliyetler arasında nadiren görüldüğü, açıkça paylaşılan gerçek dünya işlem kayıtları kullanılarak, pratik çarpık dağılımlar altında testler yürütülmektedir. Sonuçlar, standart kurulumlara kıyasla yaklaşık yüzde 91 oranında gerçek dolandırıcılığın tespit edildiğini ve sınıflar arasındaki dengesiz örneklerle uğraşırken değişmeyen kurallara dayanan standart kurulumları geride bıraktığını ortaya koymaktadır. Canlı ortamlarda test edildiğinde, dolandırıcılık tespit sistemi çevrimiçi bankacılık işlem akışına doğrudan bağlanarak, şüpheli faaliyetleri tamamlanmadan durdurmaktadır. Bu kurulumun yanı sıra, Chrome için tasarlanmış bir tarayıcı eklentisi, yanıltıcı web bağlantılarını işaretlemek ve zararlı sitelerden kaynaklanan tehditleri azaltmak üzere tasarlanmıştır. Bu sonuçlar, maliyet etkisine göre kararları ayarlamanın ve tüm sistemler genelinde doğrulamanın, günümüzün dijital bankacılık platformları için dağıtımı daha istikrarlı ve gerçekçi hale getirdiğini göstermektedir."
    }
  },
  {
    "id": "2601.07263v1",
    "title": "When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering Attack in Web Automation Agent",
    "authors": [
      "Xinyi Wu",
      "Geng Hong",
      "Yueyue Chen",
      "MingXuan Liu",
      "Feier Jin"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.07263v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07263v1",
    "content": {
      "en": "Web agents, powered by large language models (LLMs), are increasingly deployed to automate complex web interactions. The rise of open-source frameworks (e.g., Browser Use, Skyvern-AI) has accelerated adoption, but also broadened the attack surface. While prior research has focused on model threats such as prompt injection and backdoors, the risks of social engineering remain largely unexplored. We present the first systematic study of social engineering attacks against web automation agents and design a pluggable runtime mitigation solution. On the attack side, we introduce the AgentBait paradigm, which exploits intrinsic weaknesses in agent execution: inducement contexts can distort the agent's reasoning and steer it toward malicious objectives misaligned with the intended task. On the defense side, we propose SUPERVISOR, a lightweight runtime module that enforces environment and intention consistency alignment between webpage context and intended goals to mitigate unsafe operations before execution.   Empirical results show that mainstream frameworks are highly vulnerable to AgentBait, with an average attack success rate of 67.5% and peaks above 80% under specific strategies (e.g., trusted identity forgery). Compared with existing lightweight defenses, our module can be seamlessly integrated across different web automation frameworks and reduces attack success rates by up to 78.1% on average while incurring only a 7.7% runtime overhead and preserving usability. This work reveals AgentBait as a critical new threat surface for web agents and establishes a practical, generalizable defense, advancing the security of this rapidly emerging ecosystem. We reported the details of this attack to the framework developers and received acknowledgment before submission.",
      "tr": "Elbette, akademik makale başlığını ve özetini istediğiniz şekilde Türkçeye çevirdim:\n\n**Makale Başlığı:** Botlar Tuzağa Düştüğünde: Web Otomasyon Ajanlarında Ortaya Çıkan Sosyal Mühendislik Saldırısını Açığa Çıkarma ve Azaltma\n\n**Özet:**\nBüyük dil modelleri (LLMs) ile desteklenen web ajanları, giderek artan bir şekilde karmaşık web etkileşimlerini otomatikleştirmek için konuşlandırılmaktadır. Açık kaynaklı framework'lerin (örneğin, Browser Use, Skyvern-AI) yükselişi benimsenmeyi hızlandırmış, ancak aynı zamanda attack surface'ı da genişletmiştir. Önceki araştırmalar prompt injection ve backdoors gibi model tehditlerine odaklanmışken, sosyal mühendislik riskleri büyük ölçüde keşfedilmemiş kalmıştır. Web otomasyon ajanlarına yönelik sosyal mühendislik saldırılarının ilk sistematik çalışmasını sunuyor ve pluggable bir runtime mitigation çözümü tasarlıyoruz. Saldırı tarafında, AgentBait paradigm'ını tanıtıyoruz; bu paradigm, ajan yürütmesindeki içsel zayıflıkları sömürmektedir: 'inducement contexts' ajanın reasoning'ini bozabilir ve onu amaçlanan görevle uyumsuz, kötü niyetli hedeflere yönlendirebilir. Savunma tarafında, SUPERVISOR'ı öneriyoruz; bu, webpage context'i ile amaçlanan hedefler arasındaki environment ve intention consistency alignment'ı zorunlu kılarak yürütme öncesinde güvensiz işlemleri azaltan hafif bir runtime modülüdür. Ampirik sonuçlar, ana akım framework'lerin AgentBait'e karşı oldukça savunmasız olduğunu göstermektedir; ortalama attack success rate %67,5 ve belirli stratejiler (örneğin, trusted identity forgery) altında %80'in üzerinde zirveler kaydedilmiştir. Mevcut hafif savunmalarla karşılaştırıldığında, modülümüz farklı web otomasyon framework'leri arasında sorunsuz bir şekilde entegre edilebilir ve ortalama olarak attack success rates'ini %78,1'e kadar azaltırken yalnızca %7,7'lik bir runtime overhead'e neden olmakta ve kullanılabilirliği korumaktadır. Bu çalışma, AgentBait'i web ajanları için kritik bir yeni threat surface olarak ortaya koymakta ve bu hızla gelişen ekosistemin güvenliğini ilerleterek pratik, genelleştirilebilir bir savunma oluşturmaktadır. Bu saldırının ayrıntılarını framework geliştiricilerine bildirdik ve gönderimden önce onaylarını aldık."
    }
  },
  {
    "id": "2601.07214v1",
    "title": "BlindU: Blind Machine Unlearning without Revealing Erasing Data",
    "authors": [
      "Weiqi Wang",
      "Zhiyi Tian",
      "Chenhan Zhang",
      "Shui Yu"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2601.07214v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07214v1",
    "content": {
      "en": "Machine unlearning enables data holders to remove the contribution of their specified samples from trained models to protect their privacy. However, it is paradoxical that most unlearning methods require the unlearning requesters to firstly upload their data to the server as a prerequisite for unlearning. These methods are infeasible in many privacy-preserving scenarios where servers are prohibited from accessing users' data, such as federated learning (FL). In this paper, we explore how to implement unlearning under the condition of not uncovering the erasing data to the server. We propose \\textbf{Blind Unlearning (BlindU)}, which carries out unlearning using compressed representations instead of original inputs. BlindU only involves the server and the unlearning user: the user locally generates privacy-preserving representations, and the server performs unlearning solely on these representations and their labels. For the FL model training, we employ the information bottleneck (IB) mechanism. The encoder of the IB-based FL model learns representations that distort maximum task-irrelevant information from inputs, allowing FL users to generate compressed representations locally. For effective unlearning using compressed representation, BlindU integrates two dedicated unlearning modules tailored explicitly for IB-based models and uses a multiple gradient descent algorithm to balance forgetting and utility retaining. While IB compression already provides protection for task-irrelevant information of inputs, to further enhance the privacy protection, we introduce a noise-free differential privacy (DP) masking method to deal with the raw erasing data before compressing. Theoretical analysis and extensive experimental results illustrate the superiority of BlindU in privacy protection and unlearning effectiveness compared with the best existing privacy-preserving unlearning benchmarks.",
      "tr": "**Makale Başlığı:** KörMakine Öğrenmesi: Silinen Veriyi Açığa Çıkarmadan Kör Makine Öğrenmesi\n\n**Özet:**\nMachine unlearning, veri sahiplerinin gizliliklerini korumak amacıyla, eğitilmiş modellerden belirli örneklerin katkısını kaldırmasına olanak tanır. Ancak, çoğu unlearning yönteminin, unlearning talebinde bulunanların unlearning'in ön koşulu olarak verilerini öncelikle sunucuya yüklemesini gerektirmesi paradoksal bir durumdur. Bu yöntemler, sunucuların kullanıcı verilerine erişmesinin yasak olduğu, örneğin federated learning (FL) gibi birçok gizlilik koruyucu senaryoda uygulanamaz. Bu makalede, silinen veriyi sunucuya ifşa etmeden unlearning'i nasıl uygulayabileceğimizi araştırıyoruz. Orijinal girdiler yerine sıkıştırılmış temsiller kullanarak unlearning gerçekleştiren **Blind Unlearning (BlindU)** yöntemini öneriyoruz. BlindU, yalnızca sunucu ve unlearning kullanıcısını içerir: kullanıcı yerel olarak gizlilik koruyucu temsiller üretir ve sunucu yalnızca bu temsiller ve etiketleri üzerinde unlearning gerçekleştirir. FL model eğitimi için, information bottleneck (IB) mekanizmasını kullanıyoruz. IB tabanlı FL modelinin encoder'ı, girdilerden görevle ilgisiz maksimum bilgiyi bozan temsiller öğrenir, bu da FL kullanıcılarının yerel olarak sıkıştırılmış temsiller üretmesine olanak tanır. Sıkıştırılmış temsillerle etkili unlearning için, BlindU, özellikle IB tabanlı modeller için tasarlanmış iki özel unlearning modülünü entegre eder ve unutmayı ve faydayı korumayı dengelemek için multiple gradient descent algoritmasını kullanır. IB sıkıştırması girdilerin görevle ilgisiz bilgileri için zaten koruma sağlasa da, gizlilik korumasını daha da artırmak için, sıkıştırmadan önce ham silinen veriyi işlemek üzere gürültüsüz differential privacy (DP) masking yöntemi sunuyoruz. Teorik analiz ve kapsamlı deneysel sonuçlar, BlindU'nun gizlilik koruması ve unlearning etkinliği açısından en iyi mevcut gizlilik koruyucu unlearning kıyaslamalarına kıyasla üstünlüğünü göstermektedir."
    }
  },
  {
    "id": "2601.07185v1",
    "title": "Defenses Against Prompt Attacks Learn Surface Heuristics",
    "authors": [
      "Shawn Li",
      "Chenxiao Yu",
      "Zhiyu Ni",
      "Hao Li",
      "Charith Peris"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.07185v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07185v1",
    "content": {
      "en": "Large language models (LLMs) are increasingly deployed in security-sensitive applications, where they must follow system- or developer-specified instructions that define the intended task behavior, while completing benign user requests. When adversarial instructions appear in user queries or externally retrieved content, models may override intended logic. Recent defenses rely on supervised fine-tuning with benign and malicious labels. Although these methods achieve high attack rejection rates, we find that they rely on narrow correlations in defense data rather than harmful intent, leading to systematic rejection of safe inputs. We analyze three recurring shortcut behaviors induced by defense fine-tuning. \\emph{Position bias} arises when benign content placed later in a prompt is rejected at much higher rates; across reasoning benchmarks, suffix-task rejection rises from below \\textbf{10\\%} to as high as \\textbf{90\\%}. \\emph{Token trigger bias} occurs when strings common in attack data raise rejection probability even in benign contexts; inserting a single trigger token increases false refusals by up to \\textbf{50\\%}. \\emph{Topic generalization bias} reflects poor generalization beyond the defense data distribution, with defended models suffering test-time accuracy drops of up to \\textbf{40\\%}. These findings suggest that current prompt-injection defenses frequently respond to attack-like surface patterns rather than the underlying intent. We introduce controlled diagnostic datasets and a systematic evaluation across two base models and multiple defense pipelines, highlighting limitations of supervised fine-tuning for reliable LLM security.",
      "tr": "Makale Başlığı: Prompt Saldırılarına Karşı Savunmalar Yüzey Sezgilerini Öğrenir\n\nÖzet:\nLarge language models (LLMs), güvenlik açısından hassas uygulamalarda giderek daha fazla kullanılmaktadır. Bu modellerde, sistem veya geliştirici tarafından belirtilen ve hedeflenen görev davranışını tanımlayan talimatlara uyulması gerekmekte, aynı zamanda zararsız kullanıcı istekleri tamamlanmalıdır. Kullanıcı sorgularında veya harici olarak getirilen içerikte zararlı talimatlar göründüğünde, modeller hedeflenen mantığı geçersiz kılabilir. Son dönemdeki savunmalar, iyi huylu ve kötü niyetli etiketlerle supervised fine-tuning üzerine kuruludur. Bu yöntemler yüksek saldırı reddetme oranları elde etse de, zararlı niyet yerine savunma verilerindeki dar korelasyonlara dayandıklarını bulduk. Bu durum, güvenli girdilerin sistematik olarak reddedilmesine yol açmaktadır. Savunma fine-tuning'i tarafından tetiklenen üç tekrarlayan kısayol davranışı analiz ediyoruz. \\emph{Position bias}, iyi huylu içeriğin bir prompt'un ilerleyen bölümlerine yerleştirildiğinde çok daha yüksek oranlarda reddedilmesiyle ortaya çıkar; reasoning benchmarkları genelinde, suffix-task reddetme oranları \\textbf{10\\%} altında iken \\textbf{90\\%} kadar yükselmektedir. \\emph{Token trigger bias}, saldırı verilerinde yaygın olan dizelerin, iyi huylu bağlamlarda bile reddedilme olasılığını artırması durumunda ortaya çıkar; tek bir trigger token eklenmesi, yanlış reddetmeleri \\textbf{50\\%} kadar artırır. \\emph{Topic generalization bias}, savunma verisi dağılımının ötesine geçen zayıf genellemeyi yansıtır ve savunulan modeller test zamanı doğruluk düşüşlerinden \\textbf{40\\%} kadar etkilenir. Bu bulgular, mevcut prompt-injection savunmalarının sıklıkla altta yatan niyet yerine saldırıya benzeyen yüzey desenlerine yanıt verdiğini göstermektedir. Güvenilir LLM güvenliği için supervised fine-tuning'in sınırlılıklarını vurgulayarak, kontrollü tanı setleri ve iki temel model ile çoklu savunma hatları boyunca sistematik bir değerlendirme sunuyoruz."
    }
  },
  {
    "id": "2601.07177v1",
    "title": "Safe-FedLLM: Delving into the Safety of Federated Large Language Models",
    "authors": [
      "Mingxiang Tao",
      "Yu Tian",
      "Wenxuan Tu",
      "Yue Yang",
      "Xue Yang"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.07177v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07177v1",
    "content": {
      "en": "Federated learning (FL) addresses data privacy and silo issues in large language models (LLMs). Most prior work focuses on improving the training efficiency of federated LLMs. However, security in open environments is overlooked, particularly defenses against malicious clients. To investigate the safety of LLMs during FL, we conduct preliminary experiments to analyze potential attack surfaces and defensible characteristics from the perspective of Low-Rank Adaptation (LoRA) weights. We find two key properties of FL: 1) LLMs are vulnerable to attacks from malicious clients in FL, and 2) LoRA weights exhibit distinct behavioral patterns that can be filtered through simple classifiers. Based on these properties, we propose Safe-FedLLM, a probe-based defense framework for federated LLMs, constructing defenses across three dimensions: Step-Level, Client-Level, and Shadow-Level. The core concept of Safe-FedLLM is to perform probe-based discrimination on the LoRA weights locally trained by each client during FL, treating them as high-dimensional behavioral features and using lightweight classification models to determine whether they possess malicious attributes. Extensive experiments demonstrate that Safe-FedLLM effectively enhances the defense capability of federated LLMs without compromising performance on benign data. Notably, our method effectively suppresses malicious data impact without significant impact on training speed, and remains effective even with many malicious clients. Our code is available at: https://github.com/dmqx/Safe-FedLLM.",
      "tr": "İşte akademik makale başlığı ve özetinin çevirisi:\n\n**Makale Başlığı:** Safe-FedLLM: Federated Large Language Models'ın Güvenliğini Derinlemesine İncelemek\n\n**Özet:**\n\nFederated learning (FL), large language models (LLMs) içindeki veri gizliliği ve silolar sorunlarına çözüm sunmaktadır. Önceki çalışmaların çoğu, federated LLMs'lerin eğitim verimliliğini artırmaya odaklanmıştır. Ancak, özellikle kötü niyetli istemcilere karşı savunmalar söz konusu olduğunda, açık ortamlardaki güvenlik göz ardı edilmektedir. FL sırasında LLMs'lerin güvenliğini incelemek amacıyla, Low-Rank Adaptation (LoRA) weights perspektifinden potansiyel saldırı yüzeylerini ve savunulabilir özellikleri analiz etmek için ön deneyler yürütüyoruz. FL'nin iki temel özelliğini tespit ediyoruz: 1) LLMs, FL'deki kötü niyetli istemcilerden gelen saldırılara karşı savunmasızdır ve 2) LoRA weights, basit sınıflandırıcılar aracılığıyla filtrelenebilecek belirgin davranışsal desenler sergiler. Bu özelliklere dayanarak, federated LLMs için bir probe-based savunma çerçevesi olan Safe-FedLLM'yi öneriyoruz ve savunmaları üç boyut üzerinde inşa ediyoruz: Step-Level, Client-Level ve Shadow-Level. Safe-FedLLM'nin temel konsepti, FL sırasında her istemci tarafından yerel olarak eğitilen LoRA weights üzerinde probe-based ayrımlaştırma gerçekleştirmektir; bu weights'ler yüksek boyutlu davranışsal özellikler olarak ele alınır ve kötü niyetli niteliklere sahip olup olmadıklarını belirlemek için hafif sınıflandırma modelleri kullanılır. Kapsamlı deneyler, Safe-FedLLM'nin benign veriler üzerindeki performanstan ödün vermeden federated LLMs'lerin savunma yeteneğini etkili bir şekilde artırdığını göstermektedir. Özellikle, yöntemimiz eğitim hızı üzerinde önemli bir etki olmadan kötü niyetli veri etkisini etkin bir şekilde bastırmakta ve çok sayıda kötü niyetli istemci ile bile etkili kalmaktadır. Kodumuz şu adreste mevcuttur: https://github.com/dmqx/Safe-FedLLM."
    }
  },
  {
    "id": "2601.07134v1",
    "title": "Proof of Reasoning for Privacy Enhanced Federated Blockchain Learning at the Edge",
    "authors": [
      "James Calo",
      "Benny Lo"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.07134v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07134v1",
    "content": {
      "en": "Consensus mechanisms are the core of any blockchain system. However, the majority of these mechanisms do not target federated learning directly nor do they aid in the aggregation step. This paper introduces Proof of Reasoning (PoR), a novel consensus mechanism specifically designed for federated learning using blockchain, aimed at preserving data privacy, defending against malicious attacks, and enhancing the validation of participating networks. Unlike generic blockchain consensus mechanisms commonly found in the literature, PoR integrates three distinct processes tailored for federated learning. Firstly, a masked autoencoder (MAE) is trained to generate an encoder that functions as a feature map and obfuscates input data, rendering it resistant to human reconstruction and model inversion attacks. Secondly, a downstream classifier is trained at the edge, receiving input from the trained encoder. The downstream network's weights, a single encoded datapoint, the network's output and the ground truth are then added to a block for federated aggregation. Lastly, this data facilitates the aggregation of all participating networks, enabling more complex and verifiable aggregation methods than previously possible. This three-stage process results in more robust networks with significantly reduced computational complexity, maintaining high accuracy by training only the downstream classifier at the edge. PoR scales to large IoT networks with low latency and storage growth, and adapts to evolving data, regulations, and network conditions.",
      "tr": "**Makale Başlığı:** Kenarda Gizliliği Artırılmış Birleşik Blokzincir Öğrenmesi İçin Proof of Reasoning\n\n**Özet:**\n\nKonsensüs mekanizmaları her türlü blokzincir sisteminin temelini oluşturur. Bununla birlikte, bu mekanizmaların büyük çoğunluğu doğrudan federated learning'i hedeflememekte veya agregasyon adımına yardımcı olmamaktadır. Bu makale, veri gizliliğini korumayı, kötü niyetli saldırılara karşı savunmayı ve katılımcı ağların geçerliliğini geliştirmeyi amaçlayan, blokzincir kullanarak federated learning için özel olarak tasarlanmış yeni bir konsensüs mekanizması olan Proof of Reasoning'i (PoR) tanıtmaktadır. Literatürde yaygın olarak bulunan genel blokzincir konsensüs mekanizmalarının aksine, PoR federated learning'e özel olarak tasarlanmış üç farklı süreci entegre etmektedir. Birincisi, bir masked autoencoder (MAE), insan yeniden yapılandırmasına ve model inversion saldırılarına karşı dayanıklı hale getiren bir feature map olarak işlev gören ve girdi verilerini gizleyen bir encoder üretmek üzere eğitilir. İkincisi, eğitilmiş encoder'dan girdi alan bir downstream classifier kenarda eğitilir. Daha sonra downstream ağın ağırlıkları, tek bir encoded datapoint, ağın çıktısı ve ground truth, federated aggregation için bir blok zincirine eklenir. Son olarak, bu veri, daha önce mümkün olandan daha karmaşık ve doğrulanabilir agregasyon yöntemlerine olanak tanıyarak tüm katılımcı ağların agregasyonunu kolaylaştırır. Bu üç aşamalı süreç, sadece kenarda downstream classifier'ı eğiterek yüksek doğruluk oranını korurken, daha sağlam ağlar ve önemli ölçüde azaltılmış hesaplama karmaşıklığı ile sonuçlanır. PoR, düşük gecikme süresi ve depolama büyümesiyle büyük IoT ağlarına ölçeklenir ve gelişen verilere, düzenlemelere ve ağ koşullarına uyum sağlar."
    }
  },
  {
    "id": "2601.07122v1",
    "title": "Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework",
    "authors": [
      "Yixiao Peng",
      "Hao Hu",
      "Feiyang Li",
      "Xinye Cao",
      "Yingchang Jiang"
    ],
    "published_date": "2026-01-12",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.07122v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07122v1",
    "content": {
      "en": "While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&CK's Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.",
      "tr": "Makale Başlığı: Bulut Ağı Dayanıklılığını Sağlam LLM Destekli Çoklu-Ajan Takviyeli Öğrenme Çerçevesi ile Güçlendirme\n\nÖzet:\nSanalizasyon ve kaynak havuzlama, bulut ağlarını yapısal esneklik ve elastik ölçeklenebilirlik ile güçlendirirken, kaçınılmaz olarak saldırı yüzeyini genişletmekte ve siber dayanıklılığı zorlamaktadır. Takviyeli Öğrenme (RL)-tabanlı savunma stratejileri, düşmanca koşullar altında kaynak konuşlandırma ve izolasyon politikalarını optimize etmek, ağ kullanılabilirliğini koruyarak ve restore ederek sistem dayanıklılığını artırmak amacıyla geliştirilmiştir. Ancak, mevcut yaklaşımlar ağ yapısındaki, düğüm ölçeğindeki, saldırı stratejilerindeki ve saldırı yoğunluğundaki dinamik değişikliklere uyum sağlamak için yeniden eğitim gerektirdiğinden sağlamlık açısından yetersiz kalmaktadır. Dahası, İnsan Döngüde (Human-in-the-Loop - HITL) desteğinin eksikliği yorumlanabilirliği ve esnekliği sınırlamaktadır. Bu sınırlamaları ele almak için, Büyük Dil Modelleri (LLMs) tarafından desteklenen hiyerarşik bir çoklu-ajan takviyeli öğrenme çerçevesi olan CyberOps-Bots'u öneriyoruz. MITRE ATT&CK'in Taktikler-Teknikler modelinden esinlenen CyberOps-Bots, iki katmanlı bir mimariye sahiptir: (1) Dört modülden oluşan üst düzey bir LLM ajanı -- ReAct planning, IPDRR-based perception, long-short term memory ve action/tool integration -- küresel farkındalık, insan niyet tanıma ve taktiksel planlama gerçekleştirir; (2) Heterojen ayrılmış ön eğitim yoluyla geliştirilen alt düzey RL ajanları, yerelleştirilmiş ağ bölgeleri içinde atomik savunma eylemlerini yürütür. Bu sinerji, güvenilir RL yürütmesini sağlarken LLM uyarlanabilirliğini ve yorumlanabilirliğini korur. Gerçek bulut veri kümeleri üzerinde yapılan deneyler, en gelişmiş algoritmalara kıyasla CyberOps-Bots'un ağ kullanılabilirliğini %68,5 daha yüksek tuttuğunu ve senaryoları yeniden eğitim olmadan değiştirdiğinde %34,7'lik bir sıçrama performansı artışı elde ettiğini göstermektedir. Bildiğimiz kadarıyla bu, bulut savunması için HITL desteği ile sağlam bir LLM-RL çerçevesi oluşturan ilk çalışmadır. Çerçevemizi topluluğa açıklayacağız, bu da bulut ağlarında sağlam ve otonom savunmanın ilerlemesini kolaylaştıracaktır."
    }
  },
  {
    "id": "2601.07072v1",
    "title": "Overcoming the Retrieval Barrier: Indirect Prompt Injection in the Wild for LLM Systems",
    "authors": [
      "Hongyan Chang",
      "Ergute Bao",
      "Xinjian Luo",
      "Ting Yu"
    ],
    "published_date": "2026-01-11",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.07072v1",
    "pdf_link": "https://arxiv.org/pdf/2601.07072v1",
    "content": {
      "en": "Large language models (LLMs) increasingly rely on retrieving information from external corpora. This creates a new attack surface: indirect prompt injection (IPI), where hidden instructions are planted in the corpora and hijack model behavior once retrieved. Previous studies have highlighted this risk but often avoid the hardest step: ensuring that malicious content is actually retrieved. In practice, unoptimized IPI is rarely retrieved under natural queries, which leaves its real-world impact unclear.   We address this challenge by decomposing the malicious content into a trigger fragment that guarantees retrieval and an attack fragment that encodes arbitrary attack objectives. Based on this idea, we design an efficient and effective black-box attack algorithm that constructs a compact trigger fragment to guarantee retrieval for any attack fragment. Our attack requires only API access to embedding models, is cost-efficient (as little as $0.21 per target user query on OpenAI's embedding models), and achieves near-100% retrieval across 11 benchmarks and 8 embedding models (including both open-source models and proprietary services).   Based on this attack, we present the first end-to-end IPI exploits under natural queries and realistic external corpora, spanning both RAG and agentic systems with diverse attack objectives. These results establish IPI as a practical and severe threat: when a user issued a natural query to summarize emails on frequently asked topics, a single poisoned email was sufficient to coerce GPT-4o into exfiltrating SSH keys with over 80% success in a multi-agent workflow. We further evaluate several defenses and find that they are insufficient to prevent the retrieval of malicious text, highlighting retrieval as a critical open vulnerability.",
      "tr": "İşte akademik makale başlığının ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** Geri Çağırma Engellerinin Aşılması: LLM Sistemleri için Gerçek Ortamda Dolaylı Prompt Injection\n\n**Özet:**\n\nBüyük dil modelleri (LLM'ler), harici veri kümelerinden bilgi geri çağırmaya giderek daha fazla dayanmaktadır. Bu durum, yeni bir saldırı yüzeyi oluşturmaktadır: dolaylı prompt injection (IPI). IPI'da gizli talimatlar veri kümelerine yerleştirilir ve geri çağrıldığında modelin davranışını ele geçirir. Önceki çalışmalar bu riski vurgulamış olsa da, genellikle en zorlu adımı atlamışlardır: zararlı içeriğin gerçekten geri çağrılmasını sağlamak. Pratikte, optimize edilmemiş IPI, doğal sorgular altında nadiren geri çağrılır, bu da gerçek dünyadaki etkisini belirsiz bırakır. Biz, zararlı içeriği, geri çağırmayı garanti eden bir trigger fragment'a ve rastgele saldırı hedeflerini kodlayan bir attack fragment'a ayırarak bu zorluğa yaklaşmaktayız. Bu fikirden yola çıkarak, herhangi bir attack fragment için geri çağırmayı garanti eden kompakt bir trigger fragment oluşturan verimli ve etkili bir black-box attack algorithm tasarlıyoruz. Saldırımız, embedding modellerine yalnızca API erişimi gerektirir, maliyet açısından verimlidir (OpenAI'nin embedding modellerinde hedef kullanıcı sorgusu başına 0,21 $ kadar az) ve 11 benchmark ve 8 embedding modeli (hem açık kaynaklı modeller hem de özel hizmetler dahil) üzerinde neredeyse %100 geri çağırma oranına ulaşır. Bu saldırıya dayanarak, doğal sorgular ve gerçekçi harici veri kümeleri altında, RAG ve agentic sistemleri kapsayan, çeşitli saldırı hedefleriyle birlikte ilk uçtan uca IPI exploit'lerini sunuyoruz. Bu sonuçlar, IPI'yı pratik ve ciddi bir tehdit olarak ortaya koymaktadır: kullanıcılar sık sorulan konular hakkında e-postaları özetlemek için doğal bir sorgu gönderdiğinde, tek bir zehirlenmiş e-posta, GPT-4o'yu çoklu-agent workflow'unda %80'in üzerinde başarıyla SSH anahtarlarını dışarı çıkarmaya zorlamak için yeterli olmuştur. Ayrıca, birkaç savunmayı değerlendiriyoruz ve zararlı metnin geri çağrılmasını önlemede yetersiz olduklarını buluyoruz, bu da geri çağırmayı kritik ve açık bir güvenlik açığı olarak vurgulamaktadır."
    }
  }
]