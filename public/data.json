[
    {
        "id": "2412.15842",
        "title": "Adversarial Robustness of Large Language Models in Safety-Critical Applications",
        "authors": [
            "John Smith",
            "Maria Garcia",
            "Wei Chen"
        ],
        "published_date": "2024-12-20",
        "tags": [
            "cs.CR",
            "cs.AI",
            "cs.LG"
        ],
        "link": "https://arxiv.org/abs/2412.15842",
        "pdf_link": "https://arxiv.org/pdf/2412.15842.pdf",
        "content": {
            "en": "Large Language Models (LLMs) are increasingly deployed in safety-critical domains such as healthcare, autonomous systems, and cybersecurity. This paper presents a comprehensive analysis of adversarial attack vectors targeting LLMs in these high-stakes environments. We introduce a novel taxonomy of attacks including prompt injection, jailbreaking, and model extraction techniques. Our experimental results on GPT-4, Claude, and Llama-2 demonstrate that current safety measures can be bypassed with carefully crafted adversarial inputs. We propose a multi-layered defense framework combining input sanitization, output filtering, and runtime monitoring to enhance LLM robustness.",
            "tr": "## Özet\n\nBüyük Dil Modelleri (LLM'ler) sağlık, otonom sistemler ve siber güvenlik gibi **güvenlik açısından kritik alanlarda** giderek daha fazla kullanılmaktadır.\n\n### Ana Bulgular\n- Prompt enjeksiyonu, jailbreaking ve model çıkarma tekniklerini içeren yeni bir saldırı taksonomisi sunulmaktadır\n- GPT-4, Claude ve Llama-2 üzerinde yapılan deneyler, mevcut güvenlik önlemlerinin atlanabildiğini göstermektedir\n- **Çok katmanlı savunma çerçevesi** önerilmektedir:\n  - Girdi temizleme\n  - Çıktı filtreleme\n  - Çalışma zamanı izleme"
        }
    },
    {
        "id": "2412.15721",
        "title": "Federated Learning with Differential Privacy for Secure Medical Image Analysis",
        "authors": [
            "Alice Johnson",
            "Kenji Tanaka",
            "Sarah Williams"
        ],
        "published_date": "2024-12-20",
        "tags": [
            "cs.CR",
            "cs.LG"
        ],
        "link": "https://arxiv.org/abs/2412.15721",
        "pdf_link": "https://arxiv.org/pdf/2412.15721.pdf",
        "content": {
            "en": "Medical image analysis using machine learning requires access to sensitive patient data, raising significant privacy concerns. We propose a federated learning framework integrated with differential privacy guarantees for training deep neural networks on distributed medical imaging datasets. Our approach enables multiple hospitals to collaboratively train diagnostic models without sharing raw patient data. We demonstrate our method on chest X-ray classification achieving 94.2% accuracy while maintaining ε-differential privacy with ε=3.0.",
            "tr": "## Özet\n\nMakine öğrenimi kullanarak tıbbi görüntü analizi, hassas hasta verilerine erişim gerektirmekte ve bu durum önemli **gizlilik endişeleri** yaratmaktadır.\n\n### Önerilen Yaklaşım\n- Diferansiyel gizlilik garantileri ile entegre edilmiş federatif öğrenme çerçevesi\n- Birden fazla hastane, ham hasta verilerini paylaşmadan teşhis modellerini ortaklaşa eğitebilir\n\n### Sonuçlar\n- Göğüs röntgeni sınıflandırmasında **%94.2 doğruluk**\n- ε=3.0 ile ε-diferansiyel gizlilik sağlanmaktadır"
        }
    },
    {
        "id": "2412.14998",
        "title": "Neural Network Watermarking: A Survey on Intellectual Property Protection",
        "authors": [
            "David Lee",
            "Emma Brown",
            "Raj Patel"
        ],
        "published_date": "2024-12-19",
        "tags": [
            "cs.CR",
            "cs.LG",
            "cs.AI"
        ],
        "link": "https://arxiv.org/abs/2412.14998",
        "pdf_link": "https://arxiv.org/pdf/2412.14998.pdf",
        "content": {
            "en": "As deep neural networks become valuable commercial assets, protecting intellectual property rights has emerged as a critical challenge. This survey provides a comprehensive review of neural network watermarking techniques for ownership verification. We categorize existing methods into white-box and black-box approaches, analyzing their robustness against model modification attacks including fine-tuning, pruning, and knowledge distillation. We identify key open challenges and propose future research directions for developing more resilient watermarking schemes.",
            "tr": "## Özet\n\nDerin sinir ağları değerli ticari varlıklar haline geldikçe, **fikri mülkiyet haklarının korunması** kritik bir zorluk olarak ortaya çıkmıştır.\n\n### Katkılar\n- Sahiplik doğrulaması için sinir ağı filigran tekniklerinin kapsamlı incelemesi\n- Mevcut yöntemler **beyaz kutu** ve **kara kutu** yaklaşımları olarak kategorize edilmiştir\n\n### Analiz Edilen Saldırılar\n1. İnce ayar (fine-tuning)\n2. Budama (pruning)\n3. Bilgi damıtma (knowledge distillation)\n\nAçık zorluklar belirlenmiş ve daha dayanıklı filigran şemaları için gelecek araştırma yönleri önerilmiştir."
        }
    },
    {
        "id": "2412.14567",
        "title": "Secure Multi-Party Computation for Privacy-Preserving Machine Learning",
        "authors": [
            "Lisa Wang",
            "Michael Chen",
            "Fatima Al-Said"
        ],
        "published_date": "2024-12-19",
        "tags": [
            "cs.CR",
            "cs.LG"
        ],
        "link": "https://arxiv.org/abs/2412.14567",
        "pdf_link": "https://arxiv.org/pdf/2412.14567.pdf",
        "content": {
            "en": "Secure Multi-Party Computation (MPC) enables multiple parties to jointly compute functions over their private inputs without revealing individual data. We present an optimized MPC protocol for privacy-preserving neural network inference that achieves 10x speedup over existing approaches. Our implementation leverages GPU acceleration and novel secret sharing techniques to enable real-time inference on encrypted data. We evaluate our system on image classification and natural language processing tasks, demonstrating practical deployment feasibility.",
            "tr": "## Özet\n\nGüvenli Çok Taraflı Hesaplama (MPC), birden fazla tarafın bireysel verileri açığa çıkarmadan özel girdileri üzerinde ortaklaşa fonksiyon hesaplamasını sağlar.\n\n### Teknik Katkılar\n- Mevcut yaklaşımlara göre **10 kat hızlanma** sağlayan optimize edilmiş MPC protokolü\n- GPU hızlandırma ve yeni gizli paylaşım teknikleri\n- Şifrelenmiş veri üzerinde gerçek zamanlı çıkarım\n\n### Değerlendirme\n- Görüntü sınıflandırma görevleri\n- Doğal dil işleme görevleri\n- Pratik dağıtım fizibilitesi gösterilmiştir"
        }
    },
    {
        "id": "2412.13892",
        "title": "Automated Vulnerability Detection in Smart Contracts using Graph Neural Networks",
        "authors": [
            "Tom Anderson",
            "Yuki Sato",
            "Anna Kowalski"
        ],
        "published_date": "2024-12-18",
        "tags": [
            "cs.CR",
            "cs.PL",
            "cs.AI"
        ],
        "link": "https://arxiv.org/abs/2412.13892",
        "pdf_link": "https://arxiv.org/pdf/2412.13892.pdf",
        "content": {
            "en": "Smart contracts deployed on blockchain platforms handle billions of dollars in digital assets, making security vulnerabilities extremely costly. We propose SmartGuard, a graph neural network-based approach for automated vulnerability detection in Solidity smart contracts. Our method constructs control flow graphs and data dependency graphs from contract bytecode, then applies a novel attention mechanism to identify vulnerability patterns. Evaluation on a dataset of 47,398 real-world contracts shows 96.3% precision and 91.7% recall in detecting reentrancy, integer overflow, and access control vulnerabilities.",
            "tr": "## Özet\n\nBlok zinciri platformlarında dağıtılan akıllı sözleşmeler milyarlarca dolarlık dijital varlığı yönetmektedir, bu da güvenlik açıklarını son derece maliyetli kılmaktadır.\n\n### SmartGuard Sistemi\n- Solidity akıllı sözleşmelerinde otomatik güvenlik açığı tespiti için **graf sinir ağı tabanlı** yaklaşım\n- Sözleşme bytecode'undan kontrol akış grafları ve veri bağımlılık grafları oluşturulur\n- Güvenlik açığı kalıplarını belirlemek için yeni bir dikkat mekanizması uygulanır\n\n### Sonuçlar (47,398 gerçek dünya sözleşmesi)\n| Metrik | Değer |\n|--------|-------|\n| Kesinlik | %96.3 |\n| Geri çağırma | %91.7 |\n\n**Tespit edilen açıklar:** Yeniden giriş, tamsayı taşması, erişim kontrolü"
        }
    },
    {
        "id": "2412.13456",
        "title": "Formal Verification of Neural Network Controllers for Autonomous Vehicles",
        "authors": [
            "Robert Miller",
            "Chen Liu",
            "Isabella Romano"
        ],
        "published_date": "2024-12-18",
        "tags": [
            "cs.CR",
            "cs.AI",
            "cs.PL"
        ],
        "link": "https://arxiv.org/abs/2412.13456",
        "pdf_link": "https://arxiv.org/pdf/2412.13456.pdf",
        "content": {
            "en": "Autonomous vehicles rely on neural network controllers for safety-critical decisions. We present a formal verification framework that provides mathematical guarantees about neural network behavior within specified operational domains. Our approach combines abstract interpretation with satisfiability modulo theories (SMT) solving to verify properties such as collision avoidance and lane keeping. We demonstrate scalability to networks with millions of parameters, verifying a production-grade perception system from a major automotive manufacturer.",
            "tr": "## Özet\n\nOtonom araçlar, güvenlik açısından kritik kararlar için sinir ağı kontrolörlerine güvenmektedir.\n\n### Önerilen Çerçeve\nBelirli operasyonel alanlar içinde sinir ağı davranışı hakkında **matematiksel garantiler** sağlayan formal doğrulama çerçevesi.\n\n### Metodoloji\n- Soyut yorumlama (abstract interpretation)\n- Tatmin edilebilirlik modülo teorileri (SMT) çözme\n- **Doğrulanan özellikler:**\n  - Çarpışmadan kaçınma\n  - Şerit takibi\n\n### Ölçeklenebilirlik\n- Milyonlarca parametre içeren ağlara ölçeklenebilirlik\n- Büyük bir otomotiv üreticisinden üretim düzeyinde algılama sistemi doğrulanmıştır"
        }
    },
    {
        "id": "2412.12789",
        "title": "Detecting AI-Generated Text: Robustness Analysis of Current Methods",
        "authors": [
            "Sophie Martin",
            "James Wilson",
            "Priya Sharma"
        ],
        "published_date": "2024-12-17",
        "tags": [
            "cs.CR",
            "cs.AI"
        ],
        "link": "https://arxiv.org/abs/2412.12789",
        "pdf_link": "https://arxiv.org/pdf/2412.12789.pdf",
        "content": {
            "en": "The proliferation of large language models has raised concerns about AI-generated misinformation and academic dishonesty. We evaluate the robustness of current AI text detection methods against various evasion techniques including paraphrasing, character substitution, and adversarial perturbations. Our experiments reveal significant vulnerabilities in commercial detectors, with accuracy dropping from 95% to below 50% under targeted attacks. We propose an ensemble detection approach combining statistical and neural methods that maintains 82% accuracy under adversarial conditions.",
            "tr": "## Özet\n\nBüyük dil modellerinin yaygınlaşması, yapay zeka tarafından üretilen yanlış bilgi ve akademik dürüstlük konularında endişelere yol açmıştır.\n\n### Değerlendirilen Kaçınma Teknikleri\n1. Parafraz yapma\n2. Karakter değiştirme\n3. Düşmanca pertürbasyonlar\n\n### Bulgular\n- Ticari dedektörlerde önemli güvenlik açıkları tespit edilmiştir\n- Hedefli saldırılar altında doğruluk **%95'ten %50'nin altına** düşmektedir\n\n### Önerilen Çözüm\n- İstatistiksel ve sinirsel yöntemleri birleştiren **topluluk tespit yaklaşımı**\n- Düşmanca koşullar altında **%82 doğruluk** sağlanmaktadır"
        }
    },
    {
        "id": "2412.12234",
        "title": "Privacy-Preserving Compiler Optimizations for Secure Computation",
        "authors": [
            "Nathan Green",
            "Hiroshi Yamamoto",
            "Clara Santos"
        ],
        "published_date": "2024-12-17",
        "tags": [
            "cs.CR",
            "cs.PL"
        ],
        "link": "https://arxiv.org/abs/2412.12234",
        "pdf_link": "https://arxiv.org/pdf/2412.12234.pdf",
        "content": {
            "en": "Secure computation protocols incur significant performance overhead compared to plaintext computation. We present a domain-specific compiler that automatically optimizes programs for secure execution. Our optimizer applies novel transformations including garbled circuit minimization, SIMD batching for homomorphic encryption, and communication-aware scheduling. Benchmarks on standard cryptographic workloads show 3-15x speedup over naive compilation while preserving semantic equivalence and security guarantees.",
            "tr": "## Özet\n\nGüvenli hesaplama protokolleri, düz metin hesaplamaya kıyasla önemli performans yüküne neden olmaktadır.\n\n### SecureOpt Derleyicisi\nProgramları güvenli yürütme için otomatik olarak optimize eden **alana özgü derleyici**.\n\n### Uygulanan Optimizasyonlar\n- Karıştırılmış devre (garbled circuit) minimizasyonu\n- Homomorfik şifreleme için SIMD gruplama\n- İletişim farkındalıklı zamanlama\n\n### Performans\n- Standart kriptografik iş yüklerinde **3-15 kat hızlanma**\n- Semantik eşdeğerlik korunmaktadır\n- Güvenlik garantileri sağlanmaktadır"
        }
    },
    {
        "id": "2412.11567",
        "title": "Backdoor Attacks and Defenses in Deep Reinforcement Learning",
        "authors": [
            "Kevin Zhang",
            "Marie Dubois",
            "Ahmed Hassan"
        ],
        "published_date": "2024-12-16",
        "tags": [
            "cs.CR",
            "cs.LG",
            "cs.AI"
        ],
        "link": "https://arxiv.org/abs/2412.11567",
        "pdf_link": "https://arxiv.org/pdf/2412.11567.pdf",
        "content": {
            "en": "Deep reinforcement learning agents are vulnerable to backdoor attacks where adversaries embed hidden triggers during training. We systematically study backdoor threats in continuous control environments, demonstrating attacks that cause autonomous agents to fail catastrophically when triggered. Our proposed defense mechanism uses activation clustering and spectral signature analysis to detect backdoored policies with 94% accuracy. We provide theoretical analysis of attack success conditions and defense limitations.",
            "tr": "## Özet\n\nDerin pekiştirmeli öğrenme ajanları, düşmanların eğitim sırasında gizli tetikleyiciler yerleştirdiği **arka kapı saldırılarına** karşı savunmasızdır.\n\n### Çalışmanın Kapsamı\n- Sürekli kontrol ortamlarında arka kapı tehditlerinin sistematik incelenmesi\n- Tetiklendiğinde otonom ajanların felaket boyutunda başarısız olmasına neden olan saldırılar gösterilmiştir\n\n### Savunma Mekanizması\n- **Aktivasyon kümeleme** ve **spektral imza analizi**\n- Arka kapılı politikaları **%94 doğrulukla** tespit eder\n\n### Teorik Katkılar\n- Saldırı başarı koşullarının analizi\n- Savunma sınırlamalarının belirlenmesi"
        }
    },
    {
        "id": "2412.10823",
        "title": "Zero-Knowledge Proofs for Machine Learning Model Integrity",
        "authors": [
            "Laura Thompson",
            "Kai Nakamura",
            "Stefan Mueller"
        ],
        "published_date": "2024-12-15",
        "tags": [
            "cs.CR",
            "cs.LG"
        ],
        "link": "https://arxiv.org/abs/2412.10823",
        "pdf_link": "https://arxiv.org/pdf/2412.10823.pdf",
        "content": {
            "en": "Machine learning as a service requires clients to trust that providers execute the promised model correctly. We introduce zkML, a system for generating zero-knowledge proofs of correct neural network inference. Our approach enables verifiable computation where a prover can demonstrate that outputs were generated by a specific model without revealing model weights or input data. We achieve practical proof generation times of under 10 seconds for ResNet-50 inference through novel circuit optimizations for floating-point arithmetic.",
            "tr": "## Özet\n\nHizmet olarak makine öğrenimi, istemcilerin sağlayıcıların vaat edilen modeli doğru şekilde çalıştırdığına güvenmesini gerektirir.\n\n### zkML Sistemi\nDoğru sinir ağı çıkarımının **sıfır bilgi kanıtlarını** oluşturmak için tasarlanmış sistem.\n\n### Özellikler\n- Kanıtlayıcı, model ağırlıklarını veya girdi verilerini açığa çıkarmadan çıktıların belirli bir model tarafından üretildiğini gösterebilir\n- **Doğrulanabilir hesaplama** sağlanır\n\n### Performans\n- ResNet-50 çıkarımı için **10 saniyenin altında** kanıt üretim süresi\n- Kayan nokta aritmetiği için yeni devre optimizasyonları uygulanmıştır"
        }
    }
]