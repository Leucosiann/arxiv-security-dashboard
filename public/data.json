[
  {
    "id": "2601.09647v1",
    "title": "Identifying Models Behind Text-to-Image Leaderboards",
    "authors": [
      "Ali Naseh",
      "Yuefeng Peng",
      "Anshuman Suri",
      "Harsh Chaudhari",
      "Alina Oprea"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.09647v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09647v1",
    "content": {
      "en": "Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.",
      "tr": "**Makale Başlığı: Text-to-Image Lider Tablolarının Arkasındaki Modellerin Belirlenmesi**\n\n**Özet:**\n\nText-to-image (T2I) modelleri giderek daha popüler hale gelmekte ve çevrimiçi olarak üretilen yapay zeka (AI) görsellerinin büyük bir kısmını oluşturmaktadır. Model kalitesini karşılaştırmak amacıyla, adilliği sağlamak için anonimleştirilmiş model çıktılarına dayanan oylama tabanlı lider tabloları standart hale gelmiştir. Bu çalışmada, bu tür anonimliğin kolayca kırılabileceğini gösteriyoruz. Her bir T2I modelinden elde edilen üretimlerin, image embedding space'te ayırt edici kümeler oluşturduğunu ve prompt control veya training data olmadan doğru bir şekilde anonimlikten çıkarma (deanonymization) olanağı sağladığını buluyoruz. 22 model ve 280 prompt (150K görsel) kullanarak, centroid-based methodumuz yüksek doğruluk oranı elde etmekte ve sistematik model-spesifik imzaları ortaya çıkarmaktadır. Ayrıca, prompt-level distinguishability metric'ini tanıtıyoruz ve bazı prompt'ların nasıl neredeyse mükemmel distinguishability'ye yol açabileceğini gösteren büyük ölçekli analizler yürütüyoruz. Bulgularımız, T2I lider tablolarındaki temel güvenlik açıklarını ortaya koymakta ve daha güçlü anonimleştirme savunmalarını teşvik etmektedir."
    }
  },
  {
    "id": "2601.09625v1",
    "title": "The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware",
    "authors": [
      "Ben Nassi",
      "Bruce Schneier",
      "Oleg Brodt"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.09625v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09625v1",
    "content": {
      "en": "The rapid adoption of large language model (LLM)-based systems -- from chatbots to autonomous agents capable of executing code and financial transactions -- has created a new attack surface that existing security frameworks inadequately address. The dominant framing of these threats as \"prompt injection\" -- a catch-all phrase for security failures in LLM-based systems -- obscures a more complex reality: Attacks on LLM-based systems increasingly involve multi-step sequences that mirror traditional malware campaigns. In this paper, we propose that attacks targeting LLM-based applications constitute a distinct class of malware, which we term \\textit{promptware}, and introduce a five-step kill chain model for analyzing these threats. The framework comprises Initial Access (prompt injection), Privilege Escalation (jailbreaking), Persistence (memory and retrieval poisoning), Lateral Movement (cross-system and cross-user propagation), and Actions on Objective (ranging from data exfiltration to unauthorized transactions). By mapping recent attacks to this structure, we demonstrate that LLM-related attacks follow systematic sequences analogous to traditional malware campaigns. The promptware kill chain offers security practitioners a structured methodology for threat modeling and provides a common vocabulary for researchers across AI safety and cybersecurity to address a rapidly evolving threat landscape.",
      "tr": "İşte akademik makalenin başlığı ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** Promptware Kill Chain: Prompt Enjeksiyonları Nasıl Aşamalı Olarak Çok Adımlı Zararlı Yazılımlara Dönüştü\n\n**Özet:**\nSohbet robotlarından kod ve finansal işlemler gerçekleştirebilen otonom ajanlara kadar büyük dil modeli (LLM) tabanlı sistemlerin hızla benimsenmesi, mevcut güvenlik çerçevelerinin yetersiz kaldığı yeni bir saldırı yüzeyi oluşturmuştur. Bu tehditlerin hakim olarak \"prompt injection\" olarak çerçevelenmesi -- LLM tabanlı sistemlerdeki güvenlik hataları için kullanılan genel bir ifade -- daha karmaşık bir gerçeği gizlemektedir: LLM tabanlı sistemlere yönelik saldırılar giderek artan bir şekilde geleneksel zararlı yazılım kampanyalarını yansıtan çok adımlı diziler içermektedir. Bu makalede, LLM tabanlı uygulamaları hedef alan saldırıların, bizin \\textit{promptware} olarak adlandırdığımız ayırt edici bir zararlı yazılım sınıfı oluşturduğunu öne sürüyor ve bu tehditleri analiz etmek için beş adımlı bir kill chain modeli sunuyoruz. Çerçeve, İlk Erişim (prompt injection), Yetki Yükseltme (jailbreaking), Kalıcılık (memory and retrieval poisoning), Yanal Hareket (cross-system and cross-user propagation) ve Amaç Üzerindeki Eylemler (veri sızdırmadan yetkisiz işlemlere kadar) içermektedir. Son saldırıları bu yapıya haritalayarak, LLM ile ilgili saldırıların geleneksel zararlı yazılım kampanyalarına benzer sistematik dizileri takip ettiğini gösteriyoruz. Promptware kill chain, güvenlik uygulayıcılarına tehdit modellemesi için yapılandırılmış bir metodoloji sunar ve AI safety ile cybersecurity alanındaki araştırmacılara hızla gelişen bir tehdit ortamıyla başa çıkmak için ortak bir kelime dağarcığı sağlar."
    }
  },
  {
    "id": "2601.09460v1",
    "title": "SoK: Enhancing Cryptographic Collaborative Learning with Differential Privacy",
    "authors": [
      "Francesco Capano",
      "Jonas Böhler",
      "Benjamin Weggenmann"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.09460v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09460v1",
    "content": {
      "en": "In collaborative learning (CL), multiple parties jointly train a machine learning model on their private datasets. However, data can not be shared directly due to privacy concerns. To ensure input confidentiality, cryptographic techniques, e.g., multi-party computation (MPC), enable training on encrypted data. Yet, even securely trained models are vulnerable to inference attacks aiming to extract memorized data from model outputs. To ensure output privacy and mitigate inference attacks, differential privacy (DP) injects calibrated noise during training. While cryptography and DP offer complementary guarantees, combining them efficiently for cryptographic and differentially private CL (CPCL) is challenging. Cryptography incurs performance overheads, while DP degrades accuracy, creating a privacy-accuracy-performance trade-off that needs careful design considerations. This work systematizes the CPCL landscape. We introduce a unified framework that generalizes common phases across CPCL paradigms, and identify secure noise sampling as the foundational phase to achieve CPCL. We analyze trade-offs of different secure noise sampling techniques, noise types, and DP mechanisms discussing their implementation challenges and evaluating their accuracy and cryptographic overhead across CPCL paradigms. Additionally, we implement identified secure noise sampling options in MPC and evaluate their computation and communication costs in WAN and LAN. Finally, we propose future research directions based on identified key observations, gaps and possible enhancements in the literature.",
      "tr": "İşte akademik makale başlığı ve özetinin Türkçeye çevirisi, istenen teknik terimlerin İngilizce bırakıldığı ve resmi, akademik bir dilin kullanıldığı haliyle:\n\n**Makale Başlığı:** SoK: Differential Privacy ile Kriptografik İşbirlikçi Öğrenmenin Geliştirilmesi\n\n**Özet:**\nİşbirlikçi öğrenmede (CL), birden fazla taraf kendi özel veri kümeleri üzerinde ortaklaşa bir makine öğrenmesi modeli eğitir. Ancak, gizlilik endişeleri nedeniyle veriler doğrudan paylaşılamaz. Girdi gizliliğini sağlamak için kriptografik teknikler, örneğin multi-party computation (MPC), şifrelenmiş veriler üzerinde eğitime olanak tanır. Yine de, güvenli bir şekilde eğitilmiş modeller bile, model çıktılarından ezberlenmiş verileri çıkarmayı amaçlayan inference attacks saldırılarına karşı savunmasızdır. Çıktı gizliliğini sağlamak ve inference attacks saldırılarını azaltmak için differential privacy (DP), eğitim sırasında kalibre edilmiş gürültü ekler. Kriptografi ve DP tamamlayıcı garantiler sunarken, bunları kriptografik ve differentially private CL (CPCL) için verimli bir şekilde birleştirmek zorludur. Kriptografi performans maliyetleri getirirken, DP doğruluk oranını düşürür, bu da dikkatli tasarım gerektiren bir privacy-accuracy-performance trade-off'u oluşturur. Bu çalışma, CPCL manzarasını sistematikleştirir. CPCL paradigmaları boyunca ortak fazları genelleştiren bir birleşik framework sunuyoruz ve CPCL'yi başarmak için temel faz olarak secure noise sampling'i tanımlıyoruz. Farklı secure noise sampling tekniklerinin, noise türlerinin ve DP mekanizmalarının trade-off'larını analiz ediyor, uygulama zorluklarını tartışıyor ve CPCL paradigmaları boyunca doğruluk oranlarını ve kriptografik maliyetlerini değerlendiriyoruz. Ek olarak, belirlenmiş secure noise sampling seçeneklerini MPC içinde uyguluyor ve WAN ve LAN'daki hesaplama ve iletişim maliyetlerini değerlendiriyoruz. Son olarak, literatürdeki belirlenen temel gözlemler, boşluklar ve olası iyileştirmelere dayanan gelecekteki araştırma yönleri öneriyoruz."
    }
  },
  {
    "id": "2601.09292v1",
    "title": "Blue Teaming Function-Calling Agents",
    "authors": [
      "Greta Dolcetti",
      "Giulio Zizzo",
      "Sergio Maffeis"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.09292v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09292v1",
    "content": {
      "en": "We present an experimental evaluation that assesses the robustness of four open source LLMs claiming function-calling capabilities against three different attacks, and we measure the effectiveness of eight different defences. Our results show how these models are not safe by default, and how the defences are not yet employable in real-world scenarios.",
      "tr": "İşte akademik makale başlığının ve özetinin teknik terimler korunarak Türkçeye çevrilmiş hali:\n\n**Makale Başlığı:** Blue Teaming Function-Calling Agents\n\n**Özet:**\n\nDört açık kaynaklı LLM'nin function-calling yetenekleri iddiasını, üç farklı saldırıya karşı dayanıklılığını değerlendiren deneysel bir inceleme sunuyoruz ve sekiz farklı savunmanın etkinliğini ölçüyoruz. Sonuçlarımız, bu modellerin varsayılan olarak güvenli olmadığını ve savunmaların henüz gerçek dünya senaryolarında kullanılamayacağını göstermektedir."
    }
  },
  {
    "id": "2601.09287v1",
    "title": "Explainable Autoencoder-Based Anomaly Detection in IEC 61850 GOOSE Networks",
    "authors": [
      "Dafne Lozano-Paredes",
      "Luis Bote-Curiel",
      "Juan Ramón Feijóo-Martínez",
      "Ismael Gómez-Talal",
      "José Luis Rojo-Álvarez"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CR",
      "cs.LG",
      "eess.SP"
    ],
    "link": "http://arxiv.org/abs/2601.09287v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09287v1",
    "content": {
      "en": "The IEC 61850 Generic Object-Oriented Substation Event (GOOSE) protocol plays a critical role in real-time protection and automation of digital substations, yet its lack of native security mechanisms can expose power systems to sophisticated cyberattacks. Traditional rule-based and supervised intrusion detection techniques struggle to detect protocol-compliant and zero-day attacks under significant class imbalance and limited availability of labeled data. This paper proposes an explainable, unsupervised multi-view anomaly detection framework for IEC 61850 GOOSE networks that explicitly separates semantic integrity and temporal availability. The approach employs asymmetric autoencoders trained only on real operational GOOSE traffic to learn distinct latent representations of sequence-based protocol semantics and timing-related transmission dynamics in normal traffic. Anomaly detection is implemented using reconstruction errors mixed with statistically grounded thresholds, enabling robust detection without specified attack types. Feature-level reconstruction analysis provides intrinsic explainability by directly linking detection outcomes to IEC 61850 protocol characteristics. The proposed framework is evaluated using real substation traffic for training and a public dataset containing normal traffic and message suppression, data manipulation, and denial-of-service attacks for testing. Experimental results show attack detection rates above 99% with false positives remaining below 5% of total traffic, demonstrating strong generalization across environments and effective operation under extreme class imbalance and interpretable anomaly attribution.",
      "tr": "**Makale Başlığı:** IEC 61850 GOOSE Ağlarında Açıklanabilir Otomatik Kodlayıcı Tabanlı Anomali Tespiti\n\n**Özet:**\n\nIEC 61850 Generic Object-Oriented Substation Event (GOOSE) protokolü, dijital trafo merkezlerinin gerçek zamanlı koruma ve otomasyonunda kritik bir rol oynamaktadır. Ancak, yerleşik güvenlik mekanizmalarının eksikliği, enerji sistemlerini sofistike siber saldırılara maruz bırakabilmektedir. Geleneksel kural tabanlı ve denetimli saldırı tespit teknikleri, önemli sınıf dengesizliği ve etiketli veri kıtlığı altında, protokole uyumlu ve sıfır gün saldırılarını tespit etmekte zorlanmaktadır. Bu makale, IEC 61850 GOOSE ağları için, semantik bütünlük ve zamansal kullanılabilirliği açıkça ayıran, açıklanabilir, denetimsiz çok görünümlü bir anomali tespit çerçevesi sunmaktadır. Yaklaşım, yalnızca gerçek operasyonel GOOSE trafiği üzerinde eğitilmiş asimetrik autoencoders kullanarak, sıra tabanlı protokol semantiği ve zamanla ilgili iletim dinamiklerinin belirgin gizli temsillerini normal trafikte öğrenir. Anomali tespiti, belirtilen saldırı türleri olmadan sağlam bir tespit sağlayan, istatistiksel olarak temellendirilmiş eşiklerle karıştırılmış yeniden yapılandırma hataları kullanılarak gerçekleştirilir. Özellik düzeyinde yeniden yapılandırma analizi, tespit sonuçlarını doğrudan IEC 61850 protokol özelliklerine bağlayarak içsel açıklanabilirlik sağlar. Önerilen çerçeve, eğitim için gerçek trafo merkezi trafiği ve test için normal trafik ile mesaj bastırma, veri manipülasyonu ve hizmet reddi saldırıları içeren bir halka açık veri kümesi kullanılarak değerlendirilmiştir. Deneysel sonuçlar, toplam trafiğin %5'inin altında yanlış pozitifler ile %99'un üzerinde saldırı tespit oranları göstermektedir. Bu, ortamlar arasında güçlü genelleme yeteneği ve aşırı sınıf dengesizliği altında etkili operasyon ile yorumlanabilir anomali atıfı sergilemektedir."
    }
  },
  {
    "id": "2601.09166v1",
    "title": "DP-FEDSOFIM: Differentially Private Federated Stochastic Optimization using Regularized Fisher Information Matrix",
    "authors": [
      "Sidhant R. Nair",
      "Tanmay Sen",
      "Mrinmay Sen"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ],
    "link": "http://arxiv.org/abs/2601.09166v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09166v1",
    "content": {
      "en": "Differentially private federated learning (DP-FL) suffers from slow convergence under tight privacy budgets due to the overwhelming noise introduced to preserve privacy. While adaptive optimizers can accelerate convergence, existing second-order methods such as DP-FedNew require O(d^2) memory at each client to maintain local feature covariance matrices, making them impractical for high-dimensional models. We propose DP-FedSOFIM, a server-side second-order optimization framework that leverages the Fisher Information Matrix (FIM) as a natural gradient preconditioner while requiring only O(d) memory per client. By employing the Sherman-Morrison formula for efficient matrix inversion, DP-FedSOFIM achieves O(d) computational complexity per round while maintaining the convergence benefits of second-order methods. Our analysis proves that the server-side preconditioning preserves (epsilon, delta)-differential privacy through the post-processing theorem. Empirical evaluation on CIFAR-10 demonstrates that DP-FedSOFIM achieves superior test accuracy compared to first-order baselines across multiple privacy regimes.",
      "tr": "Elbette, istenen çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** DP-FEDSOFIM: Düzenlenmiş Fisher Bilgi Matrisi Kullanarak Diferansiyel Olarak Gizli Federated Stokastik Optimizasyon\n\n**Özet:**\nDifferentially private federated learning (DP-FL), gizliliği korumak için eklenen aşırı gürültü nedeniyle sıkı gizlilik bütçeleri altında yavaş yakınsama sorunundan muzdariptir. Adaptif optimizerlar yakınsamayı hızlandırabilirken, mevcut ikinci dereceden yöntemler olan DP-FedNew gibi her istemcide yerel özellik kovaryans matrislerini tutmak için O(d^2) bellek gerektirir, bu da onları yüksek boyutlu modeller için pratik olmaktan çıkarır. DP-FedSOFIM'i öneriyoruz; bu, Fisher Information Matrix'i (FIM) doğal bir gradyan ön koşullandırıcısı olarak kullanan, ancak istemci başına yalnızca O(d) bellek gerektiren sunucu tabanlı bir ikinci dereceden optimizasyon çerçevesidir. Verimli matris ters çevirme için Sherman-Morrison formülünü kullanarak, DP-FedSOFIM, ikinci dereceden yöntemlerin yakınsama avantajlarını korurken tur başına O(d) hesaplama karmaşıklığına ulaşır. Analizimiz, sunucu tarafı ön koşullandırmanın post-processing theorem aracılığıyla (epsilon, delta)-differential privacy'yi koruduğunu kanıtlamaktadır. CIFAR-10 üzerindeki ampirik değerlendirme, DP-FedSOFIM'in birden fazla gizlilik rejimi boyunca birinci dereceden temel çizgilere kıyasla üstün test doğruluğu elde ettiğini göstermektedir."
    }
  },
  {
    "id": "2601.09157v1",
    "title": "Deep Learning-based Binary Analysis for Vulnerability Detection in x86-64 Machine Code",
    "authors": [
      "Mitchell Petingola"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.09157v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09157v1",
    "content": {
      "en": "While much of the current research in deep learning-based vulnerability detection relies on disassembled binaries, this paper explores the feasibility of extracting features directly from raw x86-64 machine code. Although assembly language is more interpretable for humans, it requires more complex models to capture token-level context. In contrast, machine code may enable more efficient, lightweight models and preserve all information that might be lost in disassembly. This paper approaches the task of vulnerability detection through an exploratory study on two specific deep learning model architectures and aims to systematically evaluate their performance across three vulnerability types. The results demonstrate that graph-based models consistently outperform sequential models, emphasizing the importance of control flow relationships, and that machine code contains sufficient information for effective vulnerability discovery.",
      "tr": "**Makale Başlığı:** x86-64 Makine Kodunda Güvenlik Açığı Tespiti İçin Derin Öğrenme Tabanlı İkili Analiz\n\n**Özet:**\n\nMevcut derin öğrenme tabanlı güvenlik açığı tespiti araştırmalarının büyük bir kısmı ayrıştırılmış (disassembled) ikili (binaries) verilere dayanırken, bu çalışma ham x86-64 makine kodundan doğrudan özellik çıkarmanın fizibilitesini araştırmaktadır. Assembly dili insanlar için daha anlaşılır olsa da, token-seviyesi bağlamı yakalamak için daha karmaşık modellere ihtiyaç duyar. Buna karşılık, makine kodu daha verimli, hafif modelleri mümkün kılabilir ve ayrıştırma sırasında kaybolabilecek tüm bilgileri koruyabilir. Bu çalışma, iki spesifik derin öğrenme model mimarisi üzerinde yapılan keşifsel bir çalışma aracılığıyla güvenlik açığı tespiti görevine yaklaşmakta ve üç güvenlik açığı türündeki performanslarını sistematik olarak değerlendirmeyi amaçlamaktadır. Sonuçlar, graph-based modellerin sıralı (sequential) modellere göre tutarlı bir şekilde daha iyi performans gösterdiğini, kontrol akışı (control flow) ilişkilerinin önemini vurguladığını ve makine kodunun etkili güvenlik açığı keşfi için yeterli bilgi içerdiğini göstermektedir."
    }
  },
  {
    "id": "2601.09035v1",
    "title": "A Decompilation-Driven Framework for Malware Detection with Large Language Models",
    "authors": [
      "Aniesh Chawla",
      "Udbhav Prasad"
    ],
    "published_date": "2026-01-14",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.09035v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09035v1",
    "content": {
      "en": "The parallel evolution of Large Language Models (LLMs) with advanced code-understanding capabilities and the increasing sophistication of malware presents a new frontier for cybersecurity research. This paper evaluates the efficacy of state-of-the-art LLMs in classifying executable code as either benign or malicious. We introduce an automated pipeline that first decompiles Windows executable into a C code using Ghidra disassembler and then leverages LLMs to perform the classification. Our evaluation reveals that while standard LLMs show promise, they are not yet robust enough to replace traditional anti-virus software. We demonstrate that a fine-tuned model, trained on curated malware and benign datasets, significantly outperforms its vanilla counterpart. However, the performance of even this specialized model degrades notably when encountering newer malware. This finding demonstrates the critical need for continuous fine-tuning with emerging threats to maintain model effectiveness against the changing coding patterns and behaviors of malicious software.",
      "tr": "**Makale Başlığı:** Büyük Dil Modelleri ile Kötü Amaçlı Yazılım Tespiti için Bir Dekompilasyon Odaklı Çerçeve\n\n**Özet:**\n\nBüyük Dil Modelleri (LLMs)'nin gelişmiş kod anlama yetenekleri ile paralel gelişimi ve kötü amaçlı yazılımların artan karmaşıklığı, siber güvenlik araştırmaları için yeni bir alan sunmaktadır. Bu çalışma, en ileri düzey LLM'lerin yürütülebilir kodu zararsız veya kötü amaçlı olarak sınıflandırmadaki etkinliğini değerlendirmektedir. Otomatik bir pipeline sunuyoruz; bu pipeline öncelikle Ghidra disassembler kullanarak Windows yürütülebilir dosyasını C koduna decompile etmekte ve ardından sınıflandırma işlemini gerçekleştirmek için LLM'leri kullanmaktadır. Değerlendirmemiz, standart LLM'lerin umut vaat etmesine rağmen, geleneksel antivirüs yazılımlarının yerini alacak kadar henüz sağlam olmadığını ortaya koymaktadır. Derlenmiş kötü amaçlı yazılım ve zararsız veri kümeleri üzerinde eğitilmiş ince ayar yapılmış (fine-tuned) bir modelin, standart (vanilla) modeline göre önemli ölçüde daha iyi performans gösterdiğini göstermekteyiz. Bununla birlikte, bu özel modelin performansı bile daha yeni kötü amaçlı yazılımlarla karşılaştığında belirgin şekilde düşmektedir. Bu bulgu, değişen kodlama örüntülerine ve kötü amaçlı yazılımların davranışlarına karşı model etkinliğini sürdürmek için ortaya çıkan tehditlerle sürekli ince ayar yapmanın kritik gerekliliğini göstermektedir."
    }
  },
  {
    "id": "2601.09029v1",
    "title": "Proactively Detecting Threats: A Novel Approach Using LLMs",
    "authors": [
      "Aniesh Chawla",
      "Udbhav Prasad"
    ],
    "published_date": "2026-01-13",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.09029v1",
    "pdf_link": "https://arxiv.org/pdf/2601.09029v1",
    "content": {
      "en": "Enterprise security faces escalating threats from sophisticated malware, compounded by expanding digital operations. This paper presents the first systematic evaluation of large language models (LLMs) to proactively identify indicators of compromise (IOCs) from unstructured web-based threat intelligence sources, distinguishing it from reactive malware detection approaches. We developed an automated system that pulls IOCs from 15 web-based threat report sources to evaluate six LLM models (Gemini, Qwen, and Llama variants). Our evaluation of 479 webpages containing 2,658 IOCs (711 IPv4 addresses, 502 IPv6 addresses, 1,445 domains) reveals significant performance variations. Gemini 1.5 Pro achieved 0.958 precision and 0.788 specificity for malicious IOC identification, while demonstrating perfect recall (1.0) for actual threats.",
      "tr": "**Makale Başlığı: Proaktif Tehdit Tespiti: LLM'ler Kullanılarak Yeni Bir Yaklaşım**\n\n**Özet:**\nKurumsal güvenlik, gelişmiş kötü amaçlı yazılımlardan kaynaklanan ve dijital operasyonların genişlemesiyle daha da karmaşıklaşan tehditlerle karşı karşıyadır. Bu makale, yapılandırılmamış web tabanlı tehdit istihbaratı kaynaklarından uzlaşma göstergelerini (IOCs) proaktif olarak tanımlamak için büyük dil modellerinin (LLMs) ilk sistematik değerlendirmesini sunmaktadır. Bu yaklaşım, reaktif kötü amaçlı yazılım tespit yöntemlerinden farklıdır. 15 web tabanlı tehdit raporu kaynağından IOC'leri çeken otomatik bir sistem geliştirdik ve altı LLM modelini (Gemini, Qwen ve Llama varyantları) değerlendirdik. 2.658 IOC (711 IPv4 adresi, 502 IPv6 adresi, 1.445 alan adı) içeren 479 web sayfasının değerlendirmemiz, önemli performans farklılıklarını ortaya koymaktadır. Gemini 1.5 Pro, kötü niyetli IOC tanımlamasında 0.958 precision ve 0.788 specificity elde etmiş, gerçek tehditler için ise mükemmel recall (1.0) sergilemiştir."
    }
  },
  {
    "id": "2601.08623v1",
    "title": "SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models",
    "authors": [
      "Renyang Liu",
      "Kangjie Chen",
      "Han Qiu",
      "Jie Zhang",
      "Kwok-Yan Lam"
    ],
    "published_date": "2026-01-13",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.08623v1",
    "pdf_link": "https://arxiv.org/pdf/2601.08623v1",
    "content": {
      "en": "Image generation models (IGMs), while capable of producing impressive and creative content, often memorize a wide range of undesirable concepts from their training data, leading to the reproduction of unsafe content such as NSFW imagery and copyrighted artistic styles. Such behaviors pose persistent safety and compliance risks in real-world deployments and cannot be reliably mitigated by post-hoc filtering, owing to the limited robustness of such mechanisms and a lack of fine-grained semantic control. Recent unlearning methods seek to erase harmful concepts at the model level, which exhibit the limitations of requiring costly retraining, degrading the quality of benign generations, or failing to withstand prompt paraphrasing and adversarial attacks. To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection. Without modifying the underlying IGMs, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework comprises two core components: a latent-aware multi-modal safety classifier for identifying unsafe generation trajectories, and a token-level delta generator for precise semantic redirection, equipped with auxiliary predictors for token masking and adaptive scaling to localize and regulate the intervention. Empirical results across multiple representative unlearning tasks demonstrate that SafeRedir achieves effective unlearning capability, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks. Furthermore, SafeRedir generalizes effectively across a variety of diffusion backbones and existing unlearned models, validating its plug-and-play compatibility and broad applicability. Code and data are available at https://github.com/ryliu68/SafeRedir.",
      "tr": "Makale Başlığı: SafeRedir: Image Generation Modellerinde Robust Unlearning için Prompt Embedding Redirection\n\nÖzet:\nImage generation models (IGMs), etkileyici ve yaratıcı içerikler üretme yeteneklerine rağmen, eğitim verilerinden geniş bir yelpazede istenmeyen kavramları sıklıkla ezberleyerek, NSFW görseller ve telif hakkıyla korunan sanatsal stiller gibi güvenli olmayan içeriklerin yeniden üretilmesine yol açmaktadır. Bu tür davranışlar, gerçek dünya konuşlandırmalarında kalıcı güvenlik ve uyumluluk riskleri oluşturmakta ve bu mekanizmaların sınırlı sağlamlığı ve ince taneli anlamsal kontrol eksikliği nedeniyle, sonradan yapılan filtreleme ile güvenilir bir şekilde azaltılamamaktadır. Son unlearning yöntemleri, modeli düzeyinde zararlı kavramları silmeyi amaçlamakta olup, bu yöntemler pahalı yeniden eğitim gerektirme, zararsız üretimlerin kalitesini düşürme veya prompt'ların yeniden ifade edilmesine ve adversarial saldırılara karşı koyamama gibi sınırlılıklar sergilemektedir. Bu zorlukların üstesinden gelmek için, prompt embedding redirection yoluyla robust unlearning için hafif bir inference-time framework olan SafeRedir'i sunuyoruz. Temel IGMs'leri değiştirmeden, SafeRedir embedding alanında token-level müdahaleler aracılığıyla güvensiz prompt'ları güvenli anlamsal bölgelere uyarlanabilir bir şekilde yönlendirir. Framework, iki temel bileşenden oluşmaktadır: güvensiz üretim yörüngelerini belirlemek için latent-aware multi-modal safety classifier ve token masking ve adaptif ölçeklendirme için yardımcı tahmincilerle donatılmış hassas anlamsal yönlendirme için token-level delta generator. Birden fazla temsili unlearning görevi boyunca elde edilen ampirik sonuçlar, SafeRedir'in etkili unlearning yeteneği, yüksek anlamsal ve algısal koruma, sağlam görüntü kalitesi ve adversarial saldırılara karşı geliştirilmiş direnç sağladığını göstermektedir. Dahası, SafeRedir, çeşitli diffusion backbones ve mevcut unlearned modeller arasında etkili bir şekilde genelleme yaparak, plug-and-play uyumluluğunu ve geniş uygulanabilirliğini doğrulamaktadır. Kod ve veriler https://github.com/ryliu68/SafeRedir adresinde mevcuttur."
    }
  }
]