[
  {
    "id": "2512.17902v1",
    "title": "Adversarial Robustness of Vision in Open Foundation Models",
    "authors": [
      "Jonathon Fox",
      "William J Buchanan",
      "Pavlos Papadopoulos"
    ],
    "published_date": "2025-12-19",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.17902v1",
    "pdf_link": "https://arxiv.org/pdf/2512.17902v1",
    "content": {
      "en": "With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.",
      "tr": "İşte makale başlığı ve özetinin Türkçe çevirisi:\n\n**Makale Başlığı:** Açık Temel Modellerde Görüşün Karşıt Saldırıya Direnci (Adversarial Robustness of Vision in Open Foundation Models)\n\n**Özet:**\nDerin öğrenmenin artışıyla birlikte, yapay zeka sistemlerinin nesneleri tanımlayabildiği modelleri anlamak giderek zorlaşmaktadır. Dolayısıyla, bir saldırgan, yapay zekayı varlık tanıma konusunda kafasını karıştıracak şekilde görülmemiş öğeler ekleyerek bir görüntüyü değiştirmeyi hedefleyebilir. Bu makale, LLaVA-1.5-13B ve Meta'nın Llama 3.2 Vision-8B-2'nin karşıt saldırıya direncini (adversarial robustness) araştırmaktadır. Bunlar, görsel girdi modallitesine (visual input modality) karşı hedefsiz PGD (Projected Gradient Descent) için test edilmiş ve Visual Question Answering (VQA) v2 veri kümesi alt kümesi üzerinde deneysel olarak değerlendirilmiştir. Bu karşıt saldırıların (adversarial attacks) sonuçları daha sonra standart VQA accuracy metriği kullanılarak ölçülür. Bu değerlendirme, LLaVA ve Llama 3.2 Vision'ın accuracy degradation'ı (accuracy drop) ile karşılaştırılır. Önemli bir bulgu, Llama 3.2 Vision'ın, bu kurulumda daha düşük bir baseline accuracy'ye sahip olmasına rağmen, özellikle daha yüksek pertürbasyon seviyelerinde LLaVA'ya kıyasla saldırı altındaki performans düşüşünün daha az olduğunu göstermesidir. Genel olarak, bulgular, görsel modallitenin, Meta'nın Llama 3.2 Vision'ı da dahil olmak üzere çağdaş açık ağırlıklı VLM'lerin performansını düşürmek için uygulanabilir bir saldırı vektörü (attack vector) olduğunu doğrulamaktadır. Ayrıca, karşıt saldırıya direncin (adversarial robustness) standart benchmark performansı ile doğrudan ilişkili olmayabileceğini ve altta yatan mimari ve eğitim faktörlerinden etkilenebileceğini vurgulamaktadır."
    }
  },
  {
    "id": "2512.17722v1",
    "title": "Digital and Web Forensics Model Cards, V1",
    "authors": [
      "Paola Di Maio"
    ],
    "published_date": "2025-12-19",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.17722v1",
    "pdf_link": "https://arxiv.org/pdf/2512.17722v1",
    "content": {
      "en": "This paper introduces a standardized model card framework specifically designed for digital and web forensics. Building upon established model card methodologies and recent work on abstract models for digital forensic analysis, this paper presents a web based framework that generates model cards specifically designed to represent knowledge in the forensic domain. The framework includes controlled vocabularies for classification, reasoning types, bias identification, and error categorization, along with a web-based generator tool to facilitate adoption. The paper describes the model card structure, presents the controlled vocabularies, and introduces the beta version of the generator tool, inviting community feedback to refine this emerging standard. Ultimately, the systemic risk is that that the anti fraud and digital and web forensics processes are controlled by the mobs.",
      "tr": "**Makale Başlığı:** Digital and Web Forensics Model Cards, V1\n\n**Özet:**\n\nBu makale, dijital ve web forensics alanına özel olarak tasarlanmış standartlaştırılmış bir model kart çerçevesini tanıtmaktadır. Dijital adli analiz için soyut modeller üzerine yapılan güncel çalışmalar ve yerleşik model kart metodolojilerinden yararlanarak, bu çalışma adli alanındaki knowledge'ı temsil etmek üzere özel olarak tasarlanmış model kartları üreten web tabanlı bir çerçeve sunmaktadır. Çerçeve, classification, reasoning types, bias identification ve error categorization için kontrollü kelime dağarcığını içermekte olup, benimsenmesini kolaylaştırmak üzere web tabanlı bir generator tool'u da kapsamaktadır. Makale, model kart yapısını tarif etmekte, kontrollü kelime dağarcığını sunmakta ve generator tool'unun beta sürümünü tanıtarak, bu gelişmekte olan standardı iyileştirmek üzere topluluk geri bildirimini davet etmektedir. Nihayetinde, systemic risk, anti fraud ve dijital ile web forensics süreçlerinin mobs tarafından kontrol edilmesidir."
    }
  },
  {
    "id": "2512.17667v1",
    "title": "STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting",
    "authors": [
      "Yifei Cheng",
      "Yujia Zhu",
      "Baiyang Li",
      "Xinhao Deng",
      "Yitong Cai"
    ],
    "published_date": "2025-12-19",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "link": "http://arxiv.org/abs/2512.17667v1",
    "pdf_link": "https://arxiv.org/pdf/2512.17667v1",
    "content": {
      "en": "Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR's datasets and code to support reproducibility and future research.",
      "tr": "**Makale Başlığı:** STAR: Sıfır Atışlı HTTPS Web Sitesi Parmak İzi İçin Semantik-Trafik Hizalaması ve Erişimi\n\n**Özet:**\n\nEncrypted Client Hello (ECH) ve şifrelenmiş DNS gibi modern HTTPS mekanizmaları gizliliği artırır ancak şifrelenmiş trafik örüntülerinden ziyaret edilen siteleri çıkarsayan website fingerprinting (WF) saldırılarına karşı savunmasız kalır. Mevcut WF yöntemleri, siteye özgü etiketli izlerle denetimli öğrenmeye dayanır, bu da ölçeklenebilirliği sınırlar ve daha önce görülmemiş web sitelerini işleyememe sorununu ortaya çıkarır. Bu sınırlamaları, WF'yi zero-shot cross-modal retrieval problemi olarak yeniden formüle ederek ve STAR'ı tanıtarak ele alıyoruz. STAR, dual-encoder mimarisi kullanarak şifrelenmiş trafik izleri ve crawl-time logic profile'lar için ortak bir embedding space öğrenir. Kontrastif ve tutarlılık hedefleri ile structure-aware augmentation kullanılarak otomatik olarak toplanan 150K trafik-mantık çifti üzerinde eğitilen STAR, eğitim sırasında hedef taraf trafik gerektirmeden bir iz için en semantik olarak hizalanmış profile'ı erişir. 1.600 görülmemiş web sitesi üzerinde yapılan deneyler, STAR'ın open-world detection'da %87,9 top-1 doğruluğu ve 0,963 AUC elde ettiğini göstermektedir, bu da denetimli ve few-shot temelli yaklaşımlardan daha iyi performans gösterir. Site başına sadece dört etiketli iz içeren bir adapter eklemek, top-5 doğruluğunu %98,8'e çıkarmaktadır. Analizimiz, modern web protokollerindeki içsel semantik-trafik hizalamasını ortaya çıkarmakta ve semantik sızıntıyı şifrelenmiş HTTPS trafiğindeki baskın gizlilik riski olarak belirlemektedir. Tekrarlanabilirlik ve gelecekteki araştırmaları desteklemek için STAR'ın veri kümelerini ve kodunu yayınlıyoruz."
    }
  },
  {
    "id": "2512.17594v1",
    "title": "MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification",
    "authors": [
      "Tosin Ige",
      "Christopher Kiekintveld",
      "Aritran Piplai",
      "Asif Rahman",
      "Olukunle Kolade"
    ],
    "published_date": "2025-12-19",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.17594v1",
    "pdf_link": "https://arxiv.org/pdf/2512.17594v1",
    "content": {
      "en": "Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.",
      "tr": "**Makale Başlığı: MAD-OOD: Dağılım Dışı Kötü Amaçlı Yazılım Tespiti ve Sınıflandırması İçin Derin Öğrenme Kümesi Odaklı Bir Çerçeve**\n\n**Özet:**\n\nDağılım dışı (OOD) tespit, polimorfik ve metamorfik kötü amaçlı yazılım varyantlarının getirdiği aile içi önemli değişkenlik nedeniyle kötü amaçlı yazılım sınıflandırmasında kritik bir zorluk olmaya devam etmektedir. Mevcut derin öğrenme tabanlı kötü amaçlı yazılım tespit cihazlarının çoğu kapalı dünya varsayımlarına dayanmakta ve bu sınıf içi değişkenliği yeterince modellemekte başarısız olmaktadır, bu da daha önce görülmemiş kötü amaçlı yazılım aileleriyle karşılaşıldığında performansın düşmesine yol açmaktadır. Bu makalede, sağlam OOD kötü amaçlı yazılım tespiti ve sınıflandırması için yeni, iki aşamalı, küme odaklı bir derin öğrenme çerçevesi olan MADOOD sunulmaktadır. İlk aşamada, Gaussian Discriminant Analysis (GDA)'dan türetilen sınıf koşullu küresel karar sınırları kullanılarak kötü amaçlı yazılım aile gömülmeleri (embeddings) modellenmektedir. Bu, eğitim sırasında OOD verisine ihtiyaç duymadan dağılım-içi (in-distribution) ve OOD örneklerinin istatistiksel olarak temellendirilmiş bir ayrımını sağlar. Gizli alandaki (latent space) anormal örnekleri güvenilir bir şekilde tanımlamak için birden fazla sınıf merkez noktası (centroids) üzerinden Z-skoru tabanlı mesafe analizi kullanılmaktadır. İkinci aşamada, derin bir sinir ağı, son sınıflandırma doğruluğunu artırmak için küme tabanlı tahminleri, iyileştirilmiş gömülmeleri (refined embeddings) ve denetimli sınıflandırıcı çıktılarını entegre eder. 25 bilinen aile ve birden fazla yeni OOD varyantını içeren kıyaslama kötü amaçlı yazılım veri kümeleri üzerinde yapılan kapsamlı değerlendirmeler, MADOOD'un en gelişmiş OOD tespit yöntemlerinden önemli ölçüde daha iyi performans gösterdiğini ve görülmemiş kötü amaçlı yazılım ailelerinde 0.911'e kadar AUC (Area Under the Curve) elde ettiğini göstermektedir. Önerilen çerçeve, gelişen siber güvenlik ortamlarında gerçek dünya kötü amaçlı yazılım tespiti ve anomali tespiti için ölçeklenebilir, yorumlanabilir ve istatistiksel olarak prensipli bir çözüm sunmaktadır."
    }
  },
  {
    "id": "2512.17527v1",
    "title": "SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals",
    "authors": [
      "Muhammad Haris Khan"
    ],
    "published_date": "2025-12-19",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.17527v1",
    "pdf_link": "https://arxiv.org/pdf/2512.17527v1",
    "content": {
      "en": "Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate \"never-before-seen\" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.",
      "tr": "**Makale Başlığı:** SafeBench-Seq: Protein Tehlike Tarama İçin Homoloji Kümelenmiş, Yalnızca CPU Tabanlı Bir Temel Model ve Fizikokimyasal/Bileşim Özellikleri ile Küme Farkındalığına Sahip Güven Aralıkları\n\n**Özet:**\n\nProtein tasarımı için geliştirilen temel modeller somut biyogüvenlik riskleri barındırmaktadır; ancak toplulukta, homoloji kontrolü altında açıkça değerlendirilen ve standart CPU'larda çalışabilen, sekans düzeyinde tehlike taraması için basit ve tekrarlanabilir bir baseline'a gereksinim duyulmaktadır. SafeBench-Seq'i tanıtıyoruz; bu, yalnızca metadata içeren, tekrarlanabilir bir benchmark ve baseline sınıflandırıcıdır ve tamamen kamuya açık verilerden (SafeProtein tehlikeleri ve UniProt iyi huyluları) ve yorumlanabilir özelliklerden (global fizikokimyasal tanımlayıcılar ve aminoasit bileşimi) inşa edilmiştir. \"Daha önce hiç görülmemiş\" tehditleri yaklaştırmak için, birleşik veri kümesini <=%40 özdeşlikte homology-cluster'lıyoruz ve küme düzeyinde ayırmalar (train/test arasında küme örtüşmesi yok) gerçekleştiriyoruz. Diskriminasyon (AUROC/AUPRC) ve tarama-işletim noktalarını (TPR@1% FPR; FPR@95% TPR) 95% bootstrap confidence intervals (n=200) ile raporluyoruz ve CalibratedClassifierCV aracılığıyla kalibre edilmiş olasılıklar sunuyoruz (Logistic Regression / Random Forest için isotonic; Linear SVM için Platt sigmoid). Olasılık kalitesini Brier score, Expected Calibration Error (ECE; 15 bins) ve reliability diagrams kullanarak ölçüyoruz. Shortcut susceptibility, bileşim koruyan kalıntı karıştırmaları ve yalnızca uzunluk-/bileşim tabanlı ablasyonlar aracılığıyla sorgulanmaktadır. Ampirik olarak, rastgele ayırmalar homoloji kümelenmiş değerlendirmeye kıyasla dayanıklılığı önemli ölçüde abartmaktadır; kalibre edilmiş lineer modeller karşılaştırılabilir düzeyde iyi bir kalibrasyon sergilerken, tree ensemble'lar biraz daha yüksek Brier/ECE değerlerini korumaktadır. SafeBench-Seq yalnızca CPU üzerinde çalışır, tekrarlanabilir ve sadece metadata (erişim bilgileri, küme kimlikleri, ayırma etiketleri) yayınlar, böylece tehlikeli dizileri dağıtmadan titiz bir değerlendirmeye olanak tanır."
    }
  },
  {
    "id": "2512.17519v1",
    "title": "Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models",
    "authors": [
      "Muhammad Haris Khan"
    ],
    "published_date": "2025-12-19",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.17519v1",
    "pdf_link": "https://arxiv.org/pdf/2512.17519v1",
    "content": {
      "en": "We present a simple, PEFT-compatible mechanism that enforces secret-key access control in instruction-tuned language models. K-OTG trains on a dual-path corpus: authorized examples (prefixed with a role key) learn the task output, while unauthorized examples learn a visible block token. At inference, a pre-lm_head hook applies an orthonormal transform to the hidden state: with the correct key/role the inverse map restores the model's native basis; otherwise a session-ephemeral scrambler (permutation, sign flips, Householders) makes logits uninformative and the system short-circuits to BLOCK. Keys are not added as special tokens, and the method composes cleanly with LoRA on 4-bit bases. We evaluate an hour-scale protocol on 1-3B-class instruction models (Llama 3.2, Qwen2.5 1.5B) across utility (XSum ROUGE/BLEU, GSM8K accuracy, WikiText-2 perplexity), selectivity (3by3 role-key unlock matrices), nonce invariance, block suppression, and throughput. Authorized utility remains close to the base on summarization with the expected modest PPL increase from instruction tuning; unauthorized utility collapses (near-zero sequence metrics with exploding PPL), indicating practical unusability without the key. Unlock matrices are diagonally dominant (high on-target unlock, low cross-unlock), authorized block emission is 0 per N via robust bad-word lists, and greedy outputs match exactly across nonces, confirming correct inverse cancellation. The runtime overhead of the Python-level hook is 40% tokens per sec versus the base. K-OTG therefore provides a pragmatic, model-agnostic way to prevent unauthorized use while preserving authorized utility.",
      "tr": "**Makale Başlığı:** Key-Conditioned Orthonormal Transform Gating (K-OTG): LoRA-Ayarlandı Modeller İçin Gizli Durum Karıştırmalı Çoklu Anahtar Erişim Kontrolü\n\n**Özet:**\n\nTalimatla ayarlanmış dil modellerinde gizli anahtar erişim kontrolünü zorlayan basit, PEFT-compatible bir mekanizma sunuyoruz. K-OTG, çift yollu bir corpus üzerinde eğitilir: yetkili örnekler (bir role key ile önekli) görev çıktısını öğrenirken, yetkisiz örnekler görünür bir block token öğrenir. Çıkarım sırasında, bir pre-lm_head hook, hidden state'e ortonormal bir transform uygular: doğru key/role ile ters harita modelin doğal basis'ini geri yükler; aksi takdirde, oturuma özgü geçici bir scrambler (permutation, sign flips, Householders) logits'i bilgilendirmez hale getirir ve sistem BLOCK'a kısa devre yapar. Anahtarlar özel tokenlar olarak eklenmez ve yöntem, 4-bit tabanlarda LoRA ile temiz bir şekilde birleşir. 1-3B sınıfı talimat modellerinde (Llama 3.2, Qwen2.5 1.5B) bir saat ölçekli protokolü fayda (XSum ROUGE/BLEU, GSM8K accuracy, WikiText-2 perplexity), seçicilik (3by3 role-key unlock matrices), nonce invariance, block suppression ve throughput üzerinde değerlendiriyoruz. Yetkili fayda, talimat ayarlamasından beklenen mütevazı PPL artışıyla, özetlemede tabana yakın kalır; yetkisiz fayda, anahtar olmadan pratik kullanılamazlığı göstererek (düşük sıra metrikleri ve patlayan PPL ile) çöker. Unlock matrices çapraz olarak baskındır (yüksek hedefli unlock, düşük çapraz unlock), yetkili blok emisyonu sağlam kötü kelime listeleri aracılığıyla N başına 0'dır ve açgözlü çıktılar nonces boyunca tam olarak eşleşir, doğru ters iptali onaylar. Python seviyesindeki hook'un çalışma zamanı ek yükü, tabana kıyasla saniyede %40 token'dır. Dolayısıyla K-OTG, yetkili faydayı korurken yetkisiz kullanımı önlemek için pragmatik, modelden bağımsız bir yol sunar."
    }
  },
  {
    "id": "2512.17411v1",
    "title": "Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques",
    "authors": [
      "Xingyu Feng"
    ],
    "published_date": "2025-12-19",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.17411v1",
    "pdf_link": "https://arxiv.org/pdf/2512.17411v1",
    "content": {
      "en": "Blockchain technology, lauded for its transparent and immutable nature, introduces a novel trust model. However, its decentralized structure raises concerns about potential inclusion of malicious or illegal content. This study focuses on Ethereum, presenting a data identification and restoration algorithm. Successfully recovering 175 common files, 296 images, and 91,206 texts, we employed the FastText algorithm for sentiment analysis, achieving a 0.9 accuracy after parameter tuning. Classification revealed 70,189 neutral, 5,208 positive, and 15,810 negative texts, aiding in identifying sensitive or illicit information. Leveraging the NSFWJS library, we detected seven indecent images with 100% accuracy. Our findings expose the coexistence of benign and harmful content on the Ethereum blockchain, including personal data, explicit images, divisive language, and racial discrimination. Notably, sensitive information targeted Chinese government officials. Proposing preventative measures, our study offers valuable insights for public comprehension of blockchain technology and regulatory agency guidance. The algorithms employed present innovative solutions to address blockchain data privacy and security concerns.",
      "tr": "**Makale Başlığı:** Makine Öğrenmesi Teknikleri Kullanarak Ethereum Blok Zincirinde Hassas ve Yasa Dışı İçeriğin Tespiti ve Analizi\n\n**Özet:**\n\nBlok zinciri teknolojisi, şeffaf ve değişmez doğasıyla övülerek yeni bir güven modeli sunmaktadır. Ancak, merkeziyetsiz yapısı zararlı veya yasa dışı içeriğin potansiyel dahiliyeti konusunda endişelere yol açmaktadır. Bu çalışma Ethereum'a odaklanarak bir veri tanımlama ve kurtarma algoritması sunmaktadır. Başarıyla 175 yaygın dosya, 296 resim ve 91.206 metin kurtaran çalışmamızda, parametre ayarlamasının ardından 0.9 doğruluk oranı elde ederek duygu analizi için FastText algoritmasını kullandık. Sınıflandırma, 70.189 nötr, 5.208 pozitif ve 15.810 negatif metin ortaya koyarak hassas veya yasa dışı bilgilerin tanımlanmasına yardımcı olmuştur. NSFWJS kütüphanesinden yararlanarak, %100 doğruluk oranıyla yedi müstehcen resim tespit ettik. Bulgularımız, Ethereum blok zincirinde kişisel veriler, müstehcen resimler, ayrımcı dil ve ırkçılık dahil olmak üzere hem zararsız hem de zararlı içeriğin bir arada bulunduğunu ortaya koymaktadır. Özellikle, hassas bilgiler Çinli hükümet yetkililerini hedef almıştır. Önleyici tedbirler öneren çalışmamız, blok zinciri teknolojisinin kamu tarafından anlaşılması ve düzenleyici kurumların rehberliği için değerli bilgiler sunmaktadır. Kullanılan algoritmalar, blok zinciri veri gizliliği ve güvenlik endişelerini gidermeye yönelik yenilikçi çözümler sunmaktadır."
    }
  },
  {
    "id": "2512.17398v1",
    "title": "DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference",
    "authors": [
      "Yonathan Bornfeld",
      "Shai Avidan"
    ],
    "published_date": "2025-12-19",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.17398v1",
    "pdf_link": "https://arxiv.org/pdf/2512.17398v1",
    "content": {
      "en": "Private Inference (PI) uses cryptographic primitives to perform privacy preserving machine learning. In this setting, the owner of the network runs inference on the data of the client without learning anything about the data and without revealing any information about the model. It has been observed that a major computational bottleneck of PI is the calculation of the gate (i.e., ReLU), so a considerable amount of effort have been devoted to reducing the number of ReLUs in a given network.   We focus on the DReLU, which is the non-linear step function of the ReLU and show that one DReLU can serve many ReLU operations. We suggest a new activation module where the DReLU operation is only performed on a subset of the channels (Prototype channels), while the rest of the channels (replicate channels) replicates the DReLU of each of their neurons from the corresponding neurons in one of the prototype channels. We then extend this idea to work across different layers.   We show that this formulation can drastically reduce the number of DReLU operations in resnet type network. Furthermore, our theoretical analysis shows that this new formulation can solve an extended version of the XOR problem, using just one non-linearity and two neurons, something that traditional formulations and some PI specific methods cannot achieve. We achieve new SOTA results on several classification setups, and achieve SOTA results on image segmentation.",
      "tr": "**Makale Başlığı:** DeepShare: Verimli Gizli Çıkarım İçin Kanallar ve Katmanlar Boyunca ReLU Paylaşımı\n\n**Özet:**\nPrivate Inference (PI), gizlilik korumalı makine öğrenmesi gerçekleştirmek için kriptografik önlemler kullanır. Bu bağlamda, ağın sahibi, veriler hakkında hiçbir şey öğrenmeden ve model hakkında herhangi bir bilgi ifşa etmeden istemcinin verileri üzerinde çıkarım yapar. PI'nin önemli bir hesaplama darboğazının kapı (yani, ReLU) hesaplaması olduğu gözlemlenmiştir, bu nedenle belirli bir ağdaki ReLU sayısı azaltmaya önemli ölçüde çaba harcanmıştır. ReLU'nun doğrusal olmayan adım fonksiyonu olan DReLU'ya odaklanıyoruz ve bir DReLU'nun birçok ReLU işlemine hizmet edebileceğini gösteriyoruz. DReLU işleminin yalnızca kanalların bir alt kümesi (Prototype channels) üzerinde gerçekleştirildiği yeni bir aktivasyon modülü öneriyoruz, diğer kanallar (replicate channels) ise prototip kanallardan birindeki karşılık gelen nöronlarından her birinin nöronlarının DReLU'sunu kopyalar. Ardından, bu fikri farklı katmanlar boyunca çalışacak şekilde genişletiyoruz. Bu formülasyonun resnet tipi ağlarda DReLU işlemlerinin sayısını büyük ölçüde azaltabileceğini gösteriyoruz. Dahası, teorik analizimiz, bu yeni formülasyonun, geleneksel formülasyonların ve bazı PI'ye özgü yöntemlerin başaramadığı, yalnızca bir doğrusal olmayanlık ve iki nöron kullanarak XOR probleminin genişletilmiş bir versiyonunu çözebileceğini göstermektedir. Çeşitli sınıflandırma kurulumlarında yeni SOTA sonuçlar elde ettik ve görüntü segmentasyonunda SOTA sonuçlar elde ettik."
    }
  },
  {
    "id": "2512.17375v1",
    "title": "AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens",
    "authors": [
      "Tung-Ling Li",
      "Yuhao Wu",
      "Hongliang Liu"
    ],
    "published_date": "2025-12-19",
    "tags": [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.17375v1",
    "pdf_link": "https://arxiv.org/pdf/2512.17375v1",
    "content": {
      "en": "Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.",
      "tr": "Makale Başlığı: AdvJudge-Zero: Adversarial Control Token'lar Aracılığıyla LLM-as-a-Judge'da İkili Karar Çevirmeleri\n\nÖzet:\nReward modelleri ve LLM-as-a-Judge sistemleri, modern post-training pipeline'larında (RLHF, DPO ve RLAIF gibi) merkezi bir rol oynamakta olup, model seçimi ve RL tabanlı fine-tuning'i yönlendiren scalar feedback ve binary decision'lar sağlamaktadır. Bu judge sistemlerinin tekrarlayan bir zafiyet sergilediğini göstermekteyiz: kısa, low-perplexity control token dizileri, last-layer logit gap'ı yönlendirerek birçok binary evaluation'ı doğru \"No\" yargısından yanlış \"Yes\" yargısına çevirebilir. Bu control token'lar, bir policy modelinin post-training sırasında makul olarak üretebileceği desenler olup, worst-case adversarial strings'ten ziyade gerçekçi reward-hacking risklerini temsil etmektedir. Yöntemimiz AdvJudge-Zero, modelin next-token distribution'unu ve beam-search exploration'ını kullanarak diverse control-token dizilerini sıfırdan keşfeder ve analizimiz, indüklenen hidden-state perturbation'ların, judge'ın refusal direction'ı ile anti-aligned olan düşük rank'lı bir \"soft mode\"da yoğunlaştığını göstermektedir. Ampirik olarak, bu token'lar, büyük açık ağırlıklı ve özel judge modelleri matematik ve reasoning benchmark'larında yanlış cevapları skorlarken çok yüksek false positive oranlarına neden olmaktadır. Son olarak, küçük kontrol token'ı ile zenginleştirilmiş örnek kümeleri üzerinde LoRA tabanlı adversarial training'in, evaluation kalitesini korurken bu false positive'leri belirgin şekilde azaltabileceğini göstermekteyiz."
    }
  },
  {
    "id": "2512.17254v1",
    "title": "Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning",
    "authors": [
      "Baolei Zhang",
      "Minghong Fang",
      "Zhuqing Liu",
      "Biao Yi",
      "Peizhao Zhou"
    ],
    "published_date": "2025-12-19",
    "tags": [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.17254v1",
    "pdf_link": "https://arxiv.org/pdf/2512.17254v1",
    "content": {
      "en": "Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.",
      "tr": "İşte makale başlığının ve özetinin akademik ve resmi bir dilde Türkçe çevirisi, belirtilen teknik terimler olduğu gibi korunarak yapılmıştır:\n\n**Makale Başlığı:** Gizlilik Korumalı ve Bizans'a Dayanıklı Federe Öğrenme için Pratik Bir Çerçeve\n\n**Özet:**\nFedere Öğrenme (FL), birden fazla istemcinin özel verilerini paylaşmadan bir modeli işbirliği içinde eğitebilmesini sağlar. Ancak FL, Bizans saldırılarına karşı hassastır; bu saldırılarda düşmanlar, federe modeli tehlikeye atmak için istemci modellerini manipüle ederler ve ayrıca istemci modellerini özel verileri tahmin etmek için kullanan gizlilik çıkarım saldırılarına da açıktır. Hem geri kapı (backdoor) hem de gizlilik çıkarım saldırılarına karşı mevcut savunmalar, teorik ve pratik arasındaki bir boşluğu doldurarak önemli hesaplama ve iletişim yükü getirmektedir. Bu sorunu ele almak için, Bizans'a dayanıklı ve gizlilik korumalı FL için pratik bir çerçeve olan ABBR'yi öneriyoruz. Gizlilik korumalı FL'de karmaşık filtreleme kurallarının özel hesaplamasını hızlandırmak için boyut azaltmayı kullanan ilk çalışmayı gerçekleştiriyoruz. Ek olarak, düşük boyutlu uzayda vektör bazlı filtrelemenin doğruluk kaybını analiz ediyoruz ve filtrelemeyi geçen kötü niyetli modellerin küresel model üzerindeki etkisini en aza indirmek için adaptif bir ayarlama stratejisi sunuyoruz. ABBR'yi en gelişmiş Bizans'a dayanıklı agregasyon kurallarıyla uyguladık ve halka açık veri kümeleri üzerinde değerlendirdik. Bu değerlendirme, ABBR'nin önemli ölçüde daha hızlı çalıştığını, minimum iletişim yüküne sahip olduğunu ve temel karşılaştırma yöntemleriyle neredeyse aynı Bizans dayanıklılığını koruduğunu göstermektedir."
    }
  }
]