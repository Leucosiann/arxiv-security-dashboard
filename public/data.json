[
  {
    "id": "2602.14135v1",
    "title": "ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI",
    "authors": [
      "Haibo Tong",
      "Feifei Zhao",
      "Linghao Feng",
      "Ruoyu Wu",
      "Ruolin Chen"
    ],
    "published_date": "2026-02-15",
    "tags": [
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "link": "http://arxiv.org/abs/2602.14135v1",
    "pdf_link": "https://arxiv.org/pdf/2602.14135v1",
    "content": {
      "en": "Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the \"ForesightSafety Bench\" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.",
      "tr": "Elbette, istediğiniz çeviriyi aşağıda bulabilirsiniz:\n\n**Makale Başlığı:** ForesightSafety Bench: Yapay Zekayı Güvenli Hale Getirmeye Yönelik Bir İleriye Dönük Risk Değerlendirme ve Yönetişim Çerçevesi\n\n**Özet:**\n\nHızla gelişen yapay zeka, giderek artan otonomi ve hedef odaklı yetenekler sergilemekte olup, bununla birlikte daha öngörülemez, kontrolü zor ve potansiyel olarak geri döndürülemez türevsel sistemik riskler de beraberinde getirmektedir. Ancak, mevcut yapay zeka güvenlik değerlendirme sistemleri, sınırlı risk boyutları ve ileriye dönük risklerin tespit edilememesi gibi kritik sınırlılıklardan muzdariptir. Geri planda kalan güvenlik benchmarkları ve alignment teknolojileri, en son yapay zeka modellerinin ortaya çıkardığı karmaşık zorluklarla güçlükle başa çıkabilmektedir. Bu boşluğu doldurmak için, 7 ana Temel Güvenlik (Fundamental Safety) sütunu ile başlayan ve ilerleyen aşamalarda Gelişmiş Vücutlu Yapay Zeka Güvenliği (Embodied AI Safety), Yapay Zeka Bilimi Güvenliği (AI4Science Safety), Sosyal ve Çevresel Yapay Zeka Riskleri, Felaket ve Varoluşsal Riskler (Catastrophic and Existential Risks) ile 8 kritik endüstriyel güvenlik alanına genişleyen ve toplamda 94 adet rafine edilmiş risk boyutu oluşturan \"ForesightSafety Bench\" Yapay Zeka Güvenlik Değerlendirme Çerçevesi'ni öneriyoruz. Bugüne kadar, benchmark, on binlerce yapılandırılmış risk veri noktasını ve değerlendirme sonucunu biriktirerek, yaygın bir kapsama sahip, hiyerarşik olarak net ve dinamik olarak gelişen bir yapay zeka güvenlik değerlendirme çerçevesi oluşturmuştur. Bu benchmark temelinde, yirmiyi aşkın ana akım ileri büyük model üzerinde sistematik değerlendirmeler ve derinlemesine analizler yürüterek, temel risk örüntülerini ve onların yetenek sınırlarındaki etkilerini tespit ettik. Güvenlik yetenek değerlendirme sonuçları, Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety ve Catastrophic and Existential Risks'e özellikle odaklanarak, ileriye dönük yapay zekanın birden fazla sütunda yaygın güvenlik açıkları olduğunu ortaya koymaktadır. Benchmark'ımız https://github.com/Beijing-AISI/ForesightSafety-Bench adresinde yayımlanmıştır. Proje web sitesine https://foresightsafety-bench.beijing-aisi.ac.cn/ adresinden ulaşılabilir."
    }
  },
  {
    "id": "2602.14106v1",
    "title": "Anticipating Adversary Behavior in DevSecOps Scenarios through Large Language Models",
    "authors": [
      "Mario Marín Caballero",
      "Miguel Betancourt Alonso",
      "Daniel Díaz-López",
      "Angel Luis Perales Gómez",
      "Pantaleone Nespoli"
    ],
    "published_date": "2026-02-15",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.14106v1",
    "pdf_link": "https://arxiv.org/pdf/2602.14106v1",
    "content": {
      "en": "The most valuable asset of any cloud-based organization is data, which is increasingly exposed to sophisticated cyberattacks. Until recently, the implementation of security measures in DevOps environments was often considered optional by many government entities and critical national services operating in the cloud. This includes systems managing sensitive information, such as electoral processes or military operations, which have historically been valuable targets for cybercriminals. Resistance to security implementation is often driven by concerns over losing agility in software development, increasing the risk of accumulated vulnerabilities. Nowadays, patching software is no longer enough; adopting a proactive cyber defense strategy, supported by Artificial Intelligence (AI), is crucial to anticipating and mitigating threats. Thus, this work proposes integrating the Security Chaos Engineering (SCE) methodology with a new LLM-based flow to automate the creation of attack defense trees that represent adversary behavior and facilitate the construction of SCE experiments based on these graphical models, enabling teams to stay one step ahead of attackers and implement previously unconsidered defenses. Further detailed information about the experiment performed, along with the steps to replicate it, can be found in the following repository: https://github.com/mariomc14/devsecops-adversary-llm.git.",
      "tr": "**Makale Başlığı:** Large Language Models Aracılığıyla DevSecOps Senaryolarında Rakip Davranışını Öngörme\n\n**Özet:**\n\nHerhangi bir bulut tabanlı kuruluşun en değerli varlığı, giderek sofistike siber saldırılara maruz kalan verilerdir. Yakın zamana kadar, DevOps ortamlarında güvenlik önlemlerinin uygulanması, bulutta faaliyet gösteren birçok kamu kurumu ve kritik ulusal hizmet tarafından genellikle isteğe bağlı görülüyordu. Bu durum, seçim süreçleri veya askeri operasyonlar gibi hassas bilgileri yöneten ve tarihsel olarak siber suçlular için değerli hedefler haline gelen sistemleri de kapsamaktadır. Güvenlik uygulamalarına yönelik direnç, genellikle yazılım geliştirme çevikliğini kaybetme endişelerinden kaynaklanmakta ve bu da birikmiş zafiyet riskini artırmaktadır. Günümüzde yazılım yamalamak yeterli değildir; tehditleri öngörmek ve azaltmak için yapay zeka (AI) tarafından desteklenen proaktif bir siber savunma stratejisi benimsemek esastır. Bu nedenle, bu çalışma, rakip davranışını temsil eden saldırı savunma ağaçlarının otomatik olarak oluşturulmasını sağlamak ve bu grafik modeller temelinde SCE deneylerinin yapısını kolaylaştırmak için Güvenlik Kaos Mühendisliği (SCE) metodolojisini yeni bir LLM tabanlı akışla entegre etmeyi önermektedir. Bu entegrasyon, ekiplerin saldırganların bir adım önünde olmasını ve daha önce dikkate alınmamış savunmaların uygulanmasını sağlayacaktır. Gerçekleştirilen deney hakkında daha ayrıntılı bilgi ve deneyi tekrarlamak için gereken adımlar aşağıdaki repozitörde bulunabilir: https://github.com/mariomc14/devsecops-adversary-llm.git."
    }
  },
  {
    "id": "2602.14095v1",
    "title": "NEST: Nascent Encoded Steganographic Thoughts",
    "authors": [
      "Artem Karpov"
    ],
    "published_date": "2026-02-15",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2602.14095v1",
    "pdf_link": "https://arxiv.org/pdf/2602.14095v1",
    "content": {
      "en": "Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.",
      "tr": "**Makale Başlığı:** NEST: Nascent Encoded Steganographic Thoughts\n\n**Özet:**\n\nChain-of-thought (CoT) reasoning'in izlenmesi, büyük dil modeli (LLM) ajanları için temel bir güvenlik tekniğidir; ancak, modellerin akıl yürütmelerini gizlemeyi öğrenmeleri durumunda bu denetim tehlikeye girer. Risk değerlendirmesini ve dağıtım politikalarını bilgilendirmek için modellerin masum metinler içinde gizli akıl yürütme gizlediği steganographic CoT potansiyelini inceliyoruz. Geçmiş nesillerden mevcut en yeni modellere kadar 28 modeldeki steganographic yeteneklerin sınırlarını sistematik olarak değerlendiriyoruz. Dört veri kümesi üzerinden monitor evasion, refusal rates, encoding fidelity ve hidden task accuracy'yi ölçüyor, steganographic acrostics'leri plain reasoning ve filler-token baselines'larla karşılaştırıyoruz. Mevcut modellerin henüz karmaşık matematik ve aritmetik görevler için gizli akıl yürütmeyi sürdüremediğini buluyoruz. Ancak, basitleştirilmiş bir sayım deneyinde, Claude Opus 4.5, hidden task üzerinde %92 doğruluk elde ederek nascent capability göstermiştir. Özellikle, nadir durumlarda (<%1), GPT-5.2, steganographic talimatlara uyarken aynı anda bunları reddedebilir. Bulgularımız, steganographic risklerin sürekli değerlendirilmesi ihtiyacını vurgulamaktadır. Bu çalışma, misaligned scheming ve aldatıcı davranışı güçlendirebilecek gizli akıl yürütmeyi önleyici olarak tespit etmek ve önlemek için bir metodoloji sağlamaktadır."
    }
  },
  {
    "id": "2602.14030v1",
    "title": "MC$^2$Mark: Distortion-Free Multi-Bit Watermarking for Long Messages",
    "authors": [
      "Xuehao Cui",
      "Ruibo Chen",
      "Yihan Wu",
      "Heng Huang"
    ],
    "published_date": "2026-02-15",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2602.14030v1",
    "pdf_link": "https://arxiv.org/pdf/2602.14030v1",
    "content": {
      "en": "Large language models now produce text indistinguishable from human writing, which increases the need for reliable provenance tracing. Multi-bit watermarking can embed identifiers into generated text, but existing methods struggle to keep both text quality and watermark strength while carrying long messages. We propose MC$^2$Mark, a distortion-free multi-bit watermarking framework designed for reliable embedding and decoding of long messages. Our key technical idea is Multi-Channel Colored Reweighting, which encodes bits through structured token reweighting while keeping the token distribution unbiased, together with Multi-Layer Sequential Reweighting to strengthen the watermark signal and an evidence-accumulation detector for message recovery. Experiments show that MC$^2$Mark improves detectability and robustness over prior multi-bit watermarking methods while preserving generation quality, achieving near-perfect accuracy for short messages and exceeding the second-best method by nearly 30% for long messages.",
      "tr": "**Makale Başlığı:** MC$^2$Mark: Uzun Mesajlar İçin Bozulma Olmayan Çok Bitli Filigranlama\n\n**Özet:**\n\nBüyük dil modelleri artık insan yazısından ayırt edilemeyen metinler üretmektedir, bu da güvenilir kaynak takibi ihtiyacını artırmaktadır. Çok bitli filigranlama, üretilen metinlere tanımlayıcılar gömebilir, ancak mevcut yöntemler uzun mesajları taşırken hem metin kalitesini hem de filigran gücünü korumakta zorlanmaktadır. Güvenilir gömme ve uzun mesajların çözümlenmesi için tasarlanmış, bozulma olmayan bir çok bitli filigranlama framework'ü olan MC$^2$Mark'ı öneriyoruz. Anahtar teknik fikrimiz, token dağılımını tarafsız tutarken yapılandırılmış token yeniden ağırlıklandırması yoluyla bitleri kodlayan Multi-Channel Colored Reweighting'dir; bununla birlikte, filigran sinyalini güçlendirmek için Multi-Layer Sequential Reweighting ve mesaj kurtarma için bir evidence-accumulation detector kullanılmıştır. Deneyler, MC$^2$Mark'ın kısa mesajlar için neredeyse mükemmel doğruluk ve uzun mesajlar için ikinci en iyi yöntemin neredeyse %30 üzerine çıkarak, nesil kalitesini korurken önceki çok bitli filigranlama yöntemlerine göre tespit edilebilirliği ve sağlamlığı iyileştirdiğini göstermektedir."
    }
  },
  {
    "id": "2602.14012v1",
    "title": "From SFT to RL: Demystifying the Post-Training Pipeline for LLM-based Vulnerability Detection",
    "authors": [
      "Youpeng Li",
      "Fuxun Yu",
      "Xinda Wang"
    ],
    "published_date": "2026-02-15",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2602.14012v1",
    "pdf_link": "https://arxiv.org/pdf/2602.14012v1",
    "content": {
      "en": "The integration of LLMs into vulnerability detection (VD) has shifted the field toward interpretable and context-aware analysis. While post-training methods have shown promise in general coding tasks, their systematic application to VD remains underexplored. In this paper, we present the first comprehensive investigation into the post-training pipeline for LLM-based VD, spanning from cold-start SFT to off-policy preference optimization and on-policy RL, uncovering how data curation, stage interactions, reward mechanisms, and evaluation protocols collectively dictate the efficacy of model training and assessment. Our study identifies practical guidelines and insights: (1) SFT based on rejection sampling greatly outperforms rationalization-based supervision, which can introduce hallucinations due to ground-truth leakage. (2) While increased SFT epochs constantly benefit preference optimization, excessive SFT inhibits self-exploration during RL, ultimately limiting performance gains. (3) Coarse-grained reward signals often mislead RL, whereas fine-grained root-cause judgments ensure reliable credit assignment. Specification-based rewards offer further benefits but incur significant effort in specification generation. (4) Although filtering extremely hard-to-detect vulnerability samples improves RL training efficiency, the cost of performance loss should be considered in practical applications. (5) Models trained under GRPO significantly outperform those using SFT and preference optimization (i.e., DPO and ORPO), as well as a series of zero-shot SOTA LLMs, underscoring the significant potential of on-policy RL for LLM-based VD. (6) In contrast to binary matching that tends to overestimate performance, LLM-as-a-Judge based on root-cause analysis provides a more robust evaluation protocol, although its accuracy varies across judge models with different levels of security expertise.",
      "tr": "Makale Başlığı: SFT'den RL'ye: LLM Tabanlı Güvenlik Açığı Tespiti için Eğitim Sonrası Sürecinin Anlaşılması\n\nÖzet:\nLLM'lerin güvenlik açığı tespitine (VD) entegrasyonu, alanı yorumlanabilir ve bağlama duyarlı analize doğru yönlendirmiştir. Eğitim sonrası yöntemler genel kodlama görevlerinde umut vaat etse de, VD'ye sistematik uygulamaları yeterince araştırılmamıştır. Bu makalede, LLM tabanlı VD için eğitim sonrası süreci üzerine ilk kapsamlı incelemeyi sunuyoruz; bu inceleme, cold-start SFT'den off-policy preference optimization ve on-policy RL'ye kadar uzanmakta ve veri kürasyonu, aşama etkileşimleri, reward mekanizmaları ve değerlendirme protokollerinin model eğitimi ve değerlendirmesinin etkinliğini topluca nasıl belirlediğini ortaya koymaktadır. Çalışmamız pratik kılavuzlar ve içgörüler tanımlamaktadır: (1) Rejection sampling tabanlı SFT, ground-truth sızması nedeniyle halüsinasyonlara yol açabilen rasyonalizasyon tabanlı süpervizyondan önemli ölçüde daha iyi performans gösterir. (2) Artan SFT epoch'ları preference optimization'ı sürekli olarak faydalarken, aşırı SFT RL sırasında self-exploration'ı engeller ve nihayetinde performans artışlarını sınırlar. (3) Coarse-grained reward sinyalleri genellikle RL'yi yanıltır; oysa fine-grained root-cause judgments, güvenilir credit assignment'ı sağlar. Specification-based rewards daha fazla fayda sunar ancak specification generation'da önemli bir çaba gerektirir. (4) Tespit edilmesi son derece zor güvenlik açığı örneklerini filtrelemek RL eğitim verimliliğini artırsa da, performans kaybının maliyeti pratik uygulamalarda göz önünde bulundurulmalıdır. (5) GRPO altında eğitilen modeller, SFT ve preference optimization (yani DPO ve ORPO) kullanan modellerin yanı sıra bir dizi zero-shot SOTA LLM'den önemli ölçüde daha iyi performans göstermektedir; bu da LLM tabanlı VD için on-policy RL'nin önemli potansiyelini vurgulamaktadır. (6) Performansı aşırı tahmin etme eğiliminde olan binary matching'in aksine, root-cause analysis'e dayalı LLM-as-a-Judge, daha sağlam bir değerlendirme protokolü sunar, ancak doğruluğu farklı düzeylerde güvenlik uzmanlığına sahip judge modelleri arasında değişiklik gösterir."
    }
  },
  {
    "id": "2602.13576v1",
    "title": "Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges",
    "authors": [
      "Ruomeng Ding",
      "Yifei Pang",
      "He Sun",
      "Yizhong Wang",
      "Zhiwei Steven Wu"
    ],
    "published_date": "2026-02-14",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2602.13576v1",
    "pdf_link": "https://arxiv.org/pdf/2602.13576v1",
    "content": {
      "en": "Evaluation and alignment pipelines for large language models increasingly rely on LLM-based judges, whose behavior is guided by natural-language rubrics and validated on benchmarks. We identify a previously under-recognized vulnerability in this workflow, which we term Rubric-Induced Preference Drift (RIPD). Even when rubric edits pass benchmark validation, they can still produce systematic and directional shifts in a judge's preferences on target domains. Because rubrics serve as a high-level decision interface, such drift can emerge from seemingly natural, criterion-preserving edits and remain difficult to detect through aggregate benchmark metrics or limited spot-checking. We further show this vulnerability can be exploited through rubric-based preference attacks, in which benchmark-compliant rubric edits steer judgments away from a fixed human or trusted reference on target domains, systematically inducing RIPD and reducing target-domain accuracy up to 9.5% (helpfulness) and 27.9% (harmlessness). When these judgments are used to generate preference labels for downstream post-training, the induced bias propagates through alignment pipelines and becomes internalized in trained policies. This leads to persistent and systematic drift in model behavior. Overall, our findings highlight evaluation rubrics as a sensitive and manipulable control interface, revealing a system-level alignment risk that extends beyond evaluator reliability alone. The code is available at: https://github.com/ZDCSlab/Rubrics-as-an-Attack-Surface. Warning: Certain sections may contain potentially harmful content that may not be appropriate for all readers.",
      "tr": "İşte akademik makalenin başlığı ve özetinin istenen şekilde Türkçeye çevrilmiş hali:\n\n**Makale Başlığı:** Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges\n\n**Özet:**\n\nBüyük dil modelleri için değerlendirme ve alignment pipeline'ları, giderek artan bir şekilde LLM tabanlı yargıçlara dayanmaktadır. Bu yargıçların davranışları, natural-language rubrics tarafından yönlendirilir ve benchmarklar üzerinde doğrulanır. Bu iş akışında daha önce yeterince fark edilmemiş bir güvenlik açığını tespit ediyoruz ve bunu Rubric-Induced Preference Drift (RIPD) olarak adlandırıyoruz. Rubric düzenlemeleri benchmark doğrulamayı geçse bile, hedef alanlarda bir yargıcın tercihlerinde sistematik ve yönlü kaymalara neden olabilirler. Rubrics yüksek seviyeli bir karar arayüzü görevi gördüğünden, bu tür bir kayma görünüşte doğal, criterion-preserving düzenlemelerden ortaya çıkabilir ve aggregate benchmark metrikleri veya sınırlı spot-checking yoluyla tespit edilmesi zor kalabilir. Ayrıca, bu güvenlik açığının rubric tabanlı preference attacks yoluyla istismar edilebileceğini gösteriyoruz. Bu attacks'larda, benchmark uyumlu rubric düzenlemeleri, hedef alanlarda sabit bir insan veya güvenilir bir referanstan uzaklaşan yargıları yönlendirerek, sistematik olarak RIPD'yi tetikler ve hedef alan doğruluğunu %9.5'e kadar (helpfulness) ve %27.9'a kadar (harmlessness) düşürür. Bu yargılar, downstream post-training için preference labels oluşturmak amacıyla kullanıldığında, tetiklenen bias alignment pipeline'ları aracılığıyla yayılır ve eğitilmiş politikalara internalization olur. Bu durum, model davranışında persistent ve sistematik bir kaymaya yol açar. Genel olarak, bulgularımız değerlendirme rubrics'lerini hassas ve manipüle edilebilir bir kontrol arayüzü olarak vurgulamakta, sadece evaluator reliability'yi aşan sistem seviyesinde bir alignment riskini ortaya koymaktadır. Kod şuradan edinilebilir: https://github.com/ZDCSlab/Rubrics-as-an-Attack-Surface. Uyarı: Belirli bölümler, tüm okuyucular için uygun olmayabilecek potansiyel olarak zararlı içerik barındırabilir."
    }
  },
  {
    "id": "2602.13562v1",
    "title": "Mitigating the Safety-utility Trade-off in LLM Alignment via Adaptive Safe Context Learning",
    "authors": [
      "Yanbo Wang",
      "Minzheng Wang",
      "Jian Liang",
      "Lu Wang",
      "Yongcan Yu"
    ],
    "published_date": "2026-02-14",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2602.13562v1",
    "pdf_link": "https://arxiv.org/pdf/2602.13562v1",
    "content": {
      "en": "While reasoning models have achieved remarkable success in complex reasoning tasks, their increasing power necessitates stringent safety measures. For safety alignment, the core challenge lies in the inherent trade-off between safety and utility. However, prevailing alignment strategies typically construct CoT training data with explicit safety rules via context distillation. This approach inadvertently limits reasoning capabilities by creating a rigid association between rule memorization and refusal. To mitigate the safety-utility trade-off, we propose the Adaptive Safe Context Learning (ASCL) framework to improve the reasoning given proper context. ASCL formulates safety alignment as a multi-turn tool-use process, empowering the model to autonomously decide when to consult safety rules and how to generate the ongoing reasoning. Furthermore, to counteract the preference for rule consultation during RL, we introduce Inverse Frequency Policy Optimization (IFPO) to rebalance advantage estimates. By decoupling rule retrieval and subsequent reasoning, our method achieves higher overall performance compared to baselines.",
      "tr": "**Makale Başlığı:** Mitigating the Safety-utility Trade-off in LLM Alignment via Adaptive Safe Context Learning\n\n**Özet:**\n\nBu makalede, LLM Alignment'da Adaptive Safe Context Learning ile Safety-Utility Trade-off'unun Azaltılması ele alınmaktadır.\n\nReasoning modelleri karmaşık muhakeme görevlerinde dikkate değer başarılar elde etmiş olsa da, artan güçleri titiz güvenlik önlemlerini gerektirmektedir. Safety alignment için temel zorluk, safety ve utility arasındaki doğal trade-off'ta yatmaktadır. Bununla birlikte, yaygın alignment stratejileri tipik olarak context distillation aracılığıyla explicit safety rules ile CoT training data'sı oluşturur. Bu yaklaşım, rule memorization ve refusal arasında katı bir ilişki yaratarak istemeden reasoning yeteneklerini sınırlar. Safety-utility trade-off'unu azaltmak için, proper context verildiğinde reasoning'i iyileştirmek üzere Adaptive Safe Context Learning (ASCL) framework'ünü öneriyoruz. ASCL, safety alignment'ı multi-turn tool-use process'i olarak formüle eder, modelin ne zaman safety rules'a danışacağına ve ongoing reasoning'i nasıl üreteceğine otonom olarak karar verme yeteneğini güçlendirir. Dahası, RL sırasında rule consultation tercihine karşı koymak için, advantage estimates'leri yeniden dengelemek üzere Inverse Frequency Policy Optimization (IFPO)'yu tanıtıyoruz. Rule retrieval ve subsequent reasoning'i ayırarak, yöntemimiz baselines'a kıyasla daha yüksek genel performans elde etmektedir."
    }
  },
  {
    "id": "2602.13547v1",
    "title": "AISA: Awakening Intrinsic Safety Awareness in Large Language Models against Jailbreak Attacks",
    "authors": [
      "Weiming Song",
      "Xuan Xie",
      "Ruiping Yin"
    ],
    "published_date": "2026-02-14",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.13547v1",
    "pdf_link": "https://arxiv.org/pdf/2602.13547v1",
    "content": {
      "en": "Large language models (LLMs) remain vulnerable to jailbreak prompts that elicit harmful or policy-violating outputs, while many existing defenses rely on expensive fine-tuning, intrusive prompt rewriting, or external guardrails that add latency and can degrade helpfulness. We present AISA, a lightweight, single-pass defense that activates safety behaviors already latent inside the model rather than treating safety as an add-on. AISA first localizes intrinsic safety awareness via spatiotemporal analysis and shows that intent-discriminative signals are broadly encoded, with especially strong separability appearing in the scaled dot-product outputs of specific attention heads near the final structural tokens before generation. Using a compact set of automatically selected heads, AISA extracts an interpretable prompt-risk score with minimal overhead, achieving detector-level performance competitive with strong proprietary baselines on small (7B) models. AISA then performs logits-level steering: it modulates the decoding distribution in proportion to the inferred risk, ranging from normal generation for benign prompts to calibrated refusal for high-risk requests -- without changing model parameters, adding auxiliary modules, or requiring multi-pass inference. Extensive experiments spanning 13 datasets, 12 LLMs, and 14 baselines demonstrate that AISA improves robustness and transfer while preserving utility and reducing false refusals, enabling safer deployment even for weakly aligned or intentionally risky model variants.",
      "tr": "Elbette, akademik makale başlığını ve özetini belirttiğiniz şekilde Türkçeye çevirdim:\n\n**Makale Başlığı:** AISA: Jailbreak Saldırılarına Karşı Büyük Dil Modellerinde İçsel Güvenlik Farkındalığını Uyandırmak\n\n**Özet:**\nBüyük Dil Modelleri (LLM'ler), zararlı veya politika ihlali içeren çıktılara neden olan jailbreak promptlarına karşı hala savunmasızdır. Mevcut savunmaların çoğu pahalı fine-tuning, müdahaleci prompt yeniden yazma veya gecikme ekleyen ve yardımseverliği azaltabilen harici guardrail'lara dayanmaktadır. Biz, güvenliği bir eklenti olarak ele almak yerine, modelin içinde zaten gizli olan güvenlik davranışlarını etkinleştiren hafif, tek geçişli bir savunma olan AISA'yı sunuyoruz. AISA, öncelikle spatiotemporal analysis yoluyla intrinsic safety awareness'ı yerelleştirir ve niyet ayırt edici sinyallerin geniş çapta kodlandığını, özellikle üretmeden önceki son yapısal token'lara yakın belirli attention head'lerin scaled dot-product çıktılarıda güçlü bir ayrışma göründüğünü gösterir. Otomatik olarak seçilmiş kompakt bir head kümesi kullanarak, AISA minimal ek yük ile yorumlanabilir bir prompt-risk score çıkarır ve küçük (7B) modellerde güçlü özel baselines ile rekabetçi detector-level performans elde eder. AISA daha sonra logits-level steering gerçekleştirir: çıkarılan riskle orantılı olarak decoding distribution'ı modüle eder, zararsız prompt'lar için normal üretimden yüksek riskli istekler için kalibre edilmiş refuzal'a kadar değişir - model parametrelerini değiştirmeden, yardımcı modüller eklemeden veya multi-pass inference gerektirmeden. 13 veri seti, 12 LLM ve 14 baseline kapsayan kapsamlı deneyler, AISA'nın daha zayıf hizalanmış veya kasıtlı olarak riskli model varyantları için bile daha güvenli dağıtım sağlayarak, yardımcı programı korurken ve yanlış refuzal'ları azaltırken dayanıklılığı ve transferi iyileştirdiğini göstermektedir."
    }
  },
  {
    "id": "2602.13427v1",
    "title": "Backdooring Bias in Large Language Models",
    "authors": [
      "Anudeep Das",
      "Prach Chantasantitam",
      "Gurjot Singh",
      "Lipeng He",
      "Mariia Ponomarenko"
    ],
    "published_date": "2026-02-13",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2602.13427v1",
    "pdf_link": "https://arxiv.org/pdf/2602.13427v1",
    "content": {
      "en": "Large language models (LLMs) are increasingly deployed in settings where inducing a bias toward a certain topic can have significant consequences, and backdoor attacks can be used to produce such models. Prior work on backdoor attacks has largely focused on a black-box threat model, with an adversary targeting the model builder's LLM. However, in the bias manipulation setting, the model builder themselves could be the adversary, warranting a white-box threat model where the attacker's ability to poison, and manipulate the poisoned data is substantially increased. Furthermore, despite growing research in semantically-triggered backdoors, most studies have limited themselves to syntactically-triggered attacks. Motivated by these limitations, we conduct an analysis consisting of over 1000 evaluations using higher poisoning ratios and greater data augmentation to gain a better understanding of the potential of syntactically- and semantically-triggered backdoor attacks in a white-box setting. In addition, we study whether two representative defense paradigms, model-intrinsic and model-extrinsic backdoor removal, are able to mitigate these attacks. Our analysis reveals numerous new findings. We discover that while both syntactically- and semantically-triggered attacks can effectively induce the target behaviour, and largely preserve utility, semantically-triggered attacks are generally more effective in inducing negative biases, while both backdoor types struggle with causing positive biases. Furthermore, while both defense types are able to mitigate these backdoors, they either result in a substantial drop in utility, or require high computational overhead.",
      "tr": "İşte akademik makalenin başlığı ve özetinin istenen şekilde çevrilmiş hali:\n\n**Makale Başlığı:** Large Language Models'da Biasın Gizlice Yerleştirilmesi (Backdooring Bias in Large Language Models)\n\n**Özet:**\n\nLarge language models (LLMs), belirli bir konuya yönelik bir yanlılığı (bias) teşvik etmenin önemli sonuçlara yol açabileceği ortamlarda giderek daha fazla kullanılmaktadır ve bu tür modelleri üretmek için backdoor attacks kullanılabilir. Backdoor attacks üzerine yapılan önceki çalışmalar büyük ölçüde black-box tehdit modeline odaklanmış ve bir saldırganın model oluşturucunun LLM'ini hedeflemesine dayanmıştır. Ancak, bias manipülasyonu bağlamında, model oluşturucunun kendisi saldırgan olabilir, bu da saldırganın zehirleme (poisoning) ve zehirlenen veriyi manipüle etme yeteneğinin önemli ölçüde arttığı white-box tehdit modelini gerektirir. Dahası, anlamsal olarak tetiklenen (semantically-triggered) backdoors üzerine artan araştırmalara rağmen, çoğu çalışma kendini sözdizimsel olarak tetiklenen (syntactically-triggered) saldırılarla sınırlamıştır. Bu kısıtlamalardan ilham alarak, daha yüksek zehirleme oranları (poisoning ratios) ve daha fazla veri artırımı (data augmentation) kullanarak 1000'den fazla değerlendirme içeren bir analiz gerçekleştiriyoruz. Bu analiz, white-box bir ortamda sözdizimsel ve anlamsal olarak tetiklenen backdoor attacks'ların potansiyelini daha iyi anlamamızı sağlamaktadır. Ek olarak, iki temsili savunma paradigmasının, yani model-intrinsik ve model-extrinsik backdoor temizleme (backdoor removal) yöntemlerinin bu saldırıları azaltıp azaltamadığını inceliyoruz. Analizimiz çok sayıda yeni bulguyu ortaya koymaktadır. Hem sözdizimsel hem de anlamsal olarak tetiklenen saldırıların hedef davranışı etkili bir şekilde tetikleyebildiğini ve büyük ölçüde faydayı (utility) koruyabildiğini keşfettik. Bununla birlikte, anlamsal olarak tetiklenen saldırılar genel olarak negatif yanlılıklar (negative biases) oluşturmada daha etkilidir, oysa her iki backdoor türü de pozitif yanlılıklar (positive biases) oluşturmada zorlanmaktadır. Dahası, her iki savunma türü de bu backdoors'ları azaltabilse de, ya faydada önemli bir düşüşe yol açmakta ya da yüksek hesaplama maliyeti (computational overhead) gerektirmektedir."
    }
  },
  {
    "id": "2602.13379v1",
    "title": "Unsafer in Many Turns: Benchmarking and Defending Multi-Turn Safety Risks in Tool-Using Agents",
    "authors": [
      "Xu Li",
      "Simon Yu",
      "Minzhou Pan",
      "Yiyou Sun",
      "Bo Li"
    ],
    "published_date": "2026-02-13",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2602.13379v1",
    "pdf_link": "https://arxiv.org/pdf/2602.13379v1",
    "content": {
      "en": "LLM-based agents are becoming increasingly capable, yet their safety lags behind. This creates a gap between what agents can do and should do. This gap widens as agents engage in multi-turn interactions and employ diverse tools, introducing new risks overlooked by existing benchmarks. To systematically scale safety testing into multi-turn, tool-realistic settings, we propose a principled taxonomy that transforms single-turn harmful tasks into multi-turn attack sequences. Using this taxonomy, we construct MT-AgentRisk (Multi-Turn Agent Risk Benchmark), the first benchmark to evaluate multi-turn tool-using agent safety. Our experiments reveal substantial safety degradation: the Attack Success Rate (ASR) increases by 16% on average across open and closed models in multi-turn settings. To close this gap, we propose ToolShield, a training-free, tool-agnostic, self-exploration defense: when encountering a new tool, the agent autonomously generates test cases, executes them to observe downstream effects, and distills safety experiences for deployment. Experiments show that ToolShield effectively reduces ASR by 30% on average in multi-turn interactions. Our code is available at https://github.com/CHATS-lab/ToolShield.",
      "tr": "**Makale Başlığı:** Çok Turlu Güvenlik Risklerinde Daha Güvensiz: Araç Kullanan Ajanlarda Çok Turlu Güvenlik Risklerinin Kıyaslanması ve Savunulması\n\n**Özet:**\n\nLLM tabanlı ajanlar giderek daha yetenekli hale gelmekle birlikte, güvenlikleri bu gelişimin gerisinde kalmaktadır. Bu durum, ajanların yapabilecekleri ile yapmaları gerekenler arasında bir boşluk yaratmaktadır. Ajanlar çok turlu etkileşimlere girdiğinde ve çeşitli araçlar kullandığında bu boşluk daha da genişler, mevcut kıyaslamalar tarafından gözden kaçırılan yeni riskler ortaya çıkar. Güvenlik testlerini sistematik olarak çok turlu, araç gerçekli ortamlara ölçeklendirmek için, tek turlu zararlı görevleri çok turlu saldırı dizilerine dönüştüren prensipli bir taksonomi öneriyoruz. Bu taksonomiyi kullanarak, çok turlu araç kullanan ajan güvenliğini değerlendiren ilk kıyaslama olan MT-AgentRisk'i (Multi-Turn Agent Risk Benchmark) inşa ediyoruz. Deneylerimiz önemli güvenlik bozulmaları ortaya koymaktadır: açık ve kapalı modellerde çok turlu ortamlarda ortalama olarak Saldırı Başarı Oranı (ASR) %16 oranında artmaktadır. Bu boşluğu kapatmak için, eğitim gerektirmeyen, araçtan bağımsız, kendi kendine keşfeden bir savunma olan ToolShield'ı öneriyoruz: yeni bir araçla karşılaştığında, ajan test senaryolarını otonom olarak üretir, aşağı akış etkilerini gözlemlemek için bunları yürütür ve dağıtım için güvenlik deneyimlerini damıtarak özetler. Deneyler, ToolShield'ın çok turlu etkileşimlerde ortalama olarak ASR'yi %30 oranında etkili bir şekilde azalttığını göstermektedir. Kodumuz https://github.com/CHATS-lab/ToolShield adresinde mevcuttur."
    }
  }
]