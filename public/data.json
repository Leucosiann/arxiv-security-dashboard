[
  {
    "id": "2512.16874v1",
    "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking",
    "authors": [
      "Tomáš Souček",
      "Pierre Fernandez",
      "Hady Elsahar",
      "Sylvestre-Alvise Rebuffi",
      "Valeriu Lacatusu"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16874v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16874v1",
    "content": {
      "en": "Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.",
      "tr": "Makale Başlığı: Pixel Seal: Görünmez Görüntü ve Video Filigranlama İçin Yalnızca Çekişmeli Eğitim\n\nÖzet:\nGörünmez filigranlama, dijital içeriğin kökenini izlemek için esastır. Bununla birlikte, en gelişmiş modellerin eğitimi son derece zorlu olmaya devam etmektedir; mevcut yaklaşımlar genellikle sağlamlığı gerçek algılanamazlık dengesinde sürdürmekte zorlanmaktadır. Bu çalışma, görüntü ve video filigranlamada yeni bir state-of-the-art oluşturan Pixel Seal'ı tanıtmaktadır. Öncelikle, mevcut yöntemlerin üç temel sorununu tespit ediyoruz: (i) insan algısını taklit etmekte başarısız olan ve görünür filigran artefaktlarına yol açan MSE ve LPIPS gibi proxy algısal kayıplara güvenilmesi; (ii) çakışan hedeflerden kaynaklanan ve kapsamlı hiperparametre ayarlaması gerektiren optimizasyon kararsızlığı; ve (iii) modellerin yüksek çözünürlüklü görüntülere ve videolara ölçeklendirilmesi sırasında filigranların azalan sağlamlığı ve algılanamazlığı. Bu sorunların üstesinden gelmek için, öncelikle güvenilmez piksel bazında algılanamazlık kayıplarını ortadan kaldıran bir adversarial-only training paradigmı öneriyoruz. İkinci olarak, sağlamlık ve algılanamazlığı ayrıştırarak yakınsamayı stabilize eden üç aşamalı bir eğitim programı tanıtıyoruz. Üçüncü olarak, JND-based attenuation ve upscaling artefaktlarını ortadan kaldırmak için training-time inference simulation kullanarak yüksek çözünürlüklü adaptasyon yoluyla resolution gap sorununu ele alıyoruz. Pixel Seal'ın farklı görüntü türleri ve çok çeşitli dönüşümler karşısındaki sağlamlığını ve algılanamazlığını kapsamlı bir şekilde değerlendiriyoruz ve state-of-the-art'a göre belirgin iyileştirmeler gösteriyoruz. Son olarak, modelin temporal watermark pooling aracılığıyla videoya verimli bir şekilde adapte olduğunu göstererek Pixel Seal'ı gerçek dünya görüntü ve video ortamlarında güvenilir köken takibi için pratik ve ölçeklenebilir bir çözüm olarak konumlandırıyoruz."
    }
  },
  {
    "id": "2512.16851v1",
    "title": "PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy",
    "authors": [
      "Ripan Kumar Kundu",
      "Istiak Ahmed",
      "Khaza Anuarul Hoque"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "link": "http://arxiv.org/abs/2512.16851v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16851v1",
    "content": {
      "en": "The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.",
      "tr": "İşte makale başlığı ve özetinin çevirisi:\n\n**Makale Başlığı:** PrivateXR: Açıklanabilir Yapay Zeka Güdümlü Diferansiyel Gizlilik Yoluyla Genişletilmiş Gerçeklikte Gizlilik Saldırılarına Karşı Savunma\n\n**Özet:**\nYapay zeka ve XR teknolojilerinin (AI XR) yakınsaması, birçok alanda yenilikçi uygulamalar vaat etmektedir. Ancak, bu sistemlerde kullanılan hassas nitelikteki veriler (örneğin, eye-tracking), üye çıkarım saldırıları (MIA) ve yeniden kimliklendirme (RDA) yoluyla kötü niyetli kişilerin bu verileri ve modelleri yüksek başarı oranıyla istismar ederek kişisel bilgileri çıkarmasına ve sızdırmasına neden olabileceğinden önemli gizlilik endişeleri doğurmaktadır. Araştırmacılar, diferansiyel gizlilik (DP) dahil olmak üzere bu tür gizlilik saldırılarını hafifletmek için çeşitli teknikler önermişlerdir. Bununla birlikte, AI XR veri kümeleri sıklıkla çok sayıda özellik içermekte ve DP'nin homojen bir şekilde uygulanması, daha az alakalı özelliklere gereksiz gürültü ekleyebilir, model doğruluğunu düşürebilir ve çıkarım süresini artırarak gerçek zamanlı XR konuşlandırmalarını sınırlayabilir. Bu durumdan hareketle, açıklanabilir yapay zeka (XAI) ve DP özellikli gizlilik koruyucu mekanizmaları birleştiren, gizlilik saldırılarına karşı savunma sağlayan yeni bir çerçeve öneriyoruz. Spesifik olarak, AI XR modellerindeki en etkili özellikleri belirlemek ve çıkarım sırasında bu özelliklere seçici olarak DP uygulamak için post-hoc açıklamalarından yararlanıyoruz. XAI güdümlü DP yaklaşımımızı, üç adet son teknoloji AI XR modeli ve cybersickness, duygu ve aktivite sınıflandırması olmak üzere üç veri kümesi üzerinde değerlendiriyoruz. Sonuçlarımız, önerilen yöntemin cybersickness görevlerinde MIA ve RDA başarı oranlarını sırasıyla %43 ve %39'a kadar azalttığını, Transformer modellerini kullanarak %97'ye varan doğrulukla model faydasını koruduğunu göstermektedir. Ayrıca, geleneksel DP yaklaşımlarına kıyasla çıkarım süresini yaklaşık 2 katına kadar iyileştirmektedir. Uygulanabilirliği göstermek için, XAI güdümlü DP AI XR modellerini bir HTC VIVE Pro başlığına konuşlandırıyoruz ve kullanıcıların gizlilik seviyelerini (örneğin, düşük, orta, yüksek) ayarlamalarına ve gerçek zamanlı görev tahminleri almalarına olanak tanıyan, kullanıcı gizliliğini XR oyun deneyimi sırasında koruyan PrivateXR adında bir kullanıcı arayüzü (UI) geliştiriyoruz."
    }
  },
  {
    "id": "2512.16778v1",
    "title": "Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences",
    "authors": [
      "Theshani Nuradha",
      "Ian George",
      "Christoph Hirche"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "quant-ph",
      "cs.CR",
      "cs.IT",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16778v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16778v1",
    "content": {
      "en": "Data-processing is a desired property of classical and quantum divergences and information measures. In information theory, the contraction coefficient measures how much the distinguishability of quantum states decreases when they are transmitted through a quantum channel, establishing linear strong data-processing inequalities (SDPI). However, these linear SDPI are not always tight and can be improved in most of the cases. In this work, we establish non-linear SDPI for quantum hockey-stick divergence for noisy channels that satisfy a certain noise criterion. We also note that our results improve upon existing linear SDPI for quantum hockey-stick divergences and also non-linear SDPI for classical hockey-stick divergence. We define $F_γ$ curves generalizing Dobrushin curves for the quantum setting while characterizing SDPI for the sequential composition of heterogeneous channels. In addition, we derive reverse-Pinsker type inequalities for $f$-divergences with additional constraints on hockey-stick divergences. We show that these non-linear SDPI can establish tighter finite mixing times that cannot be achieved through linear SDPI. Furthermore, we find applications of these in establishing stronger privacy guarantees for the composition of sequential private quantum channels when privacy is quantified by quantum local differential privacy.",
      "tr": "**Makale Başlığı:** Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences\n\n**Özet:**\n\nVeri işleme, klasik ve kuantum ıraksaklıkları ve bilgi ölçümleri için arzu edilen bir özelliktir. Bilgi teorisinde, contraction coefficient kuantum durumlarının ayırt edilebilirliğinin bir kuantum kanalı aracılığıyla iletildiğinde ne kadar azaldığını ölçer ve linear strong data-processing inequalities (SDPI) oluşturur. Ancak, bu linear SDPI her zaman sıkı değildir ve çoğu durumda iyileştirilebilir. Bu çalışmada, belirli bir gürültü kriterini karşılayan noisy kanallar için quantum hockey-stick divergence için non-linear SDPI kuruyoruz. Ayrıca, sonuçlarımızın mevcut linear SDPI for quantum hockey-stick divergences ve classical hockey-stick divergence için non-linear SDPI üzerine iyileştirmeler getirdiğini de belirtiyoruz. Dobrushin curves'i kuantum ortamı için genelleştiren $F_γ$ eğrilerini tanımlarken, heterogeneous kanalların ardışık bileşimi için SDPI'yı karakterize ediyoruz. Ek olarak, hockey-stick divergences üzerine ek kısıtlamalarla $f$-divergences için reverse-Pinsker type inequalities türetiyoruz. Bu non-linear SDPI'ların, linear SDPI ile başarılamayacak daha sıkı finite mixing times kurabileceğini gösteriyoruz. Ayrıca, bu sonuçların, privacy quantum local differential privacy ile niceliklendirildiğinde, ardışık private quantum channels'in bileşimi için daha güçlü privacy guarantees kurma hususunda uygulamalarını buluyoruz."
    }
  },
  {
    "id": "2512.16717v1",
    "title": "Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering",
    "authors": [
      "Rudra Dubey",
      "Arpit Mani Tripathi",
      "Archit Srivastava",
      "Sarvpal Singh"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.16717v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16717v1",
    "content": {
      "en": "In actuality, phishing attacks remain one of the most prevalent cybersecurity risks in existence today, with malevolent actors constantly changing their strategies to successfully trick users. This paper presents an AI model for a phishing detection system that uses an ensemble approach to combine character-level Convolutional Neural Networks (CNN) and LightGBM with engineered features. Our system uses a character-level CNN to extract sequential features after extracting 36 lexical, structural, and domain-based features from the URLs. On a test dataset of 19,873 URLs, the ensemble model achieves an accuracy of 99.819 percent, precision of 100 percent, recall of 99.635 percent, and ROC-AUC of 99.947 percent. Through a FastAPI-based service with an intuitive user interface, the suggested system has been utilised to offer real-time detection. In contrast, the results demonstrate that the suggested solution performs better than individual models; LightGBM contributes 40 percent and character-CNN contributes 60 percent to the final prediction. The suggested method maintains extremely low false positive rates while doing a good job of identifying contemporary phishing techniques. Index Terms - Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis",
      "tr": "Aşağıda akademik makalenin başlığı ve özetinin Türkçe çevirisi bulunmaktadır:\n\n**Makale Başlığı:** Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering\n\n**Özet:**\n\nGünümüzde oltalama (phishing) saldırıları varlığını sürdüren en yaygın siber güvenlik risklerinden biri olup, kötü niyetli aktörler kullanıcıları başarıyla kandırmak için stratejilerini sürekli olarak değiştirmektedir. Bu çalışma, oltalama tespiti sistemi için bir AI modelini sunmaktadır. Model, karakter-seviyesi Convolutional Neural Networks (CNN) ve LightGBM'i mühendislik ürünü özelliklerle birleştiren bir ensemble yaklaşımı kullanmaktadır. Sistemimiz, URL'lerden 36 adet leksikal, yapısal ve domain tabanlı özellik çıkarıldıktan sonra karakter-seviyesi CNN kullanarak sıralı özellikleri çıkarmaktadır. 19.873 URL'den oluşan bir test veri kümesinde, ensemble model %99.819 doğruluk (accuracy), %100 kesinlik (precision), %99.635 geri çağırma (recall) ve %99.947 ROC-AUC elde etmiştir. FastAPI tabanlı bir servis ve sezgisel bir kullanıcı arayüzü aracılığıyla önerilen sistem, gerçek zamanlı tespit sağlamak için kullanılmıştır. Buna karşılık, elde edilen sonuçlar önerilen çözümün bireysel modellerden daha iyi performans gösterdiğini ortaya koymaktadır; nihai tahmine LightGBM %40 ve character-CNN %60 katkıda bulunmaktadır. Önerilen yöntem, çağdaş oltalama tekniklerini iyi bir şekilde tanımlarken son derece düşük yanlış pozitif oranlarını korumaktadır. Index Terms - Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis"
    }
  },
  {
    "id": "2512.16658v1",
    "title": "Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking",
    "authors": [
      "Sangeeth B",
      "Serena Nicolazzo",
      "Deepa K.",
      "Vinod P"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.16658v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16658v1",
    "content": {
      "en": "The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.",
      "tr": "**Makale Başlığı:** Kaos Tabanlı White-Box Watermarking ile Derin Sinir Ağı Fikri Mülkiyetinin Korunması\n\n**Özet:**\n\nDerin sinir ağlarının (DNN) çeşitli alanlarda hızla yaygınlaşması, fikri mülkiyet (IP) korunması ve model kötüye kullanımıyla ilgili endişeleri artırmıştır. Eğitilmiş DNN'ler, genellikle önemli yatırımlarla geliştirilen değerli varlıkları temsil eder. Bununla birlikte, modellerin kopyalanmasının, yeniden dağıtılmasının veya yeniden amaçlandırılmasının kolaylığı, model sahipliğini iddia etmek ve doğrulamak için etkili mekanizmalara duyulan acil ihtiyacı vurgulamaktadır. Bu çalışmada, kaos dizileri kullanarak bir DNN'nin iç parametrelerine sahiplik bilgisini yerleştiren verimli ve dayanıklı bir white-box watermarking çerçevesi önerilmektedir. Watermark, iyi bilinen bir kaos fonksiyonu olan ve başlatma parametrelerine duyarlı bir dizi üreten bir lojistik harita (logistic map) kullanılarak oluşturulur. Bu dizi, modelin yapısal değişiklikleri veya öngörüsel performansta bozulma gerektirmeden, seçilen bir ara katmanın ağırlıklarına enjekte edilir. Sahipliği doğrulamak için, çıkarılan ve yeniden oluşturulan diziler arasındaki benzerliği optimize ederek orijinal kaos parametrelerini geri kazanan bir genetik algoritmaya dayalı doğrulama süreci sunulmaktadır. Önerilen yaklaşımın etkinliği, MNIST ve CIFAR-10 veri kümelerini kullanan görüntü sınıflandırma görevleri üzerindeki kapsamlı deneylerle gösterilmektedir. Sonuçlar, yerleştirilen watermark'ın ince ayardan sonra tespit edilebilir kaldığını ve model doğruluğunda ihmal edilebilir bir kayıp olduğunu göstermektedir. Watermark'ın sayısal kurtarılmasına ek olarak, ağırlık yoğunluk grafikleri (weight density plots) kullanarak görsel analizler gerçekleştiriyoruz ve orijinal, watermark'lı ve kurcalanmış modelleri ayırt etmek için aktivasyon tabanlı sınıflandırıcılar (activation-based classifiers) oluşturuyoruz. Genel olarak, önerilen yöntem, fikri mülkiyetin kritik olduğu gerçek dünya senaryolarına uygun white-box ortamlarda model sahipliğini yerleştirmek ve doğrulamak için esnek ve ölçeklenebilir bir çözüm sunmaktadır."
    }
  },
  {
    "id": "2512.16650v1",
    "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
    "authors": [
      "Jirui Yang",
      "Hengqi Guo",
      "Zhihui Lu",
      "Yi Zhao",
      "Yuansen Zhang"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2512.16650v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16650v1",
    "content": {
      "en": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
      "tr": "Makale Başlığı: Prefix Probing: Büyük Dil Modelleri İçin Hafif Zararlı İçerik Tespiti\n\nÖzet:\nBüyük dil modelleri, gerçek dünyada güvenlik açısından hassas uygulamalarda kullanıldıklarında genellikle tespit doğruluğu, çıkarım gecikmesi ve dağıtım maliyeti arasında üç yönlü bir ödünleşme ile karşı karşıya kalır. Bu makale, \"rıza/uygulama\" ve \"reddetme/güvenlik\" açılış prefixlerinin koşullu log-olasılıklarını karşılaştıran ve tespit ek yükünü ilk token gecikmesine yakın bir seviyeye indirmek için prefix caching'den yararlanan siyah kutu zararlı içerik tespit yöntemi olan Prefix Probing'i sunmaktadır. Çıkarım sırasında, herhangi bir ek model veya çok aşamalı çıkarım çağırmadan, zararlılık puanı üretmek ve bir eşik değeri uygulamak için yalnızca prefixler üzerinden tek bir log-olasılık hesaplaması gerektirir. Prefixlerin ayırt edici gücünü daha da artırmak için, son derece bilgilendirici prefixleri otomatik olarak keşfeden, tespit performansını önemli ölçüde artıran verimli bir prefix inşa algoritması tasarlıyoruz. Kapsamlı deneyler, Prefix Probing'in, yalnızca minimum hesaplama maliyeti gerektirip ek model dağıtımı istemeden, ana akım harici güvenlik modellerine kıyasla karşılaştırılabilir bir tespit etkinliği sağladığını göstermektedir; bu da güçlü pratikliğini ve verimliliğini vurgulamaktadır."
    }
  },
  {
    "id": "2512.16538v1",
    "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
    "authors": [
      "Xiao Li",
      "Yue Li",
      "Hao Wu",
      "Yue Zhang",
      "Yechao Zhang"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16538v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16538v1",
    "content": {
      "en": "As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.",
      "tr": "Makale Başlığı: LLM Tabanlı Açık Tespiti Karşısında Kod Gizleme Üzerine Sistematik Bir Çalışma\n\nÖzet:\nBüyük dil modellerinin (LLM'ler) kod açığı tespiti için benimsenmesi arttıkça, çeşitli açık türlerindeki güvenilirlik ve sağlamlıkları acil bir endişe kaynağı haline gelmiştir. Geleneksel düşmanca ayarlarda, kod gizleme (code obfuscation) uzun süredir genel bir strateji olarak denetim araçlarını atlatmak, araçlara dokunmadan exploit kabiliyetini korumak için kullanılmıştır. Sayısız çaba, gizleme yöntemlerini ve araçlarını araştırmış olsa da, desteklenen teknikler, granülerlik ve programlama dilleri açısından yetenekleri farklılık göstermekte, bu da LLM tabanlı açık tespitine etkilerini sistematik olarak değerlendirmeyi zorlaştırmaktadır. Bu boşluğu gidermek için, gizleme tekniklerinin yapılandırılmış bir sistematizasyonunu sunuyor ve bunları birleşik bir çerçeve altında değerlendiriyoruz. Spesifik olarak, mevcut gizleme yöntemlerini üç ana sınıfa (layout, data flow ve control flow) ayırarak, 11 alt kategori ve 19 somut tekniği kapsıyoruz. Bu teknikleri dört programlama dilinde (Solidity, C, C++, ve Python) tutarlı bir LLM-driven yaklaşımla uyguluyor ve dört model ailesini (DeepSeek, OpenAI, Qwen ve LLaMA) kapsayan 15 LLM üzerindeki etkilerini ve iki kodlama ajanı (GitHub Copilot ve Codex) üzerindeki etkilerini değerlendiriyoruz. Bulgularımız, LLM tabanlı açık tespitinde kod gizlemenin hem olumlu hem de olumsuz etkilerini ortaya koymakta, gizlemenin performans iyileştirmelerine veya bozulmalarına yol açtığı koşulları vurgulamaktadır. Ayrıca, bu sonuçları açık özellikleri, kod özellikleri ve model nitelikleri açısından analiz ediyoruz. Son olarak, birkaç açık problemi özetliyor ve gerçek dünya açık tespiti için LLM'lerin sağlamlığını artırmak üzere gelecek yönleri öneriyoruz."
    }
  },
  {
    "id": "2512.16310v1",
    "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
    "authors": [
      "Yuxuan Qiao",
      "Dongqin Liu",
      "Hongchang Yang",
      "Wei Zhou",
      "Songlin Hu"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2512.16310v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16310v1",
    "content": {
      "en": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
      "tr": "**Makale Başlığı:** Agent Tools Orchestration Leaks More: Dataset, Benchmark, ve Mitigation\n\n**Özet:**\n\nBüyük Dil Modelleri (Large Language Models) tarafından yönlendirilen, tek ajanlı, çok araçlı (single-agent, multi-tool) mimari, basitliği ve etkinliği nedeniyle otonom ajanlar için popüler bir paradigm haline gelmiştir. Bununla birlikte, bu mimari aynı zamanda yeni ve ciddi bir gizlilik riski de ortaya çıkarmaktadır; biz buna \"Tools Orchestration Privacy Risk\" (TOP-R) adını veriyoruz. Bu riskte ajan, iyi niyetli bir kullanıcı hedefine ulaşmak için otonom olarak birden çok araçtan bilgi parçacıklarını toplamakta ve unexpected sensitive information sentezlemek için kendi reasoning yeteneklerini kullanmaktadır. Biz bu riskin ilk sistematik çalışmasını sunuyoruz. İlk olarak, riski ajanın yanlış hizalanmış hedef fonksiyonuna bağlayan formal bir framework oluşturuyoruz: helpfulness için aşırı optimizasyon yaparken privacy awareness'ı ihmal etmek. İkinci olarak, bu riski kapsamlı bir şekilde değerlendirmek için paired leakage ve benign senaryolarını içeren TOP-Bench'i oluşturuyoruz. Safety ve robustness arasındaki trade-off'u ölçmek için H-Score'u bütünsel bir metrik olarak tanıtıyoruz. Değerlendirme sonuçları, TOP-R'ın ciddi bir risk olduğunu ortaya koymaktadır: sekiz temsili modelin ortalama Risk Leakage Rate (RLR) %90.24'e ulaşırken, ortalama H-Score sadece 0.167'dir ve hiçbir model 0.3'ü aşamamıştır. Son olarak, TOP-R'ı etkili bir şekilde azaltan, Risk Leakage Rate'i %46.58'e düşüren ve H-Score'u önemli ölçüde 0.624'e yükselten Privacy Enhancement Principle (PEP) yöntemini öneriyoruz. Çalışmamız, hem yeni bir risk sınıfını hem de mevcut ajan mimarilerindeki içsel yapısal sınırlılıkları ortaya koymakta, aynı zamanda uygulanabilir mitigation stratejileri sunmaktadır."
    }
  },
  {
    "id": "2512.16307v1",
    "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks",
    "authors": [
      "Safwan Shaheer",
      "G. M. Refatul Islam",
      "Mohammad Rafid Hamid",
      "Tahsin Zaman Jilan"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2512.16307v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16307v1",
    "content": {
      "en": "In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.",
      "tr": "**Makale Başlığı:** Benchmark'ın Ötesinde: Prompt Injection Saldırılarına Karşı Yenilikçi Savunmalar\n\n**Özet:**\n\nBüyük dil modellerinin (LLMs) hızla gelişen bu alanında, makalemiz prompt injection saldırılarının sunduğu önemli güvenlik riskini ele almaktadır. Özellikle LLaMA ailesi modelleri gibi küçük açık kaynaklı modellere odaklanmaktadır. Otomatik savunmalar üretebilen yenilikçi savunma mekanizmaları sunuyoruz ve bu üretilen savunmaları kapsamlı bir benchmark'lanmış saldırı setine karşı sistematik olarak değerlendiriyoruz. Böylece, LLM'lerdeki hedef kaçırma (goal-hijacking) güvenlik açıklarını azaltmada yaklaşımımızın önerdiği iyileşmeyi ampirik olarak göstermiş bulunuyoruz. Çalışmamız, küçük açık kaynaklı LLM'lerin artan önemini ve kenar cihazlarda (edge devices) geniş çapta dağıtılma potansiyellerini, LLM uygulamalarındaki gelecekteki eğilimlerle uyumlu olarak tanımaktadır. Açık kaynaklı LLM'lerin ve güvenliklerinin daha büyük ekosistemine aşağıdaki katkılarda bulunuyoruz: (1) Mevcut prompt tabanlı savunmaları en son saldırılara karşı değerlendirmek, (2) savunma prompt'larını iteratif olarak iyileştirmek için bir tohum savunması (Chain Of Thoughts) kullanan yeni bir framework tanıtmak ve (3) hedef kaçırma saldırılarını tespit etmede önemli iyileşmeler göstermek. Stratejilerimiz, saldırıların başarı oranlarını ve yanlış tespit oranlarını önemli ölçüde azaltırken, aynı zamanda hedef kaçırma yeteneklerini etkili bir şekilde tespit ederek, kaynak kısıtlı ortamlarda küçük ve açık kaynaklı LLM'lerin daha güvenli ve verimli dağıtımlarının önünü açmaktadır."
    }
  },
  {
    "id": "2512.16292v1",
    "title": "In-Context Probing for Membership Inference in Fine-Tuned Language Models",
    "authors": [
      "Zhexi Lu",
      "Hongliang Chi",
      "Nathalie Baracaldo",
      "Swanand Ravindra Kadhe",
      "Yuseok Jeon"
    ],
    "published_date": "2025-12-18",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2512.16292v1",
    "pdf_link": "https://arxiv.org/pdf/2512.16292v1",
    "content": {
      "en": "Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.",
      "tr": "Elbette, makale başlığını ve özetini istediğiniz şekilde Türkçeye çevireyim:\n\n**Makale Başlığı:** In-Context Probing for Membership Inference in Fine-Tuned Language Models\n\n**Özet:**\n\nFine-tuned büyük dil modelleri (LLMs) için membership inference attacks (MIAs), özellikle modeller hassas veriler kullanılarak alan-özgü görevlere adapte edildiğinde kritik bir gizlilik tehdidi oluşturmaktadır. Önceki black-box MIA teknikleri confidence scores veya token likelihoods'e dayanırken, bu sinyaller sıklıkla bir örneğin içsel özellikleriyle -örneğin içeriğin zorluğu veya nadirliği gibi- iç içe geçmiş durumdadır. Bu durum, zayıf genelleştirme ve düşük signal-to-noise oranlarına yol açmaktadır. Bu çalışmada, eğitim dinamikleri teorisine, özellikle optimizasyon sırasındaki diminishing returns olgusuna dayanan yeni bir MIA framework'ü olan ICP-MIA'yı öneriyoruz. Üyeliğin temel bir sinyali olarak Optimization Gap'i tanıtıyoruz: yakınsama noktasında, üye örnekleri minimal kalan loss-reduction potential'ı gösterirken, üye olmayanlar daha fazla optimizasyon için önemli bir potansiyel barındırmaya devam eder. Bu gap'i black-box bir ortamda tahmin etmek için, stratejik olarak oluşturulmuş input contexts aracılığıyla fine-tuning benzeri bir davranışı simüle eden, training-free bir yöntem olan In-Context Probing (ICP)'yi öneriyoruz. İki probing stratejisi öneriyoruz: reference-data-based (semantik olarak benzer kamuya açık örnekleri kullanarak) ve self-perturbation (maskeleme veya üretme yoluyla). Üç görev ve birden fazla LLM üzerinde yapılan deneyler, ICP-MIA'nın önceki black-box MIAs'lardan, özellikle düşük false positive oranlarında, önemli ölçüde daha iyi performans gösterdiğini ortaya koymaktadır. Ayrıca, reference data alignment, model tipi, PEFT configurations ve training schedules'ın saldırı etkinliğini nasıl etkilediğini analiz ediyoruz. Bulgularımız, ICP-MIA'yı konuşlandırılmış LLMs'lerdeki gizlilik risklerini denetlemek için pratik ve teorik olarak temellendirilmiş bir framework olarak belirlemektedir."
    }
  }
]