[
  {
    "id": "2601.22159v1",
    "title": "RedSage: A Cybersecurity Generalist LLM",
    "authors": [
      "Naufal Suryanto",
      "Muzammal Naseer",
      "Pengfei Li",
      "Syed Talal Wasim",
      "Jinhui Yi"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "link": "http://arxiv.org/abs/2601.22159v1",
    "pdf_link": "https://arxiv.org/pdf/2601.22159v1",
    "content": {
      "en": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.",
      "tr": "İşte makale başlığı ve özetinin çevirisi:\n\n**Makale Başlığı:** RedSage: Siber Güvenlik Alanında Genelci Bir LLM\n\n**Özet:**\nSiber güvenlik operasyonları, hassas verileri açığa çıkarmadan çeşitli iş akışlarını destekleyen yardımcı LLM'ler gerektirir. Mevcut çözümler ya gizlilik riskleri barındıran proprietary API'lere dayanmakta ya da alan adaptasyonu eksik olan açık modelleri kullanmaktadır. Bu boşluğu doldurmak için, framework'ler, saldırı teknikleri ve güvenlik araçlarını kapsayan 28.6K belge boyunca, büyük ölçekli web filtreleme ve yüksek kaliteli kaynakların manuel toplanması yoluyla 11.8B token'lık siber güvenliğe odaklanmış sürekli ön eğitim verisi derledik. Buna dayanarak, denetimli ince ayar için 266K çok turlu siber güvenlik örneği üretmek amacıyla uzman iş akışlarını simüle eden agentic augmentation pipeline'ı tasarladık. Genel açık kaynaklı LLM verileriyle birleştirilen bu kaynaklar, alan farkındalığına sahip ön eğitim ve sonrası eğitime sahip, açık kaynaklı, yerel olarak konuşlandırılabilir bir siber güvenlik yardımcısı olan RedSage'in eğitimini sağlamaktadır. Modelleri titizlikle değerlendirmek için, siber güvenlik bilgisi, becerileri ve araç uzmanlığını kapsayan 30K çoktan seçmeli ve 240 açık uçlu Soru-Cevap öğesi içeren bir benchmark olan RedSage-Bench'i tanıtıyoruz. RedSage, daha geniş genelleştirmeyi değerlendirmek için yerleşik siber güvenlik benchmarklarında (örneğin, CTI-Bench, CyberMetric, SECURE) ve genel LLM benchmarklarında daha da değerlendirilmiştir. 8B ölçeğinde, RedSage sürekli olarak daha iyi sonuçlar elde ederek, siber güvenlik benchmarklarında +5.59 puana ve Open LLM Leaderboard görevlerinde +5.05 puana kadar temel modelleri aşmıştır. Bu bulgular, alan farkındalığına sahip agentic augmentation ve ön/son eğitiminin yalnızca siber güvenliğe özgü uzmanlığı geliştirmekle kalmayıp, aynı zamanda genel reasoning ve instruction-following'i iyileştirmeye de yardımcı olabileceğini göstermektedir. Tüm modeller, veri kümeleri ve kodlar kamuya açıktır."
    }
  },
  {
    "id": "2601.22136v1",
    "title": "StepShield: When, Not Whether to Intervene on Rogue Agents",
    "authors": [
      "Gloria Felicia",
      "Michael Eniolade",
      "Jinfeng He",
      "Zitha Sasindran",
      "Hemant Kumar"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.SE"
    ],
    "link": "http://arxiv.org/abs/2601.22136v1",
    "pdf_link": "https://arxiv.org/pdf/2601.22136v1",
    "content": {
      "en": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.",
      "tr": "Elbette, istenen çeviri aşağıdadır:\n\n**Makale Başlığı:** StepShield: Sahte Ajanlara Müdahale Zamanı, Müdahale Edilip Edilmeyeceği Değil\n\n**Özet:**\nMevcut ajan güvenliği karşılaştırma ölçütleri (benchmarks), erken müdahale ile müdahale sonrası analizi karıştırarak ikili doğruluk (binary accuracy) raporlamaktadır. 8. adımda bir ihlali işaretleyen bir dedektör müdahale imkanı sağlarken, 48. adımda bildiren bir dedektör yalnızca adli (forensic) değer sunar. Bu ayrım kritik öneme sahiptir, ancak mevcut karşılaştırma ölçütleri bunu ölçemez. StepShield'ı tanıtıyoruz, yani yalnızca ihlallerin olup olmadığını değil, ne zaman tespit edildiğini değerlendiren ilk karşılaştırma ölçütü. StepShield, 9.213 kod ajanı yörüngesi içermekte olup, bunlardan 1.278'i özenle etiketlenmiş eğitim çiftleri ve gerçekçi %8,1'lik sahte (rogue) oranına sahip 7.935 yörüngelik bir test setidir. Sahte davranışlar, altı kategoriye yayılan gerçek dünya güvenlik olaylarına dayanmaktadır. Üç yeni zamansal metrik öneriyoruz: Early Intervention Rate (EIR), Intervention Gap ve Tokens Saved. Şaşırtıcı bir şekilde, değerlendirmemiz LLM tabanlı bir yargıcın %59 EIR elde ederken, statik bir analizcinin yalnızca %26 elde ettiğini ortaya koymaktadır; bu, standart doğruluk metrikleri için tamamen görünmez olan 2.3 katlık bir performans farkıdır. Erken tespitin doğrudan ekonomik faydaları olduğunu da göstermekteyiz: kaskatlı (cascaded) HybridGuard dedektörümüz izleme maliyetlerini %75 oranında azaltmakta ve kurumsal ölçekte beş yıl boyunca kümülatif 108 milyon dolarlık tasarruf öngörmektedir. StepShield, değerlendirme odağını \"olup olmadığını\"ndan \"ne zaman\" sorusuna kaydırarak, daha güvenli ve ekonomik olarak daha uygulanabilir yapay zeka ajanları inşa etmek için yeni bir temel sağlamaktadır. Kod ve veriler Apache 2.0 lisansı altında yayımlanmıştır."
    }
  },
  {
    "id": "2601.21902v1",
    "title": "Hardware-Triggered Backdoors",
    "authors": [
      "Jonas Möller",
      "Erik Imgrund",
      "Thorsten Eisenhofer",
      "Konrad Rieck"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.LG",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.21902v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21902v1",
    "content": {
      "en": "Machine learning models are routinely deployed on a wide range of computing hardware. Although such hardware is typically expected to produce identical results, differences in its design can lead to small numerical variations during inference. In this work, we show that these variations can be exploited to create backdoors in machine learning models. The core idea is to shape the model's decision function such that it yields different predictions for the same input when executed on different hardware. This effect is achieved by locally moving the decision boundary close to a target input and then refining numerical deviations to flip the prediction on selected hardware. We empirically demonstrate that these hardware-triggered backdoors can be created reliably across common GPU accelerators. Our findings reveal a novel attack vector affecting the use of third-party models, and we investigate different defenses to counter this threat.",
      "tr": "**Makale Başlığı:** Hardware-Triggered Backdoors\n\n**Özet:**\n\nMakine öğrenmesi modelleri rutin olarak geniş bir yelpazede hesaplama donanımında dağıtılmaktadır. Bu tür donanımların tipik olarak aynı sonuçları üretmesi beklenmesine rağmen, tasarımlarındaki farklılıklar çıkarım (inference) sırasında küçük sayısal değişimlere yol açabilir. Bu çalışmada, bu değişimlerin makine öğrenmesi modellerinde backdoor'lar oluşturmak için istismar edilebileceğini göstermekteyiz. Temel fikir, modelin karar fonksiyonunu (decision function) öyle şekillendirmektir ki, farklı donanımlarda çalıştırıldığında aynı girdi için farklı tahminler üretsin. Bu etki, karar sınırını (decision boundary) yerel olarak bir hedef girdiye yaklaştırıp, ardından sayısal sapmaları (numerical deviations) seçilen donanımda tahmini değiştirecek şekilde rafine ederek elde edilir. Yaygın GPU hızlandırıcılar (GPU accelerators) boyunca bu hardware-triggered backdoors'un güvenilir bir şekilde oluşturulabileceğini ampirik olarak göstermekteyiz. Bulgularımız, üçüncü taraf modellerin kullanımını etkileyen yeni bir saldırı vektörünü ortaya koymaktadır ve bu tehdidi savuşturmak için farklı savunmaları (defenses) araştırmaktayız."
    }
  },
  {
    "id": "2601.21898v1",
    "title": "Making Models Unmergeable via Scaling-Sensitive Loss Landscape",
    "authors": [
      "Minwoo Jang",
      "Hoyoung Kim",
      "Jabin Koo",
      "Jungseul Ok"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.AI",
      "cs.CR"
    ],
    "link": "http://arxiv.org/abs/2601.21898v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21898v1",
    "content": {
      "en": "The rise of model hubs has made it easier to access reusable model components, making model merging a practical tool for combining capabilities. Yet, this modularity also creates a \\emph{governance gap}: downstream users can recompose released weights into unauthorized mixtures that bypass safety alignment or licensing terms. Because existing defenses are largely post-hoc and architecture-specific, they provide inconsistent protection across diverse architectures and release formats in practice. To close this gap, we propose \\textsc{Trap}$^{2}$, an architecture-agnostic protection framework that encodes protection into the update during fine-tuning, regardless of whether they are released as adapters or full models. Instead of relying on architecture-dependent approaches, \\textsc{Trap}$^{2}$ uses weight re-scaling as a simple proxy for the merging process. It keeps released weights effective in standalone use, but degrades them under re-scaling that often arises in merging, undermining unauthorized merging.",
      "tr": "**Makale Başlığı:** Making Models Unmergeable via Scaling-Sensitive Loss Landscape\n\n**Özet:**\n\nModel merkezlerinin (model hubs) yükselişi, yeniden kullanılabilir model bileşenlerine erişimi kolaylaştırmış ve model birleştirmeyi (model merging) yetenekleri birleştirmek için pratik bir araç haline getirmiştir. Ancak bu modülerlik aynı zamanda bir *yönetişim açığı* yaratmaktadır: Son kullanıcılar, güvenlik uyumunu veya lisans koşullarını atlayan yetkisiz karışımlara yeniden dönüştürülebilen yayımlanmış ağırlıkları (released weights) kullanabilirler. Mevcut savunmalar büyük ölçüde sonradan uygulanan (post-hoc) ve mimariye özgü olduğundan, pratikte çeşitli mimariler ve yayın formatları arasında tutarsız bir koruma sağlamaktadırlar. Bu açığı kapatmak için, mimariden bağımsız bir koruma çerçevesi olan \\textsc{Trap}$^{2}$'yi öneriyoruz. \\textsc{Trap}$^{2}$, ister adaptör (adapters) ister tam modeller (full models) olarak yayımlansın, ince ayar (fine-tuning) sırasında güncellemelerin içine korumayı kodlar. Mimariye bağlı yaklaşımlara dayanmak yerine, \\textsc{Trap}$^{2}$ birleştirme sürecinin basit bir vekilini (proxy) olarak ağırlık yeniden ölçeklendirmesini (weight re-scaling) kullanır. Yayımlanan ağırlıkları bağımsız kullanımda etkili tutar, ancak birleştirme sırasında sıklıkla ortaya çıkan yeniden ölçeklendirme altında bu ağırlıkları bozarak yetkisiz birleştirmeyi baltalar."
    }
  },
  {
    "id": "2601.21682v1",
    "title": "FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning",
    "authors": [
      "Xiaoyu Xu",
      "Minxin Du",
      "Kun Fang",
      "Zi Liang",
      "Yaxin Xiao"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.21682v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21682v1",
    "content": {
      "en": "Large language models (LLMs) demonstrate impressive capabilities across diverse tasks but raise concerns about privacy, copyright, and harmful materials. Existing LLM unlearning methods rarely consider the continual and high-volume nature of real-world deletion requests, which can cause utility degradation and catastrophic forgetting as requests accumulate. To address this challenge, we introduce \\fit, a framework for continual unlearning that handles large numbers of deletion requests while maintaining robustness against both catastrophic forgetting and post-unlearning recovery. \\fit mitigates degradation through rigorous data \\underline{F}iltering, \\underline{I}mportance-aware updates, and \\underline{T}argeted layer attribution, enabling stable performance across long sequences of unlearning operations and achieving a favorable balance between forgetting effectiveness and utility retention. To support realistic evaluation, we present \\textbf{PCH}, a benchmark covering \\textbf{P}ersonal information, \\textbf{C}opyright, and \\textbf{H}armful content in sequential deletion scenarios, along with two symmetric metrics, Forget Degree (F.D.) and Retain Utility (R.U.), which jointly assess forgetting quality and utility preservation. Extensive experiments on four open-source LLMs with hundreds of deletion requests show that \\fit achieves the strongest trade-off between F.D. and R.U., surpasses existing methods on MMLU, CommonsenseQA, and GSM8K, and remains resistant against both relearning and quantization recovery attacks.",
      "tr": "İşte makale başlığının ve özetinin çevirisi:\n\n**Makale Başlığı:** FIT: Sürekli LLM Unlearning'de Katastrofik Unutkanlığı Yenmek\n\n**Özet:**\nLarge language models (LLMs), çeşitli görevlerde etkileyici yetenekler sergilese de gizlilik, telif hakkı ve zararlı materyallerle ilgili endişeleri artırmaktadır. Mevcut LLM unlearning yöntemleri, gerçek dünyadaki silme isteklerinin sürekli ve yüksek hacimli doğasını nadiren dikkate almakta, bu da istekler biriktikçe kullanılabilirlik düşüşüne ve catastrophic forgetting'e neden olabilmektedir. Bu zorluğun üstesinden gelmek için, büyük sayıda silme isteğini ele alan, hem catastrophic forgetting'e hem de post-unlearning recovery'ye karşı dayanıklılığı koruyan sürekli bir unlearning çerçevesi olan \\fit'i sunuyoruz. \\fit, titiz veri **F**iltering, **I**mportance-aware updates ve **T**argeted layer attribution yoluyla düşüşü azaltır, uzun süreli unlearning işlemleri dizilerinde kararlı bir performans sağlar ve forgetting effectiveness ile utility retention arasında olumlu bir denge kurar. Gerçekçi değerlendirmeyi desteklemek için, sıralı silme senaryolarında **P**ersonal information, **C**opyright ve **H**armful content'i kapsayan bir benchmark olan \\textbf{PCH}'yi ve forgetting quality ile utility preservation'ı ortaklaşa değerlendiren Forget Degree (F.D.) ve Retain Utility (R.U.) olmak üzere iki simetrik metrik sunuyoruz. Yüzlerce silme isteğiyle dört açık kaynak LLM üzerinde yapılan kapsamlı deneyler, \\fit'in F.D. ve R.U. arasında en güçlü trade-off'u sağladığını, MMLU, CommonsenseQA ve GSM8K üzerinde mevcut yöntemleri geride bıraktığını ve hem relearning hem de quantization recovery attacks'lerine karşı dirençli kaldığını göstermektedir."
    }
  },
  {
    "id": "2601.21636v1",
    "title": "Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation",
    "authors": [
      "Jan Schuchardt",
      "Nikita Kalinin"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "link": "http://arxiv.org/abs/2601.21636v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21636v1",
    "content": {
      "en": "We study privacy amplification for differentially private model training with matrix factorization under random allocation (also known as the balls-in-bins model). Recent work by Choquette-Choo et al. (2025) proposes a sampling-based Monte Carlo approach to compute amplification parameters in this setting. However, their guarantees either only hold with some high probability or require random abstention by the mechanism. Furthermore, the required number of samples for ensuring $(ε,δ)$-DP is inversely proportional to $δ$. In contrast, we develop sampling-free bounds based on Rényi divergence and conditional composition. The former is facilitated by a dynamic programming formulation to efficiently compute the bounds. The latter complements it by offering stronger privacy guarantees for small $ε$, where Rényi divergence bounds inherently lead to an over-approximation. Our framework applies to arbitrary banded and non-banded matrices. Through numerical comparisons, we demonstrate the efficacy of our approach across a broad range of matrix mechanisms used in research and practice.",
      "tr": "**Makale Başlığı:** Sampling-Free Privacy Accounting for Matrix Mechanisms under Random Allocation\n\n**Özet:**\n\nBu çalışmada, rastgele tahsis (diğer adıyla balls-in-bins modeli) altında matris ayrıştırma ile gerçekleştirilen diferansiyel olarak özel model eğitiminde gizlilik amplifikasyonunu inceliyoruz. Choquette-Choo et al. (2025) tarafından yapılan yakın tarihli bir çalışma, bu ayarda amplifikasyon parametrelerini hesaplamak için sampling-based Monte Carlo bir yaklaşım önermektedir. Ancak, sundukları garantiler ya yalnızca belirli bir yüksek olasılıkla geçerlidir ya da mekanizma tarafından rastgele abstention gerektirir. Dahası, $(ε,δ)$-DP'yi güvence altına almak için gereken örnek sayısı $δ$'ya ters orantılıdır. Buna karşılık, Rényi divergence ve conditional composition'a dayanan sampling-free bounds geliştiriyoruz. Birincisi, bounds'u verimli bir şekilde hesaplamak için bir dynamic programming formulation ile kolaylaştırılmıştır. İkincisi, Rényi divergence bounds'un doğal olarak bir over-approximation'a yol açtığı küçük $ε$ değerleri için daha güçlü gizlilik garantileri sunarak bunu tamamlar. Çerçevemiz, keyfi olarak banded ve non-banded matrislere uygulanabilir. Sayısal karşılaştırmalar aracılığıyla, araştırma ve uygulamada kullanılan geniş bir matrix mechanisms yelpazesindeki yaklaşımımızın etkinliğini gösteriyoruz."
    }
  },
  {
    "id": "2601.21628v1",
    "title": "Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise",
    "authors": [
      "Puwei Lian",
      "Yujun Cai",
      "Songze Li",
      "Bingkun Bao"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.21628v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21628v1",
    "content": {
      "en": "Diffusion models have achieved remarkable progress in image generation, but their increasing deployment raises serious concerns about privacy. In particular, fine-tuned models are highly vulnerable, as they are often fine-tuned on small and private datasets. Membership inference attacks (MIAs) are used to assess privacy risks by determining whether a specific sample was part of a model's training data. Existing MIAs against diffusion models either assume obtaining the intermediate results or require auxiliary datasets for training the shadow model. In this work, we utilized a critical yet overlooked vulnerability: the widely used noise schedules fail to fully eliminate semantic information in the images, resulting in residual semantic signals even at the maximum noise step. We empirically demonstrate that the fine-tuned diffusion model captures hidden correlations between the residual semantics in initial noise and the original images. Building on this insight, we propose a simple yet effective membership inference attack, which injects semantic information into the initial noise and infers membership by analyzing the model's generation result. Extensive experiments demonstrate that the semantic initial noise can strongly reveal membership information, highlighting the vulnerability of diffusion models to MIAs.",
      "tr": "**Gürültü Bir Sorgulama Aracı Olarak: İlk Gürültüden Yararlanan Yayılma Modellerine Üyelik Çıkarım Saldırıları**\n\n**Özet:**\n\nYayılma modelleri (diffusion models) görüntü üretiminde dikkate değer ilerlemeler kaydetmiş olsa da, giderek artan kullanımları gizlilik konusunda ciddi endişelere yol açmaktadır. Özellikle, küçük ve özel veri kümeleri üzerinde ince ayarlanmış (fine-tuned) modeller yüksek derecede savunmasızdır. Üyelik çıkarım saldırıları (Membership Inference Attacks - MIAs), belirli bir örneğin modelin eğitim verilerinin bir parçası olup olmadığını belirleyerek gizlilik risklerini değerlendirmek için kullanılır. Yayılma modellerine karşı mevcut MIAs ya ara sonuçların elde edildiğini varsayar ya da gölge modelin (shadow model) eğitilmesi için yardımcı veri kümeleri gerektirir. Bu çalışmada, kritik ancak göz ardı edilmiş bir zafiyetten yararlandık: yaygın olarak kullanılan gürültü çizelgeleri (noise schedules) görüntülerdeki anlamsal bilgiyi tam olarak ortadan kaldırmakta başarısız olarak, en yüksek gürültü adımında bile kalıntı anlamsal sinyallerin oluşmasına neden olur. Deneyimsel olarak, ince ayarlanmış yayılma modelinin, ilk gürültüdeki kalıntı anlambilimler (residual semantics) ile orijinal görüntüler arasındaki gizli ilişkileri yakaladığını göstermekteyiz. Bu içgörüden yola çıkarak, ilk gürültüye anlamsal bilgi enjekte eden ve modelin üretim sonucunu analiz ederek üyeliği çıkarsayan basit ama etkili bir üyelik çıkarım saldırısı öneriyoruz. Kapsamlı deneyler, anlamsal ilk gürültünün (semantic initial noise) üyelik bilgisini güçlü bir şekilde ortaya çıkarabileceğini ve yayılma modellerinin MIAs'a karşı savunmasızlığını vurgulamaktadır."
    }
  },
  {
    "id": "2601.21531v1",
    "title": "On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression",
    "authors": [
      "Xinwei Zhang",
      "Hangcheng Liu",
      "Li Bai",
      "Hao Wang",
      "Qingqing Ye"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "link": "http://arxiv.org/abs/2601.21531v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21531v1",
    "content": {
      "en": "Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.",
      "tr": "**Makale Başlığı:** Görsel Token Sıkıştırması Altında Büyük Görsel-Dil Modellerinin Çekişmeli Dayanıklılığı Üzerine\n\n**Özet:**\n\nGörsel token sıkıştırması, görsel tokenları budayarak veya birleştirerek büyük görsel-dil modellerini (LVLMs) hızlandırmak için yaygın olarak kullanılmaktadır; ancak bunun çekişmeli dayanıklılığı henüz keşfedilmemiştir. Mevcut encoder tabanlı saldırıların, sıkıştırılmış LVLMs'lerin dayanıklılığını önemli ölçüde abartabileceğini gösteriyoruz. Bu durum, bir optimizasyon-çıkarım uyumsuzluğundan kaynaklanmaktadır: bozulmalar tam token temsili üzerinde optimize edilirken, çıkarım bir token sıkıştırma darboğazı aracılığıyla gerçekleştirilir. Bu boşluğu gidermek için, konuşlandırılmış sıkıştırma mekanizmasına veya token bütçesine erişim varsayımı olmadan bozulma optimizasyonunu sıkıştırma çıkarımıyla hizalayan Compression-AliGnEd attack (CAGE)'i öneriyoruz. CAGE, (i) olası bütçeler boyunca hayatta kalması muhtemel tokenlar üzerinde bozulmayı yoğunlaştıran beklenen özellik bozulması ve (ii) yüksek derecede bozulmuş kanıtların korunmasını teşvik etmek için token bozulmalarını rank skorlarıyla aktif olarak hizalayan rank bozulma hizalaması'nı birleştirir. Çeşitli temsili plug-and-play sıkıştırma mekanizmaları ve veri setleri üzerinde elde ettiğimiz sonuçlar, CAGE'in temel modele kıyasla tutarlı bir şekilde daha düşük sağlam doğruluk sağladığını göstermektedir. Bu çalışma, sıkıştırmayı göz ardı eden dayanıklılık değerlendirmelerinin aşırı iyimser olabileceğini vurgulamakta ve verimli LVLMs'ler için sıkıştırma-farkındalığına sahip güvenlik değerlendirmeleri ve savunmaları çağrısında bulunmaktadır."
    }
  },
  {
    "id": "2601.21189v1",
    "title": "Adaptive and Robust Cost-Aware Proof of Quality for Decentralized LLM Inference Networks",
    "authors": [
      "Arther Tian",
      "Alex Ding",
      "Frank Chen",
      "Simon Wu",
      "Aaron Chan"
    ],
    "published_date": "2026-01-29",
    "tags": [
      "cs.CR",
      "cs.AI"
    ],
    "link": "http://arxiv.org/abs/2601.21189v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21189v1",
    "content": {
      "en": "Decentralized large language model inference networks require lightweight mechanisms to reward high quality outputs under heterogeneous latency and cost. Proof of Quality provides scalable verification by sampling evaluator nodes that score candidate outputs, then aggregating their scores into a consensus signal that determines rewards. However, evaluator heterogeneity and malicious score manipulation can distort consensus and inflate payouts, which weakens incentive alignment in open participation settings.   This paper extends a cost-aware Proof of Quality mechanism by adding adversary-resilient consensus formation. We study robust aggregation rules, including median and trimmed mean, and an adaptive trust-weighted consensus that updates evaluator weights from deviation signals. Using question answering and summarization workloads with a ground truth proxy for offline analysis, we quantify evaluator reliability and show strong variance across evaluators, including task-dependent misalignment that can invert correlations. We then evaluate robustness under four adversarial strategies, including noise injection, boosting, sabotage, and intermittent manipulation, across a sweep of malicious ratios and evaluator sample sizes. Our results show that robust aggregation improves consensus alignment with the ground truth proxy and reduces sensitivity to noisy and strategic attacks compared with simple averaging. We further characterize the operational trade-off introduced by evaluator sampling, where larger evaluator sets reduce evaluator rewards and increase payoff variance while inference rewards remain relatively stable in our configuration. These findings motivate robust consensus as a default component for cost-aware Proof of Quality and provide practical guidance for selecting evaluator sampling parameters under adversarial risk and resource constraints.",
      "tr": "Makale Başlığı: Merkezi Olmayan LLM Çıkarım Ağları İçin Uyarlanabilir ve Dayanıklı Maliyet-Farkında Kalite Kanıtı\n\nÖzet:\nMerkezi olmayan büyük dil modeli (LLM) çıkarım ağları, heterojen gecikme ve maliyet altında yüksek kaliteli çıktıları ödüllendirmek için hafif mekanizmalar gerektirir. Proof of Quality, aday çıktıları puanlayan değerlendirici düğümleri örnekleyerek ve ardından puanlarını ödülleri belirleyen bir consensus sinyaline toplayarak ölçeklenebilir doğrulama sağlar. Ancak, değerlendirici heterojenliği ve kötü niyetli puan manipülasyonu consensus'u bozabilir ve ödemeleri şişirebilir, bu da açık katılım ortamlarında teşvik hizalamasını zayıflatır. Bu makale, düşmanlara dayanıklı consensus oluşumu ekleyerek maliyet-farkında bir Proof of Quality mekanizmasını genişletmektedir. Median ve trimmed mean dahil olmak üzere sağlam toplama kurallarını ve sapma sinyallerinden değerlendirici ağırlıklarını güncelleyen uyarlanabilir bir trust-weighted consensus'u inceliyoruz. Çevrimdışı analiz için bir ground truth proxy ile soru yanıtlama ve özetleme iş yüklerini kullanarak, değerlendirici güvenilirliğini ölçer ve değerlendiriciler arasında güçlü varyanslar gösteririz; görev bağımlı hizalamasızlık korelasyonları tersine çevirebilir. Ardından, kötü niyetli oranlar ve değerlendirici örneklem boyutlarının bir aralığında, gürültü enjeksiyonu, boosting, sabotage ve aralıklı manipülasyon dahil olmak üzere dört düşman stratejisi altındaki dayanıklılığı değerlendiririz. Sonuçlarımız, sağlam toplamanın, basit ortalamaya kıyasla ground truth proxy ile consensus hizalamasını iyileştirdiğini ve gürültülü ve stratejik saldırılara duyarlılığı azalttığını göstermektedir. Ayrıca, değerlendirici örnekleme tarafından tanıtılan operasyonel ödünleşmeyi karakterize ederiz; burada daha büyük değerlendirici setleri değerlendirici ödüllerini azaltır ve ödeme varyansını artırırken, bizim konfigürasyonumuzda çıkarım ödülleri nispeten sabit kalır. Bu bulgular, maliyet-farkında Proof of Quality için varsayılan bir bileşen olarak sağlam consensus'u teşvik eder ve düşman riski ve kaynak kısıtlamaları altında değerlendirici örnekleme parametrelerini seçmek için pratik rehberlik sağlar."
    }
  },
  {
    "id": "2601.21051v1",
    "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
    "authors": [
      "Zhuoran Yang",
      "Ed Li",
      "Jianliang He",
      "Aman Priyanshu",
      "Baturay Saglam"
    ],
    "published_date": "2026-01-28",
    "tags": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "link": "http://arxiv.org/abs/2601.21051v1",
    "pdf_link": "https://arxiv.org/pdf/2601.21051v1",
    "content": {
      "en": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning.",
      "tr": "Makale Başlığı: Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Teknik Raporu\n\nÖzet:\nFoundation-Sec-8B-Reasoning, siber güvenlik alanına yönelik ilk açık kaynaklı native reasoning modelini sunmaktayız. Daha önce yayımlanan ve Llama-3.1-8B-Base modelinden türetilmiş olan Foundation-Sec-8B temel modelimiz üzerine inşa edilen bu model, supervised fine-tuning (SFT) ve verifiable rewards'dan (RLVR) elde edilen reinforcement learning yöntemlerini birleştiren iki aşamalı bir süreçle eğitilmiştir. Eğitimimiz, siber güvenlik analizi, instruction-following ve mathematical reasoning'i kapsayan özel reasoning verilerini kullanmaktadır. 10 siber güvenlik benchmark'ı ve 10 genel amaçlı benchmark üzerindeki değerlendirmeler, genel yeteneklerini korurken siber güvenlik görevlerinde önemli ölçüde daha büyük modellerle rekabetçi performans sergilediğini göstermektedir. Model, multi-hop reasoning görevlerinde etkili genelleme ve uygun system prompts ile guardrails ile konuşlandırıldığında güçlü güvenlik performansı sergilemektedir. Bu çalışma, alana özgü specialized reasoning modellerinin geniş genel yetenekleri korurken özel görevlerde güçlü performans gösterebileceğini ortaya koymaktadır. Modeli kamuya açık olarak https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Reasoning adresinde yayımlıyoruz."
    }
  }
]